,relevant,title,authors,url,venue,year,keywords,abstract
0,,Systematic Mutation-Based Evaluation of the Soundness of Security-Focused Android Static Analysis Techniques.,"Amit Seal Ami,Kaushal Kafle,Kevin Moran,Adwait Nadkarni,Denys Poshyvanyk",https://doi.org/10.1145/3439802,TOPS,2021,[],
1,,Analyzing Dynamic Code - A Sound Abstract Interpreter for Evil Eval.,"Vincenzo Arceri,Isabella Mastroeni",https://doi.org/10.1145/3426470,TOPS,2021,[],
2,,Friendly Fire - Cross-app Interactions in IoT Platforms.,"Musard Balliu,Massimo Merro,Michele Pasqua,Mikhail Shcherbakov",https://doi.org/10.1145/3444963,TOPS,2021,[],
3,,Attack Context Embedded Data Driven Trust Diagnostics in Smart Metering Infrastructure.,"Shameek Bhattacharjee,Venkata Praveen Kumar Madhavarapu,Simone Silvestri,Sajal K. Das 0001",https://doi.org/10.1145/3426739,TOPS,2021,[],
4,,One Size Does Not Fit All - A Longitudinal Analysis of Brazilian Financial Malware.,"Marcus Botacin,Hojjat Aghakhani,Stefano Ortolani,Christopher Kruegel,Giovanni Vigna,Daniela A. S. de Oliveira,Paulo Lício de Geus,André Grégio",https://doi.org/10.1145/3429741,TOPS,2021,[],
5,,On Generating Network Traffic Datasets with Synthetic Attacks for Intrusion Detection.,"Carlos Garcia Cordero,Emmanouil Vasilomanolakis,Aidmar Wainakh,Max Mühlhäuser,Simin Nadjm-Tehrani",https://doi.org/10.1145/3424155,TOPS,2021,[],
6,,Privacy-preserving Dynamic Symmetric Searchable Encryption with Controllable Leakage.,"Shujie Cui,Xiangfu Song,Muhammad Rizwan Asghar,Steven D. Galbraith,Giovanni Russello",https://doi.org/10.1145/3446920,TOPS,2021,[],
7,,An Extensive Formal Analysis of Multi-factor Authentication Protocols.,"Charlie Jacomme,Steve Kremer",https://doi.org/10.1145/3440712,TOPS,2021,[],
8,,Two-factor Password-authenticated Key Exchange with End-to-end Security.,"Stanislaw Jarecki,Mohammed Jubur,Hugo Krawczyk,Nitesh Saxena,Maliheh Shirvanian",https://doi.org/10.1145/3446807,TOPS,2021,[],
9,,The Android Platform Security Model.,"René Mayrhofer,Jeffrey Vander Stoep,Chad Brubaker,Nick Kralevich",https://doi.org/10.1145/3448609,TOPS,2021,[],
10,1,A Multi-view Approach to Preserve Privacy and Utility in Network Trace Anonymization.,"Meisam Mohammady,Momen Oqaily,Lingyu Wang 0001,Yuan Hong,Habib Louafi,Makan Pourzandi,Mourad Debbabi",https://doi.org/10.1145/3439732,TOPS,2021,[],
11,,Exploiting Mixed Binaries.,"Michalis Papaevripides,Elias Athanasopoulos",https://doi.org/10.1145/3418898,TOPS,2021,[],
12,,A Large-Scale Analysis of the Semantic Password Model and Linguistic Patterns in Passwords.,"Rafael Veras,Christopher Collins 0001,Julie Thorpe",https://doi.org/10.1145/3448608,TOPS,2021,[],
13,,Designing Strong Privacy Metrics Suites Using Evolutionary Optimization.,"Isabel Wagner,Iryna Yevseyeva",https://doi.org/10.1145/3439405,TOPS,2021,[],
447,,GraphBoot - Quantifying Uncertainty in Node Feature Learning on Large Networks.,"Cuneyt Gurcan Akcora,Yulia R. Gel,Murat Kantarcioglu,Vyacheslav Lyubchich,Bhavani M. Thuraisingham",https://doi.org/10.1109/TKDE.2019.2925355,TKDE,2021,"Uncertainty,Estimation,Reliability,Drugs,Data mining,Twitter","In recent years, as online social networks continue to grow in size, estimating node features, such as sociodemographics, preferences and health status, in a scalable and reliable way has become a primary research direction in social network mining. Although many techniques have been developed for estimating various node features, quantifying uncertainty in such estimations has received little attention. Furthermore, most existing methods study networks parametrically, which limits insights about necessary quantity of queried data, reliable feature estimation, and estimator uncertainty. Uncertainty quantification is critical for answering key questions, such as, given a limited availability of social network data, how much data should be queried from the network?, and which node features can be learned reliably? More importantly, how can we evaluate uncertainty of our estimators? Uncertainty quantification is not equivalent to network sampling but constitutes a key complementary concept to sampling and the associated reliability analysis. To our knowledge, this paper is the first work that sheds light on uncertainty quantification and uncertainty propagation in social network feature mining. We propose a novel non-parametric bootstrap method for uncertainty analysis of node features in social network mining, derive its asymptotic properties, and demonstrate its effectiveness with extensive experiments. Furthermore, we develop a new metric based on dispersion of estimations, enabling analysts to assess how much more information is needed for increasing prediction reliability based on the estimated uncertainty. We demonstrate the effectiveness of our new uncertainty quantification methodology with extensive experiments on real life social networks, and a case study of mental health on Twitter."
448,,Polygraph - A Plug-n-Play Framework to Quantify Application Anomalies.,"Yazeed Alabdulkarim,Marwan Almaymoni,Shahram Ghandeharizadeh",https://doi.org/10.1109/TKDE.2019.2939520,TKDE,2021,"Benchmark testing,Tools,Servers,Real-time systems,Schedules,Plugs,Law","Polygraph is a tool to quantify application anomalies attributed to violating atomicity, isolation, and linearizability properties of transactions. It is a plug-n-play framework that includes visualization tools to empower an experimentalist to (a) quickly incorporate Polygraph into an existing application or benchmark and (b) quantify the number of anomalies. We demonstrate Polygraph using existing benchmarks, including TPC-C, SEATS, TATP, YCSB, and BG. We highlight Polygraph as an on-line tool by showing it scales for almost all benchmarks to process their transaction log records faster than their rate of production."
449,,Evolutionary Clustering via Message Passing.,"Natalia M. Arzeno,Haris Vikalo",https://doi.org/10.1109/TKDE.2019.2954869,TKDE,2021,"Clustering algorithms,Heuristic algorithms,Clustering methods,Message passing,Bayes methods,Hidden Markov models,Inference algorithms","We are often interested in clustering objects that evolve over time and identifying solutions to the clustering problem for every time step. Evolutionary clustering provides insight into cluster evolution and temporal changes in cluster memberships while enabling performance superior to that achieved by independently clustering data collected at different time points. In this article we introduce evolutionary affinity propagation (EAP), an evolutionary clustering algorithm that groups data points by exchanging messages on a factor graph. EAP promotes temporal smoothness of the solution to clustering time-evolving data by linking the nodes of the factor graph that are associated with adjacent data snapshots, and introduces consensus nodes to enable cluster tracking and identification of cluster births and deaths. Unlike existing evolutionary clustering methods that require additional processing to approximate the number of clusters or match them across time, EAP determines the number of clusters and tracks them automatically. A comparison with existing methods on simulated and experimental data demonstrates effectiveness of the proposed EAP algorithm."
450,,Learning Spatiotemporal Latent Factors of Traffic via Regularized Tensor Factorization - Imputing Missing Values and Forecasting.,"Abdelkader Baggag,Sofiane Abbar,Ankit Sharma 0004,Tahar Zanouda,Abdulaziz Al-Homaid,Abhiraj Mohan,Jaideep Srivastava",https://doi.org/10.1109/TKDE.2019.2954868,TKDE,2021,"Roads,Tensile stress,Forecasting,Sensors,Urban areas,Junctions,Real-time systems","Intelligent transportation systems are a key component in smart cities, and the estimation and prediction of the spatiotemporal traffic state is critical to capture the dynamics of traffic congestion, i.e., its generation, propagation and mitigation, in order to increase operational efficiency and improve livability within smart cities. And while spatiotemporal data related to traffic is becoming common place due to the wide availability of cheap sensors and the rapid deployment of IoT platforms, the data still suffer some challenges related to sparsity, incompleteness, and noise which makes the traffic analytics difficult. In this article, we investigate the problem of missing data or noisy information in the context of real-time monitoring and forecasting of traffic congestion for road networks in a city. The road network is represented as a directed graph in which nodes are junctions (intersections) and edges are road segments. We assume that the city has deployed high-fidelity sensors for speed reading in a subset of edges; and the objective is to infer the speed readings for the remaining edges in the network; and to estimate the missing values in the segments for which sensors have stopped generating data due to technical problems (e.g., battery, network, etc.). We propose a tensor representation for the series of road network snapshots, and develop a regularized factorization method to estimate the missing values, while learning the latent factors of the network. The regularizer, which incorporates spatial properties of the road network, improves the quality of the results. The learned factors, with a graph-based temporal dependency, are then used in an autoregressive algorithm to predict the future state of the road network with a large horizon. Extensive numerical experiments with real traffic data from the cities of Doha (Qatar) and Aarhus (Denmark) demonstrate that the proposed approach is appropriate for imputing the missing data and predicting the traffic state. It is accurate and efficient and can easily be applied to other traffic datasets."
451,,Position-Aware Deep Character-Level CTR Prediction for Sponsored Search.,"Xiao Bai 0002,Reza Abasi,Bora Edizel,Amin Mantrach",https://doi.org/10.1109/TKDE.2019.2941881,TKDE,2021,"Predictive models,Search engines,Deep learning,Context modeling,Advertising,Engines,Data models","Predicting the click-through rate of an advertisement is a critical component of online advertising platforms. In sponsored search, the click-through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. This inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the click-through rate of a query-advertisement pair. Specifically, the proposed architectures consider as input only the textual content appearing in a query-advertisement pair and the page position at which the advertisement appears on the search result page of the query, and produce as output a click-through rate prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. Through extensive experiments using billions of query-advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-the-art word2vec-based approach. We also show the importance of the position feature in the proposed approaches in improving the prediction accuracy. When combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the click-through rate prediction of the production system. We also show the potential of leveraging the CTR prediction of the proposed deep learning models for query-ad relevance modeling and query-ad matching tasks in sponsored search."
452,,ConcurDB - Concurrent Query Authentication for Outsourced Databases.,"Sumeet Bajaj,Anrin Chakraborti,Radu Sion",https://doi.org/10.1109/TKDE.2019.2943557,TKDE,2021,"Authentication,Databases,Concurrent computing,Cryptography,Gold,Protocols,Data structures","Clients of outsourced databases need Query Authentication (QA) guaranteeing the integrity and authenticity of query results returned by potentially compromised providers. Prior work provides QA assurances for a limited class of queries by deploying several software-based cryptographic constructs. The constructs are often designed assuming read-only or infrequently updated databases. For dynamic datasets, the data owner is required to perform all updates on behalf of clients. Hence, for concurrent updates by multiple clients, such as for OLTP workloads, existing QA solutions are inefficient. We present ConcurDB, a concurrent QA scheme that enables simultaneous updates by multiple clients. To realize concurrent QA, we have designed several new mechanisms. First, we identify and use an important relationship between QA and memory checking to decouple query execution and verification. We allow clients to execute transactions concurrently and perform verifications in parallel using an offline memory checking based protocol. Then, to extend QA to a multi-client scenario, we design new protocols that enable clients to securely exchange a small set of authentication data even when using the untrusted provider as a communication hub. Finally, we overcome provider-side replay attacks. Using ConcurDB, we provide and evaluate concurrent QA for the full TPC-C benchmark. For updates, ConcurDB shows a 4x performance increase over existing solutions."
453,,User-Driven Geolocated Event Detection in Social Media.,"Anes Bendimerad,Marc Plantevit,Céline Robardet,Sihem Amer-Yahia",https://doi.org/10.1109/TKDE.2019.2931340,TKDE,2021,"Geology,Event detection,Data mining,Twitter,Urban areas,Time series analysis","Event detection is one of the most important research topics in social media analysis. Despite this interest, few researchers have addressed the problem of identifying geolocated events in an unsupervised way, and none includes user interests during the process. In this paper, we tackle the problem of local event detection from social media data. We present a method to automatically identify events by evaluating the burstiness of hashtags in a geographical area and a time interval, and at the same time integrating user feedback. We devise two algorithms to discover user-driven events. The first one relies on an exact enumeration process, while the other directly samples the space of events. In our empirical study, we provide evidence that geolocated events cannot be detected by non location-aware methods. We also show that our methods (i) outperform by a factor of two to several orders of magnitude state-of-the-art methods designed to discover geolocated events, (ii) are more robust to noise, and (iii) produce high quality events with respect to user interests."
454,,Node Embedding with Adaptive Similarities for Scalable Learning over Graphs.,"Dimitris Berberidis,Georgios B. Giannakis",https://doi.org/10.1109/TKDE.2019.2931542,TKDE,2021,"Measurement,Task analysis,Symmetric matrices,Numerical models,Scalability,Feature extraction,Sparse matrices","Node embedding is the task of extracting informative and descriptive features over the nodes of a graph. The importance of node embedding for graph analytics as well as learning tasks, such as node classification, link prediction, and community detection, has led to a growing interest and a number of recent advances. Nonetheless, node embedding faces several major challenges. Practical embedding methods have to deal with real-world graphs that arise from different domains, with inherently diverse underlying processes as well as similarity structures and metrics. On the other hand, similar to principal component analysis in feature vector spaces, node embedding is an inherently unsupervised task. Lacking metadata for validation, practical schemes motivate standardization and limited use of tunable hyperparameters. Finally, node embedding methods must be scalable in order to cope with large-scale real-world graphs of networks with ever-increasing size. The present work puts forth an adaptive node embedding framework that adjusts the embedding process to a given underlying graph, in a fully unsupervised manner. This is achieved by leveraging the notion of a tunable node similarity matrix that assigns weights on multihop paths. The design of multihop similarities ensures that the resultant embeddings also inherit interpretable spectral properties. The proposed model is thoroughly investigated, interpreted, and numerically evaluated using stochastic block models. Moreover, an unsupervised algorithm is developed for training the model parameters effieciently. Extensive node classification, link prediction, and clustering experiments are carried out on many real-world graphs from various domains, along with comparisons with state-of-the-art scalable and unsupervised node embedding alternatives. The proposed method enjoys superior performance in many cases, while also yielding interpretable information on the underlying graph structure."
455,,Event Detection on Microposts - A Comparison of Four Approaches.,"Akansha Bhardwaj,Albert Blarer,Philippe Cudré-Mauroux,Vincent Lenders,Boris Motik,Axel Tanner,Alberto Tonon",https://doi.org/10.1109/TKDE.2019.2944815,TKDE,2021,"Event detection,Twitter,Semantics,Training,Terrorism,Feature extraction,Time series analysis","Microblogging services such as Twitter are important, up-to-date, and live sources of information on a multitude of topics and events. An increasing number of systems use such services to detect and analyze events in real-time as they unfold. In this context, we recently proposed ArmaTweet-a system developed in collaboration among armasuisse and the Universities of Oxford and Fribourg to support semantic event detection on Twitter streams. Our experiments have shown that ArmaTweet is successful at detecting many complex events that cannot be detected by simple keyword-based search methods alone. Building up on this work, we explore in this paper several approaches for event detection on microposts. In particular, we describe and compare four different approaches based on keyword search (Plain-Seed-Query), information retrieval (Temporal Query Expansion), Word2Vec word embeddings (Embedding), and semantic retrieval (ArmaTweet). We provide an extensive empirical evaluation of these techniques using a benchmark dataset of about 200 million tweets on six event categories that we collected. While the performance of individual systems varies depending on the event category, our results show that ArmaTweet outperforms the other approaches on five out of six categories, and that a combined approach offers highest recall without adversely affecting precision of event detection."
456,,A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search.,Deng Cai 0001,https://doi.org/10.1109/TKDE.2019.2953897,TKDE,2021,"Machine learning algorithms,Machine learning,Nearest neighbor methods,Approximation algorithms,Search problems,Reproducibility of results,Indexes","Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims to outperform Locality Sensitive Hashing (LSH), which is the most popular hashing method. However, the evaluation of these hashing article was not thorough enough, and the claim should be re-examined. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing article only report the performance with the code length shorter than 128. In this article, we carefully revisit the problem of search-with-a-hash-index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a simple but effective novel hash index search approach and made a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing ranked the first, which is in contradiction to the claims in all the other 10 hashing article. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the article are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms."
457,,BS-SC - An Unsupervised Approach for Detecting Shilling Profiles in Collaborative Recommender Systems.,"Hongyun Cai 0002,Fuzhi Zhang",https://doi.org/10.1109/TKDE.2019.2946247,TKDE,2021,"Feature extraction,Recommender systems,Collaboration,Training,Correlation,Clustering algorithms,Detection algorithms","Collaborative recommender systems are vulnerable to shilling attacks. To address this issue, many methods including supervised and unsupervised have been proposed. However, supervised detection methods require training classifiers and they only apply to detect known types of attacks. The existing unsupervised detection methods need to know the prior knowledge of attacks, otherwise they suffer from low detection precision. In this paper, we present BS-SC, an unsupervised approach for detecting shilling profiles, which does not need to know the attack size or to label the candidate spammers. BS-SC starts from an in-depth analysis of user behaviors and uses two key mechanisms (i.e., behavior features extraction and behavior similarity matrix clustering) to distinguish shilling profiles from genuine ones. The behavior features reflect the behavior difference between genuine and shilling profiles, and the behavior similarity matrix clustering is to cluster shilling profiles based on their highly similar behaviors. Experimental results on the MovieLens and the sampled Amazon review datasets indicate that BS-SC outperforms the baseline unsupervised approaches, even when the prior knowledge is given for them."
458,,Social-Enhanced Attentive Group Recommendation.,"Da Cao,Xiangnan He 0001,Lianhai Miao,Guangyi Xiao,Hao Chen 0051,Jia Xu 0005",https://doi.org/10.1109/TKDE.2019.2936475,TKDE,2021,"Recommender systems,Neural networks,Aggregates,Collaboration,Deep learning,Social networking (online),Task analysis","With the proliferation of social networks, group activities have become an essential ingredient of our daily life. A growing number of users share their group activities online and invite their friends to join in. This imposes the need of an in-depth study on the group recommendation task, i.e., recommending items to a group of users. Despite its value and significance, group recommendation remains an unsolved problem due to 1) the weights of group members are crucial to the recommendation performance but are rarely learnt from data; 2) social followee information is beneficial to understand users' preferences but is rarely considered; and 3) user-item interactions are helpful to reinforce the performance of group recommendation but are seldom investigated. Toward this end, we devise neural network-based solutions by utilizing the recent developments of attention network and neural collaborative filtering (NCF). First of all, we adopt an attention network to form the representation of a group by aggregating the group members' embeddings, which allows the attention weights of group members to be dynamically learnt from data. Second, the social followee information is incorporated via another attention network to enhance the representation of individual user, which is helpful to capture users' personal preferences. Third, considering that many online group systems also have abundant interactions of individual users on items, we further integrate the modeling of user-item interactions into our method. Through this way, the recommendation for groups and users can be mutually reinforced. Extensive experiments on the scope of both macro-level performance comparison and micro-level analyses justify the effectiveness and rationality of our proposed approaches."
459,,Deep Multi-View Learning to Rank.,"Guanqun Cao,Alexandros Iosifidis,Moncef Gabbouj,Vijay Raghavan 0001,Raju Gottumukkala",https://doi.org/10.1109/TKDE.2019.2942590,TKDE,2021,"Correlation,Transforms,Neural networks,Training,Optimization,Data models,Data mining","We study the problem of learning to rank from multiple information sources. Though multi-view learning and learning to rank have been studied extensively leading to a wide range of applications, multi-view learning to rank as a synergy of both topics has received little attention. The aim of the paper is to propose a composite ranking method while keeping a close correlation with the individual rankings simultaneously. We present a generic framework for multi-view subspace learning to rank (MvSL2R), and two novel solutions are introduced under the framework. The first solution captures information of feature mappings from within each view as well as across views using autoencoder-like networks. Novel feature embedding methods are formulated in the optimization of multi-view unsupervised and discriminant autoencoders. Moreover, we introduce an end-to-end solution to learning towards both the joint ranking objective and the individual rankings. The proposed solution enhances the joint ranking with minimum view-specific ranking loss, so that it can achieve the maximum global view agreements in a single optimization process. The proposed method is evaluated on three different ranking problems, i.e., university ranking, multi-view lingual text ranking, and image data ranking, providing superior results compared to related methods."
460,1,Protecting Spatiotemporal Event Privacy in Continuous Location-Based Services.,"Yang Cao 0011,Yonghui Xiao,Li Xiong 0001,Liquan Bai,Masatoshi Yoshikawa",https://doi.org/10.1109/TKDE.2019.2963312,TKDE,2021,"Privacy,Spatiotemporal phenomena,Measurement,Trajectory,Transmission line matrix methods,Markov processes,Hospitals","Location privacy-preserving mechanisms (LPPMs) have been extensively studied for protecting users’ location privacy by releasing a perturbed location to third parties such as location-based service providers. However, when a user’s perturbed locations are released continuously, existing LPPMs may not protect the sensitive information about the user’s real-world activities, such as “visited hospital in the last week” or “regularly commuting between location A and location B every weekday” (it is easy to infer that location A and location B may be home and office), which we call it 
<i>spatiotemporal event</i>
. In this paper, we first formally define spatiotemporal event as Boolean expressions between location and time predicates, and then we define 
<inline-formula><tex-math notation=""LaTeX"">$ \epsilon$</tex-math></inline-formula>
-
<i>spatiotemporal event privacy</i>
 by extending the notion of differential privacy. Second, to understand how much spatiotemporal event privacy that existing LPPMs can provide, we design computationally efficient algorithms to quantify the spatiotemporal event privacy leakage of state-of-the-art LPPMs. It turns out that the existing LPPMs may not adequately protect spatiotemporal event privacy. Third, we propose a framework, PriSTE, to transform an existing LPPM into one protecting spatiotemporal event privacy by calibrating the LPPM’s privacy budgets. Our experiments on real-life and synthetic data verified that the proposed method is effective and efficient."
461,,The Power of Bounds - Answering Approximate Earth Mover&apos;s Distance with Parametric Bounds.,"Tsz Nam Chan,Man Lung Yiu,Leong Hou U",https://doi.org/10.1109/TKDE.2019.2931969,TKDE,2021,"Upper bound,Histograms,Earth,Optimization,Approximation algorithms,Databases,Time factors","The Earth Mover's Distance (EMD) is a robust similarity measure between two histograms (e.g., probability distributions). It has been extensively used in a wide range of applications, e.g., multimedia, data mining, computer vision, etc. As EMD is a computationally intensive operation, many efficient lower and upper bound functions of EMD have been developed. However, they provide no guarantee on the error. In this work, we study how to compute approximate EMD value with bounded error. First, we develop a parametric dual bound function for EMD, in order to offer sufficient trade-off points for optimization. After that, we propose an approximation framework that leverages on lower and upper bound functions to compute approximate EMD with error guarantee. Then, we present three solutions to solve our problem. Experimental results on real data demonstrate the efficiency and the effectiveness of our proposed solutions."
462,,MSQ-Index - A Succinct Index for Fast Graph Similarity Search.,"Xiaoyang Chen,Hongwei Huo,Jun Huan,Jeffrey Scott Vitter,Weiguo Zheng,Lei Zou 0001",https://doi.org/10.1109/TKDE.2019.2954527,TKDE,2021,"Indexing,Encoding,Boosting,Search problems,Data structures","Graph similarity search under the graph edit distance constraint has received considerable attention in many applications, such as bioinformatics, data mining, pattern recognition and social networks. Existing methods for this problem have limited scalability because of the huge amount of memory they consume when handling very large graph databases with tens of millions of graphs. In this article, we present a succinct index that incorporates succinct data structures and hybrid encoding to achieve improved query time performance with minimal space usage. Specifically, the space usage of our index requires only 5–15 percent of the previous state-of-the-art indexing size while at the same time achieving several times acceleration in query time on the tested data. We also improve the query performance by augmenting the global filter with range searching, which allows us to perform similarity search in a reduced region. In addition, we propose two effective lower bounds together with a boosting technique to obtain the smallest possible candidate set. Extensive experiments demonstrate that our proposed approach is superior both in space and filtering to the state-of-the-art approaches. To the best of our knowledge, our index is the first in-memory index for this problem that successfully scales to cope with the large dataset of 25 million chemical structure graphs from the PubChem dataset. The source code is available online."
463,,Incremental Factorization of Big Time Series Data with Blind Factor Approximation.,"Dan Chen 0001,Yunbo Tang,Hao Zhang,Lizhe Wang 0001,Xiaoli Li 0002",https://doi.org/10.1109/TKDE.2019.2931687,TKDE,2021,"Time series analysis,Brain modeling,Graphics processing units,Complex systems,Big Data,Bayes methods","Extracting the latent factors of big time series data is an important means to examine the dynamic complex systems under observation. These low-dimensional and “small” representations reveal the key insights to the overall mechanisms, which can otherwise be obscured by the notoriously high dimensionality and scale of big data as well as the enormously complicated interdependencies amongst data elements. However, grand challenges still remain: (1) to incrementally derive the multi-mode factors of the augmenting big data and (2) to achieve this goal under the circumstance of insufficient a priori knowledge. This study develops an incrementally parallel factorization solution (namely I-PARAFAC) for huge augmenting tensors (multi-way arrays) consisting of three phases over a cutting-edge GPU cluster: in the “giant-step” phase, a variational Bayesian inference (VBI) model estimates the distribution of the close neighborhood of each factor in a high confidence level without the need for a priori knowledge of the tensor or problem domain; in the “baby-step” phase, a massively parallel Fast-HALS algorithm (namely G-HALS) has been developed to derive the accurate subfactors of each subtensor on the basis of the initial factors; in the final fusion phase, I-PARAFAC fuses the known factors of the original tensor and those accurate subfactors of the “increment” to achieve the final full factors. Experimental results indicate that: (1) the VBI model enables a blind factor approximation, where the distribution of the close neighborhood of each final factor can be quickly derived (10 iterations for the test case). As a result, the model of a low time complexity significantly accelerates the derivation of the final accurate factors and lowers the risks of errors; (2) I-PARAFAC significantly outperforms even the latest high performance counterpart when handling augmenting tensors, e.g., the increased overhead is only proportional to the increment while the latter has to repeatedly factorize the whole tensor, and the overhead in fusing subfactors is always minimal; (3) I-PARAFAC can factorize a huge tensor (volume up to 500 TB over 50 nodes) as a whole with the capability several magnitudes higher than conventional methods, and the runtime is in the order of 1/n to the number of compute nodes; (4) I-PARAFAC supports correct factorization-based analysis of a real 4-order EEG dataset captured from a variety of epilepsy patients. Overall, it should also be noted that counterpart methods have to derive the whole tensor from the scratch if the tensor is augmented in any dimension; as a contrast, the I-PARAFAC framework only needs to incrementally compute the full factors of the huge augmented tensor."
464,,A Domain Adaptive Density Clustering Algorithm for Data With Varying Density Distribution.,"Jianguo Chen,Philip S. Yu",https://doi.org/10.1109/TKDE.2019.2954133,TKDE,2021,"Clustering algorithms,Density measurement,Partitioning algorithms,Computational complexity,Shape,Stability analysis,Data mining","As one type of efficient unsupervised learning methods, clustering algorithms have been widely used in data mining and knowledge discovery with noticeable advantages. However, clustering algorithms based on density peak have limited clustering effect on data with varying density distribution (VDD), equilibrium distribution (ED), and multiple domain-density maximums (MDDM), leading to the problems of sparse cluster loss and cluster fragmentation. To address these problems, we propose a Domain-Adaptive Density Clustering (DADC) algorithm, which consists of three steps: domain-adaptive density measurement, cluster center self-identification, and cluster self-ensemble. For data with VDD features, clusters in sparse regions are often neglected by using uniform density peak thresholds, which results in the loss of sparse clusters. We define a domain-adaptive density measurement method based on 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math> </inline-formula>
-Nearest Neighbors (KNN) to adaptively detect the density peaks of different density regions. We treat each data point and its KNN neighborhood as a subgroup to better reflect its density distribution in a domain view. In addition, for data with ED or MDDM features, a large number of density peaks with similar values can be identified, which results in cluster fragmentation. We propose a cluster center self-identification and cluster self-ensemble method to automatically extract the initial cluster centers and merge the fragmented clusters. Experimental results demonstrate that compared with other comparative algorithms, the proposed DADC algorithm can obtain more reasonable clustering results on data with VDD, ED and MDDM features. Benefitting from a few parameter requirement and non-iterative nature, DADC achieves low computational complexity and is suitable for large-scale data clustering."
465,,Making Online Sketching Hashing Even Faster.,"Xixian Chen,Haiqin Yang,Shenglin Zhao,Irwin King,Michael R. Lyu",https://doi.org/10.1109/TKDE.2019.2934687,TKDE,2021,"Training,Computational efficiency,Data models,Approximation algorithms,Hash functions,Economics,Principal component analysis","Data-dependent hashing methods have demonstrated good performance in various machine learning applications to learn a low-dimensional representation from the original data. However, they still suffer from several obstacles: First, most of existing hashing methods are trained in a batch mode, yielding inefficiency for training streaming data. Second, the computational cost and the memory consumption increase extraordinarily in the big data setting, which perplexes the training procedure. Third, the lack of labeled data hinders the improvement of the model performance. To address these difficulties, we utilize online sketching hashing (OSH) and present a FasteR Online Sketching Hashing (FROSH) algorithm to sketch the data in a more compact form via an independent transformation. We provide theoretical justification to guarantee that our proposed FROSH consumes less time and achieves a comparable sketching precision under the same memory cost of OSH. We also extend FROSH to its distributed implementation, namely DFROSH, to further reduce the training time cost of FROSH while deriving the theoretical bound of the sketching precision. Finally, we conduct extensive experiments on both synthetic and real datasets to demonstrate the attractive merits of FROSH and DFROSH."
466,,Succinct Representation of Dynamic Networks.,"Kaiqi Chen,Lanlan Yu,Tingting Zhu,Ping Li 0024,Jürgen Kurths",https://doi.org/10.1109/TKDE.2019.2960240,TKDE,2021,"Feature extraction,Principal component analysis,Task analysis,Marine vehicles,Knowledge engineering,Data mining,Encoding","Many network analysis tasks like classification over nodes require careful efforts in engineering features used by learning algorithms. Most of recent studies have been made and succeeded in the field of static network representation learning. However, real-world networks are often dynamic and little work has been done on how to describe dynamic networks. In this work, we pose the problem of condensing dynamic networks and introduce 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SuRep</i>
, an encoding-decoding framework which utilizes matrix factorization technique to derive a succinct representation of a dynamic network in any stationary phase. We show that the succinct representation method can uncover the invariant structural properties in the network evolution and derive dense feature representations of the nodes as the byproduct. This method can be easily extended to dynamic attribute networks. For experiments on detecting change points in dynamic networks and network classification with real-world datasets we demonstrate 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SuRep</i>
’s potential for capturing latent patterns among nodes."
467,,Follow the Title Then Read the Article - Click-Guide Network for Dwell Time Prediction.,"Jingwu Chen,Fuzhen Zhuang,Tianxin Wang,Leyu Lin,Feng Xia 0006,Lihuan Du,Qing He 0003",https://doi.org/10.1109/TKDE.2019.2960693,TKDE,2021,"Task analysis,Predictive models,Lifting equipment,Mobile applications,Collaboration,User experience","In article recommendation, the amount of time user spends on viewing articles, dwell time, is an important metric to measure the post-click engagement of user on content and has been widely used as a proxy to user satisfaction, complementing the click feedback. Recently, the sequential pattern of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">impression-click-read</i>
 has become one of the most popular type of article recommendation service in real world, where users are presented with a list of titles at first, then get interested in one and click in for reading. Predicting dwell time in such service is conditioned on the click, since the user reads the article only after he clicks the corresponding title. We argue that conventional models for dwell time prediction, which mainly focus on the relevance between the content and the general preference of user, are not well-designed for such service. There is a natural assumption in recommendation system that the click indicates user's getting attracted by the item. Therefore, in the pattern of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">impression-click-read</i>
, the user might get interested and curious on some other concepts different from his general preference while reading, due to the attraction of the title. Conventional models tend to ignore the gap between such temporary interest and the general preference of user in the reading behavior, which fails to use the pattern of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">impression-click-read</i>
 and the assumption of the click very well. In this work, we propose a framework, 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">C</b>
lick-
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">g</b>
uide 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">N</b>
etwork (CGN) for dwell time prediction, which makes good use of the sequential pattern and the assumption to model the ”guidance” of the click on user preference. CGN is a joint learner for dwell time and click through rate (CTR). We introduce the CTR task as an auxiliary task to help us better learn the preference of user and the representation of title. Besides, we propose the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Guider</i>
 to capture the user's temporary interest raised by the title. We collect the data from 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">WeChat</i>
, a widely-used mobile app in China, for experiments. The results demonstrate the advantages of CGN over several competitive baselines on dwell time prediction, while our case studies show how the Guider effectively capture the temporary interest of user."
468,,Fast Algorithms for Semantic Association Search and Pattern Mining.,"Gong Cheng 0001,Daxin Liu,Yuzhong Qu",https://doi.org/10.1109/TKDE.2019.2942031,TKDE,2021,"Search problems,Semantics,Data mining,Partitioning algorithms,Skeleton,Indexes,Google","Given a large graph representing relations between entities, searching for complex relationships (called semantic associations, or SAs for short) between a set of entities is a common type of information needs in many domains. Further, numerous SAs are often abstracted into a few frequent high-level conceptual graph patterns (called SA patterns, or SAPs for short), which organize SAs into interpretable subgroups. Whereas the quality and usefulness of SAs and SAPs have been extensively studied in the literature, in this article we aim to develop faster algorithms for SA search and frequent SAP mining. For the former problem, we leverage distances to prune the search space, and implement a distance oracle to balance the time and space for distance calculation. For the latter problem, we exploit both graph structure and labels to induce fine-grained skeleton-based partitions of SAs, which may be pruned to reduce SAP enumeration. Besides, we generate canonical codes for SAs, which not only enable result deduplication but also are reused in SAP mining to improve the overall performance. We extensively evaluate the efficiency of our algorithms on four large graphs, using both random queries and simulated queries which reproduce the extreme case of finding numerous SAs."
469,,Event-Participant and Incremental Planning over Event-Based Social Networks.,"Yurong Cheng,Ye Yuan 0001,Lei Chen 0002,Christophe G. Giraud-Carrier,Guoren Wang,Boyang Li 0006",https://doi.org/10.1109/TKDE.2019.2931906,TKDE,2021,"Planning,Upper bound,Approximation algorithms,Social networking (online),Computer science,Sports,Games","In recent years, online Event Based Social Network (EBSN) platforms have become increasingly popular. One typical task of EBSN platforms is to help users make suitable and personalized plans for participating in different interesting social events. Existing techniques either ignore the minimum-participant requirement constraint for each event, which is crucially needed for some events to be held successfully, or assume that events would not change once announced. In this paper, we address the above inadequacies of existing EBSN techniques. We formally define the Global Event Planning with Constraints (GEPC) problem, and its incremental variant. Since these problems are NP-hard, and provide approximate solutions. Finally, we verify the effectiveness and efficiency of our proposed algorithms through extensive experiments over real and synthetic datasets."
470,,Clustering with Local Density Peaks-Based Minimum Spanning Tree.,"Dongdong Cheng,Qingsheng Zhu,Jinlong Huang,Quanwang Wu,Lijun Yang",https://doi.org/10.1109/TKDE.2019.2930056,TKDE,2021,"Clustering algorithms,Partitioning algorithms,Shape,Machine learning algorithms,Merging,Image edge detection,Approximation algorithms","Clustering analysis has been widely used in statistics, machine learning, pattern recognition, image processing, and so on. It is a great challenge for most existing clustering algorithms to discover clusters with arbitrary shapes. Clustering algorithms based on Minimum spanning tree (MST) are able to discover clusters with arbitrary shapes, but they are time consuming and susceptible to noise points. In this paper, we employ local density peaks (LDP) to represent the whole data set and define a shared neighbors-based distance between local density peaks to better measure the dissimilarity between objects on manifold data. On the basis of local density peaks and the new distance, we propose a novel MST-based clustering algorithm called LDP-MST. It first uses local density peaks to construct MST and then repeatedly cuts the longest edge until a given number of clusters are found. The experimental results on synthetic data sets and real data sets show that our algorithm is competent with state-of-the-art methods when discovering clusters with complex structures."
471,,POLARIS - Probabilistic and Ontological Activity Recognition in Smart-Homes.,"Gabriele Civitarese,Timo Sztyler,Daniele Riboni,Claudio Bettini,Heiner Stuckenschmidt",https://doi.org/10.1109/TKDE.2019.2930050,TKDE,2021,"Activity recognition,Semantics,Probabilistic logic,Ontologies,Correlation","Recognition of activities of daily living (ADLs) is an enabling technology for several ubiquitous computing applications. Most activity recognition systems rely on supervised learning to extract activity models from labeled datasets. A problem with that approach is the acquisition of comprehensive activity datasets, which is an expensive task. The problem is particularly challenging when focusing on complex ADLs characterized by large variability of execution. Moreover, several activity recognition systems are limited to offline recognition, while many applications claim for online activity recognition. In this paper, we propose POLARIS, a framework for unsupervised activity recognition. POLARIS can recognize complex ADLs exploiting the semantics of activities, context data, and sensors. Through ontological reasoning, our algorithm derives semantic correlations among activities and sensor events. By matching observed events with semantic correlations, a statistical reasoner formulates initial hypotheses about the occurred activities. Those hypotheses are refined through probabilistic reasoning, exploiting semantic constraints derived from the ontology. Our system supports online recognition, thanks to a novel segmentation algorithm. Extensive experiments with real-world datasets show that the accuracy of our unsupervised method is comparable to the one of supervised approaches. Moreover, the online version of our system achieves essentially the same accuracy of the offline version."
472,1,Constrained Private Mechanisms for Count Data.,"Graham Cormode,Tejas Kulkarni,Divesh Srivastava",https://doi.org/10.1109/TKDE.2019.2912179,TKDE,2021,"Differential privacy,Privacy,Linear programming,Optimization,Heating systems,Pathology","Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. Differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of n individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can lead to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters."
473,,Automating IoT Data-Intensive Application Allocation in Clustered Edge Computing.,"Rustem Dautov,Salvatore Distefano",https://doi.org/10.1109/TKDE.2019.2923638,TKDE,2021,"Task analysis,Big Data,Peer-to-peer computing,Middleware,Topology,Sensors","Enabling data processing at the network edge, as close to the actual source of data as possible, is a challenging, yet realistic goal to be achieved by the Internet of Things (IoT), which still primarily relies on the Cloud for data processing. By further extending the Fog and Edge computing principles, recent research advancements enabled aggregation of computing resources from multiple edge devices to support data-intensive task processing using Big Data clustering middleware. The use of these existing solutions, however, is hindered by the heterogeneous, dynamic, mobile, resource-constrained, and time-critical nature of IoT ecosystems. More specifically, a particularly challenging goal is to discover, select, and cluster suitable edge devices - on the one hand, and decompose and allocate data-intensive tasks with respect to discovered resources - on the other. To address this challenge, this paper introduces a novel decentralized architecture for clustering heterogeneous edge devices and executing data-intensive IoT workflows. The proposed approach first breaks down a complex workflow into simpler tasks, then discovers and selects suitable edge devices, and finally allocates the tasks to the selected nodes, connecting them to recompose the original workflow. The proposed approach benefits from an intelligent mapping algorithm that takes into account available cluster resources and processing demands to efficiently allocate fine-grained tasks to selected nodes. To support the clusterisation process, the proposed solution relies on a unified semantic knowledge base that provides a common vocabulary of terms for modelling task requirements and edge device properties, as well as enables automated task grouping and match-making for device discovery and selection, using built-in reasoning capabilities."
474,,Label Propagation on K-Partite Graphs with Heterophily.,"Dingxiong Deng,Fan Bai,Yiqi Tang,Shuigeng Zhou,Cyrus Shahabi,Linhong Zhu",https://doi.org/10.1109/TKDE.2019.2937493,TKDE,2021,"Inference algorithms,Labeling,Fans,Time complexity,Partitioning algorithms,Heterogeneous networks,Data models","In this paper, for the first time, we study label propagation in heterogeneous graphs under heterophily assumption. Homophily label propagation (i.e., two connected nodes share similar labels) in homogeneous graph (with same types of vertices and relations) has been extensively studied before. Unfortunately, real-life networks (e.g., social networks) are heterogeneous, they contain different types of vertices (e.g., users, images, and texts) and relations (e.g., friendships and co-tagging) and allow for each node to propagate both the same and opposite copy of labels to its neighbors. We propose a IC-partite label propagation model to handle the mystifying combination of heterogeneous nodes/relations and heterophily propagation. With this model, we develop a novel label inference algorithm framework with update rules in near-linear time complexity. Since real networks change overtime, we devise an incremental approach, which supports fast updates for both new data and evidence (e.g., ground truth labels) with guaranteed efficiency. We further provide a utility function to automatically determine whether an incremental or a re-modeling approach is favored. Extensive experiments on real datasets have verified the effectiveness and efficiency of our approach, and its superiority over the state-of-the-art label propagation methods."
475,1,A Novel Privacy Preserving Framework for Large Scale Graph Data Publishing.,"Xiaofeng Ding,Cui Wang,Kim-Kwang Raymond Choo,Hai Jin 0001",https://doi.org/10.1109/TKDE.2019.2931903,TKDE,2021,"Differential privacy,Privacy,Publishing,Facebook,Task analysis","The need to efficiently store and query large scale graph datasets is evident in the growing number of data-intensive applications, particularly to maximize the mining of intelligence from these data (e.g., to inform decision making). However, directly releasing graph dataset for analysis may leak sensitive information of an individual even if the graph is anonymized, as demonstrated by the re-identification attacks on the DBpedia datasets. A key challenge in the design of graph sanitization methods is scalability, as existing execution models generally have significant memory requirements. In this paper, we propose a novel k-decomposition algorithm and define a new information loss matrix designed for utility measurement in massively large graph datasets. We also propose a novel privacy preserving framework that can be seamlessly integrated with graph storage, anonymization, query processing, and analysis. Our experimental studies show that the proposed solution achieves privacy-preserving, utility, and efficiency."
476,,Sampler Design for Bayesian Personalized Ranking by Leveraging View Data.,"Jingtao Ding,Guanghui Yu,Xiangnan He 0001,Fuli Feng,Yong Li 0008,Depeng Jin",https://doi.org/10.1109/TKDE.2019.2931327,TKDE,2021,"Business process re-engineering,Data models,Recommender systems,Training,Bayes methods,Streaming media,Learning systems","Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of negative sampler. In this paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of E-commerce, we propose a negative sampler for BPR by leveraging the additional view data. In our proposed sampler, users' viewed interactions are considered as an intermediate feedback between the purchased and unobserved interactions. We jointly learn the pairwise rankings of user preference among these three types of interactions and design a user-oriented weighting strategy during learning process, which is more effective and flexible. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-enhanced sampler enhances BPR with a relative improvement over 36.64 and 16.40 percent on Beibei and Tmall datasets, respectively. Empirical studies demonstrate the importance of considering users' additional feedback when modeling their preference on different items, which can effectively improve the quality of sampled negative items towards learning a better personalized ranking function. Our implementation is available at https://github.com/dingjingtao/NegativeSamplerBPR."
477,,Mining Spatio-Temporal Reachable Regions With Multiple Sources over Massive Trajectory Data.,"Yichen Ding,Xun Zhou,Guojun Wu,Yanhua Li,Jie Bao 0003,Yu Zheng 0004,Jun Luo 0007",https://doi.org/10.1109/TKDE.2019.2959531,TKDE,2021,"Roads,Trajectory,Business,Indexes,Query processing,Public transportation,Data mining","Given a set of user-specified locations and a massive trajectory dataset, the task of mining spatio-temporal reachable regions aims at finding which road segments are reachable from these locations within a given temporal period based on the historical trajectories. Determining such spatio-temporal reachable regions with high accuracy is vital for many urban applications, such as location-based recommendations and advertising. Traditional approaches to answering such queries essentially perform a distance-based range query over the given road network, which does not consider dynamic travel time at different time of day. By contrast, we propose a data-driven approach to formulate the problem as mining actual reachable regions based on a real historical trajectory dataset. Efficient algorithms for the Single-location spatio-temporal reachability Query (S-Query) and the Union-of-multi-location spatio-temporal reachability Query (U-Query) were presented in our recent work. In this paper, we extend the previous ideas by introducing a new type of reachability query with multiple sources, namely, the Intersection-of-multi-location spatio-temporal reachability Query (I-Query). As we demonstrate, answering I-Queries efficiently is generally more computationally challenging than answering either S-Queries or U-Queries because I-Queries involve complicated intersect conditions. We propose two new algorithms called the Intersection-of-Multi-location Query Maximum Bounding region search (I-MQMB) algorithm and the I-Query Trace Back Search (I-TBS) algorithm to efficiently answer I-Queries, which utilize an indexing schema composed of a spatio-temporal index and a connection index. We evaluate our system extensively by using a large-scale real taxi trajectory dataset that records taxi rides in Shenzhen, China. Our results demonstrate that the proposed approach reduces the running time of I-Queries by 50 percent on average compared to the baseline method."
478,,Unveiling Hidden Implicit Similarities for Cross-Domain Recommendation.,"Quan Do,Wei Liu 0007,Jin Fan,Dacheng Tao",https://doi.org/10.1109/TKDE.2019.2923904,TKDE,2021,"Matrix decomposition,Correlation,Sparse matrices,Fans,Collaboration,Task analysis","E-commerce businesses are increasingly dependent on recommendation systems to introduce personalized services and products to targeted customers. Providing effective recommendations requires sufficient knowledge about user preferences and product (item) characteristics. Given the current abundance of available data across domains, achieving a thorough understanding of the relationship between users and items can bring in more collaborative filtering power and lead to a higher recommendation accuracy. However, how to effectively utilize different types of knowledge obtained across domains is still a challenging problem. In this paper, we propose to discover both explicit and implicit similarities from latent factors across domains based on matrix tri-factorization. In our research, common factors in a shared dimension (users or items) of two coupled matrices are discovered, while at the same time, domain-specific factors of the shared dimension are also preserved. We will show that such preservation of both common and domain-specific factors are significantly beneficial to cross-domain recommendations. Moreover, on the non-shared dimension, we propose to use the middle matrix of the tri-factorization to match the unique factors, and align the matched unique factors to transfer cross-domain implicit similarities and thus further improve the recommendation. This research is the first that proposes the transfer of knowledge across the non-shared (non-coupled) dimensions. Validated on real-world datasets, our approach outperforms existing algorithms by more than two times in terms of recommendation accuracy. These empirical results illustrate the potential of utilizing both explicit and implicit similarities for making across-domain recommendations."
479,,Feature Re-Learning with Data Augmentation for Video Relevance Prediction.,"Jianfeng Dong,Xun Wang,Leimin Zhang,Chaoxi Xu,Gang Yang 0001,Xirong Li",https://doi.org/10.1109/TKDE.2019.2947442,TKDE,2021,"Visualization,Task analysis,Feature extraction,Training,Image color analysis,Standards,Metadata","Predicting the relevance between two given videos with respect to their visual content is a key component for content-based video recommendation and retrieval. Thanks to the increasing availability of pre-trained image and video convolutional neural network models, deep visual features are widely used for video content representation. However, as how two videos are relevant is task-dependent, such off-the-shelf features are not always optimal for all tasks. Moreover, due to varied concerns including copyright, privacy and security, one might have access to only pre-computed video features rather than original videos. We propose in this paper feature re-learning for improving video relevance prediction, with no need of revisiting the original video content. In particular, re-learning is realized by projecting a given deep feature into a new space by an affine transformation. We optimize the re-learning process by a novel negative-enhanced triplet ranking loss. In order to generate more training data, we propose a new data augmentation strategy which works directly on frame-level and video-level features. Extensive experiments in the context of the Hulu Content-based Video Relevance Prediction Challenge 2018 justify the effectiveness of the proposed method and its state-of-the-art performance for content-based video relevance prediction."
480,,Deep Air Quality Forecasting Using Hybrid Deep Learning Framework.,"Shengdong Du,Tianrui Li 0001,Yan Yang 0001,Shi-Jinn Horng",https://doi.org/10.1109/TKDE.2019.2954510,TKDE,2021,"Air quality,Forecasting,Atmospheric modeling,Time series analysis,Deep learning,Predictive models,Data models","Air quality forecasting has been regarded as the key problem of air pollution early warning and control management. In this article, we propose a novel deep learning model for air quality (mainly PM2.5) forecasting, which learns the spatial-temporal correlation features and interdependence of multivariate air quality related time series data by hybrid deep learning architecture. Due to the nonlinear and dynamic characteristics of multivariate air quality time series data, the base modules of our model include one-dimensional Convolutional Neural Networks (1D-CNNs) and Bi-directional Long Short-term Memory networks (Bi-LSTM). The former is to extract the local trend features and spatial correlation features, and the latter is to learn spatial-temporal dependencies. Then we design a jointly hybrid deep learning framework based on one-dimensional CNNs and Bi-LSTM for shared representation features learning of multivariate air quality related time series data. We conduct extensive experimental evaluations using two real-world datasets, and the results show that our model is capable of dealing with PM2.5 air pollution forecasting with satisfied accuracy."
481,,POLAR++ - Active One-Shot Personalized Article Recommendation.,"Zhengxiao Du,Jie Tang 0001,Yuhui Ding",https://doi.org/10.1109/TKDE.2019.2953721,TKDE,2021,"Bayes methods,Uncertainty,Neural networks,Deep learning,Learning systems,Collaboration,Recommender systems","We study the problem of personalized article recommendation, in particular when the user's preference data is missing or limited, which is knowns as the user cold-start issue in recommender systems. We propose POLAR++, an active recommendation framework that utilizes Bayesian neural networks to capture the uncertainty of user preference, actively selects articles to query the user for feedback, and adaptively learns user preference with one-shot learning. For the article recommendation, we design an attention-based CNN to quantify the similarity between user preference and recommended articles, which significantly improves the performance with only a few articles rated by the users. We evaluate the proposed POLAR++ on datasets of different scale and sources. Experimental results demonstrate the effectiveness of the proposed model. We have successfully deployed POLAR++ into AMiner as the recommendation engine for article recommendation, which further confirms the effectiveness of the proposed model."
482,,Metagraph-Based Learning on Heterogeneous Graphs.,"Yuan Fang 0001,Wenqing Lin,Vincent W. Zheng,Min Wu 0008,Jiaqi Shi,Kevin Chen-Chuan Chang,Xiaoli Li 0001",https://doi.org/10.1109/TKDE.2019.2922956,TKDE,2021,"Social networking (online),Semantics,Machine learning,Search problems,Particle measurements,Computational efficiency,Task analysis","Data in the form of graphs are prevalent, ranging from biological and social networks to citation graphs and the Web. In particular, most real-world graphs are heterogeneous, containing objects of multiple types, which present new opportunities for many problems on graphs. Consider a typical proximity search problem on graphs, which boils down to measuring the proximity between two given nodes. Most earlier studies on homogeneous or bipartite graphs only measure a generic form of proximity, without accounting for different “semantic classes”-for instance, on a social network two users can be close for different reasons, such as being classmates or family members, which represent two distinct semantic classes. Learning these semantic classes are made possible on heterogeneous graphs through the concept of metagraphs. In this study, we identify metagraphs as a novel and effective means to characterize the common structures for a desired class of proximity. Subsequently, we propose a family of metagraph-based proximity, and employ a learning-to-rank technique that automatically learns the right parameters to suit the desired semantic class. In terms of efficiency, we develop a symmetry-based matching algorithm to speed up the computation of metagraph instances. Empirically, extensive experiments reveal that our metagraph-based proximity substantially outperforms the best competitor by more than 10 percent, and our matching algorithm can reduce matching time by more than half. As a further generalization, we aim to derive a general node and edge representation for heterogeneous graphs, in order to support arbitrary machine learning tasks beyond proximity search. In particular, we propose the finer-grained anchored metagraph, which is capable of discriminating the roles of nodes within the same metagraph. Finally, further experiments on the general representation show that we can outperform the state of the art significantly and consistently across various machine learning tasks."
483,,A-DSP - An Adaptive Join Algorithm for Dynamic Data Stream on Cloud System.,"Junhua Fang,Rong Zhang 0002,Yan Zhao 0008,Kai Zheng 0001,Xiaofang Zhou 0001,Aoying Zhou",https://doi.org/10.1109/TKDE.2019.2947055,TKDE,2021,"Task analysis,Routing,Data models,Heuristic algorithms,Adaptation models,Parallel processing,Computational modeling","The join operations, including both equi and non-equi joins, are essential to the complex data analytics in the big data era. However, they are not inherently supported by existing 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">DSPEs</i>
 (
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">D</u>
istributed 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">S</u>
tream 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">P</u>
rocessing 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">E</u>
ngines). The state-of-the-art join solutions on DSPEs rely on either complicated routing strategies or resource-inefficient processing structures, which are susceptible to dynamic workload, especially when the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">DSPEs</i>
 face various join predicate operations and skewed data distribution. In this paper, we propose a new cost-effective stream join framework, named 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">A-DSP</i>
 (
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">A</u>
daptive 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">D</u>
imensional 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">S</u>
pace 
<underline xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">P</u>
rocessing), which enhances the adaptability of real-time join model and minimizes the resource used over the dynamic workloads. Our proposal includes: 1) a join model generation algorithm devised to adaptively switch between different join schemes so as to minimize the number of processing task required; 2) a load-balancing mechanism which maximizes the processing throughput; and 3) a lightweight algorithm designed for cutting down unnecessary migration cost. Extensive experiments are conducted to compare our proposal against state-of-the-art solutions on both benchmark and real-world workloads. The experimental results verify the effectiveness of our method, especially on reducing the operational cost under pay-as-you-go pricing scheme."
484,,Graph Adversarial Training - Dynamically Regularizing Based on Graph Structure.,"Fuli Feng,Xiangnan He 0001,Jie Tang 0001,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2019.2957786,TKDE,2021,"Perturbation methods,Training,Neural networks,Robustness,Predictive models,Task analysis,Standards","Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (e.g., articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Adversarial Training</i>
 (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples. In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Graph Adversarial Training</i>
 (GraphAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GraphAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GraphAT, we employ it on a state-of-the-art graph neural network model — 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Graph Convolutional Network</i>
 (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GraphAT which outperforms normal training on GCN by 4.51 percent in node classification accuracy. Codes are available via: 
<uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/fulifeng/GraphAT</uri>
."
485,,Link Prediction Under Imperfect Detection - Collaborative Filtering for Ecological Networks.,"Xiao Fu 0001,Eugene Seo 0001,Justin Clarke,Rebecca A. Hutchinson",https://doi.org/10.1109/TKDE.2019.2962031,TKDE,2021,"Biological system modeling,Motion pictures,Data models,Mixture models,Mathematical model,Collaboration,Predictive models","<i>Matrix completion</i>
 based 
<i>collaborative filtering</i>
 is considered scalable and effective for online service link prediction (e.g., movie recommendation) but does not meet the challenges of link prediction in ecological networks. A unique challenge of ecological networks is that the observed data are subject to systematic 
<i>imperfect detection</i>
, due to the difficulty of accurate field sampling. In this work, we propose a new framework customized for ecological bipartite network link prediction. Our approach starts with incorporating the Poisson 
<inline-formula><tex-math notation=""LaTeX"">$N$</tex-math></inline-formula>
-mixture model, a widely used framework in statistical ecology for modeling imperfect detection of a single species in field sampling. Despite its extensive use for single species analysis, this model has never been considered for link prediction between different species, perhaps because of the complex nature of both link prediction and 
<inline-formula><tex-math notation=""LaTeX"">$N$</tex-math></inline-formula>
-mixture model inference. By judiciously combining the Poisson 
<inline-formula><tex-math notation=""LaTeX"">$N$</tex-math></inline-formula>
-mixture model with a probabilistic nonnegative matrix factorization (NMF) model in latent space, we propose an intuitive statistical model for the problem of interest. We also offer a scalable and convergence-guaranteed optimization algorithm to handle the associated maximum likelihood identification problem. Experimental results on synthetic data and two real-world ecological networks data are employed to validate our proposed approach."
486,,Learning to Rerank Schema Matches.,"Avigdor Gal,Haggai Roitman,Roee Shraga",https://doi.org/10.1109/TKDE.2019.2962124,TKDE,2021,"Urban areas,Predictive models,Uncertainty,Prediction algorithms,Data integration,Data models,Task analysis","Schema matching is at the heart of integrating structured and semi-structured data with applications in data warehousing, data analysis recommendations, Web table matching, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">etc.</i>
 Schema matching is known as an uncertain process and a common method to overcome this uncertainty introduces a human expert with a ranked list of possible schema matches to choose from, known as 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">top-<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math><alternatives><mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href=""shraga-ieq1-2962124.gif""/></alternatives></inline-formula> matching</i>
. In this work we propose a learning algorithm that utilizes an innovative set of features to rerank a list of schema matches and improves upon the ranking of the best match. We provide a bound on the size of an initial match list, tying the number of matches with a desired level of confidence in finding the best match. We also propose the use of matching predictors as features in a learning task, and tailored nine new matching predictors for this purpose. The proposed algorithm assists the matching process by introducing a quality set of alternative matches to a human expert. It also serves as a step towards eliminating the involvement of human experts as decision makers in a matching process altogether. A large scale empirical evaluation with real-world benchmark shows the effectiveness of the proposed algorithmic solution."
487,,A Survey of Utility-Oriented Pattern Mining.,"Wensheng Gan,Jerry Chun-Wei Lin,Philippe Fournier-Viger,Han-Chieh Chao,Vincent S. Tseng,Philip S. Yu",https://doi.org/10.1109/TKDE.2019.2942594,TKDE,2021,"Data mining,Itemsets,Task analysis,Biomedical measurement,Gallium nitride,Taxonomy","The main purpose of data mining and analytics is to find novel, potentially useful patterns that can be utilized in real-world applications to derive beneficial knowledge. For identifying and evaluating the usefulness of different kinds of patterns, many techniques and constraints have been proposed, such as support, confidence, sequence order, and utility parameters (e.g., weight, price, profit, quantity, satisfaction, etc.). In recent years, there has been an increasing demand for utility-oriented pattern mining (UPM, or called utility mining). UPM is a vital task, with numerous high-impact applications, including cross-marketing, e-commerce, finance, medical, and biomedical applications. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods of UPM. First, we introduce an in-depth understanding of UPM, including concepts, examples, and comparisons with related concepts. A taxonomy of the most common and state-of-the-art approaches for mining different kinds of high-utility patterns is presented in detail, including Apriori-based, tree-based, projection-based, vertical-/horizontal-data-format-based, and other hybrid approaches. A comprehensive review of advanced topics of existing high-utility pattern mining techniques is offered, with a discussion of their pros and cons. Finally, we present several well-known open-source software packages for UPM. We conclude our survey with a discussion on open and practical challenges in this field."
488,,Learning to Recommend With Multiple Cascading Behaviors.,"Chen Gao,Xiangnan He 0001,Dahua Gan,Xiangning Chen,Fuli Feng,Yong Li 0008,Tat-Seng Chua,Lina Yao,Yang Song,Depeng Jin",https://doi.org/10.1109/TKDE.2019.2958808,TKDE,2021,"Collaboration,Neural networks,Semantics,Recommender systems,Predictive models,Data models,Gallium nitride","Most existing recommender systems leverage user behavior data of one type only, such as the purchase behavior in E-commerce that is directly related to the business Key Performance Indicator (KPI) of conversion rate. Besides the key behavioral data, we argue that other forms of user behaviors also provide valuable signal, such as views, clicks, adding a product to shopping carts and so on. They should be taken into account properly to provide quality recommendation for users. In this work, we contribute a new solution named short for 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">N</b>
eural 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</b>
ulti-
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">T</b>
ask 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">R</b>
ecommendation (NMTR) for learning recommender systems from user multi-behavior data. We develop a neural network model to capture the complicated and multi-type interactions between users and items. In particular, our model accounts for the cascading relationship among different types of behaviors (e.g., a user must click on a product before purchasing it). To fully exploit the signal in the data of multiple types of behaviors, we perform a joint optimization based on the multi-task learning framework, where the optimization on a behavior is treated as a task. Extensive experiments on two real-world datasets demonstrate that NMTR significantly outperforms state-of-the-art recommender systems that are designed to learn from both single-behavior data and multi-behavior data. Further analysis shows that modeling multiple behaviors is particularly useful for providing recommendation for sparse users that have very few interactions."
489,,Learning ADL Daily Routines with Spatiotemporal Neural Networks.,"Shan Gao,Ah-Hwee Tan,Rossi Setchi",https://doi.org/10.1109/TKDE.2019.2924623,TKDE,2021,"Subspace constraints,Spatiotemporal phenomena,Clustering algorithms,Neural networks,Adaptation models,Senior citizens,Medical services","Activities of daily living (ADLs) refer to the activities performed by individuals on a daily basis and are the indicators of a person's habits, lifestyle, and wellbeing. Consequently, learning an individual's ADL daily routines has significant value in the healthcare domain. Specifically, ADL recognition and inter-ADL pattern learning problems have been studied extensively in the past couple of decades. However, discovering the patterns of ADLs performed in a day and clustering them into ADL daily routines has been a relatively unexplored research area. In this paper, a self-organizing neural network model, called the Spatiotemporal ADL Adaptive Resonance Theory (STADLART), is proposed for learning ADL daily routines. STADLART integrates multimodal contextual information that involves the time and space wherein the ADLs are performed. By encoding spatiotemporal information explicitly as input features, STADLART enables the learning of time-sensitive knowledge. Moreover, a STADLART variation named STADLART-NC is proposed to normalize and customize ADL weighting for daily routine learning. A weighting assignment scheme is presented that facilitates the assignment of weighting according to ADL importance in specific domains. Empirical experiments using both synthetic and real-world public data sets validate the performance of STADLART and STADLART-NC when compared with alternative pattern discovery methods. The results show that STADLART could cluster ADL routines with better performance than baseline algorithms."
490,,Popularity Prediction for Single Tweet Based on Heterogeneous Bass Model.,"Xiaofeng Gao,Zuowu Zheng,Quanquan Chu,Shaojie Tang,Guihai Chen,Qianni Deng",https://doi.org/10.1109/TKDE.2019.2952856,TKDE,2021,"Predictive models,Twitter,Data models,Market research,Context modeling,YouTube","Predicting the popularity of a single tweet is useful for both users and enterprises. However, adopting existing topic or event prediction models cannot obtain satisfactory results. The reason is that one topic or event that consists of multiple tweets, has more features and characteristics than a single tweet. In this article, we propose two variations of Heterogeneous Bass models (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">HBass</i>
), originally developed in the field of marketing science, namely Spatial-Temporal Heterogeneous Bass Model (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ST-HBass</i>
) and Feature-Driven Heterogeneous Bass Model (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">FD-HBass</i>
), to predict the popularity of a single tweet at the early stage and the stable stage. We further design an Interaction Enhancement to improve the performance, which considers the competition and cooperation from different tweets with the common topic. In addition, it is often difficult to depict popularity quantitatively. We design an experiment to get the weight of favorite, retweet and reply, and apply the linear regression to calculate the popularity. Furthermore, we design a clustering method to bound the popular threshold. Once the weight and popular threshold are determined, the status whether a tweet will be popular or not can be justified. Our model is validated by conducting experiments on real-world Twitter data, and the results show the efficiency and accuracy of our model, with less absolute percent error and the best 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Precision</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">F-score</i>
. In all, we introduce Bass model into social network single-tweet prediction to show it can achieve excellent performance."
491,,A Hybrid Recommender System for Improving Automatic Playlist Continuation.,"Anna Gatzioura,João Vinagre,Alípio Mário Jorge,Miquel Sànchez-Marrè",https://doi.org/10.1109/TKDE.2019.2952099,TKDE,2021,"Recommender systems,Coherence,Semantics,Cognition,Music,Prediction algorithms,User experience","Although widely used, the majority of current music recommender systems still focus on recommendations’ accuracy, user preferences and isolated item characteristics, without evaluating other important factors, like the joint item selections and the recommendation moment. However, when it comes to playlist recommendations, additional dimensions, as well as the notion of user experience and perception, should be taken into account to improve recommendations’ quality. In this work, HybA, a hybrid recommender system for automatic playlist continuation, that combines Latent Dirichlet Allocation and Case-Based Reasoning, is proposed. This system aims to address “similar concepts” rather than similar users. More than generating a playlist based on user requirements, like automatic playlist generation methods, HybA identifies the semantic characteristics of a started playlist and reuses the most similar past ones, to recommend relevant playlist continuations. In addition, support to beyond accuracy dimensions, like increased coherence or diverse items’ discovery, is provided. To overcome the semantic gap between music descriptions and user preferences, identify playlist structures and capture songs’ similarity, a graph model is used. Experiments on real datasets have shown that the proposed algorithm is able to outperform other state of the art techniques, in terms of accuracy, while balancing between diversity and coherence."
492,,Route Recommendations for Intelligent Transportation Services.,"Yong Ge,Huayu Li,Alexander Tuzhilin",https://doi.org/10.1109/TKDE.2019.2937864,TKDE,2021,"Public transportation,Vehicles,Global Positioning System,Trajectory,Germanium,Business","The accumulated large amount of mobility data and the ability to track moving people or objects have enabled us to develop advanced mobile recommendations, which are essential to recommend a sequence of locations to an individual user on the move. In this paper, we study a particular case of mobile recommendations, route recommendations to drivers, by utilizing vehicle GPS data. Specifically, we formulate a new Route Recommendation with Relaxed Assumptions (RR-RA) problem, the goal of which is to recommend a sequence of locations to a driver based on his current location in order to maximize his business success. To make our recommendation practical and scalable for real practice, we need to produce recommendation results in a timely fashion once a request emerges. Therefore, we propose an efficient algorithm to efficiently generate recommendations. Furthermore, we identify and address a destination-oriented route recommendation (DORR) problem. Without solving DORR problem, RR-RA alone does not work well in practice because drivers may encounter the destination constraint on a daily basis. We develop a dedicated and efficient algorithm for solving DORR problem. The package of solutions for both RR-RA and DORR problems provide a comprehensive approach for route recommendations to drivers. We evaluate our methods using both real-world GPS data and synthetic data, and demonstrate the effectiveness and efficiency of proposed methods with different evaluation metrics."
493,,Computing Co-Location Patterns in Spatial Data with Extended Objects - A Scalable Buffer-Based Approach.,"Yong Ge,Zijun Yao,Huayu Li",https://doi.org/10.1109/TKDE.2019.2930598,TKDE,2021,"Upper bound,Computational modeling,Spatial databases,Data models,Computational efficiency,Data mining,Computational complexity","Spatial co-location patterns are subsets of spatial features usually located together in geographic space. Recent literature has provided different approaches to discover co-location patterns over point spatial data. However, most approaches consider the neighborhood relationship among spatial objects as binary and are mainly designed for point spatial features, thus are not appropriate for extended spatial features such as line strings and polygons, the neighborhood relationship among which is naturally continuous. This paper adopts a buffer-based model for measuring the spatial relationship of extended objects and mining co-location patterns. While the buffer-based model has several advantages for extended spatial features, it involves high computational complexity due to the expensive buffer-level overlay operation. To tackle this challenge, we introduce a coarse-level co-location mining framework, which follows a filter-and-refine paradigm. Within the framework, we develop a serious of rigorous upper bounds based on geometric property and progressively prune search space with these upper bounds. Moreover, we develop a join-less schema to further reduce computation cost of size-k(k > 2) co-location patterns. Finally, we conduct experiments with large-scale spatial data to validate the efficiency of the developed algorithms against several state-of-art methods. All experimental results demonstrate the superiority of our methods."
494,,Data-Driven Link Screening for Increasing Network Predictability.,"Tomer Geva,Inbal Yahav",https://doi.org/10.1109/TKDE.2019.2955650,TKDE,2021,"Training,Task analysis,Prediction algorithms,Tagging,Social networking (online),Supervised learning,Industries","Prediction methods applied to digital network data offer powerful capabilities that have radically affected a host of industries and services. Specifically, numerous works have shown that the use of network information to predict focal node properties produces significantly more accurate results compared with exclusive reliance on other types of data. In this study, we propose that it may be possible to improve network-information-based predictions by identifying network links that actually carry predictive power for a given prediction task. For this purpose, we suggest a problem referred to as the problem of Increasing Network Predictability (INP) by data-driven link screening. To address this problem we develop a new algorithm with three different implementations. We find that the algorithm is robust and consistently outperforms baseline link selection methods. We thus suggest that our algorithm has the potential to improve the efficacy of network data use for classification purposes."
495,,Improving the Quality of Web-Based Data Imputation With Crowd Intervention.,"Binbin Gu,Zhixu Li,An Liu 0002,Jiajie Xu 0001,Lei Zhao 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2954087,TKDE,2021,"Correlation,Data models,Web pages,Databases,Data mining,Data collection,Task analysis","Data incompleteness is a common data quality problem in databases. Recent work proposes to retrieve missing string values from the World Wide Web for higher imputation recall, but on the other hand, takes the risk of introducing web noises into the imputation results. So far there lacks an effective way to control the quality of web-based data imputation, given the complexity of the quality model and lacking of enough ground truth data. In this article, an EM-based quality model is first built for web-based data imputation which investigates three key factors jointly, i.e., precision of web sources, correlation among web sources, and precision and recall of the employed extractors. However, the accuracy of the EM-based quality model could be harmed when the EM (Expectation Maximization) assumption that “the majority agree on the truth” does not hold in some cases. To solve this problem, we introduce crowd intervention to help improve the quality model. While a straightforward but expensive way is to let the crowd to identify all these undesirable cases and provide the right imputation values for these blanks, a most crowd-economic way is to select a small set of blanks for crowd-based imputation, whose results could help to adjust the EM-based quality model towards a better one. To achieve this, an adaptive blank selection strategy is proposed to select a sequence of blanks for crowd-based imputation. Also, we work on finding a proper time to stop further crowd intervention for the balance of crowd efficiency and quality improvement. Our experiments performed on three real world and one simulated data collections prove that the proposed quality model can effectively help improve the quality of the web-based imputation results by more than 15 percent, while our crowd cost saving strategy saves more than 75 percent crowd cost."
496,,Document Vector Extension for Documents Classification.,"Shun Guo,Nianmin Yao",https://doi.org/10.1109/TKDE.2019.2961343,TKDE,2021,"Containers,Task analysis,Windows,Neural networks,Training,Semantics,Clustering algorithms","Simple linear models, which usually learn word-level representations that are later combined to form document representations, have recently shown impressive performance. To improve the performance of document-level classification, it is crucial to explore the factors affecting the quality of the document vector. In this paper, we propose the concept of containers and further explore the properties of word containers and document containers by experiments and theoretical demonstrations. We find that the document container has a fixed capacity and that the document vector obtained by a simple average of too many word embeddings undoubtedly cannot be fully loaded by the container and will lose some semantic and syntactic information on very large text datasets. We also propose an efficient approach for document representation, using clustering algorithms to divide a document container into several subcontainers and establishing the relationship between the subcontainers. We additionally report and discuss the properties of two methods of clustering algorithms, DVEM-Kmeans and DVEM-Random, on large text datasets by sentiment analysis and topic classification tasks. Compared to simple linear models, the results show that our models outperform the existing state-of-the-art in generating high-quality document representations for document-level classification relatedness tasks. Our approaches can also be introduced to other models based on neural networks, such as convolutional neural networks, recurrent neural networks and generative adversarial networks, in supervised or semisupervised settings."
497,,On the Efficient Representation of Datasets as Graphs to Mine Maximal Frequent Itemsets.,"Zahid Halim,Omer Ali,Muhammad Ghufran Khan",https://doi.org/10.1109/TKDE.2019.2945573,TKDE,2021,"Itemsets,Data mining,Databases,Data structures,Task analysis,Benchmark testing,Machine intelligence","Frequent itemsets mining is an active research problem in the domain of data mining and knowledge discovery. With the advances in database technology and an exponential increase in data to be stored, there is a need for efficient approaches that can quickly extract useful information from such large datasets. Frequent Itemsets (FIs) mining is a data mining task to find itemsets in a transactional database which occur together above a certain frequency. Finding these FIs usually requires multiple passes over the databases; therefore, making efficient algorithms crucial for mining FIs. This work presents a graph-based approach for representing a complete transactional database. The proposed graph-based representation enables the storing of all relevant information (for extracting FIs) of the database in one pass. Later, an algorithm that extracts the FIs from the graph-based structure is presented. Experimental results are reported comparing the proposed approach with 17 related FIs mining methods using six benchmark datasets. Results show that the proposed approach performs better than others in terms of time."
498,,SlimML - Removing Non-Critical Input Data in Large-Scale Iterative Machine Learning.,"Rui Han 0001,Chi Harold Liu,Shilin Li,Lydia Y. Chen,Guoren Wang,Jian Tang 0008,Jieping Ye",https://doi.org/10.1109/TKDE.2019.2951388,TKDE,2021,"Training,Data models,Support vector machines,Iterative algorithms,Artificial neural networks,Predictive models","The core of many large-scale machine learning (ML) applications, such as neural networks (NN), support vector machine (SVM), and convolutional neural network (CNN), is the training algorithm that iteratively updates model parameters by processing massive datasets. From a plethora of studies aiming at accelerating ML, being data parallelization and parameter server, the prevalent assumption is that all data points are equivalently relevant to model parameter updating. In this article, we challenge this assumption by proposing a criterion to measure a data point's effect on model parameter updating, and experimentally demonstrate that the majority of data points are non-critical in the training process. We develop a slim learning framework, termed SlimML, which trains the ML models only on the critical data and thus significantly improves training performance. To such an end, SlimML efficiently leverages a small number of aggregated data points per iteration to approximate the criticalness of original input data instances. The proposed approach can be used by changing a few lines of code in a standard stochastic gradient descent (SGD) procedure, and we demonstrate experimentally, on NN regression, SVM classification, and CNN training, that for large datasets, it accelerates model training process by an average of 3.61 times while only incurring accuracy losses of 0.37 percent."
499,,The Impact of Task Abandonment in Crowdsourcing.,"Lei Han 0003,Kevin Roitero,Ujwal Gadiraju,Cristina Sarasua,Alessandro Checco,Eddy Maddalena,Gianluca Demartini",https://doi.org/10.1109/TKDE.2019.2948168,TKDE,2021,"Task analysis,Crowdsourcing,Focusing,Reliability,Web pages","Crowdsourcing has become a standard methodology to collect manually annotated data such as relevance judgments at scale. On crowdsourcing platforms like Amazon MTurk or FigureEight, crowd workers select tasks to work on based on different dimensions such as task reward and requester reputation. Requesters then receive the judgments of workers who self-selected into the tasks and completed them successfully. Several crowd workers, however, preview tasks, begin working on them, reaching varying stages of task completion without finally submitting their work. Such behavior results in unrewarded effort which remains invisible to requesters. In this paper, we conduct an investigation of the phenomenon of task 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">abandonment</i>
, the act of workers previewing or beginning a task and deciding not to complete it. We follow a three-fold methodology which includes 1) investigating the prevalence and causes of task abandonment by means of a survey over different crowdsourcing platforms, 2) data-driven analysis of logs collected during a large-scale relevance judgment experiment, and 3) controlled experiments measuring the effect of different dimensions on abandonment. Our results show that task abandonment is a widely spread phenomenon. Apart from accounting for a considerable amount of wasted human effort, this bears important implications on the hourly wages of workers as they are not rewarded for tasks that they do not complete. We also show how task abandonment may have strong implications on the use of collected data (for example, on the evaluation of Information Retrieval systems)."
500,1,Privacy-Preserving Stochastic Gradual Learning.,"Bo Han 0003,Ivor W. Tsang,Xiaokui Xiao,Ling Chen 0006,Sai-Fu Fung,Celina Ping Yu",https://doi.org/10.1109/TKDE.2020.2963977,TKDE,2021,"Privacy,Optimization,Differential privacy,Robustness,Stochastic processes,Task analysis","It is challenging for stochastic optimization to handle large-scale sensitive data safely. Duchi 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">et al.</i>
 recently proposed a private sampling strategy to solve privacy leakage in stochastic optimization. However, this strategy leads to a degeneration in robustness, since this strategy is equal to noise injection on each gradient, which adversely affects updates of the primal variable. To address this challenge, we introduce a robust stochastic optimization under the framework of local privacy, which is called Privacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges private updates of the primal variable (by private sampling) with gradual curriculum learning (CL). The noise injection leads to similar issue from label noise, but the robust learning process of CL can combat with label noise. Thus, PRESTIGE yields “private but robust” updates of the primal variable on the curriculum, that is, a reordered label sequence provided by CL. In theory, we reveal the convergence rate and maximum complexity of PRESTIGE. Empirical results on six datasets show that PRESTIGE achieves a good tradeoff between privacy preservation and robustness over baselines."
501,,Structured Graph Reconstruction for Scalable Clustering.,"Junwei Han,Kai Xiong,Feiping Nie 0001,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2948850,TKDE,2021,"Laplace equations,Matrix decomposition,Scalability,Clustering algorithms,Sparse matrices,Linear programming,Clustering methods","Spectral clustering is a quite simple but effective method for solving graph clustering problem. It projects the original data points into a lower dimensional space with spectral embedding, and then relies on an algorithm to obtain the cluster labels. Since it involves eigendecomposition of the graph Laplacian matrix for embedding, spectral clustering has high time complexity and is not able to process large scale data. The performance of spectral clustering is also limited by a post-processing algorithm such as kmeans. To tackle the two issues, we propose a method called Orthogonal and Nonnegative Graph Reconstruction (ONGR) for large scale clustering. The two constraints serve as a structure constraint with which the graph reconstructed by the indicator matrix is structured. The proposed method has linear time complexity with respect to the data size that it mainly needs to implicitly construct a graph and iteratively perform economical singular value decomposition for a small size matrix. Moreover, the interpretability of the indicator matrix is offered due to the nonnegative constraint, and thus our method can provide the cluster labels with no post-processing. The experiments on benchmark datasets show the effectiveness of the proposed scalable clustering method."
502,,Identify Significant Phenomenon-Specific Variables for Multivariate Time Series.,"Yifan Hao,Huiping Cao,Abdullah Mueen 0001,Sukumar Brahma",https://doi.org/10.1109/TKDE.2019.2934464,TKDE,2021,"Time series analysis,Feature extraction,Convolutional neural nets,Sensor phenomena and characterization,Radio frequency,Legged locomotion","Multivariate time series (MTS) are collected for different variables in studying scientific phenomena or monitoring system health where each time series records the values of one variable for a time period. Among the different variables, it is common that only a few variables contribute significantly to a specific phenomenon. Furthermore, the variables contributing significantly to different phenomena are often different. We denote the different variables that contribute to the occurrences of different phenomena as Phenomenon-specific Variables (PVs). In this paper, we formulate a novel problem of identifying significant PVs from MTS datasets. To analyze MTS data, feature extraction techniques have been extensively studied. However, most of them identify important global features for one dataset and do not utilize the temporal order of time series. To solve the newly introduced problem, we propose a solution framework, CNN
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">mts</sub>
-X, which is a new variant of the Convolutional Neural Networks (CNN) and can embed other feature extraction techniques (as X). Furthermore, we design a CNN
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">mts</sub>
-LR method that implements a new feature identification approach (LR) as Xin the CNN
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">mts</sub>
-X framework. The LR method leverages both Linear Discriminant Analysis (LDA) and Random Forest (RF). Our extensive experiments on five real datasets show that the CNN
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">mts</sub>
-LR method has exhibited much better performance than several other baseline methods. Using 30 percent of the PVs discovered from the CNN
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">mts</sub>
-LR, classifications can achieve better or similar performance than using all the variables."
503,,Extended Isolation Forest.,"Sahand Hariri,Matias Carrasco Kind,Robert J. Brunner",https://doi.org/10.1109/TKDE.2019.2947676,TKDE,2021,"Forestry,Vegetation,Distributed databases,Anomaly detection,Standards,Clustering algorithms,Heating systems","We present an extension to the model-free anomaly detection algorithm, Isolation Forest. This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. We motivate the problem using heat maps for anomaly scores. These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. We explain this problem in detail and demonstrate the mechanism by which it occurs visually. We then propose two different approaches for improving the situation. First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. This approach results in remedying the artifact seen in the anomaly score heat maps. We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF."
504,,Content to Node - Self-Translation Network Embedding.,"Zhicheng He 0001,Jie Liu 0007,Yuyuan Zeng,Lai Wei,Yalou Huang",https://doi.org/10.1109/TKDE.2019.2932388,TKDE,2021,"Task analysis,Semantics,Computer science,Fuses,Complex networks,Complexity theory,Natural language processing","This paper concerns the problem of network embedding (NE), which aims to learn low-dimensional representations for network nodes. Such dense representations offer great promises for many network analysis problems. However, existing approaches are still faced with challenges posed by the characteristics of complex real-world networks. First, for networks associated with rich content information, previous methods often learn separated content and structure representations, which requires post-processing of combination. Empirical combination strategies often make the final vectors suboptimal. Second, existing methods preserve the structure information by considering short and fixed neighborhood scope, such as the first- and/or the second-order proximities. However, it is hard to decide the neighborhood scope in complex problems. To this end, we propose a novel sequence to sequence model based NE framework referred to as Self-Translation Network Embedding (STNE). With the sampled node sequences, STNE translates each sequence itself from the content sequence to the node sequence. On the one hand, the bi-directional LSTM encoder fuses the content and structure information seamlessly from the raw input. On the other hand, high-order proximity can be flexibly learned with the memories of LSTM to capture long-range structural information. Experimental results on three real-world datasets demonstrate the superiority of STNE."
505,,GLAD - A Grid and Labeling Framework with Scheduling for Conflict-Aware kNN Queries.,"Dan He,Sibo Wang 0001,Xiaofang Zhou 0001,Reynold Cheng",https://doi.org/10.1109/TKDE.2019.2942585,TKDE,2021,"Roads,Throughput,Dispatching,Public transportation,Indexing,Law enforcement","The intelligent transportation systems, e.g., DiDi and Uber, have served as essential travel tools for customers, which foster plenty of studies for the location-based queries on road network. In particular, given a set O of objects and a query point q on a road network, the k Nearest Neighbor (kNN) query returns the k nearest objects in O with the shortest road network distance to q. In literature, most existing solutions for kNN queries tend to reduce the query time, indexing storage, or throughput of the kNN queries while overlooking the correctness of the queries caused by query-query and update-query conflicts. In our work, we propose a grid-based framework on conflict-aware kNN queries on moving objects which aims to optimize system throughput while guaranteeing query correctness. In particular, we first propose efficient index structures and new query algorithms that significantly improve the throughput. We further present novel scheduling algorithms that aim to avoid conflicts and improve the system throughput. Moreover, we devise approximate solutions that provide a controllable trade-off between the conflict of kNN queries and system throughput. Finally, we propose a cost-based dispatching strategy to assign the kNN results to the corresponding queries. Extensive experiments on real-world data demonstrate the effectiveness and efficiency of our proposed solutions over alternatives."
506,,Component-Based Feature Saliency for Clustering.,"Xin Hong,Hailin Li,Paul Miller 0003,Jianjiang Zhou,Ling Li,Danny Crookes,Yonggang Lu,Xuelong Li,Huiyu Zhou 0001",https://doi.org/10.1109/TKDE.2019.2936847,TKDE,2021,"Feature extraction,Clustering algorithms,Mathematical model,Numerical models,Mixture models,Bayes methods,Parameter estimation","Simultaneous feature selection and clustering is a major challenge in unsupervised learning. In particular, there has been significant research into saliency measures for features that result in good clustering. However, as datasets become larger and more complex, there is a need to adopt a finer-grained approach to saliency by measuring it in relation to a part of a model. Another issue is learning the feature saliency and advanced model parameters. We address the first by presenting a novel Gaussian mixture model, which explicitly models the dependency of individual mixture components on each feature giving a new component-based feature saliency measure. For the second, we use Markov Chain Monte Carlo sampling to estimate the model and hidden variables. Using a synthetic dataset, we demonstrate the superiority of our approach, in terms of clustering accuracy and model parameter estimation, over an approach using a model-based feature saliency with expectation maximisation. We performed an evaluation of our approach with six synthetic trajectory datasets obtaining an average clustering accuracy of 97 percent. To demonstrate the generality of our approach, we applied it to a network traffic flow dataset obtaining an accuracy of 93 percent for intrusion detection. Finally, we performed a comparison with state-of-the-art clustering techniques using three real-world trajectory datasets of vehicle traffic. Our approach achieved an average clustering accuracy of 96 percent compared to 77-95 percent for the other techniques. In conclusion, for the datasets considered, component based feature saliency measures gave improved clustering over those based on whole models."
507,,Learning With Feature Evolvable Streams.,"Bo-Jian Hou,Lijun Zhang 0005,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2019.2954090,TKDE,2021,"Biological system modeling,Sensor phenomena and characterization,Task analysis,Predictive models,Ecosystems,Feature extraction","Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this article, we propose a novel learning paradigm: 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Feature Evolvable Streaming Learning</i>
 where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn a mapping from the overlapping period to recover old features and then we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved and we provide a tighter bound when the loss function is exponentially concave. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal."
508,,Network Intervention for Mental Disorders with Minimum Small Dense Subgroups.,"Bay-Yuan Hsu,Chih-Ya Shen,Xifeng Yan",https://doi.org/10.1109/TKDE.2019.2949294,TKDE,2021,"Social networking (online),Approximation algorithms,Internet,Mental disorders","According to the literature in psychology, the existence of small dense subgroups is closely related to many mental illnesses, such as depression, bullying, and psychotic disorders. Here, small dense subgroups refer to the small groups in the social network in which members are socially dense but have no or few links to other individuals outside the group. Therefore, in this article, we make the first attempt to address the issue of small dense subgroups with the concept of 
<i>network intervention</i>
 from Psychology. We first introduce the new notion of 
<inline-formula><tex-math notation=""LaTeX"">$\Delta$</tex-math></inline-formula>
-Subgroups (
<inline-formula><tex-math notation=""LaTeX"">$\Delta$</tex-math></inline-formula>
-SGs) to quantify the small dense subgroups. Then, following the concept of network intervention, we formulate a new research problem, 
<i>Small Subgroup Maximum Reduction Problem (SSMP)</i>
, to reduce the number of small dense subgroups (i.e., 
<inline-formula><tex-math notation=""LaTeX"">$\Delta$</tex-math></inline-formula>
-SGs) in the social network. We prove that SSMP is NP-Hard and propose a linear-time algorithm, namely 
<i>3-SMMTG</i>
, to find the optimal solution for a special case of SSMP with 
<inline-formula><tex-math notation=""LaTeX"">$\Delta =3$</tex-math></inline-formula>
. We then devise a 
<inline-formula><tex-math notation=""LaTeX"">$\frac{1}{2}(1-\frac{1}{e})$</tex-math></inline-formula>
-approximation algorithm, namely 
<i>ESGR</i>
, for the general SSMP and enhance its efficiency with effective pruning methods. We conduct a 8-week evaluation study with 812 participants to validate the proposed SSMP and ESGR. The results show that the participants with the network intervention recommended by ESGR have significant improvements on Internet addiction and depression, as compared to those individuals without any intervention. We also perform experiments on 7 real datasets, and the experimental results manifest that the proposed algorithms outperform the other baselines in both efficiency and solution quality."
509,,Bit-Oriented Sampling for Aggregation on Big Data.,"Huan Hu,Jianzhong Li 0001",https://doi.org/10.1109/TKDE.2019.2931014,TKDE,2021,"Big Data,Encoding,Query processing,Task analysis,Chebyshev approximation,Time factors,Indexes","The efficiency of big data analysis has become a bottleneck. Aggregation is a fundamental analytical task. It usually consumes a lot of time so that sampling based aggregation is often used to improve response time at a loss of result accuracy. In all of the related works, sampling is conducted at the granularity of data item. Considering the bits at different bit positions of each data item have different contributions to an aggregation result, the performance of sampling based aggregation has a chance of being improved if sampling is conducted at the granularity of bit. Thus, this paper studies bit-oriented sampling for aggregation. Two methods of bit-oriented uniform sampling based aggregation, i.e., DVBM and DVFM, are proposed which are based on the central limit theorem or the Chebyshev's inequality. They are much more efficient than the methods of the traditional data-oriented uniform sampling based aggregation. DVBM can guarantee a given error bound of aggregation with the assumption that sample variance equals dataset variance. By contrast, DVFM achieves the same goal without that assumption, but it could result in a larger sampling size. Extensive experiments are carried out and the results show that DVBM and DVFM are both efficient and effective."
510,,Multi-Task Image Clustering through Correlation Propagation.,"Shizhe Hu,Xiaoqiang Yan,Yangdong Ye",https://doi.org/10.1109/TKDE.2019.2937026,TKDE,2021,"Task analysis,Correlation,Clustering algorithms,Visualization,Clustering methods,Convergence,Collaboration","Traditional image clustering algorithms deal with single-task clustering (STC) problem on a single domain. However, with the increasing number of related images on the Web, it is challenging for STCs to perform related image clustering tasks independently without considering the between-task relationship, which mainly consists of similar visual features and image patterns among tasks. Therefore, it is intuitive to resort to multi-task clustering (MTC) algorithms. However, most existing MTCs learn a shared feature subspace, which may lead to negative transfer when facing the image clustering tasks that are not strongly related. In this paper, we propose a novel multi-task image clustering algorithm, which performs multiple image clustering tasks simultaneously and propagates the task correlation to improve clustering performance. Specifically, we first extend the information bottleneck method to cluster tasks independently. The related and unrelated images between the pairwise clusters of different tasks are then discovered. Meanwhile, two corresponding types of correlations are propagated among the tasks, where only the positive correlation benefits the clustering of each task. A sequential and collaborative method is further designed to ensure an optimal solution. Moreover, we perform a theoretical analysis of the properties on correlation propagation and the convergence of our algorithm. The experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art clustering methods."
511,,HM-Modularity - A Harmonic Motif Modularity Approach for Multi-Layer Network Community Detection.,"Ling Huang,Chang-Dong Wang,Hongyang Chao",https://doi.org/10.1109/TKDE.2019.2956532,TKDE,2021,"Harmonic analysis,Image edge detection,Couplings,Sociology,Electroencephalography,Biochemistry,Brain modeling","Multi-layer network community detection has drawn an increasing amount of attention recently. Despite success, the existing methods mainly focus on the lower-order connectivity structure at the level of individual nodes and edges. And the higher-order connectivity structure has been largely ignored, which contains better signature of community compared with edges. The main challenges in utilizing higher-order structure for multi-layer network community detection are that the most representative higher-order structure may vary from one layer to another and the connectivity structure formed by the same node subset may exhibit different higher-order connectivity patterns in different layers. To this end, this paper proposes a novel higher-order structure, termed harmonic motif, which is a dense subgraph having on average the largest statistical significance in each layer. Based on the harmonic motif, a primary layer is constructed by integrating higher-order structural information from all layers. Additionally, the higher-order structural information of each individual layer is taken as the auxiliary information. A coupling is established between the primary layer and each auxiliary layer. Accordingly, a harmonic motif modularity is designed to generate the community structure. Extensive experiments on eleven real-world multi-layer network datasets have been conducted to confirm the effectiveness of the proposed method."
512,,Statistical Inference of Diffusion Networks.,"Hao Huang 0001,Qian Yan 0001,Lu Chen 0001,Yunjun Gao,Christian S. Jensen",https://doi.org/10.1109/TKDE.2019.2930060,TKDE,2021,"Diffusion processes,Complexity theory,Monitoring,Knowledge engineering,Heuristic algorithms,Probabilistic logic,Computational modeling","To infer structures in diffusion networks, existing approaches mostly need to know not only the final infection statuses of network nodes, but also the exact times when infections occur. In contrast, in many real-world settings, such as disease propagation, monitoring exact infection times is often infeasible due to a high cost. We investigate the problem of how to learn diffusion network structures based on only the final infection statuses of nodes. Instead of utilizing sequences of timestamps to determine potential parent-child influence relationships between nodes, we propose to find influence relationships with high statistical significance. To this end, we design a probabilistic generative model of the final infection statuses to quantitatively measure the likelihood of potential structures of the objective diffusion network, taking into account network complexity. Based on this model, we can infer an appropriate number of most probable parent nodes for each node in the network. Furthermore, to reduce redundant inference computations, we are able to preclude insignificant candidate parent nodes from being considered during inferencing, if their infections have little correlation with the infections of the corresponding child nodes. Extensive experiments on both synthetic and real-world networks offer evidence that the proposed approach is effective and efficient."
513,,Coupled Graphs and Tensor Factorization for Recommender Systems and Community Detection.,"Vassilis N. Ioannidis,Ahmed S. Zamzam,Georgios B. Giannakis,Nicholas D. Sidiropoulos",https://doi.org/10.1109/TKDE.2019.2941716,TKDE,2021,"Social networking (online),Recommender systems,Data models,Correlation,Artificial neural networks,Load modeling","Joint analysis of data from multiple information repositories facilitates uncovering the underlying structure in heterogeneous datasets. Single and coupled matrix-tensor factorization (CMTF) has been widely used in this context for imputation-based recommendation from ratings, social network, and other user-item data. When this side information is in the form of item-item correlation matrices or graphs, existing CMTF algorithms may fall short. Alleviating current limitations, we introduce a novel model coined coupled graph-tensor factorization (CGTF) that judiciously accounts for graph-related side information. The CGTF model has the potential to overcome practical challenges, such as missing slabs from the tensor and/or missing rows/columns from the correlation matrices. A novel alternating direction method of multipliers (ADMM) is also developed that recovers the nonnegative factors of CGTF. Our algorithm enjoys closed-form updates that result in reduced computational complexity and allow for convergence claims. A novel direction is further explored by employing the interpretable factors to detect graph communities having the tensor as side information. The resulting community detection approach is successful even when some links in the graphs are missing. Results with real data sets corroborate the merits of the proposed methods relative to state-of-the-art competing factorization techniques in providing recommendations and detecting communities."
514,,Tracking Community Consistency in Dynamic Networks - An Influence-Based Approach.,"Xiaowei Jia,Xiaoyi Li,Nan Du,Yuan Zhang,Vishrawas Gopalakrishnan,Guangxu Xun,Aidong Zhang",https://doi.org/10.1109/TKDE.2019.2933516,TKDE,2021,"Sparse matrices,Social networking (online),Companies,Internet,Smart devices,IEEE Fellows,Coherence","The dynamic network data have become ubiquitous with the rapid development of Internet and smart devices. To effectively manage the involved vertices in networks, it is crucial to track the special community patterns and analyze the relationships among vertices. In this paper, we propose a new method to measure the coherence strength, also referred to as community consistency, of a community over a specific observation period. The measurement of community consistency is especially challenging given the dynamic community structure over time, i.e., vertices can leave their original communities and join new communities. In order to interpret the causes of evolving community structure and model the influence of evolving community structure on community consistency, we introduce an influence propagation process having a causal relation with the community consistency. Specifically, a generative model is proposed to combine the influence propagation and the network topological structure at each time step. The proposed influence-based approach for modeling evolution can be instantiated in a variety of real-world network data. The comprehensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed framework in estimating the community consistency. Besides, we conduct a case study to show the effectiveness of the proposed method in real-world applications."
515,,Label Distribution Learning with Label Correlations on Local Samples.,"Xiuyi Jia,Zechao Li,Xiang Zheng,Weiwei Li,Sheng-Jun Huang",https://doi.org/10.1109/TKDE.2019.2943337,TKDE,2021,"Correlation,Clustering algorithms,Silicon compounds,Optimization,Computer science,Visualization,Trees - insulation","Label distribution learning (LDL) is proposed for solving the label ambiguity problem in recent years, which can be seen as an extension of multi-label learning. To improve the performance of label distribution learning, some existing algorithms exploit label correlations in a global manner that assumes the label correlations are shared by all instances. However, the instances in different groups may share different label correlations, and few label correlations are globally applicable in real-world tasks. In this paper, two novel label distribution learning algorithms are proposed by exploiting label correlations on local samples, which are called GD-LDL-SCL and Adam-LDL-SCL, respectively. To utilize the label correlations on local samples, the influence of local samples is encoded, and a local correlation vector is designed as the additional features for each instance, which is based on the different clustered local samples. Then, the label distribution for an unseen instance can be predicted by exploiting the original features and the additional features simultaneously. Extensive experiments on some real-world data sets validate that our proposed methods can address the label distribution problems effectively and outperform state-of-the-art methods."
516,,A Generic Ontology Framework for Indexing Keyword Search on Massive Graphs.,"Jiaxin Jiang,Byron Choi,Jianliang Xu,Sourav S. Bhowmick",https://doi.org/10.1109/TKDE.2019.2956535,TKDE,2021,"Keyword search,Ontologies,Semantics,Indexing,Organizations,Knowledge based systems","Due to the unstructuredness and the lack of schema information of knowledge graphs, social networks and RDF graphs, keyword search has been proposed for querying such graphs/networks. Recently, various keyword search semantics have been designed. In this paper, we propose a 
<i>generic</i>
 
<i>ontology-based</i>
 indexing framework for keyword search, called 
<i>Bisimulation of Generalized Graph Index</i>
 (
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
), to enhance the search performance. The novelties of 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
 reside in using an ontology graph 
<inline-formula><tex-math notation=""LaTeX"">$G_{Ont}$</tex-math> </inline-formula>
 to summarize and index a data graph 
<inline-formula><tex-math notation=""LaTeX"">$G$</tex-math> </inline-formula>
 iteratively, to form a hierarchical index structure 
<inline-formula><tex-math notation=""LaTeX"">$\mathbb {G}$</tex-math> </inline-formula>
. 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
 is generic since it only requires keyword search algorithms to generate query answers from summary graphs having two simple properties. Regarding query evaluation, we transform a keyword search 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math> </inline-formula>
 into 
<inline-formula><tex-math notation=""LaTeX"">$\mathbb {Q}$</tex-math> </inline-formula>
 according to 
<inline-formula><tex-math notation=""LaTeX"">$G_{Ont}$</tex-math> </inline-formula>
 in runtime. The transformed query is searched on the summary graphs in 
<inline-formula><tex-math notation=""LaTeX"">$\mathbb {G}$</tex-math> </inline-formula>
. The efficiency is due to the small sizes of the summary graphs and the early pruning of semantically irrelevant subgraphs. To illustrate 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
's applicability, we show popular indexing techniques for keyword search (e.g., 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {Blinks}$</tex-math> </inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {r\hbox{-}clique}$</tex-math> </inline-formula>
) can be easily implemented on top of 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
. Our extensive experiments show that 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {BiG\hbox{-}index}$</tex-math> </inline-formula>
 reduced the runtimes of popular keyword search work 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {Blinks}$</tex-math> </inline-formula>
 by 50.5 percent and 
<inline-formula><tex-math notation=""LaTeX"">$\mathsf {r\hbox{-}clique}$</tex-math> </inline-formula>
 by 29.5 percent."
517,,Geographical Hidden Markov Tree.,"Zhe Jiang 0001,Miao Xie,Arpan Man Sainju",https://doi.org/10.1109/TKDE.2019.2930518,TKDE,2021,"Hidden Markov models,Earth,Training,Forecasting,Inference algorithms,Computational modeling,Predictive models","Given a spatial raster framework with explanatory feature layers, a spatial contextual layer (e.g., a potential field), as well as a set of training samples with class labels, the spatial prediction problem aims to learn a model that can predict a class layer. The problem is important in societal applications such as flood extent mapping for disaster response and national water forecasting, but is challenging due to the noise, obstacles, and heterogeneity in feature maps, implicit spatial dependency between locations based on the contextual layer (e.g., gradient directions on a potential field), and the large number of sample locations. Existing work often assumes undirected spatial dependency, or directed dependency with a total order, and thus cannot reflect complex directed dependency with a partial order. In contrast, we recently proposed geographical hidden Markov tree, a probabilistic graphical model that generalizes the common hidden Markov model from a one-dimensional sequence to a two-dimensional map. Partial order class dependency is incorporated in the hidden class layer with a reverse tree structure. We also investigated computational algorithms for reverse tree construction, model parameter learning and class inference. This paper extends our recent model with overlaying class nodes between observation nodes and underlying hidden class nodes. The additional overlaying class layer makes the model more robust to large scale feature obstacles. We also proposed corresponding learning and inference methods. Extensive evaluations on real world datasets show that our models outperform multiple baselines in flood mapping applications, our algorithms are scalable on large data sizes, and the proposed extension enhances classification performance."
518,,Robust Detection of Link Communities With Summary Description in Social Networks.,"Di Jin,Xiaobao Wang,Dongxiao He,Jianwu Dang,Weixiong Zhang",https://doi.org/10.1109/TKDE.2019.2958806,TKDE,2021,"Network topology,Social network services,Heuristic algorithms,Inference algorithms,Clustering algorithms,Bayes methods,Probabilistic logic","Community detection has been extensively studied for various applications. Recent research has started to explore node contents to identify semantically meaningful communities. However, links in real networks typically have semantic descriptions and communities of links can better characterize community behaviors than communities of nodes. The second issue in community finding is that the most existing methods assume network topologies and descriptive contents carry the same or compatible information of node group membership, restricting them to one topic per community, which is generally violated in real networks. The third issue is that the existing methods use top ranked words or phrases to label topics when interpreting communities, which is often inadequate for comprehension. To address these issues altogether, we propose a new Bayesian probabilistic approach for modeling real networks and developing an efficient variational algorithm for model inference. Our new method explores the intrinsic correlation between communities and topics to discover link communities and extract semantically meaningful community summaries at the same time. If desired, it is able to derive more than one topical summary per community to provide rich explanations. We present experimental results to show the effectiveness of our new approach and evaluate the method by a case study."
519,1,TIDY - Publishing a Time Interval Dataset With Differential Privacy.,"Woohwan Jung,Suyong Kwon,Kyuseok Shim",https://doi.org/10.1109/TKDE.2019.2952351,TKDE,2021,"Data privacy,Publishing,Histograms,Privacy,Time-frequency analysis,Two dimensional displays,Partitioning algorithms","Log data from mobile devices generally contain a series of events with temporal information including time intervals which consist of the start and finish times. However, the problem of releasing differentially private time interval datasets has not been tackled yet. A time interval dataset can be represented by a two dimensional (2D) histogram. Most of the methods to publish 2D histograms partition the data into rectangular spaces to reduce the aggregated noise error for range queries. However, the existing algorithms to publish 2D histograms suffer from the structural error when applied to time interval datasets. To reduce the aggregated noise errors and suppress the increase in the structural error, we propose the TIDY (publishing Time Intervals via Differential privacY) algorithm. We use the frequency vectors as a compact representation of the time interval dataset. After applying the Laplace mechanism to the frequency vectors, we improve the utility of the frequency vectors based on a maximum likelihood estimation. We also develop a new partitioning method adapted for the frequency vectors to balance the trade-off between the noise and structural errors. Our empirical study on real-life and synthetic datasets confirms that TIDY outperforms the existing algorithms for 2D histograms."
520,,Impact-Based Ranking of Scientific Publications - A Survey and Experimental Evaluation.,"Ilias Kanellos,Thanasis Vergoulis,Dimitris Sacharidis,Theodore Dalamagas,Yannis Vassiliou",https://doi.org/10.1109/TKDE.2019.2941206,TKDE,2021,"Measurement,Benchmark testing,Market research,Indexes,Information retrieval,Data mining","As the rate at which scientific work is published continues to increase, so does the need to discern high-impact publications. In recent years, there have been several approaches that seek to rank publications based on their expected citation-based impact. Despite this level of attention, this research area has not been systematically studied. Past literature often fails to distinguish between short-term impact, the current popularity of an article, and long-term impact, the overall influence of an article. Moreover, the evaluation methodologies applied vary widely and are inconsistent. In this work, we aim to fill these gaps, studying impact-based ranking theoretically and experimentally. First, we provide explicit definitions for short-term and long-term impact, and introduce the associated ranking problems. Then, we identify and classify the most important ideas employed by state-of-the-art methods. After studying various evaluation methodologies of the literature, we propose a specific benchmark framework that can help us better differentiate effectiveness across impact aspects. Using this framework we investigate: (1) the practical difference between ranking by short- and long-term impact, and (2) the effectiveness and efficiency of ranking methods in different settings. To avoid reporting results that are discipline-dependent, we perform our experiments using four datasets from different scientific disciplines."
521,,Sem2Vec - Semantic Word Vectors with Bidirectional Constraint Propagations.,"Taygun Kekeç,David M. J. Tax",https://doi.org/10.1109/TKDE.2019.2942021,TKDE,2021,"Semantics,Dictionaries,Task analysis,Thesauri,Dogs,Buildings,Stability analysis","Word embeddings learn a vector representation of words, which can be utilized in a large number of natural language processing applications. Learning these vectors shares the drawback of unsupervised learning: representations are not specialized for semantic tasks. In this work, we propose a full-fledged formulation to effectively learn semantically specialized word vectors (Sem2Vec) by creating shared representations of online lexical sources such as Thesaurus and lexical dictionaries. These shared representations are treated as semantic constraints for learning the word embeddings. Our methodology addresses size limitation and weak informativeness of these lexical sources by employing a bidirectional constraint propagation step. Unlike raw unsupervised embeddings that exhibit low stability and easily subject to changes under randomness, our semantic formulation learns word vectors that are quite stable. An extensive empirical evaluation on the word similarity task comprised of 11 word similarity datasets is provided where our vectors suggest notable performance gains over state of the art competitors. We further demonstrate the merits of our formulation in document text classification task over large collections of documents."
522,,Forecasting Gathering Events through Trajectory Destination Prediction - A Dynamic Hybrid Model.,"Amin Vahedian Khezerlou,Xun Zhou,Ling Tong,Yanhua Li,Jun Luo 0007",https://doi.org/10.1109/TKDE.2019.2937082,TKDE,2021,"Predictive models,Trajectory,Forecasting,Data mining,Markov processes,Urban areas","Identifying urban gathering events is an important problem due to challenges it brings to urban management. In our prior work, we proposed a hybrid model (H-VIGO-GIS) to predict future gathering events through trajectory destination prediction. Our approach consisted of two models: historical and recent and continuously predicted future gathering events. However, H-VIGO-GIS has limitations. (1) The recent model does not capture the newly-emerged abnormal patterns effectively, since it uses all recent trajectories, including normal ones. (2) The recent model is sparse due to limited number of trajectories it learns, i.e., it cannot produce predictions in many cases, forcing us to rely only on the historical model. (3) The accuracy of both recent and historical models varies by space and time. Therefore, combining them the same way at all times and places undermines the overall accuracy of the hybrid model. Addressing these issues, in this paper we propose a Dynamic Hybrid model called (DH-VIGO-TKDE) that addresses the above-mentioned issues. We perform comprehensive evaluations using two large real-world datasets and an event simulator. The experiments show the proposed model significantly improves the prediction accuracy and timeliness of forecasting gathering events, resulting in average precision of 0.91 and recall of 0.67 as opposed to 0.74 and 0.50 of H-VIGO-GIS."
523,,A Comparative Study for Unsupervised Network Representation Learning.,"Megha Khosla,Vinay Setty,Avishek Anand",https://doi.org/10.1109/TKDE.2019.2951398,TKDE,2021,"Task analysis,Deep learning,Linear programming,Context modeling,Optimization,Systematics,Social networking (online)","There has been significant progress in unsupervised network representation learning (UNRL) approaches over graphs recently with flexible random-walk approaches, new optimization objectives, and deep architectures. However, there is no common ground for systematic comparison of embeddings to understand their behavior for different graphs and tasks. We argue that most of the UNRL approaches either model and exploit neighborhood or what we call context information of a node. These methods largely differ in their definitions and exploitation of context. Consequently, we propose a framework that casts a variety of approaches – random walk based, matrix factorization and deep learning based – into a unified context-based optimization function. We systematically group the methods based on their similarities and differences. We study their differences which we later use to explain their performance differences (on downstream tasks). We conduct a large-scale empirical study considering nine popular and recent UNRL techniques and 11 real-world datasets with varying structural properties and two common tasks – node classification and link prediction. We find that for non-attributed graphs there is no single method that is a clear winner and that the choice of a suitable method is dictated by certain properties of the embedding methods, task and structural properties of the underlying graph. In addition, we also report the common pitfalls in evaluation of UNRL methods and come up with suggestions for experimental design and interpretation of results."
524,,Making Compiling Query Engines Practical.,"André Kohn,Viktor Leis,Thomas Neumann 0001",https://doi.org/10.1109/TKDE.2019.2905235,TKDE,2021,"Engines,Throughput,Optimization,Debugging,Tools,Computer bugs,Semantics","Compiling queries to machine code is a very efficient way for executing queries. One often overlooked problem with compilation is the time it takes to generate machine code. Even with fast compilation frameworks like LLVM, generating machine code for complex queries often takes hundreds of milliseconds. Such durations can be a major disadvantage for workloads that execute many complex, but quick queries. To solve this problem, we propose an adaptive execution framework, which dynamically switches from interpretation to compilation. We also propose a fast bytecode interpreter for LLVM, which can execute queries without costly translation to machine code and dramatically reduces the query latency. Adaptive execution is fine-grained, and can execute code paths of the same query using different execution modes. Our evaluation shows that this approach achieves optimal performance in a wide variety of settings-low latency for small data sets and maximum throughput for large data sizes. Besides compilation time, we also focus on debugging, which is another important challenge of compilation-based query engines. To address this problem, we present a novel, database-specific debugger for compiling query engines."
525,,The Framework of Personalized Ranking on Poisson Factorization.,"Li-Yen Kuo,Chung-Kuang Chou,Ming-Syan Chen",https://doi.org/10.1109/TKDE.2019.2924894,TKDE,2021,"Recommender systems,Sparse matrices,Linear programming,Estimation,Logistics,Optimization,Approximation algorithms","Matrix factorization (MF) has earned great success on recommender systems. However, the common-used regression-based MF not only is sensitive to outliers but also unable to guarantee that the predicted values are in line with the user preference orders, which is the basis of common measures of recommender systems, e.g., nDCG. To overcome the aforementioned drawbacks, we propose a framework for personalized ranking of Poisson factorization that utilizes learning-to-rank based posteriori instead of the classical regression-based ones. Owing to the combination, the proposed framework not only preserves user preference but also performs well on a sparse matrix. Since the posteriori that combines learning to rank and Poisson factorization does not follow the conjugate prior relationship, we estimate variational parameters approximately and propose two optimization approaches based on variational inference. As long as the used learning-to-rank model has the 1st and 2nd order partial derivatives, by exploiting our framework, the proposed optimizing algorithm can maximize the posteriori whichever the used learning-to-rank model is. In the experiment, we show that the proposed framework outperforms the state-of-the-art methods and achieves promising results on consuming log and rating datasets for multiple recommendation tasks."
526,,An Adaptive Robust Semi-Supervised Clustering Framework Using Weighted Consensus of Random $k$k-Means Ensemble.,"Yongxuan Lai,Songyao He,Zhijie Lin,Fan Yang 0010,Qifeng Zhou,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2952596,TKDE,2021,"Clustering algorithms,Partitioning algorithms,Libraries,Noise measurement,Linear programming,Fans,Heuristic algorithms","Semi-supervised cluster ensemble usually introduces a small amount of supervision in the first stage of cluster ensemble, i.e., ensemble generation, by performing many runs of semi-supervised clustering algorithms. However, it is neither efficient in terms of computational complexity, nor flexible in a dynamic learning environment where limited supervision changes over time. In this article we propose a new framework which generates base partitions in an unsupervised manner and attributes different weights to each cluster of the base partitions. The weighting scheme considers both the internal validation measures of clustering and the degrees of satisfaction of pairwise constraints. A weighted co-association matrix based consensus approach is then applied to achieve a final partition. To handle high-dimensional data, we generate base partitions using k-means with both random sampling and random subspace techniques. The new framework retains a high accuracy, and is efficient since it avoids performing semi-supervised clustering in ensemble generation and the complexity of the weighting scheme is independent of the number of instances in a dynamic environment. It is more adaptive than the traditional approach because it does not require rerunning semi-supervised clustering algorithms when the limited supervision changes. Empirical results on 12 datasets demonstrate that it is also more robust to noisy constraints."
527,,Efficient Distance Sensitivity Oracles for Real-World Graph Data.,"Jong-Ryul Lee,Chin-Wan Chung",https://doi.org/10.1109/TKDE.2019.2924419,TKDE,2021,"Sensitivity,Heuristic algorithms,Indexes,Roads,Instruction sets,Data structures,Fault tolerance","A distance sensitivity oracle is a data structure answering queries that ask the shortest distance from a node to another in a network expecting node/edge failures. It has been mainly studied in theory literature, but all the existing oracles for a directed graph suffer from prohibitive preprocessing time and space. Motivated by this, we develop two practical distance sensitivity oracles for directed graphs as variants of Transit Node Routing. The first oracle consists of a novel fault-tolerant index structure, which is used to construct a solution path and to detect and localize the impact of network failures, and an efficient query algorithm for it. The second oracle is made by applying the A* heuristics to the first oracle, which exploits lower bound distances to effectively reduce search space. In addition, we propose additional speed-up techniques to make our oracles faster with a slight loss of accuracy. We conduct extensive experiments with real-life datasets, which demonstrate that our oracles greatly outperform all of competitors in most cases. To the best of our knowledge, our oracles are the first distance sensitivity oracles that handle real-world graph data with million-level nodes."
528,,Discovering Hidden Topical Hubs and Authorities Across Multiple Online Social Networks.,"Roy Ka-Wei Lee,Tuan-Anh Hoang,Ee-Peng Lim",https://doi.org/10.1109/TKDE.2019.2922962,TKDE,2021,"Twitter,Analytical models,Knowledge engineering,Data engineering,Task analysis,Predictive models","Finding influential users in online social networks (OSNs) is an important problem with many possible useful applications. Many methods have been proposed to identify influential users in OSNs. PageRank and HITs are two well known examples that determine influential users through link analysis. In recent years, new models that consider both content and social network links have been developed. The Hub and Authority Topic (HAT) model is one that extends HITS to identify topic-specific hubs and authorities by jointly learning hubs, authorities, and topical interests from users' relationship and textual content. However, many of the previous works are confined to identifying influential users within a single OSN. These models, when applied to multiple OSNs, could not learn influential users under a common set of topics nor address platform preferences. In this paper, we therefore propose the MPHAT model, an extension of HAT, to jointly model the topic-specific hub users, authority users, their topical interests and platform preferences. We evaluate MPHAT against several existing state-of-the-art methods in three tasks: (i) modeling of topics, (ii) platform choice prediction, and (iii) link recommendation. Based on our extensive experiments in multiple OSNs settings using synthetic datasets and real-world datasets from Twitter and Instagram, we show that MPHAT is comparable to state-of-the-art topic models in learning topics but outperforms the state-of-the-art models in platform prediction and link recommendation tasks. We also empirically demonstrate the ability of MPHAT to determine influential users within and across multiple OSNs."
529,,Finding Route Hotspots in Large Labeled Networks.,"Mingtao Lei,Xi Zhang 0008,Lingyang Chu,Zhefeng Wang,Philip S. Yu,Binxing Fang",https://doi.org/10.1109/TKDE.2019.2956924,TKDE,2021,"Computer hacking,Communication networks,Collaboration,Indexes,Trojan horses,Feature extraction","In many advanced network analysis applications, like social networks, e-commerce, and network security, hotspots are generally considered as a group of vertices that are tightly connected owing to the similar characteristics, such as common habits and location proximity. In this article, we investigate the formation of hotspots from an alternative perspective that considers the routes along the network paths as the auxiliary information, and attempt to find the route hotspots in large labeled networks. A 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">route hotspot</i>
 is a cohesive subgraph that is covered by a set of routes, and these routes correspond to the same sequential pattern consisting of vertices’ labels. To the best of our knowledge, the problem of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Finding Route Hotspots in Large Labeled Networks</i>
 has not been tackled in the literature. However, it is challenging as counting the number of hotspots in a network is #P-hard. Inspired by the observation that the sizes of hotspots decrease with the increasing lengths of patterns, we prove several anti-monotonicity properties of hotspots, and then develop a scalable algorithm called FastRH that can use these properties to effectively prune the patterns that cannot form any hotspots. In addition, to avoid the duplicate computation overhead, we judiciously design an effective index structure called 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">RH-Index</i>
 for storing the hotspot and pattern information collectively, which also enables incremental updating and efficient query processing. Our experimental results on real-world datasets clearly demonstrate the effectiveness and scalability of our proposed methods."
530,,Large-scale Cost-Aware Classification Using Feature Computational Dependency Graph.,"Qingzhe Li,Amir Alipour-Fanid,Martin Slawski,Yanfang Ye,Lingfei Wu,Kai Zeng 0001,Liang Zhao 0002",https://doi.org/10.1109/TKDE.2019.2948607,TKDE,2021,"Computational modeling,Optimization,Feature extraction,Machine learning,Standards,Data models,Runtime","With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency."
531,,Mixture Matrix Approximation for Collaborative Filtering.,"Dongsheng Li 0002,Chao Chen 0016,Tun Lu,Stephen M. Chu,Ning Gu",https://doi.org/10.1109/TKDE.2019.2955100,TKDE,2021,"Motion pictures,Collaboration,Task analysis,Toy manufacturing industry,Mixture models,Approximation methods,Computer science","Matrix approximation (MA) methods are integral parts of today's recommender systems. In standard MA methods, only one feature vector is learned for each user/item, which may not be accurate enough to characterize the diverse interests of users/items. For instance, users could have different opinions on a given item, so that they may need different feature vectors for the item to represent their unique interests. To this end, this article proposes a mixture matrix approximation (MMA) method, in which we assume that the user-item ratings follow mixture distributions and the user/item feature vectors vary among different stars to better characterize the diverse interests of users/items. Furthermore, we show that the proposed method can tackle both rating prediction and the top-N recommendation problems. Empirical studies on MovieLens, Netflix and Amazon datasets demonstrate that the proposed method can outperform state-of-the-art MA-based collaborative filtering methods in both rating prediction and top-N recommendation tasks."
532,,Signed Clique Search in Signed Networks - Concepts and Algorithms.,"Rong-Hua Li,Qiangqiang Dai,Lu Qin,Guoren Wang,Xiaokui Xiao,Jeffrey Xu Yu,Shaojie Qiao",https://doi.org/10.1109/TKDE.2019.2904569,TKDE,2021,"Proteins,Image edge detection,Measurement,Search problems,Social networking (online),Scalability,Indexes","Mining cohesive subgraphs from a network is a fundamental problem in network analysis. Most existing cohesive subgraph models are mainly tailored to unsigned networks. In this paper, we study the problem of seeking cohesive subgraphs in a signed network, in which each edge can be positive or negative, denoting friendship or conflict, respectively. We propose a novel model, called maximal (a, k)-clique, that represents a cohesive subgraph in signed networks. Specifically, a maximal (α, k)-clique is a clique in which every node has at most k negative neighbors and at least ⌈ak⌉ positive neighbors (α ≥ 1). We show that the problem of enumerating all maximal (a, k)-cliques in a signed network is NP-hard. To enumerate all maximal (a, k)-cliques efficiently, we first develop an elegant signed network reduction technique to significantly prune the signed network. Then, we present an efficient branch and bound enumeration algorithm with several carefully-designed pruning rules to enumerate all maximal (a, k)-cliques in the reduced signed network. In addition, we also propose an efficient algorithm with three novel upper-bounding techniques to find the maximum (a, k)-clique in a signed network. The results of extensive experiments on five large real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms."
533,,Modeling Influence Diffusion over Signed Social Networks.,"Dong Li,Jiming Liu 0001",https://doi.org/10.1109/TKDE.2019.2930690,TKDE,2021,"Social networking (online),Hidden Markov models,Computational modeling,Mathematical model,Data models,Predictive models,Stochastic processes","In offline or online worlds, many social systems can be represented as signed social networks including both positive and negative relationships. Although a variety of studies on signed social networks have been conducted motivated by the great application value of unique polarity characteristics, how to model the process of influence propagation over signed social networks is still an important problem that remains pretty much open. Currently, a few studies extended traditional diffusion models (e.g., Independent Cascade model and Linear Threshold model) from unsigned social networks to signed social networks for estimating positive and negative influence of user sets. However, all of above extension models are stochastic and descriptive models. In order to ensure the accuracy of estimated influence, existing models require a significant number of Monte-Carlo simulations which are very time-consuming and not scalable. Aiming at this issue, we propose the Polarity-related Linear Influence Diffusion (PLID) model which can quickly and accurately calculate polarity-related influence of user sets without simulations. To validate effectiveness and efficiency of our proposed model, we make use of our PLID model to solve the positive influence maximization problem in signed social networks under rigorous mathematical proofs. Extensive experiments demonstrate that our PLID model and approximation algorithm significantly outperform state-of-the-art methods in terms of positive influence spread and running time, using Epinions and Slashdot datasets."
534,,Lightweight Label Propagation for Large-Scale Network Data.,"Yu-Feng Li,De-Ming Liang",https://doi.org/10.1109/TKDE.2019.2949297,TKDE,2021,"Laplace equations,Loss measurement,Prototypes,Machine learning algorithms,Social networking (online),Indexes,Scalability","Label propagation spreads the soft labels from few labeled data to a large amount of unlabeled data according to the intrinsic graph structure. Nonetheless, most label propagation solutions work under relatively small-scale data and fail to cope with many real applications, such as social network analysis, where graphs usually have millions of nodes. In this paper, we propose a novel algorithm named 
<sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SLP</small>
 to deal with large-scale data. A lightweight iterative process derived from the well-known stochastic gradient descent strategy is used to accelerate the solving process. We also give a theoretical analysis on the necessity of the warm-start technique for label propagation. Experiments show that our algorithm is several times faster than state-of-the-art methods while achieving highly competitive performance."
535,,On Both Cold-Start and Long-Tail Recommendation with Social Data.,"Jingjing Li 0001,Ke Lu 0001,Zi Huang,Heng Tao Shen",https://doi.org/10.1109/TKDE.2019.2924656,TKDE,2021,"Recommender systems,Optimization,Business,Training,Iterative algorithms,Blogs,Videos","The number of “hits” has been widely regarded as the lifeblood of many web systems, e.g., e-commerce systems, advertising systems and multimedia consumption systems. However, users would not hit an item if they cannot see it, or they are not interested in the item. Recommender system plays a critical role of discovering interesting items from near-infinite inventory and exhibiting them to potential users. Yet, two issues are crippling the recommender systems. One is “how to handle new users”, and the other is “how to surprise users”. The former is well-known as cold-start recommendation. In this paper, we show that the latter can be investigated as long-tail recommendation. We also exploit the benefits of jointly challenging both cold-start and long-tail recommendation, and propose a novel approach which can simultaneously handle both of them in a unified objective. For the cold-start problem, we learn from side information, e.g., user attributes, user social relationships, etc. Then, we transfer the learned knowledge to new users. For the long-tail recommendation, we decompose the overall interesting items into two parts: a low-rank part for short-head items and a sparse part for long-tail items. The two parts are independently revealed in the training stage, and transfered into the final recommendation for new users. Furthermore, we effectively formulate the two problems into a unified objective and present an iterative optimization algorithm. A fast extension of the method is proposed to reduce the complexity, and extensive theoretical analysis are provided to proof the bounds of our approach. At last, experiments of social recommendation on various real-world datasets, e.g., images, blogs, videos and musics, verify the superiority of our approach compared with the state-of-the-art work."
536,,HisRect - Features from Historical Visits and Recent Tweet for Co-Location Judgement.,"Pengfei Li 0005,Hua Lu 0001,Qian Zheng,Shijian Li,Gang Pan 0001",https://doi.org/10.1109/TKDE.2019.2934686,TKDE,2021,"Twitter,Feature extraction,Semisupervised learning,Data models,Computer science,Location awareness","Enabled by smartphones, social media users are increasingly going mobile. This trend fosters various location based services on social media platforms (e.g., Twitter). Many services like friends notification and community detection benefit from co-location judgement, i.e., to decide whether two Twitter users are co-located in some point-of-interest (POI). This problem is challenging due to the limited information in tweets and the lack of explicit geo-tags in tweets that can be used as labeled data. Our approach to this problem is based on a novel concept of HisRect features extracted from users' historical visits and recent tweets: The former has impacts on where a user visits in general, whereas the latter gives more hints about where a user is currently. In practice, labeled data is scarce. Therefore, we design a semi-supervised learning (SSL) framework that leverages unlabeled data to extract HisRect features. Moreover, we employ an embedding neural network layer to process HisRect features of two users, which decides co-location based on the embedding difference between the two features. Our model is extensively evaluated on two large sets of real Twitter data from more than one million users. The experimental results demonstrate that our HisRect features and SSL framework are highly effective at deciding co-locations. In terms of multiple metrics, our approach clearly outperforms alternative approaches using state-of-the-art techniques."
537,,Matrix Factorization with Interval-Valued Data.,"Mao-Lin Li,Francesco Di Mauro,K. Selçuk Candan,Maria Luisa Sapino",https://doi.org/10.1109/TKDE.2019.2942310,TKDE,2021,"Matrix decomposition,Semantics,Probabilistic logic,Principal component analysis,Singular value decomposition,Knowledge engineering","With many applications relying on multi-dimensional datasets for decision making, matrix factorization (or decomposition) is becoming the basis for many knowledge discoveries and machine learning tasks, from clustering, trend detection, anomaly detection, to correlation analysis. Unfortunately, a major shortcoming of matrix analysis operations is that, despite their effectiveness when the data is scalar, these operations become difficult to apply in the presence of non-scalar data, as they are not designed for data that include non-scalar observations, such as intervals. Yet, in many applications, the available data are inherently non-scalar for various reasons, including imprecision in data collection, conflicts in aggregated data, data summarization, or privacy issues, where one is provided with a reduced, clustered, or intentionally noisy and obfuscated version of the data to hide information. In this paper, we propose matrix decomposition techniques that consider the existence of interval-valued data. We show that naive ways to deal with such imperfect data may introduce errors in analysis and present factorization techniques that are especially effective when the amount of imprecise information is large."
538,,Neural Named Entity Boundary Detection.,"Jing Li 0034,Aixin Sun,Yukun Ma",https://doi.org/10.1109/TKDE.2020.2981329,TKDE,2021,"Decoding,Recurrent neural networks,Task analysis,Vocabulary,Knowledge based systems,Logic gates","In this paper, we focus on named entity boundary detection, which is to detect the start and end boundaries of an entity mention in text, without predicting its type. The detected entities are input to entity linking or fine-grained typing systems for semantic enrichment. We propose BdryBot, a recurrent neural network encoder-decoder framework with a pointer network to detect entity boundaries from a given sentence. The encoder considers both character-level representations and word-level embeddings to represent the input words. In this way, BdryBot does not require any hand-crafted features. Because of the pointer network, BdryBot overcomes the problem of variable size output vocabulary and the issue of sparse boundary tags. We conduct two sets of experiments, in-domain detection and cross-domain detection, on six datasets. Our results show that BdryBot achieves state-of-the-art performance against five baselines. In addition, our proposed approach can be further enhanced when incorporating contextualized language embeddings into token representations."
539,,Top-$k$k Vehicle Matching in Social Ridesharing - A Price-Aware Approach.,"Yafei Li,Ji Wan,Rui Chen,Jianliang Xu,Xiaoyi Fu,Hongyan Gu,Pei Lv,Mingliang Xu",https://doi.org/10.1109/TKDE.2019.2937031,TKDE,2021,"Vehicles,Pricing,Roads,Vehicle dynamics,Social networking (online),Safety","In the past few years ridesharing has largely reshaped the transportation marketplace. It is envisioned as a promising solution to transportation-related problems in metropolitan cities, such as traffic congestion and air pollution. In the current ridesharing research, social ridesharing, which makes use of social relations among drivers and riders to address safety issues, and dynamic pricing are two active directions with important business implications. Simultaneously optimizing social cohesion and revenue is vital to a commercial ridesharing platform's sustainable development, which, however, has not been previously studied. In this paper, we first present a new pricing scheme that better incentivizes drivers and riders to participate in ridesharing, and then propose a novel type of Price-aware Top-k Matching (PTkM) queries which retrieve the top-k vehicles for a rider's request by taking into account both social relations and revenue. We design an efficient algorithm with a set of powerful pruning techniques to tackle this problem. Moreover, we propose a novel index tailored to our problem to further speed up query processing. Extensive experimental results on real datasets show that our proposed algorithms achieve desirable performance for real-world deployment."
540,,Large-Scale Nodes Classification With Deep Aggregation Network.,"Jiangtao Li,Jianshe Wu,Weiquan He,Peng Zhou",https://doi.org/10.1109/TKDE.2019.2955502,TKDE,2021,"Task analysis,Optimization,Scalability,Machine learning,Convolution,Neural networks,Collaboration","The most fundamental task of network representation learning (NRL) is nodes classification which requires an algorithm to map nodes to vectors and use machine learning models to predict nodes’ labels. Recently, many methods based on neighborhood aggregation have achieved brilliant results in this task. However, the recursive expansion of neighborhood aggregation poses scalability and efficiency problems for deep models. Existing methods are limited to shallow architectures and cannot capture the high order proximity in networks. In this article, we propose the deep aggregation network (DAN). DAN uses a layer-wise greedy optimization strategy which stacks several sequential trained base models to form the final deep model. The high order neighborhood aggregation is performed in a dynamic programming manner, which allows the recursion nature of neighborhood aggregation to be eliminated. The reverse random walk is also proposed, and combined with the classic random walk in formulating a novel sampling strategy that allows DAN to flexibly adapt to different tasks related to communities or structural roles. DAN is more efficient and effective than previous neighborhood aggregation based methods, especially when it is intended to handle large-scale networks with dense connections. Extensive experiments are conducted on both synthetic and real-world networks to empirically demonstrate the effectiveness and efficiency of the proposed method."
541,,A Comparative Study of Consistent Snapshot Algorithms for Main-Memory Database Systems.,"Liang Li 0016,Guoren Wang,Gang Wu 0007,Ye Yuan 0001,Lei Chen 0002,Xiang Lian",https://doi.org/10.1109/TKDE.2019.2930987,TKDE,2021,"Database systems,Throughput,Benchmark testing,Computer science,Instruction sets,Performance evaluation","In-memory databases (IMDBs) are gaining increasing popularity in big data applications, where clients commit updates intensively. Specifically, it is necessary for IMDBs to have efficient snapshot performance to support certain special applications (e.g., consistent checkpoint, HTAP). Formally, the in-memory consistent snapshot problem refers to taking an in-memory consistent time-in-point snapshot with the constraints that 1) clients can read the latest data items and 2) any data item in the snapshot should not be overwritten. Various snapshot algorithms have been proposed in academia to trade off throughput and latency, but industrial IMDBs such as Redis adhere to the simple fork algorithm. To understand this phenomenon, we conduct comprehensive performance evaluations on mainstream snapshot algorithms. Surprisingly, we observe that the simple fork algorithm indeed outperforms the state-of-the-arts in update-intensive workload scenarios. On this basis, we identify the drawbacks of existing research and propose two lightweight improvements. Extensive evaluations on synthetic data and Redis show that our lightweight improvements yield better performance than fork, the current industrial standard, and the representative snapshot algorithms from academia. Finally, we have opensourced the implementation of all the above snapshot algorithms so that practitioners are able to benchmark the performance of each algorithm and select proper methods for different application scenarios."
542,,Abstractive Multi-Document Summarization Based on Semantic Link Network.,"Wei Li 0107,Hai Zhuge",https://doi.org/10.1109/TKDE.2019.2922957,TKDE,2021,"Semantics,Data mining,Coherence,Security,Feature extraction,Syntactics,Benchmark testing",The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents.
543,,Discrete Matrix Factorization and Extension for Fast Item Recommendation.,"Defu Lian,Xing Xie 0001,Enhong Chen",https://doi.org/10.1109/TKDE.2019.2951386,TKDE,2021,"Binary codes,Recommender systems,Quantization (signal),Acceleration,Quadratic programming,Data models","Binary representation of users and items can dramatically improve efficiency of recommendation and reduce size of recommendation models. However, learning optimal binary codes for them is challenging due to binary constraints, even if squared loss is optimized. In this article, we propose a general framework for discrete matrix factorization based on discrete optimization, which can 1) optimize multiple loss functions; 2) handle both explicit and implicit feedback datasets; and 3) take auxiliary information into account without any hyperparameters. To tackle the challenging discrete optimization problem, we propose block coordinate descent based on semidefinite relaxation of binary quadratic programming. We theoretically show that it is equivalent to discrete coordinate descent when only one coordinate is in each block. We extensively evaluate the proposed algorithms on eight real-world datasets. The results of evaluation show that they outperform the state-of-the-art baselines significantly and that auxiliary information of items improves recommendation performance. For better showing the advantages of binary representation, we further propose a two-stage recommender system, consisting of an item-recalling stage and a subsequent fine-ranking stage. Its extensive evaluation shows hashing can dramatically accelerate item recommendation with little degradation of accuracy."
544,,MTBR - Multi-Target Boosting for Regression.,"Sangdi Lin,Bahareh Azarnoush,George C. Runger",https://doi.org/10.1109/TKDE.2019.2930516,TKDE,2021,"Boosting,Regression tree analysis,Data models,Vegetation,Predictive models,Task analysis","Gradient boosting method has been successfully used for single target prediction problems. In real world applications, however, problems involving the prediction of multiple target attributes are often of interest. In this paper, a multi-target boosting method for regression problems, named as MTBR, is proposed. Although MTBR builds one model for each target attribute separately, all the target attributes are utilized when building each model. In each boosting iteration, the base learner, the regression tree in particular, is learned by selecting the best models from all the target attributes. We also introduce a novel knowledge transfer approach. That is, the tree structure learned from one target attribute, representing a way to partition the feature space, is used to predict another target attribute. Experiments with six data sets compare MTBR to other ensemble regression methods, and prove the effectiveness of MTBR in leveraging the knowledge of multiple target attributes and improving the model accuracy."
545,,Fixed-Cost Pooling Strategies.,"Aldo Lipani,David E. Losada,Guido Zuccon,Mihai Lupu",https://doi.org/10.1109/TKDE.2019.2947049,TKDE,2021,"Resource management,Organizations,Q measurement,Uncertainty,Buildings,Standards,Benchmark testing","The empirical nature of Information Retrieval (IR) mandates strong experimental practices. A keystone of such experimental practices is the Cranfield evaluation paradigm. Within this paradigm, the collection of relevance judgments has been the subject of intense scientific investigation. This is because, on one hand, consistent, precise, and numerous judgements are keys to reducing evaluation uncertainty and test collection bias; on the other hand, however, relevance judgements are costly to collect. The selection of which documents to judge for relevance, known as pooling method, has therefore a great impact on IR evaluation. In this paper we focus on the bias introduced by the pooling method, known as pool bias, which affects the reusability of test collections, in particular when building test collections with a limited budget. In this paper we formalize and evaluate a set of 22 pooling strategies based on: traditional strategies, voting systems, retrieval fusion methods, evaluation measures, and multi-armed bandit models. To do this we run a large-scale evaluation by considering a set of 9 standard TREC test collections, in which we show that the choice of the pooling strategy has significant effects on the cost needed to obtain an unbiased test collection. We also identify the least biased pooling strategy in terms of pool bias according to three IR evaluation measures: AP, NDCG, and P@10."
546,,Modeling Submarket Effect for Real Estate Hedonic Valuation - A Probabilistic Approach.,"Zhicheng Liu,Jun Cao,Renjie Xie,Junyan Yang,Qiao Wang",https://doi.org/10.1109/TKDE.2020.3010548,TKDE,2021,"Cost accounting,Probabilistic logic,Bayes methods,Urban areas,Clustering methods,Testing,Data models","It is critical for urban planners and real estate developers to understand how the built environment and house characteristics are valued in housing market. However, this problem is challenging because of the existence of the submarket effect resulted from the heterogeneity nature of city. In this paper, we propose a probabilistic approach to residential property hedonic valuation problem modeling the full scope of submarket effect based on built environment and house characteristics. Specifically, we introduce a latent variable representing housing submarket and model both of the submarket criteria and hedonic price model(HPM) into a Bayesian network. Utilizing the probabilistic dependencies in the Bayesian network, our model is able to capture the full scope of the submarket effect. Furthermore, to analyze the relationship among the discovered submarkets, we propose a probabilistic hierarchical clustering method to infer the hierarchical structure of housing market. In particular, we perform Bayesian hypothesis testings to find the most similar submarkets and agglomerate submarkets step-by-step, thus revealing the hierarchical structure of housing market. Finally, we conduct comprehensive experiments in the housing market of Nanjing which is a metropolis in eastern China. The experimental results demonstrate the effectiveness of our proposed modeling method."
547,,EKT - Exercise-Aware Knowledge Tracing for Student Performance Prediction.,"Qi Liu 0003,Zhenya Huang,Yu Yin,Enhong Chen,Hui Xiong 0001,Yu Su 0002,Guoping Hu",https://doi.org/10.1109/TKDE.2019.2924374,TKDE,2021,"Predictive models,Knowledge acquisition,Hidden Markov models,Task analysis,Recurrent neural networks,Data mining","For offering proactive services (e.g., personalized exercise recommendation) to the students in computer supported intelligent education, one of the fundamental tasks is predicting student performance (e.g., scores) on future exercises, where it is necessary to track the change of each student's knowledge acquisition during her exercising activities. Unfortunately, to the best of our knowledge, existing approaches can only exploit the exercising records of students, and the problem of extracting rich information existed in the materials (e.g., knowledge concepts, exercise content) of exercises to achieve both more precise prediction of student performance and more interpretable analysis of knowledge acquisition remains underexplored. To this end, in this paper, we present a holistic study of student performance prediction. To directly achieve the primary goal of performance prediction, we first propose a general Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both student's exercising records and the text content of corresponding exercises. In EERNN, we simply summarize each student's state into an integrated vector and trace it with a recurrent neural network, where we design a bidirectional LSTM to learn the encoding of each exercise from its content. For making final predictions, we design two implementations on the basis of EERNN with different prediction strategies, i.e., EERNNM with Markov property and EERNNA with Attention mechanism. Then, to explicitly track student's knowledge acquisition on multiple knowledge concepts, we extend EERNN to an explainable Exercise-aware Knowledge Tracing (EKT) framework by incorporating the knowledge concept information, where the student's integrated state vector is now extended to a knowledge state matrix. In EKT, we further develop a memory network for quantifying how much each exercise can affect the mastery of students on multiple knowledge concepts during the exercising process. Finally, we conduct extensive experiments and evaluate both EERNN and EKT frameworks on a large-scale real-world data. The results in both general and cold-start scenarios clearly demonstrate the effectiveness of two frameworks in student performance prediction as well as the superior interpretability of EKT."
548,,Social Recommendation With Learning Personal and Social Latent Factors.,"Huafeng Liu,Liping Jing,Jian Yu,Michael K. Ng",https://doi.org/10.1109/TKDE.2019.2961666,TKDE,2021,"Social networking (online),Collaboration,Knowledge engineering,Data engineering,Complexity theory,Context modeling,Social groups","Due to leveraging social relationships between users as well as their past social behavior, social recommendation becomes a core component in recommendation systems. Most existing social recommendation methods only consider direct social relationships among users (e.g., explicit and observed social relations). Recently, researchers proved that indirect social relationships can be effective to improve the recommendation quality when users only have few social connections, because it can identify the user interesting group even though the users have no observed social connection. In the literature, separate two-stage methods are studied, but they cannot explicitly capture the natural relationship between indirect social relations and latent user/item factors. In this paper, the main contribution is to propose a new joint recommendation model taking advantage of the 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">In</b>
direct 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">S</b>
ocial 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">R</b>
elations detection and 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</b>
atrix 
<bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">F</b>
actorization collaborative filtering on social network and rating behavior information, which is called as 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">InSRMF</i>
. In our work, the user latent factors can simultaneously and seamlessly capture user’s personal preferences and social group characteristics. To optimize the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">InSRMF</i>
 model, we develop a parallel graph vertex programming algorithm for efficiently handling large scale social recommendation data. Experiments based on four real-world datasets (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Ciao</i>
, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Epinions</i>
, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Douban</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Yelp</i>
) are conducted to demonstrate the performance of the proposed model. The experimental results have shown that 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">InSRMF</i>
 has ability to mine the proper indirect social relations and improve the recommendation performance compared with the testing methods in the literature, especially on the users with few social neighbors, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Near-cold-start Users</i>
, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Pure-cold-start Users</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Long-tail Items</i>
."
549,,Clustering With Outlier Removal.,"Hongfu Liu,Jun Li 0027,Yue Wu,Yun Fu 0001",https://doi.org/10.1109/TKDE.2019.2954317,TKDE,2021,"Anomaly detection,Clustering algorithms,Partitioning algorithms,Clustering methods,Task analysis,Measurement,Linear programming","Cluster analysis and outlier detection are two continuously rising topics in data mining area, which in fact connect to each other deeply. Cluster structure is vulnerable to outliers; inversely, outliers are the points belonging to none of any clusters. Unfortunately, most existing studies do not notice the coupled relationship between these two tasks and handle them separately. In this article, we consider the joint cluster analysis and outlier detection problem, and propose the Clustering with Outlier Removal (COR) algorithm. Specifically, the original space is transformed into a binary space via generating basic partitions. We employ Holoentropy to measure the compactness of each cluster without involving several outlier candidates. To provide a neat and efficient solution, an auxiliary binary matrix is introduced so that COR completely and efficiently solves the challenging problem via a unified K-means— with theoretical supports. Extensive experimental results on numerous data sets in various domains demonstrate the effectiveness and efficiency of COR significantly over state-of-the-art methods in terms of cluster validity and outlier detection. Some key factors including the basic partition number and generation strategy in COR with an application on abnormal flight trajectory detection are further analyzed for practical use."
550,,Group-Based Skyline for Pareto Optimal Groups.,"Jinfei Liu,Li Xiong 0001,Jian Pei,Jun Luo 0007,Haoyu Zhang,Wenhui Yu",https://doi.org/10.1109/TKDE.2019.2960347,TKDE,2021,"Pareto optimization,Aggregates,Computational geometry,Decision making,Heuristic algorithms,Time complexity,Databases","Skyline computation, aiming at identifying a set of skyline points that are not dominated by any other point, is particularly useful for multi-criteria data analysis and decision making. Traditional skyline computation, however, is inadequate to answer queries that need to analyze not only 
<i>individual</i>
 points but also 
<i>groups</i>
 of points. To address this gap, we generalize the original skyline definition to the novel group-based skyline (G-Skyline), which represents Pareto optimal groups that are not dominated by other groups. In order to compute G-Skyline groups consisting of 
<inline-formula><tex-math notation=""LaTeX"">$s$</tex-math></inline-formula>
 points efficiently, we present a novel structure that represents the points in a directed skyline graph and captures the dominance relationships among the points based on the first 
<inline-formula><tex-math notation=""LaTeX"">$s$</tex-math></inline-formula>
 skyline layers. We propose efficient algorithms to compute the first 
<inline-formula><tex-math notation=""LaTeX"">$s$</tex-math></inline-formula>
 skyline layers. We then present two heuristic algorithms to efficiently compute the G-Skyline groups: the point-wise algorithm and the unit group-wise algorithm, using various pruning strategies. We observe that the number of G-Skyline groups of a dataset can be significantly large, we further propose the top-
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 representative G-Skyline groups based on the number of dominated points and the number of dominated groups and present efficient algorithms for computing them. The experimental results on the real NBA dataset and the synthetic datasets show that G-Skyline is interesting and useful, and our algorithms are efficient and scalable."
551,,Shifu2 - A Network Representation Learning Based Model for Advisor-Advisee Relationship Mining.,"Jiaying Liu 0006,Feng Xia 0001,Lei Wang 0134,Bo Xu 0008,Xiangjie Kong 0001,Hanghang Tong,Irwin King",https://doi.org/10.1109/TKDE.2019.2946825,TKDE,2021,"Teamwork,Data mining,Data models,Libraries,Computer science,Computational modeling","The advisor-advisee relationship represents direct knowledge heritage, and such relationship may not be readily available from academic libraries and search engines. This work aims to discover advisor-advisee relationships hidden behind scientific collaboration networks. For this purpose, we propose a novel model based on Network Representation Learning (NRL), namely Shifu2, which takes the collaboration network as input and the identified advisor-advisee relationship as output. In contrast to existing NRL models, Shifu2 considers not only the network structure but also the semantic information of nodes and edges. Shifu2 encodes nodes and edges into low-dimensional vectors respectively, both of which are then utilized to identify advisor-advisee relationships. Experimental results illustrate improved stability and effectiveness of the proposed model over state-of-the-art methods. In addition, we generate a large-scale academic genealogy dataset by taking advantage of Shifu2."
552,,Skyline Diagram - Efficient Space Partitioning for Skyline Queries.,"Jinfei Liu,Juncheng Yang,Li Xiong 0001,Jian Pei,Jun Luo 0007,Yuzhang Guo,Shuaicheng Ma,Chenglin Fan",https://doi.org/10.1109/TKDE.2019.2923914,TKDE,2021,"Heuristic algorithms,Approximation algorithms,Time complexity,Partitioning algorithms,Automobiles,Fans,Real-time systems","Skyline queries are important in many application domains. In this paper, we propose a novel structure Skyline Diagram, which given a set of points, partitions the plane into a set of regions, referred to as skyline polyominos. All query points in the same skyline polyomino have the same skyline query results. Similar to kth-order Voronoi diagram commonly used to facilitate k nearest neighbor (kNN) queries, skyline diagram can be used to facilitate skyline queries and many other applications. However, it may be computationally expensive to build the skyline diagram. By exploiting some interesting properties of skyline, we present several efficient algorithms for building the diagram with respect to three kinds of skyline queries, quadrant, global, and dynamic skylines. In addition, we propose an approximate skyline diagram which can significantly reduce the space cost. Experimental results on both real and synthetic datasets show that our algorithms are efficient and scalable."
553,,Duration Modeling with Semi-Markov Conditional Random Fields for Keyphrase Extraction.,"Xiaolei Lu,Tommy W. S. Chow",https://doi.org/10.1109/TKDE.2019.2942295,TKDE,2021,"Feature extraction,Data mining,Hidden Markov models,Decoding,Task analysis,Training,Labeling","Existing methods for keyphrase extraction need preprocessing to generate candidate phrase or post-processing to transform keyword into keyphrase. In this paper, we propose a novel approach called duration modeling with semi-Markov Conditional Random Fields (DM-SMCRFs) for keyphrase extraction. First of all, based on the property of semi-Markov chain, DM-SMCRFs can encode segment-level features and sequentially classify the phrase in the sentence as keyphrase or non-keyphrase. Second, by assuming the independence between state transition and state duration, DM-SMCRFs model the distribution of duration (length) of keyphrases to further explore state duration information, which can help identify the size of keyphrase. Based on the convexity of parametric duration feature derived from duration distribution, a constrained Viterbi algorithm is derived to improve the performance of decoding in DM-SMCRFs. We thoroughly evaluate the performance of DM-SMCRFs on the datasets from various domains. The experimental results demonstrate the effectiveness of proposed model."
554,,Nonparametric Regression via Variance-Adjusted Gradient Boosting Gaussian Process Regression.,"Hsin-Min Lu,Jih-Shin Chen,Wei-Chun Liao",https://doi.org/10.1109/TKDE.2019.2953728,TKDE,2021,"Training,Ground penetrating radar,Computational modeling,Gaussian processes,Data models,Boosting,Predictive models","Regression models have broad applications in data analytics. Gaussian process regression is a nonparametric regression model that learns nonlinear maps from input features to real-valued output using a kernel function that constructs the covariance matrix among all pairs of data. Gaussian process regression often performs well in various applications. However, the time complexity of Gaussian process regression is 
<inline-formula><tex-math notation=""LaTeX"">$O(n^3)$</tex-math> </inline-formula>
 for a training dataset of size 
<inline-formula><tex-math notation=""LaTeX"">$n$</tex-math> </inline-formula>
. The cubic time complexity hinders Gaussian process regression from scaling up to large datasets. Guided by the properties of Gaussian distributions, we developed a variance-adjusted gradient boosting algorithm for approximating a Gaussian process regression (VAGR). VAGR sequentially approximates the full Gaussian process regression model using the residuals computed from variance-adjusted predictions based on randomly sampled training subsets. VAGR has a time complexity of 
<inline-formula><tex-math notation=""LaTeX"">$O(nm^3)$</tex-math> </inline-formula>
 for a training dataset of size 
<inline-formula><tex-math notation=""LaTeX"">$n$</tex-math> </inline-formula>
 and the chosen batch size 
<inline-formula><tex-math notation=""LaTeX"">$m$</tex-math> </inline-formula>
. The reduced time complexity allows us to apply VAGR to much larger datasets compared with the full Gaussian process regression. Our experiments suggest that VAGR has a prediction performance comparable to or better than models that include random forest, gradient boosting machines, support vector regressions, and stochastic variational inference for Gaussian process regression."
555,,Demythization of Structural XML Query Processing - Comparison of Holistic and Binary Approaches.,"Petr Lukás,Radim Baca,Michal Krátký,Tok Wang Ling",https://doi.org/10.1109/TKDE.2019.2946157,TKDE,2021,"XML,Query processing,Semantics,Sorting,Time complexity,Impedance matching","XML queries can be modeled by twig pattern queries (TPQs) specifying predicates on XML nodes and XPath relationships satisfied between them. A lot of TPQ types have been proposed; this paper takes into account a TPQ model extended by a specification of output and non-output query nodes since it complies with the XQuery semantics and, in many cases, it leads to a more efficient query processing. In general, there are two types of approaches to process a TPQ: holistic joins and binary joins. Whereas the binary join approach builds a query plan as a tree of interconnected binary operators, the holistic join approach evaluates a whole query using one operator (i.e., using one complex algorithm). Surprisingly, a thorough analytical and experimental comparison is still missing despite an enormous research effort in this area. In this paper, we try to fill this gap; we analytically and experimentally show that the binary joins used in a fully-pipelined plan (i.e., the plan where each join operation does not wait for the complete result of the previous operation and no explicit sorting is used) can often outperform the holistic joins, especially for TPQs with a higher ratio of non-output query nodes. The main contributions of this paper can be summarized as follows: (i) we introduce several improvements of existing binary join approaches allowing to build a fully-pipelined plan for a TPQ considering non-output query nodes, (ii) we prove that for a certain class of TPQs such a plan has the linear time complexity with respect to the size of the input and output as well as the linear space complexity with respect to the XML document depth (i.e., the same complexity as the holistic join approaches), (iii) we show that our improved binary join approach outperforms the holistic join approaches in many situations, and (iv) we propose a simple combined approach that utilizes advantages of both types of approaches."
556,,Dynamic Ridesharing in Peak Travel Periods.,"Hui Luo,Zhifeng Bao,Farhana Murtaza Choudhury,J. Shane Culpepper",https://doi.org/10.1109/TKDE.2019.2961341,TKDE,2021,"Vehicles,Schedules,Vehicle dynamics,Indexes,Heuristic algorithms,Optimization,Roads","In this paper, we propose and study a variant of the dynamic ridesharing problem with a specific focus on peak hours: Given a set of drivers and a set of rider requests, we aim to match drivers to each rider request by achieving two objectives: maximizing the served rate and minimizing the total additional distance, subject to a series of spatio-temporal constraints. Our problem can be distinguished from existing ridesharing solutions in three aspects: (1) Previous work did not fully explore the impact of peak travel periods where the number of rider requests is much greater than the number of available drivers. (2) Existing ridesharing solutions usually rely on single objective optimization techniques, such as minimizing the total travel cost (either distance or time). (3) When evaluating the overall system performance, the runtime spent on updating drivers’ trip schedules as per newly coming rider requests should be incorporated, while it is unfortunately excluded by most existing solutions. In order to achieve our goal, we propose an underlying index structure on top of a partitioned road network, and compute the lower bounds of the shortest path distance between any two vertices. Using the proposed index together with a set of new pruning rules, we develop an efficient algorithm to dynamically include new riders directly into an existing trip schedule of a driver. In order to respond to new rider requests more effectively, we propose two algorithms that bilaterally match drivers with rider requests. Finally, we perform extensive experiments on a large-scale test collection to validate the effectiveness and efficiency of the proposed methods."
557,,Improving Data Analytics with Fast and Adaptive Regularization.,"Zhaojing Luo,Shaofeng Cai,Gang Chen 0001,Jinyang Gao,Wang-Chien Lee,Kee Yuan Ngiam,Meihui Zhang",https://doi.org/10.1109/TKDE.2019.2916683,TKDE,2021,"Adaptation models,Training,Tools,Computational modeling,Deep learning,Data analysis,Data models","Deep Learning and Machine Learning models have recently been shown to be effective in many real world applications. While these models achieve increasingly better predictive performance, their structures have also become much more complex. A common and difficult problem for complex models is overfitting. Regularization is used to penalize the complexity of the model in order to avoid overfitting. However, in most learning frameworks, regularization function is usually set with some hyper-parameters where the best setting is difficult to find. In this paper, we propose an adaptive regularization method, as part of a large end-to-end healthcare data analytics software stack, which effectively addresses the above difficulty. First, we propose a general adaptive regularization method based on Gaussian Mixture (GM) to learn the best regularization function according to the observed parameters. Second, we develop an effective update algorithm which integrates Expectation Maximization (EM) with Stochastic Gradient Descent (SGD). Third, we design a lazy update and sparse update algorithm to reduce the computational cost by 4x and 20x, respectively. The overall regularization framework is fast, adaptive, and easy-to-use. We validate the effectiveness of our regularization method through an extensive experimental study over 14 standard benchmark datasets and three kinds of deep learning/machine learning models. The results illustrate that our proposed adaptive regularization method achieves significant improvement over state-of-the-art regularization methods."
558,,Multiscale Local Community Detection in Social Networks.,"Wenjian Luo,Daofu Zhang,Li Ni,Nannan Lu",https://doi.org/10.1109/TKDE.2019.2938173,TKDE,2021,"Image edge detection,Social networking (online),Nickel,Indexes,Proteins,World Wide Web,Web sites","In real-world social networks, global information (e.g., the number of nodes and the connections between them) is incomplete or expensive to acquire; therefore, local community detection becomes especially important. Local community detection is used to identify the local community to which the given starting node belongs according to local information. For a given node, most existing local community detection methods can only find single scale local communities but not those of variable sizes. However, local communities with different scales are often required. Therefore, it is necessary and meaningful to find local communities of the given starting node with different scales; we call this multiscale local community detection. In this paper, we propose a new local modularity inspired by the global modularity and prove the equivalence of the proposed local modularity with two other typical local modularities. Furthermore, to detect local communities with different scales, we present a method based on the proposed local modularity. We test this method on several synthetic and real datasets, and the experimental results indicate that the detected community is meaningful and its scale can be changed reasonably."
559,,GM-PLL - Graph Matching Based Partial Label Learning.,"Gengyu Lyu,Songhe Feng,Tao Wang 0011,Congyan Lang,Yidong Li",https://doi.org/10.1109/TKDE.2019.2933837,TKDE,2021,"Phase locked loops,Training,Prediction algorithms,Task analysis,Labeling,Probabilistic logic,Predictive models","Partial Label Learning (PLL) aims to learn from the data where each training example is associated with a set of candidate labels, among which only one is correct. The key to deal with such problem is to disambiguate the candidate label sets and obtain the correct assignments between instances and their candidate labels. In this paper, we interpret such assignments as instance-to-label matchings, and reformulate the task of PLL as a matching selection problem. To model such problem, we propose a novel Graph Matching based Partial Label Learning (GM-PLL) framework, where Graph Matching (GM) scheme is incorporated owing to its excellent capability of exploiting the instance and label relationship. Meanwhile, since conventional one-to-one GM algorithm does not satisfy the constraint of PLL problem that multiple instances may correspond to the same label, we extend a traditional one-to-one probabilistic matching algorithm to the many-to-one constraint, and make the proposed framework accommodate to the PLL problem. Moreover, we also propose a relaxed matching prediction model, which can improve the prediction accuracy via GM strategy. Extensive experiments on both artificial and real-world data sets demonstrate that the proposed method can achieve superior or comparable performance against the state-of-the-art methods."
560,,Truth Discovery by Claim and Source Embedding.,"Shanshan Lyu,Wentao Ouyang,Yongqing Wang,Huawei Shen,Xueqi Cheng",https://doi.org/10.1109/TKDE.2019.2936189,TKDE,2021,"Reliability,Object oriented modeling,Iterative methods,Probabilistic logic,Data science,Computer aided software engineering,Data models","Information gathered from multiple sources on the Web often exhibits conflicts. This phenomenon motivates the need of truth discovery, which aims to automatically find the true claim among multiple conflicting claims. Existing truth discovery methods are mainly based on iterative updates, optimization or probabilistic models. Although these methods have shown their own effectiveness, they have a common limitation. These methods do not model relationships between each pair of source and target such that they do not well capture the underlying interactions in the data. In this paper, we propose a new model for truth discovery, learning the representations of sources and claims automatically from the interactions between sources and targets. Our model first constructs a heterogenous network including source-claim, source-source and truth-claim relationships. It then embeds the network into a low dimensional space such that trustworthy sources and true claims are close. In this way, truth discovery can be conveniently performed in the embedding space. Moreover, our model can be implemented in both semi-supervised and un-supervised manners to deal with the label scarcity problem in practical truth discovery. Experiments on three real-world datasets demonstrate that our model outperforms existing state-of-the-art methods for truth discovery."
561,,Co-Attention Memory Network for Multimodal Microblog&apos;s Hashtag Recommendation.,"Renfeng Ma,Xipeng Qiu,Qi Zhang 0001,Xiangkun Hu,Yu-Gang Jiang,Xuanjing Huang",https://doi.org/10.1109/TKDE.2019.2932406,TKDE,2021,"Twitter,Tagging,Task analysis,Visualization,Feature extraction,Neural networks,History","Hashtags are keywords describing a topic or a theme and are usually chosen by microblogging users. Hence, the hashtags can be used to categorize microblog posts. With the fast development of the social network, the task of recommending suitable hashtags has received considerable attention in recent years. Recently, most neural network methods have treated the task as a multi-class classification problem. In fact, users are constantly introducing new hashtags in a highly dynamic way. Treating the task as a multi-class classification problem with a fixed number of target categories does not allow the method to deal with the new hashtags. To address this problem, the task is reinterpreted as a matching problem and a novel co-attention memory network is proposed to represent the multimodal microblogs and hashtags. We utilize a co-attention mechanism to model the multimodal mircroblogs, and utilize the post history to represent the hashtags. Experimental results on a Twitter-based dataset demonstrated that the proposed method can achieve better performance than the current state-of-the-art methods that treat the task as a multi-class classification problem."
562,,Fast Stochastic Ordinal Embedding With Variance Reduction and Adaptive Step Size.,"Ke Ma 0001,Jinshan Zeng,Jiechao Xiong,Qianqian Xu,Xiaochun Cao,Wei Liu 0005,Yuan Yao 0011",https://doi.org/10.1109/TKDE.2019.2956700,TKDE,2021,"Convergence,Euclidean distance,Symmetric matrices,Scalability,Computational efficiency,Matrix decomposition","Learning representation from relative similarity comparisons, often called ordinal embedding, gains rising attention in recent years. Most of the existing methods are based on semi-definite programming (
<i>SDP</i>
), which is generally time-consuming and degrades the scalability, especially confronting large-scale data. To overcome this challenge, we propose a stochastic algorithm called 
<i>SVRG-SBB</i>
, which has the following features: i) achieving good scalability via dropping positive semi-definite (
<i>PSD</i>
) constraints as serving a fast algorithm, i.e., stochastic variance reduced gradient (
<i>SVRG</i>
) method, and ii) adaptive learning via introducing a new, adaptive step size called the stabilized Barzilai-Borwein (
<i>SBB</i>
) step size. Theoretically, under some natural assumptions, we show the 
<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{O}(\frac{1}{T})$</tex-math> </inline-formula>
 rate of convergence to a stationary point of the proposed algorithm, where 
<inline-formula><tex-math notation=""LaTeX"">$T$</tex-math> </inline-formula>
 is the number of total iterations. Under the further Polyak-Łojasiewicz assumption, we can show the global linear convergence (i.e., exponentially fast converging to a global optimum) of the proposed algorithm. Numerous simulations and real-world data experiments are conducted to show the effectiveness of the proposed algorithm by comparing with the state-of-the-art methods, notably, much lower computational cost with good prediction performance."
563,,Transformed Subspace Clustering.,"Jyoti Maggu,Angshul Majumdar,Émilie Chouzenoux",https://doi.org/10.1109/TKDE.2020.2969354,TKDE,2021,"Transforms,Kernel,Integrated circuits,Manifolds,Clustering algorithms,Sparse matrices,Symmetric matrices","Subspace clustering assumes that the data is separable into separate subspaces. Such a simple assumption, does not always hold. We assume that, even if the raw data is not separable into subspaces, one can learn a representation (transform coefficients) such that the learnt representation is separable into subspaces. To achieve the intended goal, we embed subspace clustering techniques (locally linear manifold clustering, sparse subspace clustering and low rank representation) into transform learning. The entire formulation is jointly learnt; giving rise to a new class of methods called transformed subspace clustering (TSC). In order to account for non-linearity, kernelized extensions of TSC are also proposed. To test the performance of the proposed techniques, benchmarking is performed on image clustering and document clustering datasets. Comparison with state-of-the-art clustering techniques shows that our formulation improves upon them."
564,,Declarative Data Analytics - A Survey.,"Nantia Makrynioti,Vasilis Vassalos",https://doi.org/10.1109/TKDE.2019.2958084,TKDE,2021,"Task analysis,Data analysis,Programming,Optimization,Mathematical model,Analytical models,Prediction algorithms","The area of declarative data analytics explores the application of the declarative paradigm on data science and machine learning. It proposes declarative languages for expressing data analysis tasks and develops systems which optimize programs written in those languages. The execution engine can be either centralized or distributed, as the declarative paradigm advocates independence from particular physical implementations. The survey explores a wide range of declarative data analysis frameworks by examining both the programming model and the optimization techniques used, in order to provide conclusions on the current state of the art in the area and identify open challenges."
565,,A Factorization Approach for Survival Analysis on Diffusion Networks.,"Giuseppe Manco 0001,Ettore Ritacco,Nicola Barbieri",https://doi.org/10.1109/TKDE.2019.2924369,TKDE,2021,"Peer-to-peer computing,Analytical models,Computational modeling,Diffusion processes,Data models,Random variables,Complexity theory","In this paper, we propose a survival factorization framework that models information cascades by tying together social influence patterns, topical structure, and temporal dynamics. This is achieved through the introduction of a latent space which encodes: (a) the relevance of an information cascade on a topic; (b) the topical authoritativeness and the susceptibility of each individual involved in the information cascade, and (c) temporal topical patterns. By exploiting the cumulative properties of the survival function and of the likelihood of the model on a given adoption log, which records the observed activation times of users and side-information for each cascade, we show that the inference phase is linear in the number of users and in the number of adoptions. The evaluation on both synthetic and real-world data shows the effectiveness of the model in detecting the interplay between topics and social influence patterns, which ultimately provides high accuracy in predicting users activation times."
566,,CRISP-DM Twenty Years Later - From Data Mining Processes to Data Science Trajectories.,"Fernando Martínez-Plumed,Lidia Contreras Ochando,Cèsar Ferri,José Hernández-Orallo,Meelis Kull,Nicolas Lachiche,María José Ramírez-Quintana,Peter A. Flach",https://doi.org/10.1109/TKDE.2019.2962680,TKDE,2021,"Data mining,Data science,Data models,Trajectory,Business,Knowledge discovery,Standards","CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">de facto</i>
 standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">data science</i>
 now the leading term being favoured over 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">data mining</i>
. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics."
567,,Disambiguation of Semantic Relations Using Evidence Aggregation According to a Sense Inventory.,"Víctor Martínez,Fernando Berzal,Juan C. Cubero",https://doi.org/10.1109/TKDE.2021.3055621,TKDE,2021,"Semantics,Knowledge based systems,Taxonomy,Automobiles,Internet,Task analysis,Encyclopedias","This paper describes EPROP, a novel technique requiring little prior knowledge for word sense disambiguation of semantic relations between pairs of ambiguous concepts in knowledge bases. Our method makes inferences by aggregating evidences from ambiguous word interpretations and propagating the acquired knowledge over a taxonomy to generalize or specialize this knowledge. This propagation process allows the estimation of the degree of belief for each possible word sense assignment given the available evidence. EPROP only requires a sense inventory structured as a taxonomy to disambiguate a knowledge base by combining evidence from the ambiguous facts stored in the knowledge base. We have performed different experiments that show that our method achieves good results on the disambiguation of the semantic relations included in WordNet and ConceptNet. We also show how our method can be used to improve the performance of state-of-the-art word sense disambiguation methods."
568,,Answering Skyline Queries Over Incomplete Data With Crowdsourcing.,"Xiaoye Miao,Yunjun Gao,Su Guo,Lu Chen 0001,Jianwei Yin,Qing Li 0001",https://doi.org/10.1109/TKDE.2019.2946798,TKDE,2021,"Crowdsourcing,Task analysis,Motion pictures,Data models,Databases,Correlation,Bayes methods","Due to the pervasiveness of incomplete data, incomplete data queries are vital in a large number of real-life scenarios. Current models and approaches for incomplete data queries mainly rely on the machine power. In this paper, we study the problem of skyline queries over incomplete data with crowdsourcing. We propose a novel query framework, termed as BayesCrowd, which takes into account the data correlation using the Bayesian network. We leverage the typical c-table model on incomplete data to represent objects. Considering budget and latency constraints, we present a suite of effective task selection strategies. Moreover, we introduce a marginal utilityfunction to measure the benefit of crowdsourcing one task. In particular, the probability computation of each object being an answer object is at least as hard as #SAT problem. To this end, we propose an adaptive DPLL (i.e., Davis-Putnam-Logemann-Loveland) algorithm to speed up the computation. Extensive experiments using both real and synthetic data sets confirm the superiority of BayesCrowd to the state-of-the-art method, in terms of execution time, monetary cost, and latency minimization."
569,,Group-Sparse SVD Models via $L_1$L1- and $L_0$L0-norm Penalties and their Applications in Biological Data.,"Wenwen Min,Juan Liu,Shihua Zhang",https://doi.org/10.1109/TKDE.2019.2932063,TKDE,2021,"Biological system modeling,Gene expression,Data models,Sparse matrices,Input variables,Iterative methods","Sparse Singular Value Decomposition (SVD) models have been proposed for biclustering high dimensional gene expression data to identify block patterns with similar expressions. However, these models do not take into account prior group effects upon variable selection. To this end, we first propose group-sparse SVD models with group Lasso (GL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
-SVD) and group L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-norm penalty (GL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-SVD) for non-overlapping group structure of variables. However, such group-sparse SVD models limit their applicability in some problems with overlapping structure. Thus, we also propose two group-sparse SVD models with overlapping group Lasso (OGL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
-SVD) and overlapping group L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-norm penalty (OGL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-SVD). We first adopt an alternating iterative strategy to solve GL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
-SVD based on a block coordinate descent method, and GL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-SVD based on a projection method. The key of solving OGL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
-SVD is a proximal operator with overlapping group Lasso penalty. We employ an alternating direction method of multipliers (ADMM) to solve the proximal operator. Similarly, we develop an approximate method to solve OGL
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-SVD. Applications of these methods and comparison with competing ones using simulated data demonstrate their effectiveness. Extensive applications of them onto several real gene expression data with gene prior group knowledge identify some biologically interpretable gene modules."
570,,TIPS - Mining Top-K Locations to Minimize User-Inconvenience for Trajectory-Aware Services.,"Shubhadip Mitra,Priya Saraf,Arnab Bhattacharya 0001",https://doi.org/10.1109/TKDE.2019.2935448,TKDE,2021,"Trajectory,Roads,Public transportation,Fuels,Indexes,Linear programming,Urban areas","Facility location problems aim to identify the best locations to set up new services. The majority of the existing works typically assume that the users are static. However, there exists a wide array of services such as fuel stations, ATMs, food joints, etc., that are widely accessed by mobile users besides the static ones. Such trajectory-aware services should, therefore, factor in the trajectories of its users rather than simply their static locations. In this work, we introduce the problem of optimal placement of facility locations for such trajectory-aware services that minimize the user inconvenience. The inconvenience of a user is the extra distance traveled by her from her regular path to avail a service. We call this the TIPS problem (Trajectory-aware Inconvenience-minimizing Placement of Services) and consider two variants of it. The goal of the first variant, MAX-TIPS, is to minimize the maximum inconvenience faced by any user, while that of the second, AVG-TIPS, is to minimize the average inconvenience over all the users. We show that both these problems are NP-hard, and propose multiple efficient heuristics to solve them. Empirical evaluation on real urban-scale road networks validate the efficiency and effectiveness of the proposed heuristics."
571,,Efficient Computation and Visualization of Multiple Density-Based Clustering Hierarchies.,"Antônio Cavalcante Araújo Neto,Jörg Sander 0001,Ricardo J. G. B. Campello,Mario A. Nascimento",https://doi.org/10.1109/TKDE.2019.2962412,TKDE,2021,"Computational efficiency,Clustering algorithms,Proposals,Data visualization,Organizations,Optics,Euclidean distance","HDBSCAN*, a state-of-the-art density-based hierarchical clustering method, produces a hierarchical organization of clusters in a dataset 
<i>w.r.t.</i>
 a parameter 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
. While a small change in 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 typically leads to a small change in the clustering structure, choosing a “good” 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 value can be challenging: depending on the data distribution, a high or low 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 value may be more appropriate, and certain clusters may reveal themselves at different values. To explore results for a range of 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 values, one has to run HDBSCAN* for each value independently, which can be computationally impractical. In this paper, we propose an approach to efficiently compute 
<i>all</i>
 HDBSCAN* hierarchies for a 
<i>range</i>
 of 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 values by building upon results from computational geometry to replace HDBSCAN*’s complete graph with a smaller equivalent graph. An experimental evaluation shows that our approach can obtain over one hundred hierarchies for the computational cost equivalent to running HDBSCAN* about twice, which corresponds to a speedup of more than 60 times, compared to running HDBSCAN* independently that many times. We also propose a series of visualizations that allow users to analyze a collection of hierarchies for a range of 
<inline-formula><tex-math notation=""LaTeX"">$mpts$</tex-math></inline-formula>
 values, along with case studies that illustrate how these analyses are performed."
572,,&quot;What Do Your Friends Think?&quot; - Efficient Polling Methods for Networks Using Friendship Paradox.,"Buddhika Nettasinghe,Vikram Krishnamurthy",https://doi.org/10.1109/TKDE.2019.2940914,TKDE,2021,"Social networking (online),Voting,Forecasting,Sampling methods,Color,Simulation,Indexes","This paper deals with randomized polling of a social network. In the case of forecasting the outcome of an election between two candidates A and B, classical intent polling asks randomly sampled individuals: who will you vote for? Expectation polling asks: who do you think will win? In this paper, we propose a novel neighborhood expectation polling (NEP) strategy that asks randomly sampled individuals: what is your estimate of the fraction of votes for A? Therefore, in NEP, sampled individuals will naturally look at their neighbors (defined by the underlying social network graph) when answering this question. Hence, the mean squared error (MSE) of NEP methods rely on selecting the optimal set of samples from the network. To this end, we propose two NEP algorithms for the following cases: (i) the social network graph is not known but, random walks (sequential exploration) can be performed on the graph, and (ii) the social network graph is unknown but, uniformly sampled nodes from the network are available. For both cases, algorithms based on a graph theoretic consequence called friendship paradox are proposed. Theoretical results on the dependence of the MSE of the algorithms on the properties of the network are established. Numerical results on real and synthetic data sets are provided to illustrate the performance of the algorithms."
573,,Structured Graph Optimization for Unsupervised Feature Selection.,"Feiping Nie 0001,Wei Zhu,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2937924,TKDE,2021,"Feature extraction,Manifolds,Dimensionality reduction,Data structures,Benchmark testing,Nonlinear optics,Optical imaging","Unsupervised feature selection has attracted more and more attention due to the rapid growth of the large amount of unlabelled and high-dimensional data. The performance of traditional spectral-based unsupervised methods always depends on the quality of constructed similarity matrix. However, real world data always contain a large number of noise samples and features that make the similarity matrix created by original data cannot be fully relied. We propose an unsupervised feature selection method which conducts feature selection and local structure learning simultaneously. Moreover, we add an important constraint on the similarity matrix to allow it to capture more accurate information of the data structure. To perform feature selection, orthogonal constraint and `2;p-norm are adopted on the projection matrix. An efficient and simple algorithm is derived to tackle the problem. We conduct comprehensive experiments on various benchmark data sets, including handwritten digit, face image, and biomedical data, to validate the effectiveness of the proposed approach."
574,,Learning Structural Node Representations Using Graph Kernels.,"Giannis Nikolentzos,Michalis Vazirgiannis",https://doi.org/10.1109/TKDE.2019.2947478,TKDE,2021,"Kernel,Matrix decomposition,Task analysis,Social networking (online),Vocabulary,Feature extraction,Memory management","Many applications require identifying nodes that perform similar functions in a graph. For instance, identifying structurally equivalent nodes can provide insight into the structure of complex networks. Learning latent representations that capture such structural role information about nodes has recently gained a lot of attention. Existing techniques for learning such representations typically rely on manually engineered features or are very expensive in terms of time and memory requirements. In this paper, we propose SEGK, a powerful framework for computing structural node representations. SEGK learns node representations by generating (or approximating) and decomposing a kernel matrix that incorporates structural similarity between nodes. To compute the similarity between two nodes, the proposed framework builds on well-established concepts from graph mining. Specifically, it compares the neighborhood subgraphs of increasing size of two nodes using graph kernels. SEGK is very flexible, and besides unlabeled graphs, it can also handle node-labeled and node-attributed graphs. We evaluate the proposed framework on several synthetic and real-world datasets, and compare its performance to state-of-the-art techniques for learning structural node embeddings. In almost all cases, the instances of the proposed framework outperform the competing methods, while their time complexity remains very attractive."
575,1,ERATO - Trading Noisy Aggregate Statistics over Private Correlated Data.,"Chaoyue Niu,Zhenzhe Zheng,Fan Wu 0006,Shaojie Tang,Xiaofeng Gao,Guihai Chen",https://doi.org/10.1109/TKDE.2019.2934100,TKDE,2021,"Aggregates,Pricing,Privacy,Differential privacy,Correlation,Data models","With the commoditization of personal privacy, pricing private data has become an intriguing problem. In this paper, we study noisy aggregate statistics trading from the perspective of a data broker in data markets. We thus propose ERATO, which enables aggrEgate statistics pRicing over privATe cOrrelated data. On one hand, ERATO guarantees arbitrage freeness against cunning data consumers. On the other hand, ERATO compensates data owners for their privacy losses using both bottom-up and top-down designs. We further apply ERATO to three practical aggregate statistics, namely weighted sum, probability distribution fitting, and degree distribution, and extensively evaluate their performances on MovieLens dataset, 2009 RECS dataset, and two SNAP large social network datasets, respectively. Our analysis and evaluation results reveal that ERATO well balances utility and privacy, achieves arbitrage freeness, and compensates data owners more fairly than differential privacy based approaches."
576,,An Embedding-Based Approach to Rule Learning in Knowledge Graphs.,"Pouya Ghiasnezhad Omran,Kewen Wang,Zhe Wang 0001",https://doi.org/10.1109/TKDE.2019.2941685,TKDE,2021,"Knowledge engineering,Task analysis,Scalability,Standards,Knowledge based systems,Predictive models,Sampling methods","It is natural and effective to use rules for representing explicit knowledge in knowledge graphs. However, it is challenging to learn rules automatically from very large knowledge graphs such as Freebase and YAGO. This paper presents a new approach, RLvLR (Rule Learning via Learning Representations), to learning rules from large knowledge graphs by using the technique of embedding in representation learning together with a new sampling method. Based on RLvLR, a new method RLvLR-Stream is developed for learning rules from streams of knowledge graphs. Both RLvLR and RLvLR-Stream have been implemented and experiments conducted to validate the proposed methods regarding the tasks of rule learning and link prediction. Experimental results show that our systems are able to handle the task of rule learning from large knowledge graphs with high accuracy and outperform some state-of-the-art systems. Specifically, for massive knowledge graphs with hundreds of predicates and over 10M facts, RLvLR is much faster and can learn much more quality rules than major systems for rule learning in knowledge graphs such as AMIE+. In the setting of knowledge graph streams, RLvLR-Stream significantly improved RLvLR for both rule learning and link prediction."
577,,Tackling Overfitting in Boosting for Noisy Healthcare Data.,"Yubin Park,Joyce C. Ho",https://doi.org/10.1109/TKDE.2019.2959988,TKDE,2021,"Boosting,Medical services,Stochastic processes,Adaptation models,Noise measurement,Training,Predictive models","Analyzing healthcare data poses several challenges including the limited number of samples, missing measurements, noisy labels, and heterogeneous data types. Tree-based boosting is well-suited for modeling such data as it is insensitive to data types and missingness. Moreover, Stochastic Gradient TreeBoost is often found in many winning solutions in public data science challenges. Unfortunately, the best performance requires extensive hyperparameter tuning and can be prone to overfitting. We propose PaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization techniques to guard against overfitting and is robust to hyperparameter settings. PaloBoost uses the out-of-bag samples to perform gradient-aware pruning and estimate adaptive learning rates. Unlike other Stochastic Gradient TreeBoost models that use the out-of-bag samples to estimate test errors, PaloBoost treats the samples as a second batch of training samples to prune the trees and adjust the learning rates. As a result, PaloBoost can dynamically adjust tree depths and learning rates to achieve faster learning at the start and slower learning as the algorithm converges. Experimental results on four datasets demonstrate that PaloBoost is robust to overfitting and is less sensitive to the hyperparameters."
578,,Stability Analysis of Denoising Autoencoders Based on Dynamical Projection System.,"Saerom Park,Jaewook Lee 0001",https://doi.org/10.1109/TKDE.2020.3010277,TKDE,2021,"Manifolds,Stability analysis,Data models,Image reconstruction,Noise reduction,Noise measurement,Perturbation methods","In this study, we give a stability analysis of denoising autoencoder(DAE) from the novel perspective of dynamical systems when the input density is defined as a distribution on a manifold. We demonstrate the connection between the corrupted distribution and the learned reconstruction function of a nonlinear DAE, which motivates the use of a dynamic projection system (DPS) associated with the learned reconstruction function. Utilizing the constructed DPS, we prove that the high-density region of the corrupted data distribution asymptotically converges to the data manifold. Then, we show that the region is the attracting stable equilibrium manifold of the DPS which is completely stable. These results serve a theoretical basis of the DAE in recognizing the high-density region of the highly corrupted data with large deviations through the DPS. The effectiveness of this analysis is verified by conducting experiments on several toy examples and real image datasets with various types of noise."
579,,ParIS+ - Data Series Indexing on Multi-Core Architectures.,"Botao Peng,Panagiota Fatourou,Themis Palpanas",https://doi.org/10.1109/TKDE.2020.2975180,TKDE,2021,"Indexing,Task analysis,Parallel processing,Hardware,Aggregates,Multicore processing","Data series similarity search is a core operation for several data series analysis applications across many different domains. Nevertheless, even state-of-the-art techniques cannot provide the time performance required for large data series collections. We propose ParIS and ParIS+, the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">first</i>
 disk-based data series indices carefully designed to inherently take advantage of multi-core architectures, in order to accelerate similarity search processing times. Our experiments demonstrate that ParIS+ completely removes the CPU latency during index construction for disk-resident data, and for exact query answering is up to 1 order of magnitude faster than the current state of the art index scan method, and up to 3 orders of magnitude faster than the optimized serial scan method. ParIS+ (which is an evolution of the ADS+ index) owes its efficiency to the effective use of multi-core and multi-socket architectures, in order to distribute and execute in parallel both index construction and query answering, and to the exploitation of the Single Instruction Multiple Data (SIMD) capabilities of modern CPUs, in order to further parallelize the execution of instructions inside each core."
580,,Optimizing Multi-Query Evaluation in Federated RDF Systems.,"Peng Peng 0001,Qi Ge,Lei Zou 0001,M. Tamer Özsu,Zhiwei Xu,Dongyan Zhao 0001",https://doi.org/10.1109/TKDE.2019.2947050,TKDE,2021,"Resource description framework,Query processing,Metadata,Optimization,Bioinformatics,Germanium","This paper revisits the classical problem of multiple query optimization in federated RDF systems. We propose a heuristic query rewriting-based approach to optimize the evaluation of multiple queries. This approach can take advantage of SPARQL 1.1 to share the common computation of multiple queries while considering the cost of both query evaluation and data shipment. Although we prove that finding the optimal rewriting for multiple queries is NP-complete, we propose a heuristic rewriting algorithm with a bounded approximation ratio. Furthermore, we propose an efficient method to use the interconnection topology between RDF sources to filter out irrelevant sources, and utilize some characteristics of SPARQL 1.1 to optimize multiple joins of intermediate matches. The extensive experimental studies show that the proposed techniques are effective, efficient and scalable."
581,,Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification.,"Hao Peng,Jianxin Li 0002,Senzhang Wang,Lihong Wang,Qiran Gong,Renyu Yang,Bo Li 0005,Philip S. Yu,Lifang He 0001",https://doi.org/10.1109/TKDE.2019.2959991,TKDE,2021,"Semantics,Deep learning,Feature extraction,Computational modeling,Taxonomy,Task analysis,Data models","CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. Most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics. However, how to coherently take them into account is still far from studied. In addition, most existing methods treat output labels as independent medoids, ignoring the hierarchical relationships among them, which leads to a substantial loss of useful semantic information. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding word matrix representation preserving both the non-consecutive, long-distance and local sequential semantics. Then the word matrix is input to the proposed attentional graph capsule recurrent CNNs for effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches."
582,,A Reliable Storage Partition for Permissioned Blockchain.,"Xiaodong Qi,Zhao Zhang,Cheqing Jin,Aoying Zhou",https://doi.org/10.1109/TKDE.2020.3012668,TKDE,2021,"Protocols,Encoding,Scalability,Fault tolerance,Fault tolerant systems,Engines","The full-replication data storage mechanism, as commonly utilized in existing blockchains, is the barrier to the system's scalability, since it retains a copy of entire blockchain at each node so that the overall storage consumption per block is O(n) with n participants. Yet another drawback is that this mechanism may limit the throughput in permissioned blockchain. Moreover, due to the existence of Byzantine nodes, existing partitioning methods, though widely adopted in distributed systems for decades, cannot suit for blockchain systems directly, so that it is critical to devise new storage mechanism for blockchain systems. This article proposes a novel storage engine, called BFT-Store, to enhance storage scalability by integrating erasure coding with Byzantine Fault Tolerance (BFT) consensus protocol. The first property of BFT-store is that the storage consumption per block can be reduced to O(1) for the first time, which enlarges overall storage capability when more nodes attend the blockchain. Second, we design an efficient online re-encoding protocol for storage scale-out and a hybrid replication scheme to enhance reading performance. Analysis in theory and extensive experimental results illustrate the scalability, availability and efficiency of BFT-Store via the implementation in an open-source permissioned blockchain Tendermint."
583,,Unsupervised Statistical Text Simplification.,"Jipeng Qiang,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2019.2947679,TKDE,2021,"Encyclopedias,Electronic publishing,Internet,Benchmark testing,Standards,Mathematical model","Most recent approaches for Text Simplification (TS) have drawn on insights from machine translation to learn simplification rewrites from the monolingual parallel corpus of complex and simple sentences, yet their effectiveness strongly relies on large amounts of parallel sentences. However, there has been a serious problem haunting TS for decades, that is, the availability of parallel TS corpora is scarce or not fit for the learning task. In this paper, we will focus on one especially useful and challenging problem of unsupervised TS without a single parallel sentence. To the best of our knowledge, we present the first unsupervised text simplification system based on phrase-based machine translation system, which leverages a careful initialization of phrase tables and language models. On the widely used WikiLarge and WikiSmall benchmarks, our system respectively obtains 39.08 and 25.12 SARI points, even outperforms some supervised baselines."
584,,Generalizing the Pigeonhole Principle for Similarity Search in Hamming Space.,"Jianbin Qin,Chuan Xiao 0001,Yaoshu Wang,Wei Wang 0011,Xuemin Lin 0001,Yoshiharu Ishikawa,Guoren Wang",https://doi.org/10.1109/TKDE.2019.2899597,TKDE,2021,"Hamming distance,Query processing,Search problems,Partitioning algorithms,Resource management,Indexes","A distance search in Hamming space finds binary vectors whose Hamming distances are no more than a threshold from a query vector. It is a fundamental problem in many applications, such as image retrieval, near-duplicate Web page detection, and scientific databases. State-of-the-art approaches to Hamming distance search are mainly based on the pigeonhole principle to generate a set of candidates and then verify them. We observe that the constraint by the pigeonhole principle is not always tight and may bring about unnecessary candidates. We also observe that the distribution in real data is often skewed, but most existing solutions adopt a simple equi-width partitioning and allocate the same threshold to all the parts, hence failing to exploit the data skewness to optimize query processing. In this paper, we propose a new form of the pigeonhole principle which allows variable partitioning and threshold allocation. Based on the new principle, we develop a tight constraint of candidates and devise cost-aware methods for partitioning and threshold allocation to optimize query processing. In addition, we extend our methods to answer Hamming distance join queries. We also discuss the application of the pigeonhole principle in set similarity search, a problem that can be converted to Hamming distance search equivalently. Our evaluation on datasets with various data distributions shows the robustness of our solution and its superior query processing performance to the state-of-the-art methods."
585,,Imputation-Based Ensemble Techniques for Class Imbalance Learning.,"Roozbeh Razavi-Far,Maryam Farajzadeh-Zanjani,Boyu Wang,Mehrdad Saif,Shiladitya Chakrabarti",https://doi.org/10.1109/TKDE.2019.2951556,TKDE,2021,"Boosting,Bagging,Training,Machine learning algorithms,Measurement,Standards,Sampling methods","Correct classification of rare samples is a vital data mining task and of paramount importance in many research domains. This article mainly focuses on the development of the novel class-imbalance learning techniques, which make use of oversampling methods integrated with bagging and boosting ensembles. Two novel oversampling strategies based on the single and the multiple imputation methods are proposed. The proposed techniques aim to create useful synthetic minority class samples, similar to the original minority class samples, by estimation of missing values that are already induced in the minority class samples. The re-balanced datasets are then used to train base-learners of the ensemble algorithms. In addition, the proposed techniques are compared with the commonly used class imbalance learning methods in terms of three performance metrics including AUC, F-measure, and G-mean over several synthetic binary class datasets. The empirical results show that the proposed multiple imputation-based oversampling combined with bagging significantly outperforms other competitors."
586,,ActiveIter - Meta Diagram Based Active Learning in Social Networks Alignment.,"Yuxiang Ren,Charu C. Aggarwal,Jiawei Zhang 0001",https://doi.org/10.1109/TKDE.2019.2947908,TKDE,2021,"Buildings,Twitter,Task analysis,Training data,Training,Feature extraction","Network alignment aims at inferring a set of anchor links matching the shared entities between different information networks, which has become a prerequisite step for effective fusion of multiple information networks. In this paper, we will study the network alignment problem to fuse online social networks specifically. Social network alignment is extremely challenging to address due to several reasons, i.e., 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">lack of training data</i>
, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">network heterogeneity</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">one-to-one constraint</i>
. Existing network alignment works usually require a large number of training instances, but such a demand can hardly be met in applications, as manual anchor link labeling is extremely expensive. Significantly different from other homogeneous network alignment works, information in online social networks is usually of heterogeneous categories, the incorporation of which in model building is not an easy task. Furthermore, the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">one-to-one</i>
 cardinality constraint on anchor links renders their inference process intertwistingly correlated. To resolve these three challenges, a novel network alignment model, namely 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ActiveIter</i>
 (Active Iterative Alignment), is introduced in this paper. The model 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ActiveIter</i>
 defines a set of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">inter-network meta diagrams</i>
 for anchor link feature extraction, adopts 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">active learning</i>
 for effective label query and uses 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">greedy link selection</i>
 for anchor link cardinality filtering. Extensive experiments were performed on a real-world aligned networks dataset, and the experimental results have demonstrated the effectiveness of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ActiveIter</i>
 compared with other state-of-the-art baseline methods."
587,,A New Pattern Representation Method for Time-Series Data.,"Roonak Rezvani,Payam M. Barnaghi,Shirin Enshaeifar",https://doi.org/10.1109/TKDE.2019.2961097,TKDE,2021,"Discrete Fourier transforms,Euclidean distance,Weight measurement,Aggregates,Data models,Internet of Things","The rapid growth of Internet of Things (IoT) and sensing technologies has led to an increasing interest in time-series data analysis. In many domains, detecting patterns of IoT data and interpreting these patterns are challenging issues. There are several methods in time-series analysis that deal with issues such as volume and velocity of IoT data streams. However, analysing the content of the data streams and extracting insights from dynamic IoT data is still a challenging task. In this paper, we propose a pattern representation method which represents time-series frames as vectors by first applying Piecewise Aggregate Approximation (PAA) and then applying Lagrangian Multipliers. This method allows representing continuous data as a series of patterns that can be used and processed by various higher-level methods. We introduce a new change point detection method which uses the constructed patterns in its analysis. We evaluate and compare our representation method with Blocks of Eigenvalues Algorithm (BEATS) and Symbolic Aggregate approXimation (SAX) methods to cluster various datasets. We have evaluated our algorithm using UCR time-series datasets and also a healthcare dataset. The evaluation results show significant improvements in analysing time-series data in our proposed method."
588,,A Survey on Data Collection for Machine Learning - A Big Data - AI Integration Perspective.,"Yuji Roh,Geon Heo,Steven Euijong Whang",https://doi.org/10.1109/TKDE.2019.2946162,TKDE,2021,"Machine learning,Data collection,Labeling,Data models,Data acquisition,Training data,Smart manufacturing","Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research."
589,,Security and Privacy Implications on Database Systems in Big Data Era - A Survey.,"G. Dumindu Samaraweera,J. Morris Chang",https://doi.org/10.1109/TKDE.2019.2929794,TKDE,2021,"Data privacy,Solid modeling,Big Data applications,Data models,Database systems,Security,Standards","For over many decades, relational database model has been considered as the leading model for data storage and management. However, as the Big Data explosion has generated a large volume of data, alternative models like NoSQL and NewSQL have emerged. With the advancement of communication technology, these database systems have given the potential to change the existing architecture from centralized mechanism to distributed in nature, to deploy as cloud-based solutions. Though all of these evolving technologies mostly focus on performance guarantees, it is still being a major concern how these systems can ensure the security and privacy of the information they handle. Different datastores support different types of integrated security mechanisms, however, most of the non-relational database systems have overlooked the security requirements of modern Big Data applications. This paper reviews security implementations in today's leading database models giving more emphasis on security and privacy attributes. A set of standard security mechanisms have been identified and evaluated based on different security classifications. Further, it provides a thorough review and a comprehensive analysis on maturity of security and privacy implementations in these database models along with future directions/enhancements so that data owners can decide on most appropriate datastore for their data-driven Big Data applications."
590,,A Semi-Automatic Design Methodology for (Big) Data Warehouse Transforming Facts into Dimensions.,"Lucile Sautot,Sandro Bimonte,Ludovic Journaux",https://doi.org/10.1109/TKDE.2019.2925621,TKDE,2021,"Data models,Design methodology,Benchmark testing,Analytical models,Data warehouses,Standards,Biological system modeling","A decision support system is used by decision makers for a long time. But, in some cases, the originally designed multidimensional schema does not cover the entire needs of decision makers, which can change over time. One such unfulfilled need, is using facts to describe dimension members. In this article, we propose a methodology to transform the constellation schema of a data warehouse by integrating factual data into a dimension. The proposed methodology and algorithms enrich a constellation multidimensional schema with new analytical possibilities for decision makers. This enrichment has repercussions for the entire multidimensional schema that are managed by multidimensional modeling, hierarchy calculation and the hierarchy version. In this article, we present a theoretical view of the proposed methodology supported by a case study, an implemented prototype and a complete evaluation based on a standard benchmark."
591,,Efficient Distributed k-Clique Mining for Large Networks Using MapReduce.,"Saeed Shahrivari,Saeed Jalili",https://doi.org/10.1109/TKDE.2019.2936027,TKDE,2021,"Cloud computing,Data mining,Memory management,Task analysis,Social networking (online),Bioinformatics,Multicore processing","Mining cliques of a network is an important problem that has many applications in different fields like social networks, bioinformatics, and web analysis. In most applications, mining fixed sized cliques, known as k-cliques, is enough. However, mining cliques of a large network is very challenging using current solutions, and it takes a considerable time using a commodity machine. Also, very large networks cannot be efficiently loaded into memory of a single machine. To overcome these limitations, we have proposed a solution named KCminer, which is based on state space search and can be totally fitted into the MapReduce framework. Using the MapReduce framework, it is possible to run KCminer on cloud computing platforms and hence, process very large networks in feasible time. Our experiments which were performed on a cloud computing platform with 100 machines show that KCminer is both fast and scalable. Besides the MapReduce framework, KCminer executes efficiently on parallel shared memory systems. We performed some experiments on a commodity multicore desktop and showed that KCminer can effectively use the power of all cores. The experimental results show that even using a single thread, KCminer is much faster than available serial tools like MACE."
592,,Sys-TM - A Fast and General Topic Modeling System.,"Yingxia Shao,Xupeng Li,Yiru Chen,Lele Yu,Bin Cui 0001",https://doi.org/10.1109/TKDE.2019.2956518,TKDE,2021,"Inference algorithms,Data models,Probabilistic logic,Cultural differences,Computational modeling,Complexity theory,Resource management","Topic models, such as LDA and its variants, are popular probabilistic models for discovering the abstract “topics” that occur in a collection of documents. However, the performance of topic models may vary a lot for different workloads, and it is not a trivial task to achieve a well-optimized implementation. In this paper, we systematically study all recently proposed samplers over LDA: AliasLDA, F+LDA, LightLDA, and WarpLDA, and discover a novel system tradeoff by considering the diversity and skewness of workloads. Then, we propose a hybrid sampler which can cleverly choose an efficient sampler with the tradeoff, and apply the hybrid sampler to LDA and its variants, including STM, TOT and CTM. Finally, we build a fast and general topic modeling system Sys-TM, which provides a unified topic modeling framework by integrating the hybrid sampler. Based on our empirical studies, the hybrid sampler outperforms the state-of-the-art samplers by up to 
<inline-formula><tex-math notation=""LaTeX"">$2\times$</tex-math> </inline-formula>
 over various topic models, and with carefully engineered implementation, Sys-TM is able to outperform the existing systems by up to 
<inline-formula><tex-math notation=""LaTeX"">$10\times$</tex-math> </inline-formula>
."
593,,Deep Variational Matrix Factorization with Knowledge Embedding for Recommendation System.,"Xiaoxuan Shen,Baolin Yi,Hai Liu 0004,Wei Zhang,Zhaoli Zhang,Sannyuya Liu,Naixue Xiong",https://doi.org/10.1109/TKDE.2019.2952849,TKDE,2021,"Neural networks,Deep learning,Bayes methods,Knowledge engineering,Predictive models,Probability density function,Collaboration","Automatic recommendation has become an increasingly relevant problem to industries, which allows users to discover new items that match their tastes and enables the system to target items to the right users. In this article, we have proposed a deep learning based fully Bayesian treatment recommendation framework, DVMF, which has high-quality performance and ability to integrate any kinds of side information handily and efficiently. In DVMF, the variational inference technique and the reparameterization tricks are introduced to make DVMF possible to be optimized by the stochastic gradient-based methods, in addition, two novel deep neural networks have been constructed to infer the hyper-parameters of the distributions of latent factors from the knowledge of user and item, which are represented as low-dimensional real-valued vectors retaining primary features. Experimental results on five public databases indicate that the proposed method performs better than the state-of-the-art recommendation algorithms on prediction accuracy in terms of quantitative assessments."
594,,Deep Collaborative Filtering with Multi-Aspect Information in Heterogeneous Networks.,"Chuan Shi,Xiaotian Han,Li Song,Xiao Wang 0017,Senzhang Wang,Junping Du,Philip S. Yu",https://doi.org/10.1109/TKDE.2019.2941938,TKDE,2021,"Neural networks,Feature extraction,Fuses,Recommender systems,Collaboration,Data mining","Recently, recommender systems play a pivotal role in alleviating the problem of information overload. Latent factor models have been widely used for recommendation. Most existing latent factor models mainly utilize the interaction information between users and items, although some recently extended models utilize some auxiliary information to learn a unified latent factor for users and items. The unified latent factor only represents the characteristics of users and the properties of items from the aspect of purchase history. However, the characteristics of users and the properties of items may stem from different aspects, e.g., the brand-aspect and category-aspect of items. Moreover, the latent factor models usually use the shallow projection, which cannot capture the characteristics of users and items well. Deep neural network has shown tremendous potential to model the non-linearity relationship between users and items. It can be used to replace shallow projection to model the complex correlation between users and items. In this paper, we propose a Neural network based Aspect-level Collaborative Filtering model (NeuACF) to exploit different aspect latent factors. Through modelling the rich object properties and relations in recommender system as a heterogeneous information network, NeuACF first extracts different aspect-level similarity matrices of users and items, respectively, through different meta-paths, and then feeds an elaborately designed deep neural network with these matrices to learn aspect-level latent factors. Finally, the aspect-level latent factors are fused for the top-N recommendation. Moreover, to fuse information from different aspects more effectively, we further propose NeuACF++ to fuse aspect-level latent factors with self-attention mechanism. Extensive experiments on three real world datasets show that NeuACF and NeuACF++ significantly outperform both existing latent factor models and recent neural network models."
595,,Semantics-Aware Hidden Markov Model for Human Mobility.,"Hongzhi Shi,Yong Li 0008,Hancheng Cao,Xiangxin Zhou,Chao Zhang 0014,Vassilis Kostakos",https://doi.org/10.1109/TKDE.2019.2937296,TKDE,2021,"Hidden Markov models,Semantics,Trajectory,Data models,Predictive models,Training,Urban areas","Understanding human mobility benefits numerous applications such as urban planning, traffic control, and city management. Previous work mainly focuses on modeling spatial and temporal patterns of human mobility. However, the semantics of trajectory are ignored, thus failing to model people's motivation behind mobility. In this paper, we propose a novel semantics-aware mobility model that captures human mobility motivation using large-scale semantic-rich spatial-temporal data from location-based social networks. In our system, we first develop a multimodal embedding method to project user, location, time, and activity on the same embedding space in an unsupervised way while preserving original trajectory semantics. Then, we use hidden Markov model to learn latent states and transitions between them in the embedding space, which is the location embedding vector, to jointly consider spatial, temporal, and user motivations. In order to tackle the sparsity of individual mobility data, we further propose a von Mises-Fisher mixture clustering for user grouping so as to learn a reliable and fine-grained model for groups of users sharing mobility similarity. We evaluate our proposed method on two large-scale real-world datasets, where we validate the ability of our method to produce high-quality mobility models. We also conduct extensive experiments on the specific task of location prediction. The results show that our model outperforms state-of-the-art mobility models with higher prediction accuracy and much higher efficiency."
596,,A Review on Dimensionality Reduction for Multi-Label Classification.,"Wissam Siblini,Pascale Kuntz,Frank Meyer",https://doi.org/10.1109/TKDE.2019.2940014,TKDE,2021,"Dimensionality reduction,Prediction algorithms,Linear programming,Feature extraction,Recommender systems,Motion pictures,Noise measurement","Multi-label classification has gained in importance in the last decade and it is today confronted to the current needs to process massive raw data from heterogeneous sources. Therefore, dimensionality reduction, which aims at reducing the number of features, labels, or both, knows a renewed interest to enhance the scaling properties of the classifiers and their predictive performances. In this paper we review more than fifty papers presenting dimensionality reduction approaches for multi-label classification and we propose an analysis in three steps : (i) a typology of the methods describing the main components of their strategies, the problem they tackle and the way they solve it (ii) a unified formalization of the problems to help to distinguish the similarities and differences between the approaches, and (iii) a meta-analysis of the published experimental results inspired by the consensus theory to identify the most efficient algorithms."
597,,A Novel Biometric Inspired Robust Security Framework for Medical Images.,"Satendra Pal Singh,Gaurav Bhatnagar",https://doi.org/10.1109/TKDE.2019.2935710,TKDE,2021,"Medical diagnostic imaging,Encryption,Fingerprint recognition,Matrix decomposition","The protection of sensitive and confidential data become a challenging task in the present scenario as more and more digital data is stored and transmitted between the end users. The privacy is vitally necessary in case of medical data, which contains the important information of the patients. In this article, a novel biometric inspired medical encryption technique is proposed based on newly introduced parameterized all phase orthogonal transformation (PR-APBST), singular value, and QR decomposition. The proposed technique utilizes the biometrics of the patient/owner to generate a key management system to obtain the parameters involved in the proposed technique. The medical image is then encrypted employing PR-APBST, QR and singular value decomposition and is ready for secure transmission or storage. Finally, a reliable decryption process is employed to reconstruct the original medical image from the encrypted image. The validity and feasibility of the proposed framework have been demonstrated using an extensive experiments on various medical images and security analysis."
598,,Self-Healing Event Logs.,"Wei Song 0003,Hans-Arno Jacobsen,Pengcheng Zhang",https://doi.org/10.1109/TKDE.2019.2956520,TKDE,2021,"Maintenance engineering,Petri nets,Knowledge engineering,Business intelligence,Performance analysis,Tools,Electronic mail","Event logs of process-aware information systems play an increasingly critical role in today's enterprises because they are the basis for a number of business intelligence applications such as complex event processing, provenance analysis, performance analysis, and process mining. However, due to incorrect manual recording, system errors, and resource constraints, event logs inevitably contain noise in the form of deviating event sequences with redundant, missing, or dislocated events. To repair event logs, existing approaches rely on predefined process models to obtain a minimum recovery for each deviating event sequence. However, process models are typically unavailable in practice, rendering existing approaches inapplicable. In this scenario, can event logs be self-healing? To address this problem, we propose an approach that leverages compliant event sequences to repair deviating sequences. Our approach is effective if the compliant event sequences contain sufficient knowledge for repair. We implement our approach in a prototype and employ the tool to conduct experiments. The experimental results demonstrate that our approach can achieve efficient repairs without the help of process models."
599,,CED - Credible Early Detection of Social Media Rumors.,"Changhe Song,Cheng Yang 0002,Huimin Chen,Cunchao Tu,Zhiyuan Liu 0001,Maosong Sun",https://doi.org/10.1109/TKDE.2019.2961675,TKDE,2021,"Social networking (online),Feature extraction,Predictive models,Reliability,Recurrent neural networks,Convolutional neural networks","Rumors spread dramatically fast through online social media services, and people are exploring methods to detect rumors automatically. Existing methods typically learn semantic representations of all reposts to a rumor candidate for prediction. However, it is crucial to efficiently detect rumors as early as possible before they cause severe social disruption, which has not been well addressed by previous works. In this paper, we present a novel early rumor detection model, Credible Early Detection (CED). By regarding all reposts to a rumor candidate as a sequence, the proposed model will seek an early point-in-time for making a credible prediction. We conduct experiments on three real-world datasets, and the results demonstrate that our proposed model can remarkably reduce the time span for prediction by more than 85 percent, with better accuracy performance than all state-of-the-art baselines."
600,,Incremental Graph Pattern Based Node Matching with Multiple Updates.,"Guohao Sun,Guanfeng Liu 0001,Yan Wang 0002,Mehmet A. Orgun,Quan Z. Sheng,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2942294,TKDE,2021,"Pattern matching,Indexes,Facebook,Sun,Fans,Computational modeling","Graph Pattern based Node Matching (GPNM) has been proposed to find all the matches of the nodes in a data graph GD based on a given pattern graph GP. GPNM has been increasingly adopted in many applications such as group finding and expert recommendation, in which data graphs are frequently updated overtime. Moreover, many typical pattern graphs frequently and repeatedly appear in users' queries in a short period of time, e.g., social graph searches on Facebook. To deliver a GPNM result in such applications, the existing GPNM methods have to perform an incremental GPNM procedure for each of the updates in the data graph, which is computationally expensive. To address this problem, in this paper, we first analyze the elimination relationships between multiple updates in GD and the hierarchical structure between these elimination relationships. Then, we generate an Elimination Hierarchy Tree (EH-Tree) to index the elimination relationships and propose an EH-Tree based GPNM method, called EHGPNM, considering the elimination relationships between multiple updates in GD. EH-GPNM first delivers the GPNM result of an initial query, and then delivers the GPNM result of a subsequent query, based on the initial GPNM result and the multiple updates of GD that occur between those two queries. The experimental results on five real-world social graphs demonstrate that our proposed EH-GPNM is much more efficient than the state-of-the-art GPNM methods."
601,,Detecting Disjoint Communities in a Social Network Based on the Degrees of Association Between Edges and Influential Nodes.,Kamal Taha,https://doi.org/10.1109/TKDE.2019.2940189,TKDE,2021,"Linear programming,Image edge detection,Network topology,Topology,Social networking (online),Buildings,Optimization","Detecting communities is crucial to understanding the dynamics of their members. However, the detection of “good” communities is deemed demonstrably problematic, which is mainly due to the following two factors. First, real-world networks are complex and require optimizing multi-objective functions for capturing their community structures, whereas most current approaches optimize only one or two objective functions. Second, most current approaches detect communities in respect of the independence of how closely associated their connections are based on the global relative influences of the edges connecting them. To overcome these limitations, a clustering method needs to optimize multi-objective functions and employ global preprocessing techniques that consider the topology of the entire network. We, therefore, proposed a system called DAVE, which optimizes four objective functions that capture the community structures in most real-word network settings, and detects communities with regards of how closely associated their connections are based on the relative influences of the edges connecting them. We proposed novel formulas that capture these functions. Our method is the first to utilize the prediction of node-node associations based on global node-edge degrees of association. After ranking nodes based on their global relative influences on the network, some of the top-ranked ones will serve as core seeds for constructing communities. Then, the degrees of association between influential edges and seed nodes are computed. DAVE assigns a node to a community, only if each edge in the shortest path from this node to the community's core seed node is both influential and has significant degree of association with the core node. We evaluated DAVE by comparing it empirically and experimentally with 16 methods. Results showed a remarkable improvement."
602,,Neural Attention Frameworks for Explainable Recommendation.,"Omer Tal,Yang Liu 0008,Jimmy Huang,Xiaohui Yu 0001,Bushra Aljbawi",https://doi.org/10.1109/TKDE.2019.2953157,TKDE,2021,"Recommender systems,Motion pictures,History,Neural networks,Cognition,Deep learning","Neural attention, an emerging technique used to identify important inputs within neural networks, have become increasingly popular in the area of recommender systems. Not only allowing to better identify what defines users and items, attention-based recommender systems are further able to provide accompanying explanations. However, these representations usually capture only part of users’ preferences and items’ attributes, resulting in limited reasoning and accuracy. We therefore propose Dual Attention Recommender with Items and Attributes (DARIA), a novel approach able to combine two dependable neural attention mechanisms to better justify its suggestions. Utilizing the personalized history of users, DARIA identifies the most relevant past activities while considering the real-world features that contributed to the similarity. In addition, we adopt the novel approach of self-attention and introduce Self-Attention Recommender based on Attributes and History (SARAH). As a variation to DARIA, SARAH utilizes two self-attention components to describe users by their most characteristic past activities and items by their best depicting attributes. Various experiments establish the significant improvement of SARAH and DARIA over seven key baselines in diverse recommendation settings. By comparing our two proposed frameworks, we demonstrate the potential benefit of applying self-attention in different scenarios."
603,,Learning Sparse PCA with Stabilized ADMM Method on Stiefel Manifold.,"Mingkui Tan,Zhibin Hu,Yuguang Yan,Jiezhang Cao,Dong Gong,Qingyao Wu",https://doi.org/10.1109/TKDE.2019.2935449,TKDE,2021,"Loading,Principal component analysis,Manifolds,Convex functions,Feature extraction,Covariance matrices,Optimization","Sparse principal component analysis (SPCA) produces principal components with sparse loadings, which is very important for handling data with many irrelevant features and also critical to interpret the results. To deal with orthogonal constraints, most previous approaches address SPCA with several components using techniques such as deflation technique and convex relaxations. However, the deflation technique usually suffers from suboptimal solutions due to poor approximations. On the other hand, the convex relaxations are often computationally expensive. To address the above issues, in this paper, we propose to address SPCA over the Stiefel manifold directly, and develop a stabilized Alternating Direction Method of Multipliers (SADMM) to handle the nonconvex orthogonal constraints. Compared to traditional ADMM, the proposed SADMM method converges well with a wide range of parameters and obtains a better solution. We also theoretically study the convergence property of the proposed SADMM method. Furthermore, most existing methods ignore an inherent drawback of SPCA - the importance of different components is not considered when doing feature selection, which often makes the selected features nonoptimal. To address this, we further propose a two-stage method which considers the importance of different components to select the most important features. Empirical studies on both synthetic and real-world datasets show that the proposed algorithms achieve better performance compared to existing state-of-the-art methods."
604,,Tensor Multi-Elastic Kernel Self-Paced Learning for Time Series Clustering.,"Yongqiang Tang,Yuan Xie 0006,Xuebing Yang,Jinghao Niu,Wensheng Zhang 0002",https://doi.org/10.1109/TKDE.2019.2937027,TKDE,2021,"Kernel,Time series analysis,Time measurement,Clustering algorithms,Optimization,Task analysis","Time series clustering has attracted growing attention due to the abundant data accessible and extensive value in various applications. The unique characteristics of time series, including high-dimension, warping, and the integration of multiple elastic measures, pose challenges for the present clustering algorithms, most of which take into account only part of these difficulties. In this paper, we make an effort to simultaneously address all aforementioned issues in time series clustering under a unified multiple kernels clustering (MKC) framework. Specifically, we first implicitly map the raw time series space into multiple kernel spaces via elastic distance measure functions. In such high-dimensional spaces, we resort to the tensor constraint based self-representation subspace clustering approach, which involves the self-paced learning paradigm, to explore the essential low-dimensional structure of the data, as well as the high-order complementary information from different elastic kernels. The proposed approach can be extended to more challenging multivariate time series clustering scenario in a direct but elegant way. Extensive experiments on 85 univariate and 10 multivariate time series datasets demonstrate the significant superiority of the proposed approach beyond the baseline and several state-of-the-art MKC methods."
605,,Deep Cross-Modal Face Naming for People News Retrieval.,"Yong Tian,Lian Zhou,Yuejie Zhang,Tao Zhang 0022,Weiguo Fan",https://doi.org/10.1109/TKDE.2019.2948875,TKDE,2021,"Face,Correlation,Semantics,Visualization,Feature extraction,Data mining,Videos","How to integrate multimodal information sources for face naming in multimodal news is a hot and yet challenging problem. A novel deep cross-modal face naming scheme is developed in this paper to facilitate more effective people news retrieval for large-scale multimodal news. This scheme integrates deep multimodal analysis, cross-modal correlation learning, and multimodal information mining, in which the efficient naming mechanism aims to cluster the deep features of different modalities into a common space to explore their inter-related correlations, and a special Web mining pattern is designed to optimize the name-face matching for rare non-celebrity. Such a cross-modal face naming model can be treated as a problem of bi-media semantic mapping and modeled as an inter-related correlation distribution over deep representations of multimodal news, in which the most important is to create more effective cross-modal name-face correlation and measure to what degree they are correlated. The experiments on a large number of public data from 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Yahoo! News</i>
 have obtained very positive results and demonstrated the effectiveness of the proposed model."
606,,Generating Actionable Interpretations from Ensembles of Decision Trees.,"Gabriele Tolomei,Fabrizio Silvestri",https://doi.org/10.1109/TKDE.2019.2945326,TKDE,2021,"Predictive models,Decision trees,Medical services,Handwriting recognition,Task analysis,Advertising,Switches","Machine-learned models are often perceived as “black boxes”: they are given inputs and hopefully produce desired outputs. There are many circumstances, however, where human-interpretability is crucial to understand (i) why a model outputs a certain prediction on a given instance, (ii) which adjustable features of that instance should be modified, and finally (iii) how to alter a prediction when the mutated instance is input back to the model. In this paper, we present a technique that exploits the feedback loop originated from the internals of any ensemble of decision trees to offer recommendations for transforming a k-labelled predicted instance into a k'-labelled one (for any possible pair of class labels k, k'). Our proposed algorithm perturbs individual feature values of an instance, so as to change the original prediction output by the ensemble on the so-transformed instance. This is also achieved under two constraints: the cost- and tolerance of transformation. Finally, we evaluate our approach on four distinct application domains: online advertising, healthcare, spam filtering, and handwritten digit recognition. Experiments confirm that our solution is able to suggest changes to feature values that help interpreting the rationale of model predictions, making it indeed useful in practice especially if implemented efficiently."
607,,Two-Sided Online Micro-Task Assignment in Spatial Crowdsourcing.,"Yongxin Tong,Yuxiang Zeng,Bolin Ding,Libin Wang,Lei Chen 0002",https://doi.org/10.1109/TKDE.2019.2948863,TKDE,2021,"Task analysis,Crowdsourcing,Resource management,Analytical models,Real-time systems,Greedy algorithms,Spatiotemporal phenomena","With the rapid development of smartphones, spatial crowdsourcing platforms are getting popular. A foundational research of spatial crowdsourcing is to allocate micro-tasks to suitable crowd workers. Many existing studies focus on the offline scenario, where all the spatiotemporal information of micro-tasks and crowd workers is given. In this paper, we focus on the online scenario and identify a more practical micro-task allocation problem, called the 
<i><u>G</u>lobal <u>O</u>nline <u>M</u>icro-task <u>A</u>llocation in spatial crowdsourcing</i>
 (GOMA) problem. We first extend the state-of-the-art algorithm for the online maximum weighted bipartite matching problem to the GOMA problem as the baseline algorithm. Although the baseline algorithm provides a theoretical guarantee for the worst case, its average performance in practice is not good enough since the worst case happens with a very low probability in the real world. Thus, we consider the average performance of online algorithms, 
<i>a.k.a.</i>
 random order model. We propose a two-phase-based framework, based on which we present the TGOA algorithm with a 
<inline-formula><tex-math notation=""LaTeX"">$\frac{1}{4}$</tex-math></inline-formula>
-competitive ratio under the random order model. To improve its efficiency, we further design the TGOA-Greedy and TGOA-OP algorithm following this framework, which runs faster than the TGOA algorithm with a competitive ratio of 
<inline-formula><tex-math notation=""LaTeX"">$\frac{1}{8}$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$\frac{1}{4}$</tex-math></inline-formula>
, respectively. We also revisit the average performance of Greedy, which has long been considered as the worst due to its unbounded competitive ratio in the worst case. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on synthetic and real datasets."
608,,Quality-Assured Synchronized Task Assignment in Crowdsourcing.,"Jiayang Tu,Peng Cheng 0003,Lei Chen 0002",https://doi.org/10.1109/TKDE.2019.2935443,TKDE,2021,"Task analysis,Synchronization,Crowdsourcing,Parameter estimation,Optimization,Approximation algorithms","With the rapid development of crowdsourcing platforms that aggregate the intelligence of Internet workers, crowdsourcing has been widely utilized to address problems that require human cognitive abilities. Considering great dynamics of worker arrival and departure, it is of vital importance to design a task assignment scheme to adaptively select the most beneficial tasks for the available workers. In this paper, in order to make the most efficient utilization of the worker labor and balance the accuracy of answers and the overall latency, we a) develop a parameter estimation model that assists in estimating worker expertise, question easiness, and answer confidence; b) propose a quality-assured synchronized task assignment scheme that executes in batches and maximizes the number of potentially completed questions (MCQ) within each batch. We prove that MCQ problem is NP-hard and present two greedy approximation solutions to address the problem. The effectiveness and efficiency of the approximation solutions are further evaluated through extensive experiments on synthetic and real datasets. The experimental results show that the accuracy and the overall latency of the MCQ approaches outperform the existing online task assignment algorithms in the synchronized task assignment scenario."
609,,Statistically Robust Evaluation of Stream-Based Recommender Systems.,"João Vinagre,Alípio Mário Jorge,Conceição Rocha,João Gama 0001",https://doi.org/10.1109/TKDE.2019.2960216,TKDE,2021,"Data models,Recommender systems,Testing,Protocols,Statistical analysis,Real-time systems,Proposals","Online incremental models for recommendation are nowadays pervasive in both the industry and the academia. However, there is not yet a standard evaluation methodology for the algorithms that maintain such models. Moreover, online evaluation methodologies available in the literature generally fall short on the statistical validation of results, since this validation is not trivially applicable to stream-based algorithms. We propose a 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-fold validation framework for the pairwise comparison of recommendation algorithms that learn from user feedback streams, using prequential evaluation. Our proposal enables continuous statistical testing on adaptive-size sliding windows over the outcome of the prequential process, allowing practitioners and researchers to make decisions in real time based on solid statistical evidence. We present a set of experiments to gain insights on the sensitivity and robustness of two statistical tests—McNemar’s and Wilcoxon signed rank—in a streaming data environment. Our results show that besides allowing a real-time, fine-grained online assessment, the online versions of the statistical tests are at least as robust as the batch versions, and definitely more robust than a simple prequential single-fold approach."
610,,Intervening Coupling Diffusion of Competitive Information in Online Social Networks.,"Pengfei Wan,Xiaoming Wang 0001,Xinyan Wang,Liang Wang 0014,Yaguang Lin,Wei Zhao 0001",https://doi.org/10.1109/TKDE.2019.2954901,TKDE,2021,"Couplings,Social networking (online),Optimal control,Computational modeling,Integrated circuit modeling,Biological system modeling,Investment","The vigorously rising of social media brings a new opportunity for information diffusion in online social networks. However, the existing models of information diffusion only consider the single information, such as rumor. What's more, most of intervention frameworks are modeled under the ideal circumstances without reality constraints. In this article, we propose a novel model of competitive information coupling diffusion to describe the complex process of information diffusion in online social networks. Especially, in order to intervene the process of competitive information coupling diffusion, we introduce three intervention strategies and propose an intervention framework. More importantly, we take the dynamic constraints into consideration such as the budget of intervention and current state of the system, and further propose the constrained intervention model. To reduce the system loss, we establish an optimal control problem with constraints to achieve the optimal allocation of intervention strategies over time and minimize the total loss. We theoretically prove the existence and uniqueness of the optimal solution of the problem, and derive the optimal control solution. Through the experiments, we verify the effectiveness of the model and analyze the efficiency of different intervention strategies about competitive information coupling diffusion with or without constraints, respectively. The results show that the collaborative intervention strategies can effectively impact the process of diffusion and get the minimum system loss. This article provides high realistic significance to the commercial marketing in online social networks."
611,,Joint Inference for Aspect-Level Sentiment Analysis by Deep Neural Networks and Linguistic Hints.,"Yanyan Wang,Qun Chen 0001,Murtadha H. M. Ahmed,Zhanhuai Li,Wei Pan 0007,Hailong Liu",https://doi.org/10.1109/TKDE.2019.2947587,TKDE,2021,"Sentiment analysis,Linguistics,Neural networks,Task analysis,Markov processes,Cognition,Analytical models","The state-of-the-art techniques for aspect-level sentiment analysis focused on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their performance may still fall short of expectation in real scenarios due to the semantic complexity of natural languages. Motivated by the observation that many linguistic hints (e.g., sentiment words and shift words) are reliable polarity indicators, we propose a joint framework, SenHint, which can seamlessly integrate the output of deep neural networks and the implications of linguistic hints in a unified model based on Markov logic network (MLN). SenHint leverages the linguistic hints for multiple purposes: (1) to identify the easy instances, whose polarities can be automatically determined by the machine with high accuracy; (2) to capture the influence of sentiment words on aspect polarities; (2) to capture the implicit relations between aspect polarities. We present the required techniques for extracting linguistic hints, encoding their implications as well as the output of DNN into the unified model, and joint inference. Finally, we have empirically evaluated the performance of SenHint on both English and Chinese benchmark datasets. Our extensive experiments have shown that compared to the state-of-the-art DNN techniques, SenHint can effectively improve polarity detection accuracy by considerable margins."
612,,Incorporating Distribution Matching into Uncertainty for Multiple Kernel Active Learning.,"Zengmao Wang,Bo Du 0001,Weiping Tu,Lefei Zhang,Dacheng Tao",https://doi.org/10.1109/TKDE.2019.2923211,TKDE,2021,"Kernel,Uncertainty,Data models,Task analysis,Redundancy,Data structures,Computational modeling","Due to the lack of the labeled data and the complex structures of various data, it is very hard to learn the uncertainty and representativeness accurately in active learning. In this paper, we propose a multiple kernel active learning framework that incorporates a group regularizer of distribution information into the estimation of uncertainty. The proposed method takes the advantage of multiple kernel learning to learn the kernel space in which the complex structures can be well captured by kernel weights. Meanwhile, we have developed an efficient optimization algorithm to solve the proposed method. Experimental results on twelve UCI benchmark data sets and eight subsets of ImageNet show that the proposed method outperforms several state-of-the-art active learning methods. Moreover, we also have applied the proposed method to multiple feature scenario on Caltech101, and the promising results are also obtained compared with single feature scenario."
613,,Efficient Similarity Search for Sets over Graphs.,"Yue Wang 0012,Zonghao Feng,Lei Chen 0002,Zijian Li 0002,Xun Jian,Qiong Luo 0001",https://doi.org/10.1109/TKDE.2019.2931901,TKDE,2021,"Estimation,Indexes,Linear systems,Task analysis,Current measurement,Artificial intelligence,Social networking (online)","Measuring similarities among different nodes is important in graph analysis tasks, such as link prediction, and recommendation. Among different similarity measures, SimRank is one of the most popular and promising ones, and has received a lot of research attention. While most current studies focus on single-pair, single-source/top-k, and all-pairs SimRank computation, few of them have studied finding similar pairs given a set of node pairs, which has attractive applications in personalized search and recommendation tasks. In this paper, we present Carmo, an efficient algorithm for retrieving the top-k similarities from an arbitrary set of pairs. In addition, we introduce two types of indexes to boost the efficiency of Carmo: one is hub-based, the other is tree-based. We show the effectiveness and efficiency of our proposed methods by extensive experiments."
614,,HGraph - I/O-Efficient Distributed and Iterative Graph Computing by Hybrid Pushing/Pulling.,"Zhigang Wang,Yu Gu 0002,Yubin Bao,Ge Yu 0001,Jeffrey Xu Yu,Zhiqiang Wei",https://doi.org/10.1109/TKDE.2019.2951407,TKDE,2021,"Switches,Fault tolerance,Fault tolerant systems,Task analysis,Runtime,Prediction algorithms,Optimization","In the big data era, distributed computation is becoming a preferred solution for iterative graph analysis. However, graphs are rapidly growing in size and more importantly, there exist a lot of messages across iterations. For better scalability, many distributed systems keep graph data and message data on disk. Now these systems solely employ either pushing or pulling mode to manage data, but neither can always work well during the entire computation. This is mainly because I/O access patterns are dynamic and complex. This article proposes a hybrid solution. It achieves the optimal performance in different scenarios by dynamically and adaptively switching modes between pushing and pulling. Specifically, we first devise a new block-centric pulling technique. It pulls messages much more I/O-efficiently than the existing vertex-centric pulling mode. We then combine pushing and pulling. For general-purpose, we categorize graph algorithms and accordingly present two seamless switching frameworks. We also design performance prediction components specialized to the two frameworks, to decide how and when we can switch modes. Some optimization strategies are also given to further enhance performance, such as priority scheduling and lightweight fault-tolerance. Extensive experiments against state-of-the-art solutions confirm the effectiveness of our proposals."
615,,Query Expansion With Local Conceptual Word Embeddings in Microblog Retrieval.,"Yashen Wang,Heyan Huang,Chong Feng",https://doi.org/10.1109/TKDE.2019.2945764,TKDE,2021,"Task analysis,Vocabulary,Semantics,Real-time systems,Computational modeling,Twitter","Since the length of microblog texts, such as tweets, is strictly limited to 140 characters, traditional Information Retrieval techniques suffer from the vocabulary mismatch problem severely and cannot yield good performance in the context of microblogosphere. To address this critical challenge, in this paper, we focus on the use of local conceptual word embeddings for enhance microblog retrieval effectiveness. In particular, we propose a novel k-Nearest Neighbor (kNN) based Query Expansion (QE) algorithm to generate words from local word embeddings to expand the original query, which leads to better understanding of the information need. Besides, in order to further satisfy users' real-time information need, we incorporate temporal evidences into the expansion algorithm, which can boost recent tweets in the retrieval results with respect to a given topic. Experimental results on the official TREC Twitter corpora demonstrate the significant superiority of our approach over baseline methods."
616,,Open Relation Extraction for Chinese Noun Phrases.,"Chengyu Wang 0001,Xiaofeng He,Aoying Zhou",https://doi.org/10.1109/TKDE.2019.2953839,TKDE,2021,"Task analysis,Semantics,Magnetic heads,Encyclopedias,Electronic publishing,Internet","Relation Extraction (RE) aims at harvesting relational facts from texts. A majority of existing research targets at knowledge acquisition from sentences, where subject-verb-object structures are usually treated as the signals of existence of relations. In contrast, relational facts expressed within noun phrases are highly implicit. Previous works mostly relies on human-compiled assertions and textual patterns in English to address noun phrase-based RE. For Chinese, the corresponding task is non-trivial because Chinese is a highly analytic language with flexible expressions. Additionally, noun phrases tend to be incomplete in grammatical structures, where clear mentions of predicates are often missing. In this article, we present an unsupervised Noun Phrase-based Open RE system for the Chinese language (NPORE), which employs a three-layer data-driven architecture. The system contains three components, i.e., Modifier-sensitive Phrase Segmenter, Candidate Relation Generator and Missing Relation Predicate Detector. It integrates with a graph clique mining algorithm to chunk Chinese noun phrases, considering how relations are expressed. We further propose a probabilistic method with knowledge priors and a hypergraph-based random walk process to detect missing relation predicates. Experiments over Chinese Wikipedia show NPORE outperforms state-of-the-art, capable of extracting 55.2 percent more relations than the most competitive baseline, with a comparable precision at 95.4 percent."
617,,Spatiotemporal Representation Learning for Driving Behavior Analysis - A Joint Perspective of Peer and Temporal Dependencies.,"Pengyang Wang,Xiaolin Li,Yu Zheng 0004,Charu Aggarwal 0001,Yanjie Fu",https://doi.org/10.1109/TKDE.2019.2935203,TKDE,2021,"Global Positioning System,Vehicles,Optimization,Task analysis,Turning,Trajectory","Driving is a complex activity that requires multi-level skilled operations (e.g., acceleration, braking, and turning). Analyzing driving behaviors can help us assess driver performances, improve traffic safety, and, ultimately, promote the development of intelligent and resilient transportation systems. While some efforts have been made for analyzing driving behaviors, existing methods can be improved via representation learning by jointly exploring the peer and temporal dependencies of driving behaviors. To that end, in this paper, we develop a Peer and Temporal-Aware Representation Learning based framework (PTARL) for driving behavior analysis with GPS trajectory data. Specifically, we first detect the driving operations and states of each driver from their GPS traces. Then, we derive a sequence of multi-view driving state transition graphs from the driving state sequences, in order to characterize a driver's driving behaviors that vary over time. In addition, we develop a peer and temporal-aware representation learning method to learn a sequence of time-varying yet relational vectorized representations from the driving state transition graphs. The proposed method can simultaneously model both the graph-graph peer dependency and the current-past temporal dependency in a unified optimization framework. Also, we provide two effective solutions for the optimization problem: (i) a joint optimization solution of representation learning and prediction; and (ii) a step-by-step solution of representation learning and prediction. Besides, we explore two strategies to fuse the learned representations from multi-view transition graphs: (i) simple alignment and (ii) collective fusion. Moreover, we apply the developed framework to the two applications of quantitative transportation safety: (i) scoring of driving performances, and (ii) detection of dangerous regions. Finally, we present extensive experimental results with big trajectory data to demonstrate the enhanced performances of the proposed method for quantitative transportation safety."
618,,Understanding and Optimizing Conjunctive Predicates Under Memory-Efficient Storage Layouts.,"Ze-ke Wang,Xue Liu 0003,Kai Zhang 0006,Haihang Zhou,Bingsheng He",https://doi.org/10.1109/TKDE.2019.2958672,TKDE,2021,"Layout,Memory management,Optimization,Acceleration,Space exploration,Computer science,Dictionaries","Database queries can contain multiple predicates. The optimization of conjunctive predicates is still vital to the overall performance of analytic data processing tasks. Prior work proposes several memory-efficient storage layouts, e.g., BitWeaving and ByteSlice, to significantly accelerate predicate evaluation, as circuit-level intra-cycle parallelism available in modern CPUs can be exploited such that the total number of instructions can be dramatically reduced. However, the performance potential of conjunctive predicates has not been harvested yet under such storage layouts as there is no accurate cost model to provide necessary insights that guide the optimization process. In this paper, we propose a hybrid empirical/analytical cost model (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Understanding</i>
) to unveil the performance characteristics of such storage layouts when applying to predicate evaluation. Our cost model takes into account effect of non-linear factors, e.g., cache miss and branch misprediction, and easily applies to different CPUs. The main finding from our cost model is to distinguish high-cost instruction (which suffers from cache miss and/or branch misprediction) from low-cost instruction (which enjoys cache hit and correct branch prediction) in the context of predicate evaluation under these storage layouts. Guided by such a finding, we propose a simple execution scheme Hebe (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Optimizing</i>
), which is order-oblivious while maintaining high performance. Hebe is attractive to the query optimizer (QO), as the QO does not need to go through a sampling process to decide the optimal evaluation order in advance. The intuition behind Hebe is to significantly reduce the number of high-cost instructions while keeping low-cost instructions unchanged. Our finding from Hebe sheds light on the importance of accurate cost model that guide us to derive an efficient execution scheme for query processing on modern CPUs."
619,,Deep Trajectory Recovery with Fine-Grained Calibration using Kalman Filter.,"Jingyuan Wang,Ning Wu,Xinxi Lu,Wayne Xin Zhao,Kai Feng",https://doi.org/10.1109/TKDE.2019.2940950,TKDE,2021,"Trajectory,Computational modeling,Task analysis,Roads,Data models,Correlation,Calibration","With the development of location-acquisition technologies, there are a huge number of mobile trajectories generated and accumulated in a variety of domains. However, due to the constraints of device and environment, many trajectories are recorded at low sampling rate, which increases the uncertainty between two consecutive sampled points in the trajectories. Our task is to recover a high-sampled trajectory based on the irregular low-sampled trajectory in free space, i.e., without road network information. There are two major problems with traditional solutions. First, many of these methods rely on heuristic search algorithms or simple probabilistic models. They cannot well capture complex sequential dependencies or global data correlations. Second, for reducing the predictive complexity of the unconstrained numerical coordinates, most of the previous studies have adopted a common preprocessing strategy by mapping the space into discrete units. As a side effect, using discrete units is likely to bring noise or inaccurate information. Hence, a principled post-calibration step is required to produce accurate results, which has been seldom studied by existing methods. To address the above difficulties, we propose a novel Deep Hybrid Trajectory Recovery model, named DHTR. Our recovery model extends the classic sequence-to-sequence generation framework by implementing a subsequence-to-sequence recovery model tailored for the current task, named subseq2seq. In order to effectively capture spatiotemporal correlations, we adopt both spatial and temporal attentions for enhancing the model performance. With the attention mechanisms, our model is able to characterize long-range correlations among trajectory points. Furthermore, we integrate the subseq2seq with a calibration component of Kalman filter (KF) for reducing the predictive uncertainty. At each timestep, the noisy predictions from the subseq2seq component will be fed into the KF component for calibration, and then the refined predictions will be forwarded to the subseq2seq component for the computation of the next timestep. Extensive results on real-world datasets have shown the superiority of the proposed model in both performance and interpretability."
620,,Unsupervised Linear Discriminant Analysis for Jointly Clustering and Subspace Learning.,"Fei Wang 0008,Quan Wang,Feiping Nie 0001,Zhongheng Li,Weizhong Yu,Rong Wang 0001",https://doi.org/10.1109/TKDE.2019.2939524,TKDE,2021,"Optimization,Clustering algorithms,Clustering methods,Feature extraction,Linear discriminant analysis,Learning systems,Principal component analysis","Linear discriminant analysis (LDA) is one of commonly used supervised subspace learning methods. However, LDA will be powerless faced with the no-label situation. In this paper, the unsupervised LDA (Un-LDA) is proposed and first formulated as a seamlessly unified objective optimization which guarantees convergence during the iteratively alternative solving process. The objective optimization is in both the ratio trace and the trace ratio forms, forming a complete framework of a new approach to jointly clustering and unsupervised subspace learning. The extension of LDA into Un-LDA enables to not only complete unsupervised subspace learning via the explicitly presented subspace projection matrix but also simultaneously finish clustering and even clustering out-of-sample data via the explicitly presented transformation matrix. To overcome the difficulty in solving the non-convex objective optimization, we mathematically prove that the Un-LDA optimization in both forms can be transformed into the simple K-means clustering optimization when the subspace is determined. The Un-LDA optimization is eventually completed by alternatively optimizing the clusters using K-means and the subspace using the supervised LDA methods and iterating this whole process until convergence or stopping criterion. The experiments demonstrate that our proposed Un-LDA algorithms are comparable or even much superior to the counterparts."
621,,Learning Graph Representation With Generative Adversarial Nets.,"Hongwei Wang 0004,Jialin Wang,Jia Wang,Miao Zhao,Weinan Zhang 0001,Fuzheng Zhang,Wenjie Li 0002,Xing Xie 0001,Minyi Guo",https://doi.org/10.1109/TKDE.2019.2961882,TKDE,2021,"Generators,Learning systems,Training,Games,Computational modeling,Task analysis,Feature extraction","Graph representation learning aims to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">generative</i>
 models that learn the underlying connectivity distribution in a graph, and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">discriminative</i>
 models that predict the probability of edge between a pair of vertices. In this paper, we propose 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">GraphGAN</i>
, an innovative graph representation learning framework unifying the above two classes of methods, in which the generative and the discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces “fake” samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, we propose a novel 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">graph softmax</i>
 as the implementation of the generative model to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">normalization</i>
, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">graph structure awareness</i>
, and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">computational efficiency</i>
. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including graph reconstruction, link prediction, node classification, recommendation, and visualization, over state-of-the-art baselines."
622,,Multi-Instance Learning With Emerging Novel Class.,"Xiu-Shen Wei,Han-Jia Ye,Xin Mu,Jianxin Wu,Chunhua Shen,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2019.2952588,TKDE,2021,"Anomaly detection,Measurement,Training,Proteins,Heuristic algorithms,Annotations,Text categorization","Diverse applications involving complicated data objects such as proteins and images are solved by applying multi-instance learning (MIL) algorithms. However, few MIL algorithms can deal with problems in an open and dynamic environment, where new categories of samples emerge. In this type of emerging novel class setting, algorithms should be able to not only classify the samples from the observed classes accurately, but also recognize the samples from the novel class. In this paper, we focus on the Multi-Instance learning with Emerging Novel class (MIEN) problem, and formulate MIEN from a metric learning perspective. We extract key instances to form the “super-bag” for each observed class, and non-key instances from all the observed classes to form a “meta super-bag”. Based on these super-bags, we propose the MIEN-metric method to learn discriminative metrics for classifying MIL bags from the observed classes and recognizing bags from the novel class. Experimental results of diverse domains, e.g., biological function annotation, text categorization, and object-centric/scene-centric image classification, show MIEN-metric outperforms other baseline methods significantly when the novel class emerges. Meanwhile, MIEN-metric is comparable with state-of-the-art MIL algorithms for binary classification in the traditional MIL setting."
623,,Anna - A KVS for Any Scale.,"Chenggang Wu 0001,Jose M. Faleiro,Yihan Lin,Joseph M. Hellerstein",https://doi.org/10.1109/TKDE.2019.2898401,TKDE,2021,"Lattices,Message systems,Multicore processing,Servers,Programming,Hardware,Scalability","Modern cloud providers offer dense hardware with multiple cores and large memories, hosted in global platforms. This raises the challenge of implementing high-performance software systems that can effectively scale from a single core to multicore to the globe. Conventional wisdom says that software designed for one scale point needs to be rewritten when scaling up by 10 - 100× [1]. In contrast, we explore how a system can be architected to scale across many orders of magnitude by design. We explore this challenge in the context of a new key-value store system called Anna: a partitioned, multi-mastered system that achieves high performance and elasticity via wait-free execution and coordination-free consistency. Our design rests on a simple architecture of coordination-free actors that perform state update via merge of lattice-based composite data structures. We demonstrate that a wide variety of consistency models can be elegantly implemented in this architecture with unprecedented consistency, smooth fine-grained elasticity, and performance that far exceeds the state of the art."
624,,Adaptive Diffusion of Sensitive Information in Online Social Networks.,"Xudong Wu,Luoyi Fu,Huan Long,Dali Yang,Yucheng Lu,Xinbing Wang,Guihai Chen",https://doi.org/10.1109/TKDE.2020.2964242,TKDE,2021,"Minimization,Facebook,Network topology,Privacy,Topology,Adaptive systems","The cascading of sensitive information such as private contents and rumors is a severe issue in online social networks. One approach for limiting the cascading of sensitive information is constraining the diffusion among social network users. However, the diffusion constraining measures limit the diffusion of non-sensitive information diffusion as well, resulting in the bad user experiences. To tackle this issue, in this paper, we study the problem of how to minimize the sensitive information diffusion while preserve the diffusion of non-sensitive information, and formulate it as a constrained minimization problem where we characterize the intention of preserving non-sensitive information diffusion as the constraint. We study the problem of interest over the fully-known network with known diffusion abilities of all users and the semi-known network where diffusion abilities of partial users remain unknown in advance. By modeling the sensitive information diffusion size as the reward of a bandit, we utilize the bandit framework to jointly design the solutions with polynomial complexity in the both scenarios. Moreover, the unknown diffusion abilities over the semi-known network induce it difficult to quantify the information diffusion size in algorithm design. For this issue, we propose to learn the unknown diffusion abilities from the diffusion process in real time and then adaptively conduct the diffusion constraining measures based on the learned diffusion abilities, relying on the bandit framework. Extensive experiments on real and synthetic datasets demonstrate that our solutions can effectively constrain the sensitive information diffusion, and enjoy a 40 percent less diffusion loss of non-sensitive information comparing with four baseline algorithms."
625,,Absorbing Diagonal Algorithm - An Eigensolver of $O\left(n{2.584963}\log \frac{1}{\varepsilon }\right)$On2.584963log1ɛ Complexity at Accuracy $\varepsilon$ɛ.,"Junfeng Wu,Jing He 0004,Chi-Hung Chi,Guangyan Huang",https://doi.org/10.1109/TKDE.2019.2949309,TKDE,2021,"Eigenvalues and eigenfunctions,Approximation algorithms,Complexity theory,Matrix decomposition,Jacobian matrices,Symmetric matrices,Big Data","Eigenvalue decomposition is widely used in dimensionality reduction for knowledge engineering, in particular principal component analysis and other similar spectral methods. Traditional eigenvalue decomposition algorithms for decomposing a matrix of size 
<inline-formula><tex-math notation=""LaTeX"">$n \times n$</tex-math></inline-formula>
 are usually of complexity 
<inline-formula><tex-math notation=""LaTeX"">$O(n^3)$</tex-math></inline-formula>
, due to a bottleneck in using Householder/Givens transforms to convert a general matrix to a tri-diagonal one. It is proposed in this article a new algorithm that takes only 
<inline-formula><tex-math notation=""LaTeX"">$O\left(n^{2.584963}\;\log \frac{1}{\varepsilon }\right)$</tex-math></inline-formula>
 computational complexity to achieve accuracy 
<inline-formula><tex-math notation=""LaTeX"">$\varepsilon$</tex-math></inline-formula>
 of eigenvalue decomposition for any 
<inline-formula><tex-math notation=""LaTeX"">$\varepsilon &gt; 0$</tex-math></inline-formula>
. The basic idea of our algorithm is to convert a matrix into a diagonal form in multi-scale divide and conquer scheme, and the conversion is to iteratively and recursively apply two phases of operations called diagonal attractions and diagonal absorptions respectively. In a diagonal attraction, it attracts the off-diagonal entries to make the entries nearer to the diagonal larger in magnitude than those farther away from the diagonal. In a diagonal absorption, it absorbs the near-to-diagonal nonzero entries into the diagonal. In such a scheme, no Householder or Givens transforms are involved. Moreover, diagonal attractions and diagonal absorptions can be implemented with fast matrix multiplications. The scheme’s divide and conquer pattern also allows our algorithm to be easily mapped to modern computer hardware. Our algorithm also complements well the family of randomized eigenvalue/SVD algorithms using sampling techinques, which are of complexity 
<inline-formula><tex-math notation=""LaTeX"">$O(n^\alpha \ \mathrm{polylog}(\frac{1}{\varepsilon }))$</tex-math></inline-formula>
 with small 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
 but very large overheads in the polylog. Their strength in the small exponent 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
 of 
<inline-formula><tex-math notation=""LaTeX"">$n$</tex-math></inline-formula>
 in complexity was easily cancelled by the exploding overheads in the polylog. Now, with their low-accuracy estimate refined by our algorithm for high accuracy, their strength can be boosted significantly."
626,,NEIST - A Neural-Enhanced Index for Spatio-Temporal Queries.,"Sai Wu,Zhifei Pang,Gang Chen 0001,Yunjun Gao,Cenjiong Zhao,Shili Xiang",https://doi.org/10.1109/TKDE.2019.2945947,TKDE,2021,"Trajectory,Indexes,Predictive models,Public transportation,Roads,Recurrent neural networks,Computational modeling","Previous work on the spatio-temporal index often adopts a simple linear model to predict the future positions of moving objects, which may generate numerous errors for complex road networks and fast moving objects. In this paper, we propose NEIST, a neural-enhanced index to process spatio-temporal queries with enhanced efficiency and accuracy, by intelligently leveraging the movement patterns among moving objects. NEIST applies a Recurrent Neural Network (RNN) model to predict future positions of moving objects based on observed trajectories. To reduce the prediction overhead, a suffix-tree is further built to index trajectories with similar suffixes, and thus similar objects within a given similarity bound are grouped together to share the same prediction result. A prediction result in NEIST represents possible positions of a group of moving objects in the next t time slots. Inside each time slot, traditional linear prediction model is then adopted and a TPR-Tree is built to support spatio-temporal queries. We use Singapore and Porto taxi trajectory datasets to evaluate NEIST. Compared to previous approaches, NEIST achieves a much more efficient query performance and is able to produce about 70 percent more accurate results."
627,,Enhanced Privacy Preserving Group Nearest Neighbor Search.,"Yuncheng Wu,Ke Wang 0001,Ruoyang Guo,Zhilin Zhang,Dan Zhao,Hong Chen 0001,Cuiping Li",https://doi.org/10.1109/TKDE.2019.2930696,TKDE,2021,"Privacy,Databases,Data privacy,Cryptography,Aggregates,Servers,Computational efficiency","Group k-nearest neighbor (kGNN) search allows a group of n mobile users to jointly retrieve k points from a location-based service provider (LSP) that minimizes the aggregate distance to them. We identify four protection objectives in the privacy preserving kGNN search: (i) every user's location should be protected from LSP; (ii) the group's query and the query answer should be protected from LSP; (iii) LSP's private database information should be protected from users; (iv) every user's location should be protected from other users in the group. We design two privacy preserving solutions under two types of threat model to the privacy preserving kGNN search in the full user collusion environment, where any n - 1 users in the group may collude to infer the location of the remaining user. Our solutions do not rely on heavy pre-computation on LSP like previous works. Though we consider kGNN, the proposed privacy preserving solutions can be easily adopted to any group query as it treats the query answering (i.e., kGNN) as a black box. Theoretical and experimental analysis suggest that our solutions are highly efficient in both communication cost and user computational cost while incurring some reasonable overhead on LSP."
628,,Knowledge Graph Embedding Based on Multi-View Clustering Framework.,"Han Xiao,Yidong Chen,Xiaodong Shi",https://doi.org/10.1109/TKDE.2019.2931548,TKDE,2021,"Semantics,Numerical models,Knowledge representation,Task analysis,Neural networks,Probabilistic logic","Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval."
629,,GraphInception - Convolutional Neural Networks for Collective Classification in Heterogeneous Information Networks.,"Yun Xiong,Yizhou Zhang,Xiangnan Kong,Huidi Chen,Yangyong Zhu",https://doi.org/10.1109/TKDE.2019.2947458,TKDE,2021,"Convolution,Complexity theory,Correlation,Deep learning,Feature extraction,Task analysis,Training","Collective classification has attracted considerable attention in the last decade, where the labels within a group of instances are correlated and should be inferred collectively, instead of independently. Conventional approaches on collective classification mainly focus on exploiting simple relational features (such as 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">count</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">exists</i>
 aggregators on neighboring nodes). However, many real-world applications involve complex dependencies among the instances, which are obscure/hidden in the networks. To capture these dependencies in collective classification, we need to go beyond simple relational features and extract deep dependencies between the instances. In this paper, we study the problem of deep collective classification in 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Heterogeneous Information Networks</i>
 (HINs), which involve different types of autocorrelations, from simple to complex relations, among the instances. Different from conventional autocorrelations, which are given explicitly by the links in the network, complex autocorrelations are obscure/hidden in HINs, and should be inferred from existing links in a hierarchical order. This problem is highly challenging due to the multiple types of dependencies among the nodes and the complexity of the relational features. In this study, we proposed a deep convolutional collective classification method, called 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">GraphInception</i>
, to learn the deep relational features in HINs. And we presented two versions of the models with different inference styles. The proposed methods can automatically generate a hierarchy of relational features with different complexities. Extensive experiments on four real-world networks demonstrate that our approach can improve the collective classification performance by considering deep relational features in HINs."
630,,Label Enhancement for Label Distribution Learning.,"Ning Xu 0009,Yun-Peng Liu,Xin Geng",https://doi.org/10.1109/TKDE.2019.2947040,TKDE,2021,"Training,Correlation,Buildings,Supervised learning,Pose estimation,Transforms","Label distribution is more general than both single-label annotation and multi-label annotation. It covers a certain number of labels, representing the degree to which each label describes the instance. The learning process on the instances labeled by label distributions is called label distribution learning (LDL). Unfortunately, many training sets only contain simple logical labels rather than label distributions due to the difficulty of obtaining the label distributions directly. To solve this problem, one way is to recover the label distributions from the logical labels in the training set via leveraging the topological information of the feature space and the correlation among the labels. Such process of recovering label distributions from logical labels is defined as label enhancement (LE), which reinforces the supervision information in the training sets. This paper proposes a novel LE algorithm called Graph Laplacian Label Enhancement (GLLE). Experimental results on one artificial dataset and fourteen real-world LDL datasets show clear advantages of GLLE over several existing LE algorithms. Furthermore, experimental results on eleven multi-label learning datasets validate the advantage of GLLE over the state-of-the-art multi-label learning approaches."
631,,Enhancing Recommender Systems With a Stimulus-Evoked Curiosity Mechanism.,"Ke Xu 0009,Junwen Mo,Yi Cai,Huaqing Min",https://doi.org/10.1109/TKDE.2019.2957770,TKDE,2021,"Psychology,Task analysis,Recommender systems,Decision making,Motion pictures,Uncertainty,Compounds","Classical algorithms in recommender systems (RS) mainly emphasis on achieving high accuracy and thus recommend items precisely matching a user's past choices. However, the user may gradually lose interest and crave something more inspiring. In psychology, curiosity is a critical human nature and can be efficient bootstrap exploratory behaviors, thus this phenomenon can be explained as insufficient stimulation to induce curiosity regard to recommended items. Inspired from the above, this work proposes a Curiosity-drive Recommendation Framework (CdRF) which incorporates a highly innovative Stimulus-evoked Curiosity mechanism (SeCM) together with a basic accuracy-oriented algorithm via Borda count. In SeCM, we first estimate the stimulus intensity appearing on each item for each user and then model personalized curiosity among the calculated intensities using Wundt curve. For the target user, the output of CdRF is a ranked list of 
<inline-formula><tex-math notation=""LaTeX"">$N$</tex-math> </inline-formula>
 items which are both relevant and highly curiousness. We conduct extensive experiments using four public datasets to evaluate the performance of each specification of SeCM as well as the whole framework CdRF. The results reveal that SeCM can flexibly generate diversified items and CdRF can increase diversity in terms of ILS, Newness and AD while compromising very little Precision. This kind of research also offers a way to understand both individual differences in curiosity and how curiosity contributes to item exploration at the level of RS."
632,,MixSp - A Framework for Embedding Heterogeneous Information Networks With Arbitrary Number of Node and Edge Types.,"Linchuan Xu,Jing Wang,Lifang He 0001,Jiannong Cao 0001,Xiaokai Wei,Philip S. Yu,Kenji Yamanishi",https://doi.org/10.1109/TKDE.2019.2955945,TKDE,2021,"Task analysis,Semantics,Predictive models,Data mining,Computational modeling,Indexes,Mathematical model","Heterogeneous information network (HIN) embedding is to encode network structure into node representations with the heterogeneous semantics of different node and edge types considered. However, since each HIN may have a unique nature, e.g., a unique set of node and edge types, a model designed for one type of networks may not be applicable to or effective on another type. In this article, we thus attempt to propose a framework for HINs with arbitrary number of node and edge types. The proposed framework constructs a novel mixture-split representation of an HIN, and hence is named as MixSp. The mixture sub-representation and the split sub-representation serve as two different views of the network. Compared with existing models which only learn from the original view, MixSp thus may exploit more comprehensive information. Node representations in each view are learned by embedding the respective network structure. Moreover, the node representations are further refined through cross-view co-regularization. The framework is instantiated in three models which differ from each other in the co-regularization. Extensive experiments on three real-world datasets show MixSp outperforms several recent models in both node classification and link prediction tasks even though MixSp is not designed for a particular type of HINs."
633,,Adaptive Classifier Ensemble Method Based on Spatial Perception for High-Dimensional Data Classification.,"Yuhong Xu,Zhiwen Yu 0002,Wenming Cao 0002,C. L. Philip Chen,Jane You",https://doi.org/10.1109/TKDE.2019.2961076,TKDE,2021,"Learning systems,Feature extraction,Principal component analysis,Classification algorithms,Data mining,Dimensionality reduction,Bagging","Classifying high-dimensional small-size data is challenging in the field of pattern recognition. Traditional ensemble learning methods have several limitations: 1) sample-space based methods are easily affected by noise and redundant features; 2) feature-space based methods cannot excavate the essential characteristics of features; 3) feature subspaces cause information loss, which leads to a decline in accuracy; 4) most selective ensemble methods only consider the diversity and performance of sub-classifiers and ignore the impact on integration systems. To address the above limitations, we propose an adaptive classifier ensemble learning method (AdaSPEL) based on spatial perception for high-dimensional data. First, we design a local-space perception method for feature transformation, which encourages both high performance and diversity of the ensemble members. Second, we design a cross-space perception method based on the distribution of samples to obtain the cross-space enhanced features to provide a macro analysis for the characteristics of data. Furthermore, an adaptive selective ensemble method based on local and global evaluation mechanisms is proposed, which considers the impact of sub-classifiers on integrated systems. Experimental results on 33 high-dimensional data sets verify that our method outperforms mainstream ensemble learning methods based on feature space and sample space, and neural network-based algorithms."
634,,Spatio-Temporal Multi-Task Learning via Tensor Decomposition.,"Jianpeng Xu,Jiayu Zhou,Pang-Ning Tan,Xi Liu,Lifeng Luo",https://doi.org/10.1109/TKDE.2019.2956713,TKDE,2021,"Tensile stress,Data models,Predictive models,Task analysis,Meteorology,Data mining,Indexes","Predictive modeling of large-scale spatio-temporal data is an important but challenging problem as it requires training models that can simultaneously predict the target variables of interest at multiple locations while preserving the spatial and temporal dependencies of the data. In this paper, we investigate the effectiveness of applying a multi-task learning approach based on supervised tensor decomposition to the spatio-temporal prediction problem. Our proposed framework, known as SMART, encodes the data as a third-order tensor and extracts a set of interpretable, spatial and temporal latent factors from the data. An ensemble of spatial and temporal prediction models are trained using the latent factors as their predictor variables. Outputs from the ensemble model are aggregated to make predictions on test instances. The framework also allows known patterns from the domain to be incorporated as constraints to guide the tensor decomposition and ensemble learning processes. As the data may grow over space and time, an incremental learning version of the framework is given to efficiently update the models. We perform extensive experiments using a global-scale climate dataset to evaluate the accuracy and efficiency of the models as well as interpretability of the latent factors."
635,,Predicting Destinations by a Deep Learning based Approach.,"Jiajie Xu 0001,Jing Zhao,Rui Zhou 0001,Chengfei Liu,Pengpeng Zhao,Lei Zhao 0001",https://doi.org/10.1109/TKDE.2019.2932984,TKDE,2021,"Trajectory,Predictive models,Adaptation models,Standards,Neural networks,Logic gates,Data models","Destination prediction is known as an important problem for many location based services (LBSs). Existing solutions generally apply probabilistic models or neural network models to predict destinations over a subtrajectory, and adopt the standard attention mechanism to improve the prediction accuracy. However, the standard attention mechanism uses fixed feature representations, and has a limited ability to represent distinct features of locations. Besides, existing methods rarely take the impact of spatial and temporal characteristics of the trajectory into account. Their accuracies in fine-granularity prediction are always not satisfactory due to the data sparsity problem. Thus, in this paper, a carefully designed deep learning model called LATL model is presented. It not only adopts an adaptive attention network to model the distinct features of locations, but also implements time gates and distance gates into the Long Short-Term Memory (LSTM) network to capture the spatial-temporal relation between consecutive locations. Furthermore, to better understand the mobility patterns in different spatial granularities, and explore the fusion of multi-granularity learning capability, a hierarchical model that utilizes tailored combination of different neural networks under multiple spatial granularities is further proposed. Extensive empirical studies verify that the newly proposed models perform effectively and settle the problem nicely."
636,,Subgraph Networks With Application to Structural Feature Space Expansion.,"Qi Xuan,Jinhuan Wang,Minghao Zhao,Junkun Yuan,Chenbo Fu,Zhongyuan Ruan,Guanrong Chen",https://doi.org/10.1109/TKDE.2019.2957755,TKDE,2021,"Heat pumps,Water heating,Refrigerants,Liquids,Heat engines","Real-world networks exhibit prominent hierarchical and modular structures, with various subgraphs as building blocks. Most existing studies simply consider distinct subgraphs as motifs and use only their numbers to characterize the underlying network. Although such statistics can be used to describe a network model, or even to design some network algorithms, the role of subgraphs in such applications can be further explored so as to improve the results. In this article, the concept of subgraph network (SGN) is introduced and then applied to network models, with algorithms designed for constructing the 1st-order and 2nd-order SGNs, which can be easily extended to build higher-order ones. Furthermore, these SGNs are used to expand the structural feature space of the underlying network, beneficial for network classification. Numerical experiments demonstrate that the network classification model based on the structural features of the original network together with the 1st-order and 2nd-order SGNs always performs the best as compared to the models based only on one or two of such networks. In other words, the structural features of SGNs can complement that of the original network for better network classification, regardless of the feature extraction method used, such as the handcrafted, network embedding and kernel-based methods."
637,,Multi-Site User Behavior Modeling and Its Application in Video Recommendation.,"Huan Yan,Chunfeng Yang,Donghan Yu,Yong Li 0008,Depeng Jin,Dah Ming Chiu",https://doi.org/10.1109/TKDE.2019.2926078,TKDE,2021,"Data models,Analytical models,Data collection,TV,Probabilistic logic,Recommender systems,Motion pictures","As online video service continues to grow in popularity, video content providers compete hard for more eyeball engagement. Some users visit multiple video sites to enjoy videos of their interest while some visit exclusively one site. However, due to the isolation of data, mining and exploiting user behaviors in multiple video websites remain unexplored so far. In this work, we try to model user preferences in six popular video websites with user viewing records obtained from a large ISP in China. The empirical study shows that users exhibit both consistent cross-site interests as well as site-specific interests. To represent this dichotomous pattern of user preferences, we propose a generative model of Multi-site Probabilistic Factorization (MPF) to capture both the cross-site as well as site-specific preferences. Besides, we discuss the design principle of our model by analyzing the sources of the observed site-specific user preferences, namely, site peculiarity and data sparsity. Through conducting extensive recommendation validation, we show that our MPF model achieves the best results compared to several other state-of-the-art factorization models with significant improvements of F-measure by 12.96, 8.24 and 6.88 percent, respectively. Our findings provide insights on the value of integrating user data from multiple sites, which stimulates collaboration between video service providers."
638,,Semi-Supervised Multi-Modal Multi-Instance Multi-Label Deep Network with Optimal Transport.,"Yang Yang 0074,Zhao-Yang Fu,De-Chuan Zhan,Zhi-Bin Liu,Yuan Jiang 0001",https://doi.org/10.1109/TKDE.2019.2932666,TKDE,2021,"Correlation,Measurement,Games,Task analysis,Bayes methods,Acceleration,Data models","Complex objects are usually with multiple labels, and can be represented by multiple modal representations, e.g., the complex articles contain text and image information as well as multiple annotations. Previous methods assume that the homogeneous multi-modal data are consistent, while in real applications, the raw data are disordered, e.g., the article constitutes with variable number of inconsistent text and image instances. Therefore, Multi-modal Multi-instance Multi-label (M3) learning provides a framework for handling such task and has exhibited excellent performance. However, M3 learning is facing two main challenges: 1) how to effectively utilize label correlation and 2) how to take advantage of multi-modal learning to process unlabeled instances. To solve these problems, we first propose a novel Multi-modal Multi-instance Multi-label Deep Network (M3DN), which considers M3 learning in an end-to-end multi-modal deep network and utilizes consistency principle among different modal bag-level predictions. Based on the M3DN, we learn the latent ground label metric with the optimal transport. Moreover, we introduce the extrinsic unlabeled multi-modal multi-instance data, and propose the M3DNS, which considers the instance-level auto-encoder for single modality and modified bag-level optimal transport to strengthen the consistency among modalities. Thereby M3DNS can better predict label and exploit label correlation simultaneously. Experiments on benchmark datasets and real world WKG Game-Hub dataset validate the effectiveness of the proposed methods."
639,,A Survey on Canonical Correlation Analysis.,"Xinghao Yang,Weifeng Liu 0001,Wei Liu 0007,Dacheng Tao",https://doi.org/10.1109/TKDE.2019.2958342,TKDE,2021,"Correlation,Kernel,Computational modeling,Probabilistic logic,Data models,Analytical models,Bayes methods","In recent years, the advances in data collection and statistical analysis promotes canonical correlation analysis (CCA) available for more advanced research. CCA is the main technique for two-set data dimensionality reduction such that the correlation between the pairwise variables in the common subspace is mutually maximized. Over 80-years of developments, a number of CCA models have been proposed according to different machine learning mechanisms. However, the field lacks an insightful review for the state-of-art developments. This survey targets to provide a well-organized overview for CCA and its extensions. Specifically, we first review the CCA theory from the perspective of both model formation and model optimization. The association between two popular solution methods, i.e., eigen value decomposition (EVD) and singular value decomposition (SVD), are discussed. Following that, we present a taxonomy of current progresses and classify them into seven groups: 1) multi-view CCA, 2) probabilistic CCA, 3) deep CCA, 4) kernel CCA, 5) discriminative CCA, 6) sparse CCA and 7) locality preserving CCA. For each group, we demonstrate two or three representative mathematical models, identifying their strengths and limitations. We summarize the representative applications and numerical results of these seven groups in real-world practices, collecting the data sets and open-sources for implementation. In the end, we provide several promising future research directions that can improve the current state of the art."
640,,Influence Analysis in Evolving Networks - A Survey.,"Yu Yang 0001,Jian Pei",https://doi.org/10.1109/TKDE.2019.2934447,TKDE,2021,"Task analysis,Monitoring,Twitter,Mathematical model,Image edge detection,Recommender systems","Influence analysis aims at detecting influential vertices in networks and utilizing them in cost-effective business strategies. Influence analysis in large-scale networks is a key technique in many important applications ranging from viral marketing and online advertisement to recommender systems, and thus has attracted great interest from both academia and industry. Early investigations on influence analysis often assume static networks. However, it is well recognized that real networks like social networks and the web network are not static but evolve rapidly over time. Thus, to make the results of influence analysis in real networks up-to-date, we have to take network evolution into consideration. Incorporating evolution of networks into influence analysis raises many new challenges, since an evolving network often updates at a fast rate and, except for the network owner, the evolution is usually even not entirely known to people. In this survey, we provide an overview on recent research in influence analysis in evolving networks, which has not been systematically reviewed in literature. We first revisit mathematical models of evolving networks and commonly used influence models. Then, we review recent research in five major tasks of evolving network influence analysis. We also discuss some future directions to explore."
641,,Neural Diffusion Model for Microscopic Cascade Study.,"Cheng Yang 0002,Maosong Sun,Haoran Liu,Shiyi Han,Zhiyuan Liu 0001,Huanbo Luan",https://doi.org/10.1109/TKDE.2019.2939796,TKDE,2021,"Microscopy,Integrated circuit modeling,Task analysis,Data models,Deep learning,Twitter","The study of information diffusion or cascade has attracted much attention over the last decade. Most related works target on studying cascade-level macroscopic properties such as the final size of a cascade. Existing microscopic cascade models which focus on user-level modeling either make strong assumptions on how a user gets infected by a cascade or limit themselves to a specific scenario where “who infected whom” information is explicitly labeled. The strong assumptions oversimplify the complex diffusion mechanism and prevent these models from better fitting real-world cascade data. Also, the methods which focus on specific scenarios cannot be generalized to a general setting where the diffusion graph is unobserved. To overcome the drawbacks of previous works, we propose a Neural Diffusion Model (NDM) for general microscopic cascade study. NDM makes relaxed assumptions and employs deep learning techniques including attention mechanism and convolutional network for cascade modeling. Both advantages enable our model to go beyond the limitations of previous methods, better fit the diffusion data and generalize to unseen cascades. Experimental results on diffusion identification task over four realistic cascade datasets show that our model can achieve a relative improvement up to 26 percent against the best performing baseline in terms of F1 score."
642,,Mining Fraudsters and Fraudulent Strategies in Large-Scale Mobile Social Networks.,"Yang Yang 0009,Yuhong Xu,Yizhou Sun,Yuxiao Dong,Fei Wu 0001,Yueting Zhuang",https://doi.org/10.1109/TKDE.2019.2924431,TKDE,2021,"Companies,Telephone sets,Dispersion,Annotations,Social networking (online),Semisupervised learning","The rapid development of modern communication technologies-in particular, (mobile) phone communications-has largely facilitated human social interactions and information exchange. However, the emergence of telemarketing frauds can significantly dissipate individual fortune and social wealth, resulting in a potential slow down or damage to economics. In this work, we propose to spot telemarketing frauds, with an emphasis on unveiling the “precise fraud” phenomenon and the strategies that are used by fraudsters to precisely select targets. To study this problem, we employ a one-month complete dataset of telecommunication metadata in Shanghai with 54 million users and 698 million call logs. Through our study, we find that user's information might have been seriously leaked, and fraudsters have a preference over the target user's age and activity in mobile network. We further propose a novel semi-supervised learning framework to distinguish fraudsters from non-fraudsters. Experimental results on a real-world data show that our approach outperforms several state-of-the-art algorithms in accuracy of detecting fraudsters (e.g., +0.278 in terms of F1 on average). We believe that our study can potentially inform policymaking for government and mobile service providers."
643,,Semi-Supervised Multi-Modal Clustering and Classification with Incomplete Modalities.,"Yang Yang 0074,De-Chuan Zhan,Yi-Feng Wu,Zhi-Bin Liu,Hui Xiong 0001,Yuan Jiang 0001",https://doi.org/10.1109/TKDE.2019.2932742,TKDE,2021,"Kernel,Matrix decomposition,Semisupervised learning,Clustering algorithms,Data models,Data collection,Feature extraction","In this paper, we propose a novel Semi-supervised Learning with Incomplete Modality (SLIM) method considering the modal consistency and complementarity simultaneously, and Kernel SLIM (SLIM-K) based on matrix completion for further solving the modal incompleteness. As is well known, most realistic data have multi-modal representations, multi-modal learning refers to the process of learning a precise model for complete modalities. However, due to the failures of data collection, self-deficiencies, or other various reasons, multi-modal examples are usually with incomplete modalities, which generate utility obstacle using previous methods. In this paper, SLIM integrates the intrinsic consistency and extrinsic complementary information for prediction and cluster simultaneously. In detail, SLIM forms different modal classifiers and clustering learner consistently in a unified framework, while using the extrinsic complementary information from unlabeled data against the insufficiencies brought by the incomplete modal issue. Moreover, in order to deal with missing modality in essence, we propose the SLIM-K, which takes the complemented kernel matrix into the classifiers and the cluster learner respectively. Thus, SLIM-K can solve the defects of missing modality in result. Finally, we give the discussion of generalization of incomplete modalities. Experiments on 13 benchmark multi-modal datasets and two real-world incomplete multi-modal datasets validate the effectiveness of our methods."
644,,Tree++ - Truncated Tree Based Graph Kernels.,"Wei Ye 0001,Zhen Wang,Rachel Redberg,Ambuj K. Singh",https://doi.org/10.1109/TKDE.2019.2946149,TKDE,2021,"Kernel,Chemical compounds,Compounds,Natural language processing,Chlorine,Shape","Graph-structured data arise ubiquitously in many application domains. A fundamental problem is to quantify their similarities. Graph kernels are often used for this purpose, which decompose graphs into substructures and compare these substructures. However, most of the existing graph kernels do not have the property of scale-adaptivity, i.e., they cannot compare graphs at multiple levels of granularities. Many real-world graphs such as molecules exhibit structure at varying levels of granularities. To tackle this problem, we propose a new graph kernel called Tree++ in this paper. At the heart of Tree++ is a graph kernel called the path-pattern graph kernel. The path-pattern graph kernel first builds a truncated BFS tree rooted at each vertex and then uses paths from the root to every vertex in the truncated BFS tree as features to represent graphs. The path-pattern graph kernel can only capture graph similarity at fine granularities. In order to capture graph similarity at coarse granularities, we incorporate a new concept called super path into it. The super path contains truncated BFS trees rooted at the vertices in a path. Our evaluation on a variety of real-world graphs demonstrates that Tree++ achieves the best classification accuracy compared with previous graph kernels."
645,,Signed-PageRank - An Efficient Influence Maximization Framework for Signed Social Networks.,"Xiaoyan Yin 0001,Xiao Hu,Yanjiao Chen,Xu Yuan,Baochun Li",https://doi.org/10.1109/TKDE.2019.2947421,TKDE,2021,"Social networking (online),Integrated circuit modeling,Adaptation models,Greedy algorithms,Diseases,Heuristic algorithms","Influence maximization in social networks is of great importance for marketing new products. Signed social networks with both positive (friends) and negative (foes) relationships pose new challenges and opportunities, since the influence of negative relationships can be leveraged to promote information propagation. In this paper, we study the problem of influence maximization for advertisement recommendation in signed social networks. We propose a new framework to characterize the information propagation process in signed social networks, which models the dynamics of individuals’ beliefs and attitudes towards the advertisement based on recommendations from both positive and negative neighbours. To achieve influence maximization in signed social networks, we design a novel Signed-PageRank (
<inline-formula><tex-math notation=""LaTeX"">${\sf SPR}$</tex-math></inline-formula>
) algorithm, which selects the initial seed nodes by jointly considering their positive and negative connections with the rest of the network. Our extensive experimental results confirm that our proposed 
<inline-formula><tex-math notation=""LaTeX"">${\sf SPR}$</tex-math></inline-formula>
 algorithm can effectively and efficiently influence a broader range of individuals in the signed social networks than benchmark algorithms on both synthetic and real datasets."
646,,An Improved Quantum Algorithm for Ridge Regression.,"Chao-Hua Yu,Fei Gao 0001,Qiao-Yan Wen",https://doi.org/10.1109/TKDE.2019.2937491,TKDE,2021,"Prediction algorithms,Approximation algorithms,Machine learning algorithms,Quantum computing,Machine learning,Encoding,Sparse matrices","Ridge regression (RR) is an important machine learning technique which introduces a regularization hyperparameter 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
 to ordinary multiple linear regression for analyzing data suffering from multicollinearity. In this paper, we present a quantum algorithm for RR, where the technique of parallel Hamiltonian simulation to simulate a number of Hermitian matrices in parallel is proposed and used to develop a quantum version of 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula>
-fold cross-validation approach, which can efficiently estimate the predictive performance of RR. Our algorithm consists of two phases: (1) using quantum 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula>
-fold cross-validation to efficiently determine a good 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
 with which RR can achieve good predictive performance, and then (2) generating a quantum state encoding the optimal fitting parameters of RR with such 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
, which can be further utilized to predict new data. Since indefinite dense Hamiltonian simulation has been adopted as a key subroutine, our algorithm can efficiently handle non-sparse data matrices. It is shown that our algorithm can achieve exponential speedup over the classical counterpart for (low-rank) data matrices with low condition numbers. But when the condition numbers of data matrices are large to be amenable to full or approximately full ranks of data matrices, only polynomial speedup can be achieved."
647,,Co-Clustering Ensembles Based on Multiple Relevance Measures.,"Xianxue Yu,Guoxian Yu,Jun Wang 0035,Carlotta Domeniconi",https://doi.org/10.1109/TKDE.2019.2942029,TKDE,2021,"Optimization,Gene expression,Fuses,Runtime,Bipartite graph,Minimization,Robustness","Co-clustering aims at discovering groups of both objects and features from a given data matrix. Co-clustering ensembles can produce robust co-clusters by combining multiple base co-clusterings. However, current co-clustering ensemble solutions either ignore the constraints resulting from feature-to-feature and object-to-object relevance information, or ignore feature-to-object relevance information. In this paper, we advocate that all three information sources contribute to the achievement of good consensus solutions, and propose a co-clustering ensemble (CoCE) approach based on multiple relevance measures. CoCE first evaluates the quality of base co-clusters and consequently measures feature-to-object relevance. The latter, along with feature-to-feature and object-to-object relevance measures, contribute to the definition of a hybrid graph. The consensus process uses the resulting hybrid graph; it's formulated as a trace minimization problem and introduces a block-wise matrix multiplication technique to perform the optimization. Experimental results on various datasets show that CoCE not only frequently outperforms other related co-clustering ensembles, but also has reduced runtime cost and is more robust to poor base co-clusterings."
648,,Target Defense Against Link-Prediction-Based Attacks via Evolutionary Perturbations.,"Shanqing Yu,Minghao Zhao,Chenbo Fu,Jun Zheng,Huimin Huang,Xincheng Shu,Qi Xuan,Guanrong Chen",https://doi.org/10.1109/TKDE.2019.2933833,TKDE,2021,"Perturbation methods,Privacy,Social networking (online),Indexes,Measurement,Prediction methods,Resource management","In social networks, by removing some target-sensitive links, privacy protection might be achieved. However, some hidden links can still be re-observed by link prediction methods on observable networks. In this paper, the conventional link prediction method named Resource Allocation Index (RA) is adopted for privacy attacks. Several defense methods are proposed, including heuristic and evolutionary approaches, to protect targeted links from RA attack. In particular, incremental computation is proposed for accelerating the calculation of fitness in evolutionary approaches. This is the first time to study privacy protection for targeted links against similarity based link prediction attacks. Some links are randomly selected from original network as targeted links for experimentation. The experimental results on nine real-world networks demonstrate the superiority of the evolutionary perturbations, especially EDA, for defending against RA attack. Moreover, experimental results show that the proposed perturbation generated by EDA is transferable and can even defend against other link prediction attacks which are based on high order similarity between pairwise nodes, although it is designed to prevent RA attack."
649,,Detection of Community Structures in Networks With Nodal Features based on Generative Probabilistic Approach.,"Hadi Zare 0001,Mahdi Hajiabadi,Mahdi Jalili",https://doi.org/10.1109/TKDE.2019.2960222,TKDE,2021,"Feature extraction,Probabilistic logic,Predator prey systems,Graphical models,Correlation,Social networking (online),Clustering algorithms","Community detection is considered as a fundamental task in analyzing social networks. Even though many techniques have been proposed for community detection, most of them are based exclusively on the connectivity structures. However, there are node features in real networks, such as gender types in social networks, feeding behavior in ecological networks, and location on e-trading networks, that can be further leveraged with the network structure to attain more accurate community detection methods. We propose a novel probabilistic graphical model to detect communities by taking into account both network structure and nodes’ features. The proposed approach learns the relevant features of communities through a generative probabilistic model without any prior assumption on the communities. Furthermore, the model is capable of determining the strength of node features and structural elements of the networks on shaping the communities. The effectiveness of the proposed approach over the state-of-the-art algorithms is revealed on synthetic and benchmark networks."
650,,Semi-Supervised Graph Based Embedding With Non-Convex Sparse Coding Techniques.,"Qi Zhang 0005,Tianguang Chu,Cishen Zhang",https://doi.org/10.1109/TKDE.2019.2953668,TKDE,2021,"Data models,Feature extraction,Correlation,Analytical models,Semisupervised learning,Adaptation models,Face recognition","We consider the problem of semi-supervised graph-based learning upon multimodal and mixmodal data. Since in semi-supervised settings, the labeled information is very limited, we first propose a non-convex sparse-coding based label propagation (
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
-SLP) method to estimate the soft labels, and thereby to enrich the supervised information. By considering the structural properties of multimodal and mixmodal data, we present a semi-supervised graph-based embedding (SGE) approach that incorporates the soft label information with the hierarchical local geometric information of within-class, between-class, and overall-class data. Based on this, subspaces characterizing the multimodal and mixmodal data structure can be derived by maximizing the weighted between-class separability and minimizing the locality-preserved within-class as well as overall-class distances of the training samples. We further extend SGE into semi-supervised sparse subspace learning scenarios and present an 
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
-structural-regularization-induced SGE (
<inline-formula><tex-math notation=""LaTeX"">$\alpha$</tex-math></inline-formula>
-SSGE) model, which can give better results in extracting discriminative groups of features by utilizing the non-convex structural regularization techniques. Experiments for multimodal and mixmodal digit as well as face recognition verify the validity and effectiveness of the proposed models."
651,,CorrectMR - Authentication of Distributed SQL Execution on MapReduce.,"Bo Zhang 0051,Boxiang Dong,Wendy Hui Wang",https://doi.org/10.1109/TKDE.2019.2935968,TKDE,2021,"Authentication,Probabilistic logic,Aggregates,Task analysis,Query processing,Data structures,Transforms","In this paper, we consider the SQL Selection-GroupBy-Aggregation (SGA) query evaluation on an untrusted MapReduce system in which mappers and reducers may return incorrect results. We design CorrectMR, a system that supports efficient verification of result correctness for both intermediate and final results of SGA queries. CorrectMR includes the design of Pedersen Merkle R-tree (PMR-tree), a new authenticated data structure (ADS). To enable efficient verification, CorrectMR includes a distributed ADS construction mechanism that allows mappers/reducers to construct PMR-trees in parallel without a centralized party. CorrectMR provides the following verification functionality: (1) correctness verification of PMR-trees by replication; (2) correctness verification of intermediate (final, resp.) query results by constructing local (global, resp.) PMR-trees and verification objects. Our experimental results demonstrate the efficiency and effectiveness of CorrectMR."
652,,Integrity Authentication for SQL Query Evaluation on Outsourced Databases - A Survey.,"Bo Zhang,Boxiang Dong,Wendy Hui Wang",https://doi.org/10.1109/TKDE.2019.2947061,TKDE,2021,"Authentication,Query processing,Aggregates,Cloud computing,Probabilistic logic","Spurred by the development of cloud computing, there has been considerable recent interest in the Database-as-a-Service (DaaS) paradigm. Users lacking in expertise or computational resources can outsource their data and database management needs to a third-party service provider. Outsourcing, however, raises an important issue of result integrity: how can the client verify with lightweight overhead that the query results returned by the service provider are correct (i.e., the same as the results of query execution locally)? This survey focuses on categorizing and reviewing the progress on the current approaches for result integrity of SQL query evaluation in the DaaS model. The survey also includes some potential future research directions for result integrity verification of the outsourced computations."
653,,Reconciling Multiple Social Networks Effectively and Efficiently - An Embedding Approach.,"Zhongbao Zhang,Li Sun 0008,Sen Su,Jielun Qu,Gen Li",https://doi.org/10.1109/TKDE.2019.2929786,TKDE,2021,"Social networking (online),Optimization,Symmetric matrices,Clustering algorithms,Convergence,Sun,Robustness","Recently, reconciling social networks, identifying the accounts belonging to the same individual across social networks, receives significant attention from both academic and industry. Most of the existing studies have limitations in the following three aspects: multiplicity, comprehensiveness and robustness. To address these limitations, we rethink this problem and, for the first time, robustly and comprehensively reconcile multiple social networks. In this paper, we propose two frameworks, MASTER and MASTER+, i.e., across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation. In MASTER, we first design a novel Constrained Dual Embedding model, simultaneously embedding and reconciling multiple social networks, to formulate this problem into a unified optimization. To address this optimization, we then design an effective NS-Alternating algorithm and prove it converges to KKT points. To further speed up MASTER, we propose a scalable framework, namely MASTER+. The core idea is to group accounts into clusters and then perform MASTER in each cluster in parallel. Specifically, we design an efficient Augmented Pre-Embedding model and Balance-aware Fuzzy Clustering algorithm for the high efficiency and the high accuracy. Extensive experiments demonstrate that both MASTER and MASTER+ outperform the state-of-the-art approaches. Moreover, MASTER+ inherits the effectiveness of MASTER and enjoys higher efficiency."
654,,Multi-Label Truth Inference for Crowdsourcing Using Mixture Models.,"Jing Zhang 0015,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2019.2951668,TKDE,2021,"Crowdsourcing,Probabilistic logic,Correlation,Task analysis,Reliability,Inference algorithms,Hidden Markov models","When acquiring labels from crowdsourcing platforms, a task may be designed to include multiple labels and the values of each label may belong to a set of various distinct options, which is the so-called multi-class multi-label annotation. To improve the quality of labels, requesters usually let one task be independently completed by a group of heterogeneous crowdsourced workers. Then, the true values of the multiple labels of each task are inferred from these repeated noisy labels. In this paper, we propose two novel probabilistic models MCMLI and MCMLD to address the multi-class multi-label inference problem in crowdsourcing. MCMLI assumes that the labels of each task are mutually independent and MCMLD utilizes a mixture of multiple independently multinoulli distributions to capture the correlation among the labels. Both models can jointly infer multiple true labels of each instance as well as estimate the reliability of crowdsourced workers modeled by a set of confusion matrices with an expectation–maximization algorithm. Experiments with three typical crowdsourcing scenarios and a real-world dataset show that our proposed models significantly outperform existing competitive alternatives. When the labels are strongly correlated, MCMLD substantially outperforms MCMLI. Furthermore, our models can be easily simplified to the one-coin models, which show more advantageous when errors are uniformly distributed, or labels are sparse."
655,,IGE+ - A Framework for Learning Node Embeddings in Interaction Graphs.,"Yao Zhang 0009,Yun Xiong,Xiangnan Kong,Zhuang Niu,Yangyong Zhu",https://doi.org/10.1109/TKDE.2019.2940459,TKDE,2021,"Task analysis,Stock markets,Bipartite graph,Image edge detection,Encoding,Investment,Computer science","Node embedding techniques have gained prominence since they produce continuous and low-dimensional features, which are effective for various tasks. Most existing approaches learn node embeddings by exploring the structure of networks and are mainly focused on static non-attributed graphs. However, many real-world applications, such as stock markets and public review websites, involve bipartite graphs with dynamic and attributed edges, called attributed interaction graphs. Different from conventional graph data, attributed interaction graphs involve two kinds of entities (e.g. investors/stocks and users/businesses) and edges of temporal interactions with attributes (e.g. transactions and reviews). In this paper, we study the problem of node embedding in attributed interaction graphs. Learning embeddings in interaction graphs is highly challenging due to the dynamics and heterogeneous attributes of edges. Different from conventional static graphs, in attributed interaction graphs, each edge can have totally different meanings when the interaction is at different times or associated with different attributes. To tackle the above challenges, we introduce the temporal dependency and conditional proximity, which are two fundamental characteristics of interaction graphs. Then, we propose a deep node embedding method called IGE+ (Interaction Graph Embedding+). By preserving these two characteristics, IGE+ is able to produce effective node embeddings in interaction graphs. We evaluate our proposed method and various comparing methods on four real-world datasets. The experimental results prove the effectiveness of the learned embeddings by IGE+ on both node-based and edge-based tasks."
656,,Leveraging Implicit Relative Labeling-Importance Information for Effective Multi-Label Learning.,"Min-Ling Zhang,Qian-Wen Zhang,Jun-Peng Fang,Yu-Kun Li,Xin Geng",https://doi.org/10.1109/TKDE.2019.2951561,TKDE,2021,"Training,Predictive models,Semantics,Reliability,Symmetric matrices,Labeling,Standards","Multi-label learning deals with training examples each represented by a single instance while associated with multiple class labels, and the task is to train a predictive model which can assign a set of proper labels for the unseen instance. Existing approaches employ the common assumption of equal labeling-importance, i.e., all associated labels are regarded to be relevant to the training instance while their 
<i>relative</i>
 importance in characterizing its semantics are not differentiated. Nonetheless, this common assumption does not reflect the fact that the importance degree of each relevant label is generally different, though the importance information is not directly accessible from the training examples. In this article, we show that it is beneficial to leverage the implicit 
<i>relative labeling-importance</i>
 (RLI) information to help induce multi-label predictive model with strong generalization performance. Specifically, RLI degrees are formalized as multinomial distribution over the label space, which can be estimated by either global label propagation procedure or local 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-nearest neighbor reconstruction. Correspondingly, the multi-label predictive model is induced by fitting modeling outputs with estimated RLI degrees along with multi-label empirical loss regularization. Extensive experiments clearly validate that leveraging implicit RLI information serves as a favorable strategy to achieve effective multi-label learning."
657,,Flexible Auto-Weighted Local-Coordinate Concept Factorization - A Robust Framework for Unsupervised Clustering.,"Zhao Zhang 0001,Yan Zhang 0053,Sheng Li 0001,Guangcan Liu,Dan Zeng 0001,Shuicheng Yan,Meng Wang 0001",https://doi.org/10.1109/TKDE.2019.2940576,TKDE,2021,"Encoding,Manifolds,Data models,Image reconstruction,Matrix decomposition,Computer science,Laplace equations","Concept Factorization (CF) and its variants may produce inaccurate representation and clustering results due to the sensitivity to noise, hard constraint on the reconstruction error, and pre-obtained approximate similarities. To improve the representation ability, a novel unsupervised Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF) framework is proposed for clustering high-dimensional data. Specifically, RFA-LCF integrates the robust flexible CF by clean data space recovery, robust sparse local-coordinate coding, and adaptive weighting into a unified model. RFA-LCF improves the representations by enhancing the robustness of CF to noise and errors, providing a flexible constraint on the reconstruction error and optimizing the locality jointly. For robust learning, RFA-LCF clearly learns a sparse projection to recover the underlying clean data space, and then the flexible CF is performed in the projected feature space. RFA-LCF also uses a L2,1-norm based flexible residue to encode the mismatch between the recovered data and its reconstruction, and uses the robust sparse local-coordinate coding to represent data using a few nearby basis concepts. For auto-weighting, RFA-LCF jointly preserves the manifold structures in the basis concept space and new coordinate space in an adaptive manner by minimizing the reconstruction errors on clean data, anchor points and coordinates. By updating the local-coordinate preserving data, basis concepts and new coordinates alternately, the representation abilities can be potentially improved. Extensive results on public databases show that RFA-LCF delivers the state-of-the-art clustering results compared with other related methods."
658,,Online Adaptive Asymmetric Active Learning With Limited Budgets.,"Yifan Zhang 0004,Peilin Zhao,Shuaicheng Niu,Qingyao Wu,Jiezhang Cao,Junzhou Huang,Mingkui Tan",https://doi.org/10.1109/TKDE.2019.2955078,TKDE,2021,"Optimization,Indexes,Adaptation models,Manganese,Sensitivity,Correlation","Online Active Learning (OAL) aims to manage unlabeled datastream by selectively querying the label of data. OAL is applicable to many real-world problems, such as anomaly detection in health-care and finance. In these problems, there are two key challenges: the query budget is often limited; the ratio between classes is highly imbalanced. In practice, it is quite difficult to handle imbalanced unlabeled datastream when only a limited budget of labels can be queried for training. To solve this, previous OAL studies adopt either asymmetric losses or queries (an isolated asymmetric strategy) to tackle the imbalance, and use first-order methods to optimize the cost-sensitive measure. However, the isolated strategy limits their performance in class imbalance, while first-order methods restrict their optimization performance. In this article, we propose a novel Online Adaptive Asymmetric Active learning algorithm, based on a new asymmetric strategy (merging both asymmetric losses and queries strategies), and second-order optimization. We theoretically analyze its mistake bound and cost-sensitive metric bounds. Moreover, to better balance performance and efficiency, we enhance our algorithm via a sketching technique, which significantly accelerates the computational speed with quite slight performance degradation. Promising results demonstrate the effectiveness and efficiency of the proposed methods."
659,,Automatic Irregularity-Aware Fine-Grained Workload Partitioning on Integrated Architectures.,"Feng Zhang 0007,Jidong Zhai,Bo Wu 0002,Bingsheng He,Wenguang Chen,Xiaoyong Du 0001",https://doi.org/10.1109/TKDE.2019.2940184,TKDE,2021,"Graphics processing units,Computer architecture,Instruction sets,Sparse matrices,Central Processing Unit,Optimization,Load modeling","The integrated architecture that features both CPU and GPU on the same die is an emerging and promising architecture for fine-grained CPU-GPU collaboration. However, the integration also brings forward several programming and system optimization challenges, especially for irregular applications such as graph processing. The complex interplay between heterogeneity and irregularity leads to very low processor utilization of running irregular applications on integrated architectures. Furthermore, fine-grained co-processing on the CPU and GPU is still an open problem. Particularly, in this paper, we show that the previous workload partitioning for CPU-GPU co-processing is far from ideal in terms of resource utilization and performance. To solve this problem, we propose a system software called FinePar, which considers architectural differences of the CPU and GPU and leverages fine-grained collaboration enabled by integrated architectures. Through irregularity-aware performance modeling and online auto-tuning, FinePar partitions irregular workloads and achieves both device-level and thread-level load balance. We evaluate FinePar with eight irregular applications in graphs and sparse matrices on two integrated architectures and compare it with state-of-the-art partitioning approaches. Results show that FinePar demonstrates better resource utilization and achieves an average of 1.6X speedup over the optimal coarse-grained partitioning method."
660,,A Recursive Regularization Based Feature Selection Framework for Hierarchical Classification.,"Hong Zhao,Qinghua Hu,Pengfei Zhu,Yu Wang 0106,Ping Wang",https://doi.org/10.1109/TKDE.2019.2960251,TKDE,2021,"Feature extraction,Task analysis,Semantics,Sparse matrices,Prediction algorithms,Software,Machine learning","The sizes of datasets in terms of the number of samples, features, and classes have dramatically increased in recent years. In particular, there usually exists a hierarchical structure among class labels as hundreds of classes exist in a classification task. We call these tasks hierarchical classification, and hierarchical structures are helpful for dividing a very large task into a collection of relatively small subtasks. Various algorithms have been developed to select informative features for flat classification. However, these algorithms ignore the semantic hyponymy in the directory of hierarchical classes, and select a uniform subset of the features for all classes. In this paper, we propose a new feature selection framework with recursive regularization for hierarchical classification. This framework takes the hierarchical information of the class structure into account. In contrast to flat feature selection, we select different feature subsets for each node in a hierarchical tree structure with recursive regularization. The proposed framework uses parent-child, sibling, and family relationships for hierarchical regularization. By imposing 
<inline-formula><tex-math notation=""LaTeX"">$\ell _{2,1}$</tex-math></inline-formula>
-norm regularization to different parts of the hierarchical classes, we can learn a sparse matrix for the feature ranking at each node. Extensive experiments on public datasets demonstrate the effectiveness and efficiency of the proposed algorithms."
661,,Predicting Taxi and Uber Demand in Cities - Approaching the Limit of Predictability.,"Kai Zhao,Denis Khryashchev,Huy T. Vo",https://doi.org/10.1109/TKDE.2019.2955686,TKDE,2021,"Three-dimensional displays,Shape,Measurement,Detectors,Feature extraction,Object recognition,Clutter","Time series prediction has wide applications ranging from stock price prediction, product demand estimation to economic forecasting. In this article, we treat the taxi and Uber demand in each location as a time series, and reduce the taxi and Uber demand prediction problem to a time series prediction problem. We answer two key questions in this area. First, time series have different temporal regularity. Some are easy to be predicted and others are not. Given a predictive algorithm such as LSTM (deep learning) or ARIMA (time series), what is the maximum prediction accuracy that it can reach if it captures all the temporal patterns of that time series? Second, given the maximum predictability, which algorithm could approach the upper bound in terms of prediction accuracy? To answer these two question, we use temporal-correlated entropy to measure the time series regularity and obtain the maximum predictability. Testing with 14 million data samples, we find that the deep learning algorithm is not always the best algorithm for prediction. When the time series has a high predictability a simple Markov prediction algorithm (training time 0.5s) could outperform a deep learning algorithm (training time 6 hours). The predictability can help determine which predictor to use in terms of the accuracy and computational costs. We also find that the Uber demand is easier to be predicted compared the taxi demand due to different cruising strategies as the former is demand driven with higher temporal regularity."
662,,SQL-G - Efficient Graph Analytics by SQL.,"Kangfei Zhao,Jiao Su,Jeffrey Xu Yu,Hao Zhang",https://doi.org/10.1109/TKDE.2019.2950620,TKDE,2021,"Structured Query Language,Sparks,Algebra,Engines,Programming,Distributed databases,Data processing","Querying graphs and conducting graph analytics become important in data processing since many real applications are dealing with massive graphs, such as online social networks, Semantic Web, knowledge graphs, etc. Over the years, many distributed graph processing systems have been developed to support graph analytics using various programming models, and many graph querying languages have been proposed. A natural question that arises is how to integrate graph data and traditional non-graph data in a distributed system for users to conduct analytics. There are two issues. One issue is related to expressiveness on how to specify graph analytics as well as data analytics by a querying language. The other issue is related to efficiency on how to process analytics in a distributed system. For the first issue, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 is a best candidate, since 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 is a well-accepted language for data processing. We concentrate on 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 for graph analytics. Our early work shows that graph analytics can be supported by 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 in a way from “semiring + while” to “relational algebra + while” via the enhanced recursive 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 queries. In this article, we focus on the second issue on how to process such enhanced recursive 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 queries based on the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">GAS</i>
 (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Gather</i>
-
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Apply</i>
-
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Scatter</i>
) model under which efficient graph processing systems can be developed. To demonstrate the efficiency, we implemented a system by tightly coupling 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Spark</i>
 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 and 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">GraphX</i>
 on 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Spark</i>
 which is one of the most popular in-memory data-flow processing platforms. First, we enhance 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Spark</i>
 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 by adding the capability of supporting the enhanced recursive 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 queries for graph analytics. In this regard, graph analytics can be processed using a distributed 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 engine alone. Second, we further propose new transformation rules to optimize/translate the operations for recursive 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 queries to the operations by 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">GraphX</i>
. In this regard, graph analytics by 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SQL</i>
 can be processed in a similar way as done by a distributed graph processing system using the APIs provided by the system. We conduct extensive performance studies to test graph analytics using large real graphs. We show that our approach can achieve similar or even higher efficiency, in comparison to the built-in graph algorithms in the existing graph processing systems."
663,,Distribution-Free One-Pass Learning.,"Peng Zhao 0006,Xinqiang Wang,Siyu Xie,Lei Guo 0001,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2019.2937078,TKDE,2021,"Data models,Random variables,Proposals,Training,Prediction algorithms,Task analysis,Compressed sensing","In many large-scale machine learning applications, data are accumulated over time, and thus, an appropriate model should be able to update in an online style. In particular, it would be ideal to have a storage independent from the data volume, and scan each data item only once. Meanwhile, the data distribution usually changes during the accumulation procedure, making distribution-free one-pass learning a challenging task. In this paper, we propose a simple yet effective approach for this task, without requiring prior knowledge about the change, where every data item can be discarded once scanned. We also present a variant for high-dimensional situations, by exploiting compressed sensing to reduce computational and storage complexity. Theoretical analysis shows that our proposal converges under mild assumptions, and the performance is validated on both synthetic and real-world datasets."
664,,Photo2Trip - Exploiting Visual Contents in Geo-Tagged Photos for Personalized Tour Recommendation.,"Pengpeng Zhao,Chengfeng Xu,Yanchi Liu,Victor S. Sheng,Kai Zheng 0001,Hui Xiong 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2943854,TKDE,2021,"Visualization,Collaboration,Probabilistic logic,Training,Planning,Correlation,Task analysis","Recently accumulated massive amounts of geo-tagged photos provide an excellent opportunity to understand human behaviors and can be used for personalized tour recommendation. However, no existing work has considered the visual content information in these photos for tour recommendation. We believe the visual features of photos provide valuable information on measuring user / Point-of-Interest (POI) similarities, which is challenging due to data sparsity. To this end, in this paper, we propose a visual feature enhanced tour recommender system, named `Photo2Trip', to utilize the visual contents and collaborative filtering models for recommendation. Specifically, we propose a Visual-enhanced Probabilistic Matrix Factorization model (VPMF), which integrates visual features into the collaborative filtering model, to learn user interests by leveraging the historical travel records. We then extend VPMF to End-to-End training framework to incorporate users (POIs) latent factors into the learning process of the visual content of photos, which generalizes the applicability of the proposed VPMF framework in tour recommendation. Extensive empirical studies verify that our proposed visual-enhanced personalized tour recommendation method outperforms other benchmark methods in terms of recommendation accuracy. The results also show that visual features are effective in alleviating the data sparsity and cold start problems on personalized tour recommendation."
665,,Ranking Users in Social Networks with Motif-Based PageRank.,"Huan Zhao,Xiaogang Xu,Yangqiu Song,Dik Lun Lee,Zhao Chen,Han Gao",https://doi.org/10.1109/TKDE.2019.2953264,TKDE,2021,"Complex networks,Task analysis,Directed graphs,Indexes,Twitter,Computer science","PageRank has been widely used to measure the authority or the influence of a user in social networks. However, conventional PageRank only makes use of edge-based relations, which represent first-order relations between two connected nodes. It ignores higher-order relations that may exist between nodes. In this article, we propose a novel framework, motif-based PageRank (MPR), to incorporate higher-order relations into the conventional PageRank computation. Motifs are subgraphs consisting of a small number of nodes. We use motifs to capture higher-order relations between nodes in a network and introduce two methods, one linear and one non-linear, to combine first-order and higher-order relations in PageRank computation. We conduct extensive experiments on three real-world networks, namely, DBLP, Epinions, and Ciao. We study different types of motifs, including 3-node simple and anchor motifs, 4-node and 5-node motifs. Besides using single motif, we also run MPR with ensemble of multiple motifs. We also design a learning task to evaluate the abilities of authority prediction with motif-based features. All experimental results demonstrate that MPR can significantly improve the performance of user ranking in social networks compared to the baseline methods."
666,,Simultaneous Clustering and Optimization for Evolving Datasets.,"Yawei Zhao,En Zhu,Xinwang Liu,Chang Tang,Deke Guo,Jianping Yin",https://doi.org/10.1109/TKDE.2019.2923239,TKDE,2021,"Optimization,Computational modeling,Convex functions,Computational efficiency,Task analysis,Predictive models","Simultaneous clustering and optimization (SCO) has recently drawn much attention due to its wide range of practical applications. Many methods have been previously proposed to solve this problem and obtain the optimal model. However, when a dataset evolves over time, those existing methods have to update the model frequently to guarantee accuracy; such updating is computationally infeasible. In this paper, we propose a new formulation of SCO to handle evolving datasets. Specifically, we propose a new variant of the alternating direction method of multipliers (ADMM) to solve this problem efficiently. The guarantee of model accuracy is analyzed theoretically for two specific tasks: ridge regression and convex clustering. Extensive empirical studies confirm the effectiveness of our method."
667,,Multi-Level Attention Networks for Multi-Step Citywide Passenger Demands Prediction.,"Xian Zhou,Yanyan Shen,Linpeng Huang,Tianzi Zang,Yanmin Zhu",https://doi.org/10.1109/TKDE.2019.2948005,TKDE,2021,"Predictive models,Neural networks,Spatiotemporal phenomena,Data models,Correlation,Logic gates","For the emerging mobility-on-demand services, it is of great significance to predict passenger demands based on historical mobility trips towards better vehicle distribution. Prior works have focused on predicting next-step passenger demands at selected locations or hotspots. However, we argue that multi-step citywide passenger demands encapsulate both time-varying demand trends and global statuses, and hence are more beneficial to avoiding demand-service mismatching and developing effective vehicle distribution/scheduling strategies. Furthermore, we find that adaptations of single-step methods are unable to achieve robust prediction with high accuracy for further steps. In this paper, we propose an end-to-end deep neural network model to the prediction task. We employ an encoder-decoder framework based on convolutional and ConvLSTM units to identify complex features that capture spatiotemporal influence and pickup-dropoff interactions on citywide passenger demands. We introduce a multi-level attention model (global attention and temporal attention) to emphasize the effects of latent citywide mobility regularities and capture relevant temporal dependencies. We evaluate our proposed method using real-world mobility trips (taxis and bikes) and the experimental results show that our method achieves higher prediction accuracy than the state-of-the-art approaches."
668,1,A Privacy-Preserving Distributed Contextual Federated Online Learning Framework with Big Data Support in Social Recommender Systems.,"Pan Zhou,Kehao Wang 0001,Linke Guo,Shimin Gong,Bolong Zheng",https://doi.org/10.1109/TKDE.2019.2936565,TKDE,2021,"Big Data,Privacy,Recommender systems,Social networking (online),Context modeling,Prediction algorithms,Differential privacy","Nowadays, the booming demand of big data analytics and the constraints of computational ability and network bandwidth have made it difficult for a stand-alone agent/service provider to provide suitable information for every user from the large volume online data within the limited time. To handle this challenge, a recommender system (RS) can call in a group of agents to collaborate to learn users' preference and taste, which is known as a distributed recommender system (DRS). DRSs can improve the accuracy of a traditional RS by requesting agents to share information with each other. However, it is challenging for DRSs to make personalized recommendations for each user due to the large amount of candidates. In addition, information sharing among agents raises a privacy concern. Thus, we propose a privacy-preserving DRS in this paper, and then model each service provider as a distributed online learner with context-awareness. Service providers collaborate to make personalized recommendations by learning users' preferences according to the user context and users' history behaviors. We adopt the federated learning framework to help train a high quality privacy- preserving centralized model over a large number of distributed agents which is probably unreliable with relatively slow network connections. To handle big data scenario, we build an item-cluster tree to deal with online and increasing datasets from top to the bottom. We further consider the structure of social network and present an efficient algorithm to avoid more performance loss adaptively. Theoretical proofs show that our proposed algorithm can achieve sublinear regret and differential privacy protection simultaneously for service providers and users. Numerical results confirm that our novel framework can handle increasing big datasets and strike a trade-off between privacy-preserving level and the prediction accuracy."
669,,Matrix Profile IX - Admissible Time Series Motif Discovery With Missing Data.,"Yan Zhu 0014,Abdullah Mueen 0001,Eamonn J. Keogh",https://doi.org/10.1109/TKDE.2019.2950623,TKDE,2021,"Time series analysis,Time measurement,Data mining,Computer science,Interpolation,Approximation algorithms,Measurement uncertainty","The discovery of time series motifs has emerged as one of the most useful primitives in time series data mining. Researchers have shown its utility for exploratory data mining, summarization, visualization, segmentation, classification, clustering, and rule discovery. Although there has been more than a decade of extensive research, there is still no technique to allow the discovery of time series motifs in the presence of missing data, despite the well-documented ubiquity of missing data in scientific, industrial, and medical datasets. In this work, we introduce a technique for motif discovery in the presence of missing data. We formally prove that our method is 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">admissible</i>
, producing no false negatives. We also show that our method can “piggy-back” off the fastest known motif discovery method with a small constant factor time/space overhead. We will demonstrate our approach on diverse datasets with varying amounts of missing data."
670,,A Data-Driven Sequential Localization Framework for Big Telco Data.,"Fangzhou Zhu,Mingxuan Yuan,Xike Xie,Ting Wang,Shenglin Zhao,Weixiong Rao,Jia Zeng",https://doi.org/10.1109/TKDE.2019.2961657,TKDE,2021,"Hidden Markov models,Knowledge engineering,Trajectory,Tuning,Radio frequency,Data engineering,Data centers","The proliferation of telco networks and mobile terminals brings the accumulation of tremendous amounts of measure report(MR) data at a rapid pace. The MR data is generated by mobile objects while connecting to data services and is stored in backend data centers. To geo-tag or localize such MR data is believed to have a profound effect on the analytics and optimizations of telco and traffic networks. However, MR records are of noisy and partial observations regarding to mobile objects’ geo-locations and hence pose challenges to accurate telco data localization. There have been quite a few attempts. Single-point localization methods map a MR record to a location, but come out with limited accuracies due to the ignorance of spatiotemporal coherence of successive MR records. Recent efforts on sequential localization techniques alleviate this by mapping a sequence of MR records to a trajectory. However, existing solutions are often with assumptions on specific models, e.g., mobility and signal strength distributions, or priori knowledge on topology space, e.g., road networks, limiting the deployment in practice. To this end, we propose a data-driven framework to tackle the challenges in sequential telco localization. We solely use raw MR records and a public third-party GPS dataset for the learning of the correlations between mobile objects’ locations and MR records, requiring no model assumptions and priori knowledge. To handle the data-intensive workloads during the learning process, we use materialized views for efficient online localization and light-weighted indexing techniques for periodical parameters tuning, in order to improve the efficiency and scalability. Results on real data show that our solution achieves 58.8 percent improvement in median localization errors compared with state-of-art sequential localization techniques that require hypothesis models and priori knowledge, making our solution superior in terms of effectiveness, efficiency, and employability."
671,,Efficient Utilization of Missing Data in Cost-Sensitive Learning.,"Xiaofeng Zhu 0001,Jianye Yang,Chengyuan Zhang,Shichao Zhang",https://doi.org/10.1109/TKDE.2019.2956530,TKDE,2021,"Data models,Analytical models,Machine learning,Decision trees,Machine learning algorithms,Knowledge discovery,Computer science","Different from previous imputation methods which impute missing values in the incomplete samples by using the information in the complete samples, this paper proposes a Date-drive Incremental imputation Model, DIM for short, which uses all available information in the data set to impute missing values economically, effectively, orderly, and iteratively. To this end, we propose a scoring rule to rank the missing features by taking into account both the economical criterion and the effective imputation information. The economical criterion takes both the imputation cost and the discriminative ability of the feature into account, while the effective imputation information enables to use all observed information in the data set including the imputed missing values to impute the left missing values. During the imputation process, our DIM first detects the neednot-impute samples for reducing the imputation cost and noise, and then selects the missing features with the top rank to impute first. The imputation process orderly imputes the missing features until all missing values are imputed or the imputation cost is exhausted. Experimental results on UCI data sets demonstrated the advantages of our proposed DIM, compared to the comparison methods, in terms of prediction accuracy and classification accuracy."
672,,Knowledge Base Reasoning with Convolutional-Based Recurrent Neural Networks.,"Qiannan Zhu,Xiaofei Zhou,Jianlong Tan,Li Guo 0001",https://doi.org/10.1109/TKDE.2019.2951103,TKDE,2021,"Cognition,Recurrent neural networks,Knowledge based systems,Feature extraction,Correlation,Computer architecture,Convolutional neural nets","Recurrent neural network(RNN) has achieved remarkable performances in complex reasoning on knowledge bases, which usually takes as inputs vector embeddings of relations along a path between an entity pair. However, it is insufficient to extract local correlations of a path due to RNN is better at capturing global sequential information of a path. In this paper, we take full advantages of convolutional neural network that can effectively extract local features, and propose a convolutional-based RNN architecture denoted as C-RNN to perform reasoning. C-RNN first utilizes CNN to extract local high-level correlation features of a path, and then feeds the correlation features into recurrent neural network to model the path representation. Our C-RNN architecture is adaptable to obtain not only local features but also global sequential features of a path. Based on C-RNN architecture, we devise two models, the unidirectional C-RNN and bidirectional C-RNN. We empirically evaluate them on a large-scale FreeBase+ClueWeb prediction task. Experimental results show that C-RNN models achieve state-of-the-art predictive performance."
673,,DynaMo - Dynamic Community Detection by Incrementally Maximizing Modularity.,"Di Zhuang,J. Morris Chang,Mingchen Li",https://doi.org/10.1109/TKDE.2019.2951419,TKDE,2021,"Heuristic algorithms,Generators,Facebook,Detection algorithms,Clustering algorithms,Machine learning algorithms","Community detection is of great importance for online social network analysis. The volume, variety and velocity of data generated by today's online social networks are advancing the way researchers analyze those networks. For instance, real-world networks, such as Facebook, LinkedIn and Twitter, are inherently growing rapidly and expanding aggressively over time. However, most of the studies so far have been focusing on detecting communities on the static networks. It is computationally expensive to directly employ a well-studied static algorithm repeatedly on the network snapshots of the dynamic networks. We propose DynaMo, a novel modularity-based dynamic community detection algorithm, aiming to detect communities of dynamic networks as effective as repeatedly applying static algorithms but in a more efficient way. DynaMo is an adaptive and incremental algorithm, which is designed for incrementally maximizing the modularity gain while updating the community structure of dynamic networks. In the experimental evaluation, a comprehensive comparison has been made among DynaMo, Louvain (static) and 5 other dynamic algorithms. Extensive experiments have been conducted on 6 real-world networks and 10,000 synthetic networks. Our results show that DynaMo outperforms all the other 5 dynamic algorithms in terms of the effectiveness, and is 2 to 5 times (by average) faster than Louvain algorithm."
14,,Code Renewability for Native Software Protection.,"Bert Abrath,Bart Coppens,Jens Van den Broeck,Brecht Wyseur,Alessandro Cabutto,Paolo Falcarin,Bjorn De Sutter",https://doi.org/10.1145/3404891,TOPS,2020,[],
15,,NoiSense Print - Detecting Data Integrity Attacks on Sensor Measurements Using Hardware-based Fingerprints.,"Chuadhry Mujeeb Ahmed,Aditya P. Mathur,Martín Ochoa",https://doi.org/10.1145/3410447,TOPS,2020,[],
16,,The Tip of the Iceberg - On the Merits of Finding Security Bugs.,"Nikolaos Alexopoulos,Sheikh Mahbub Habib,Steffen Schulz 0001,Max Mühlhäuser",https://doi.org/10.1145/3406112,TOPS,2020,[],
17,,Key Negotiation Downgrade Attacks on Bluetooth and Bluetooth Low Energy.,"Daniele Antonioli,Nils Ole Tippenhauer,Kasper Rasmussen",https://doi.org/10.1145/3394497,TOPS,2020,[],
18,,A Case for Feedforward Control with Feedback Trim to Mitigate Time Transfer Attacks.,"Fatima M. Anwar,Mani B. Srivastava",https://doi.org/10.1145/3382503,TOPS,2020,[],
19,,"Discriminative Power of Typing Features on Desktops, Tablets, and Phones for User Identification.","Amith K. Belman,Vir V. Phoha",https://doi.org/10.1145/3377404,TOPS,2020,[],
20,,Privado - Privacy-preserving Group-based Advertising Using Multiple Independent Social Network Providers.,"Sanaz Taheri Boshrooyeh,Alptekin Küpçü,Öznur Özkasap",https://doi.org/10.1145/3386154,TOPS,2020,[],
21,,"On the Security and Usability Implications of Providing Multiple Authentication Choices on Smartphones - The More, the Better?","Geumhwan Cho,Jun Ho Huh,Soolin Kim,Junsung Cho,Heesung Park,Yenah Lee,Konstantin Beznosov,Hyoungshick Kim",https://doi.org/10.1145/3410155,TOPS,2020,[],
22,,The Seven Deadly Sins of the HTML5 WebAPI - A Large-scale Study on the Risks of Mobile Sensor-based Attacks.,"Michalis Diamantaris,Francesco Marcantoni,Sotiris Ioannidis,Jason Polakis",https://doi.org/10.1145/3403947,TOPS,2020,[],
23,,A Multi-server ORAM Framework with Constant Client Bandwidth Blowup.,"Thang Hoang,Attila A. Yavuz,Jorge Guajardo",https://doi.org/10.1145/3369108,TOPS,2020,[],
24,,Adaptive Cyber Defense Against Multi-Stage Attacks Using Learning-Based POMDP.,"Zhisheng Hu,Minghui Zhu,Peng Liu 0005",https://doi.org/10.1145/3418897,TOPS,2020,[],
25,,Quantum Leap and Crash - Searching and Finding Bias in Quantum Random Number Generators.,"Darren Hurley-Smith,Julio Hernandez-Castro",https://doi.org/10.1145/3398726,TOPS,2020,[],
26,,Measuring and Analysing the Chain of Implicit Trust - A Study of Third-party Resources Loading.,"Muhammad Ikram,Rahat Masood,Gareth Tyson,Mohamed Ali Kâafar,Noha Loizon,Roya Ensafi",https://doi.org/10.1145/3380466,TOPS,2020,[],
27,,The Dilemma of User Engagement in Privacy Notices - Effects of Interaction Modes and Habituation on User Attention.,"Farzaneh Karegar,John Sören Pettersson,Simone Fischer-Hübner",https://doi.org/10.1145/3372296,TOPS,2020,[],
28,,Mimicry Attacks on Smartphone Keystroke Authentication.,"Hassan Khan,Urs Hengartner,Daniel Vogel 0001",https://doi.org/10.1145/3372420,TOPS,2020,[],
29,,A Formal Approach to Physics-based Attacks in Cyber-physical Systems.,"Ruggero Lanotte,Massimo Merro,Andrei Munteanu,Luca Viganò 0001",https://doi.org/10.1145/3373270,TOPS,2020,[],
30,,A Study on the Use of Checksums for Integrity Verification of Web Downloads.,"Alexandre Meylan,Mauro Cherubini,Bertil Chapuis,Mathias Humbert,Igor Bilogrevic,Kévin Huguenin",https://doi.org/10.1145/3410154,TOPS,2020,[],
31,,Following Passive DNS Traces to Detect Stealthy Malicious Domains Via Graph Inference.,"Mohamed Nabeel,Issa M. Khalil,Bei Guan,Ting Yu",https://doi.org/10.1145/3401897,TOPS,2020,[],
32,,The Security of Lazy Users in Out-of-Band Authentication.,"Moni Naor,Lior Rotem,Gil Segev 0001",https://doi.org/10.1145/3377849,TOPS,2020,[],
33,,"Build It, Break It, Fix It - Contesting Secure Development.","James Parker,Michael Hicks 0001,Andrew Ruef,Michelle L. Mazurek,Dave Levin,Daniel Votipka,Piotr Mardziel,Kelsey R. Fulton",https://doi.org/10.1145/3383773,TOPS,2020,[],
34,,Efficient Authorization of Graph-database Queries in an Attribute-supporting ReBAC Model.,"Syed Zain R. Rizvi,Philip W. L. Fong",https://doi.org/10.1145/3401027,TOPS,2020,[],
35,,Proactively Identifying Emerging Hacker Threats from the Dark Web - A Diachronic Graph Embedding Framework (D-GEF).,"Sagar Samtani,Hongyi Zhu,Hsinchun Chen",https://doi.org/10.1145/3409289,TOPS,2020,[],
36,,Formal Analysis of Mobile Multi-Factor Authentication with Single Sign-On Login.,"Giada Sciarretta,Roberto Carbone,Silvio Ranise,Luca Viganò 0001",https://doi.org/10.1145/3386685,TOPS,2020,[],
37,,The System That Cried Wolf - Sensor Security Analysis of Wide-area Smoke Detectors for Critical Infrastructure.,"Hocheol Shin,Juhwan Noh,Dohyun Kim,Yongdae Kim",https://doi.org/10.1145/3393926,TOPS,2020,[],
38,,&quot;So if Mr Blue Head here clicks the link...&quot; Risk Thinking in Cyber Security Decision Making.,"Benjamin Shreeve,Joseph Hallett,Matthew Edwards 0001,Pauline Anthonysamy,Sylvain Frey,Awais Rashid",https://doi.org/10.1145/3419101,TOPS,2020,[],
39,1,CrowdPrivacy - Publish More Useful Data with Less Privacy Exposure in Crowdsourced Location-Based Services.,"Fang-Jing Wu,Tie Luo",https://doi.org/10.1145/3375752,TOPS,2020,[],
40,,Using Generative Adversarial Networks to Break and Protect Text Captchas.,"Guixin Ye,Zhanyong Tang,Dingyi Fang,Zhanxing Zhu,Yansong Feng,Pengfei Xu 0003,Xiaojiang Chen,Jungong Han,Zheng Wang 0001",https://doi.org/10.1145/3378446,TOPS,2020,[],
41,,Exploiting Behavioral Side Channels in Observation Resilient Cognitive Authentication Schemes.,"Benjamin Zi Hao Zhao,Hassan Jameel Asghar,Mohamed Ali Kâafar,Francesca Trevisan,Haiyue Yuan",https://doi.org/10.1145/3414844,TOPS,2020,[],
674,,Reference-Based Framework for Spatio-Temporal Trajectory Compression and Query Processing.,"Kai Zheng 0001,Yan Zhao 0008,Defu Lian,Bolong Zheng,Guanfeng Liu 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2914449,TKDE,2020,"Trajectory,Query processing,Indexes,Measurement,Roads,Computer science,Compression algorithms","The pervasiveness of GPS-enabled devices and wireless communication technologies results in massive trajectory data, incurring expensive cost for storage, transmission, and query processing. To relieve this problem, in this paper we propose a novel framework for compressing trajectory data, REST (Reference-based Spatio-temporal trajectory compression), by which a raw trajectory is represented by concatenation of a series of historical (sub-)trajectories (called reference trajectories) that form the compressed trajectory within a given spatio-temporal deviation threshold. In order to construct a reference trajectory set that can most benefit the subsequent compression, we propose three kinds of techniques to select reference trajectories wisely from a large dataset such that the resulting reference set is more compact yet covering most footprints of trajectories in the area of interest. To address the computational issue caused by the large number of combinations of reference trajectories that may exist for resembling a given trajectory, we propose efficient greedy algorithms that run in the blink of an eye and dynamic programming algorithms that can achieve the optimal compression ratio. Compared to existing work on trajectory compression, our framework has few assumptions about data such as moving within a road network or moving with constant direction and speed, and better compression performance with fairly small spatio-temporal loss. In addition, by indexing the reference trajectories directly with an in-memory R-tree and building connections to the raw trajectories with inverted index, we develop an extremely efficient algorithm that can answer spatio-temporal range queries over trajectories in their compressed form. Extensive experiments on a real taxi trajectory dataset demonstrate the superiority of our framework over existing representative approaches in terms of both compression ratio and efficiency."
675,,Efficient Process Conformance Checking on the Basis of Uncertain Event-to-Activity Mappings.,"Han van der Aa,Henrik Leopold,Hajo A. Reijers",https://doi.org/10.1109/TKDE.2019.2897557,TKDE,2020,"Uncertainty,Adaptation models,Task analysis,Probabilistic logic,Information systems,Cryptography","Conformance checking enables organizations to automatically identify compliance violations based on the analysis of observed event data. A crucial requirement for conformance-checking techniques is that observed events can be mapped to normative process models used to specify allowed behavior. Without a mapping, it is not possible to determine if an observed event trace conforms to the specification or not. A considerable problem in this regard is that establishing a mapping between events and process model activities is an inherently uncertain task. Since the use of a particular mapping directly influences the conformance of an event trace to a specification, this uncertainty represents a major issue for conformance checking. To overcome this issue, we introduce a probabilistic conformance-checking technique that can deal with uncertain mappings. Our technique avoids the need to select a single mapping by taking the entire spectrum of possible mappings into account. A quantitative evaluation demonstrates that our technique can be applied on a considerable number of real-world processes where existing conformance-checking techniques fail."
676,,K-SPIN - Efficiently Processing Spatial Keyword Queries on Road Networks.,"Tenindra Abeywickrama,Muhammad Aamir Cheema,Arijit Khan 0001",https://doi.org/10.1109/TKDE.2019.2894140,TKDE,2020,"Roads,Indexing,Throughput,Delays,Search engines,Approximation algorithms","A significant proportion of all search volume consists of local searches. As a result, search engines must be capable of finding relevant results combining both spatial proximity and textual relevance with high query throughput. We observe that existing techniques answering these spatial keyword queries use keyword aggregated indexing, which has several disadvantages on road networks. We propose K-SPIN, a versatile framework that instead uses keyword separated indexing to delay and avoid expensive operations. At first glance, this strategy appears to have impractical pre-processing costs. However, by exploiting several useful observations, we make the indexing cost not only viable but also light-weight. For example, we propose a novel p-Approximate Network Voronoi Diagram (NVD) with one order of magnitude less space cost than exact NVDs. By carefully exploiting features of the K-SPIN framework, our query algorithms are up to two orders of magnitude more efficient than the state-of-the-art as shown in our experimental investigation on various queries, parameter settings, and real road network and keyword datasets."
677,,Mining Novel Multivariate Relationships in Time Series Data Using Correlation Networks.,"Saurabh Agrawal,Michael S. Steinbach,Daniel Boley,Snigdhansu Chatterjee,Gowtham Atluri,Anh The Dang,Stefan Liess,Vipin Kumar",https://doi.org/10.1109/TKDE.2019.2911681,TKDE,2020,"Time series analysis,Gain,Correlation,Meteorology,Eigenvalues and eigenfunctions,Neuroscience,Time measurement","In many domains, there is significant interest in capturing novel relationships between time series that represent activities recorded at different nodes of a highly complex system. In this paper, we introduce multipoles, a novel class of linear relationships between more than two time series. A multipole is a set of time series that have strong linear dependence among themselves, with the requirement that each time series makes a significant contribution to the linear dependence. We demonstrate that most interesting multipoles can be identified as cliques of negative correlations in a correlation network. Such cliques are typically rare in a real-world correlation network, which allows us to find almost all multipoles efficiently using a clique-enumeration approach. Using our proposed framework, we demonstrate the utility of multipoles in discovering new physical phenomena in two scientific domains: climate science and neuroscience. In particular, we discovered several multipole relationships that are reproducible in multiple other independent datasets and lead to novel domain insights."
678,,A Joint Two-Phase Time-Sensitive Regularized Collaborative Ranking Model for Point of Interest Recommendation.,"Mohammad Aliannejadi,Dimitrios Rafailidis,Fabio Crestani",https://doi.org/10.1109/TKDE.2019.2903463,TKDE,2020,"Collaboration,Data models,Motion pictures,Social networking (online),Urban areas,Sparse matrices,Heuristic algorithms","The popularity of location-based social networks (LBSNs) has led to a tremendous amount of user check-in data. Recommending points of interest (POIs) plays a key role in satisfying users needs in LBSNs. While recent work has explored the idea of adopting collaborative ranking (CR) for recommendation, there have been few attempts to incorporate temporal information for POI recommendation using CR. In this article, we propose a two-phase CR algorithm that incorporates the geographical influence of POIs and is regularized based on the variance of POIs popularity and users activities over time. The time-sensitive regularizer penalizes user and POIs that have been more time-sensitive in the past, helping the model to account for their long-term behavioral patterns while learning from user-POI interactions. Moreover, in the first phase, it attempts to rank visited POIs higher than the unvisited ones, and at the same time, apply the geographical influence. In the second phase, our algorithm tries to rank users favorite POIs higher on the recommendation list. Both phases employ a collaborative learning strategy that enables the model to capture complex latent associations from two different perspectives. Experiments on real-world datasets show that our proposed time-sensitive collaborative ranking model beats state-of-the-art POI recommendation methods."
679,,On the Usefulness of SQL-Query-Similarity Measures to Find User Interests.,"Natalia Arzamasova,Klemens Böhm,Bertrand Goldman,Christian Saaler,Martin Schäler",https://doi.org/10.1109/TKDE.2019.2913381,TKDE,2020,"Feature extraction,Relational databases,Extraterrestrial measurements,Companies,Clustering algorithms","In the sciences and elsewhere, the use of relational databases has become ubiquitous. An important challenge is finding hot spots of user interests. In principle, one can discover user interests by clustering the queries in the query log. Such a clustering requires a notion of query similarity. This, in turn, raises the question of what features of SQL queries are meaningful. We have studied the query representations proposed in the literature and corresponding similarity functions and have identified shortcomings of all of them. To overcome these limitations, we propose new similarity functions for SQL queries. They rely on the so-called access area of a query and, more specifically, on the overlap and the closeness of the access areas. We have carried out experiments systematically to compare the various similarity functions described in this article. The first series of experiments measures the quality of clustering and compares it to a ground truth. In the second series, we focus on the query log from the well-known SkyServer database. Here, a domain expert has interpreted various clusters by hand. We conclude that clusters obtained with our new measures of similarity seem to be good indicators of user interests."
680,,Member Behavior in Dynamic Online Communities - Role Affiliation Frequency Model.,"Alon Bartal,Gilad Ravid",https://doi.org/10.1109/TKDE.2019.2911067,TKDE,2020,"Analytical models,Social networking (online),Time-frequency analysis,Heuristic algorithms,Data models,Cultural differences,Logic gates","People's social life has become more embedded in dynamic online communities. Each online community can be viewed as a temporal online social network (OSN). The interaction level among OSN members leads to the emergence of dynamic social roles, which change and evolve over time, creating a sequence of temporal roles. These role sequences show diversity in the role-affiliation frequency of members. That diversity enables modeling the dynamic behaviors of individuals. This paper proposes a temporal role-affiliation frequency model (RAFM) which detects the time evolving roles of each member and analyzes her/his role-affiliation frequency to infer her/his latent behavior. Applying the RAFM to real interaction data, collected in four online communities, revealed the identity of influential members. In addition, members with similar temporal behavioral patterns were found to have similar latent behavior patterns. These patterns are manifested via similar role transitions in different OSNs whose temporal interaction rhythms were compatible. These two research findings contribute to OSN research and knowledge via improved understanding of member behavior online based on role-affiliation frequency and role transitions. Thus, member latent behavior can be inferred, and influential members can be identified."
681,,"Graph K-means Based on Leader Identification, Dynamic Game, and Opinion Dynamics.","Zhan Bu,Hui-Jia Li,Chengcui Zhang,Jie Cao 0001,Aihua Li,Yong Shi 0001",https://doi.org/10.1109/TKDE.2019.2903712,TKDE,2020,"Social networking (online),Indexes,Games,Optimization,Computational modeling,Economics,Cultural differences","With the explosion of social media networks, many modern applications are concerning about people's connections, which leads to the so-called social computing. An elusive question is to study how opinion communities form and evolve in real-world networks with great individual diversity and complex human connections. In this scenario, the classic K-means technique and its extended versions could not be directly applied, as they largely ignore the relationship among interactive objects. On the other side, traditional community detection approaches in statistical physics would be neither adequate nor fair: they only consider the network topological structure but ignore the heterogeneous-objects' attributive information. To this end, we attempt to model a realistic social media network as a discrete-time dynamical system, where the opinion matrix and the community structure could mutually affect each other. In this paper, community detection in social media networks is naturally formulated as a multi-objective optimization problem (MOOP), i.e., finding a set of densely connected components with similar opinion vectors. We propose a novel and powerful graph K-means framework, which is composed of three coupled phases in each discrete-time period. Specifically, the first phase uses a fast heuristic approach to identify those opinion leaders who have relatively high local reputation; the second phase adopts a novel dynamic game model to find the locally Pareto-optimal community structure; and the final phase employs a robust opinion dynamics model to simulate the evolution of the opinion matrix. We conduct a series of comprehensive experiments on real-world benchmark networks to validate the performance of GK-means through comparisons with the state-of-the-art graph clustering technologies."
682,,Similarity Search for Dynamic Data Streams.,"Marc Bury,Chris Schwiegelshohn,Mara Sorella",https://doi.org/10.1109/TKDE.2019.2916858,TKDE,2020,"Heuristic algorithms,Hash functions,Approximation algorithms,Indexes,Measurement,Knowledge engineering,Data engineering","Nearest neighbor searching systems are an integral part of many online applications, including but not limited to pattern recognition, plagiarism detection, and recommender systems. With increasingly larger data sets, scalability has become an important issue. Many of the most space and running time efficient algorithms are based on locality-sensitive hashing. Here, we view the data set as an n by lUl matrix where each row corresponds to one of n users and the columns correspond to items drawn from a universe U. The de-facto standard approach to quickly answer nearest neighbor queries on such a data set is usually a form of min-hashing. Not only is min-hashing very fast, but it is also space efficient and can be implemented in many computational models aimed at dealing with large data sets such as MapReduce and streaming. However, a significant drawback is that minhashing and related methods are only able to handle insertions to user profiles and tend to perform poorly when items may be removed. We initiate the study of scalable locality-sensitive hashing (LSH) for fully dynamic data-streams. Specifically, using the Jaccard index as similarity measure, we design (1) a collaborative filtering mechanism maintainable in dynamic data streams and (2) a sketching algorithm for similarity estimation. Our algorithms have little overhead in terms of running time compared to previous LSH approaches for the insertion only case, and drastically outperform previous algorithms in case of deletions."
683,,ChronoGraph - Enabling Temporal Graph Traversals for Efficient Information Diffusion Analysis over Time.,"Jaewook Byun,Sungpil Woo,Daeyoung Kim 0001",https://doi.org/10.1109/TKDE.2019.2891565,TKDE,2020,"Semantics,Parallel processing,Time factors,Syntactics,Databases,Prototypes,Standards","ChronoGraph is a novel system enabling temporal graph traversals. Compared to snapshot-oriented systems, this traversal-oriented system is suitable for analyzing information diffusion over time without violating a time constraint on temporal paths. The cornerstone of ChronoGraph aims at bridging the chasm between point-based semantics and period-based semantics and the gap between temporal graph traversals and static graph traversals. Therefore, our graph model and traversal language provide the temporal syntax for both semantics, and we present a method converting point-based semantics to period-based ones. Also, ChronoGraph exploits the temporal support and parallelism to handle the temporal degree, which explosively increases compared to static graphs. We demonstrate how three traversal recipes can be implemented on top of our system: temporal breadth-first search (tBFS), temporal depth-first search (tDFS), and temporal single source shortest path (tSSSP). According to our evaluation, our temporal support and parallelism enhance temporal graph traversals in terms of convenience and efficiency. Also, ChronoGraph outperforms existing property graph databases in terms of temporal graph traversals. We prototype ChronoGraph by extending Tinkerpop, a de facto standard for property graphs. Therefore, we expect that our system would be readily accessible to existing property graph users."
684,,Trust Relationship Prediction in Alibaba E-Commerce Platform.,"Yukuo Cen,Jing Zhang 0001,Gaofei Wang,Yujie Qian,Chuizheng Meng,Zonghong Dai,Hongxia Yang,Jie Tang 0001",https://doi.org/10.1109/TKDE.2019.2893939,TKDE,2020,"Correlation,Feature extraction,Business,Twitter,Graphical models,Prediction algorithms","This paper introduces how to infer trust relationships from billion-scale networked data to benefit Alibaba E-Commerce business. To effectively leverage the network correlations between labeled and unlabeled relationships to predict trust relationships, we formalize trust into multiple types and propose a graphical model to incorporate type-based dyadic and triadic correlations, namely eTrust. We also present a fast learning algorithm in order to handle billion-scale networks. Systematically, we evaluate the proposed methods on four different genres of datasets with labeled trust relationships: Alibaba, Epinions, Ciao, and Advogato. Experimental results show that the proposed methods achieve significantly better performance than several comparison methods (+1.7-32.3% by accuracy; p <; <; 0:01, with t-test). Most importantly, when handling the real large networked data with over 1,200,000,000 edges (Ali-large), our method achieves 2,000× speedup to infer trust relationships, comparing with the traditional graph learning algorithms. Finally, we have applied the inferred trust relationships to Alibaba E-commerce platform: Taobao, and achieved 2.75 percent improvement on gross merchandise volume (GMV)."
685,,A Review of Judgment Analysis Algorithms for Crowdsourced Opinions.,"Sujoy Chatterjee,Anirban Mukhopadhyay 0001,Malay Bhattacharyya 0001",https://doi.org/10.1109/TKDE.2019.2904064,TKDE,2020,"Task analysis,Crowdsourcing,Companies,Noise measurement,Labeling,Gold","The crowd-powered systems have been shown to be highly successful in the current decade to manage collective contribution of online workers for solving different complex tasks. It can also be used for soliciting opinions from a large set of people working in a distributed manner. Unfortunately, the online community of crowd workers might involve non-experts as opinion providers. As a result, such approaches may give rise to noise making it hard to predict the appropriate (gold) judgment. Judgment analysis is in general a way of learning about human decision from multiple opinions. A spectrum of algorithms has been proposed in the last few decades to address this problem. They are broadly made up of supervised or unsupervised types. However, they have been readdressed in recent years having focus on different strategies for obtaining the gold judgment from crowdsourced opinions, viz., estimating the accuracy of opinions, difficulties of the problem, spammer identification, handling noise, etc. Besides this, investigation of various types of crowdsourced opinions to solve complex real-life problems provide new insights in this domain. In this survey, we provide a comprehensive overview of the judgment analysis problem and some of its novel variants, addressed with different approaches, where the opinions are crowdsourced."
686,,Reducing Web Page Complexity to Facilitate Effective User Navigation.,Min Chen 0027,https://doi.org/10.1109/TKDE.2019.2893242,TKDE,2020,"Complexity theory,Navigation,Web pages,Usability,Measurement,Maintenance engineering,Data models","As a website evolves to align with users' changing information needs and interests, its structure can outgrow the original design, accumulating links and pages in unanticipated places. This increases complexity to both web pages and the navigation structure, which could cause difficulty in locating relevant links and information. Though the increasing complexity of website and its impact on users' psychological perception have been anecdotally well-recognized, the need to address it with a formal and rigorous method is understudied in the literature. This paper is one of the first studies to examine how to streamline website structures to enhance navigation. We use a widely used metric in the literature - a page's outdegree (the number of links in a page) - as the measurement complexity, because it not only serves as a good proximity for page complexity but also has a significant implication on website structure. We propose a method based on mathematical programming (MP) model that can significantly reduce users' cognitive load by effectively eliminating appropriate links from pages with high complexity. We have performed extensive experiments on both a real dataset and very large synthetic datasets with statistical similarities to the real dataset. The results indicate that our method not only significantly reduces web page and structure complexity with very small impact to user navigation, but also can be effectively solved and scales up remarkably well, suggesting it is useful for website maintenance on a progressive basis. In addition, we conduct a study to evaluate the performance of streamlined website structures using the real dataset and the results confirm the validity of our method."
687,,ESPM - Efficient Spatial Pattern Matching.,"Hongmei Chen,Yixiang Fang,Ying Zhang 0001,Wenjie Zhang 0001,Lizhen Wang",https://doi.org/10.1109/TKDE.2019.2947505,TKDE,2020,"Pattern matching,Indexes,Planning,Information technology,Business,Australia,Marine vehicles","With recent advances in information technologies such as global position system and mobile internet, a huge volume of spatio-textual objects have been generated from location-based services, which enable a wide range of spatial keyword queries. Recently, researchers have proposed a novel query, called Spatial Pattern Matching (SPM), which uses a pattern to capture the user's intention. It has been demonstrated to be fundamental and useful for many real applications. Despite its usefulness, the SPM problem is computationally intractable. Existing algorithms suffer from the low efficiency issue, especially on large scale datasets. To enhance the performance of SPM, in this paper we propose a novel Efficient Spatial Pattern Matching (ESPM) algorithm, which exploits the inverted linear quadtree index and computes matched node pairs and object pairs level by level in a top-down manner. In particular, it focuses on pruning unpromising nodes and node pairs at the high levels, resulting in a large number of unpromising objects and object pairs to be pruned before accessing them from disk. We experimentally evaluate the performance of ESPM on real large datasets. Our results show that ESPM is over one order of magnitude faster than the state-of-the-art algorithm, and also uses much less I/O cost."
688,,Inferring Full Diffusion History from Partial Timestamps.,"Zhen Chen 0005,Hanghang Tong,Lei Ying",https://doi.org/10.1109/TKDE.2019.2905210,TKDE,2020,"History,Diffusion processes,Monitoring,Heuristic algorithms,Computational modeling,Privacy,Reconstruction algorithms","Understanding diffusion processes in networks has emerged as an important research topic because of its wide range of applications. Analysis of diffusion traces can help us answer important questions such as the source(s) of diffusion and the role of each node during the diffusion process. However, in large-scale networks, due to the cost and privacy concerns, it is almost impossible to monitor the entire network and collect the complete diffusion trace. In this paper, we tackle the problem of reconstructing the diffusion history from a partial observation. We formulate the diffusion history reconstruction problem as a maximum a posteriori (MAP) problem and prove the problem is NP-hard. Then, we propose a step-by-step reconstruction algorithm, which can always produce a diffusion history that is consistent with the partial observation. Our experimental results based on synthetic and real networks show that the algorithm significantly outperforms some existing methods."
689,,Affinity Regularized Non-Negative Matrix Factorization for Lifelong Topic Modeling.,"Yong Chen 0008,Junjie Wu 0002,Jianying Lin,Rui Liu 0007,Hui Zhang 0028,Zhiwen Ye",https://doi.org/10.1109/TKDE.2019.2904687,TKDE,2020,"Data models,Semantics,Task analysis,Graphics processing units,Big Data,Convergence,Maintenance engineering","Lifelong topic model (LTM), an emerging paradigm for never-ending topic learning, aims to yield higher-quality topics as time passes through knowledge accumulated from the past yet learned for the future. In this paper, we propose a novel lifelong topic model based on non-negative matrix factorization (NMF), called Affinity Regularized NMF for LTM (NMF-LTM), which to our best knowledge is distinctive from the popular LDA-based LTMs. NMF-LTM achieves lifelong learning by introducing word-word graph Laplacian as semantic affinity regularization. Other priors such as sparsity, diversity, and between-class affinity are incorporated as well for better performance, and a theoretical guarantee is provided for the algorithmic convergence to a local minimum. Extensive experiments on various public corpora demonstrate the effectiveness of NMF-LTM, particularly its human-like behaviors in two carefully designed learning tasks and the ability in topic modeling of big data. A further exploration of semantic relatedness in knowledge graphs and a case study on a large-scale real-world corpus exhibit the strength of NMF-LTM in discovering high-quality topics in an efficient and robust way."
690,,Semi-Supervised Feature Selection via Sparse Rescaled Linear Square Regression.,"Xiaojun Chen 0006,Guowen Yuan,Feiping Nie 0001,Zhong Ming 0001",https://doi.org/10.1109/TKDE.2018.2879797,TKDE,2020,"Feature extraction,Computational complexity,Laplace equations,Knowledge discovery,Data engineering,Iterative methods,Adaptation models","With the rapid increase of the data size, it has increasing demands for selecting features by exploiting both labeled and unlabeled data. In this paper, we propose a novel semi-supervised embedded feature selection method. The new method extends the least square regression model by rescaling the regression coefficients in the least square regression with a set of scale factors, which is used for evaluating the importance of features. An iterative algorithm is proposed to optimize the new model. It has been proved that solving the new model is equivalent to solving a sparse model with a flexible and adaptable ℓ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2;p</sub>
 norm regularization. Moreover, the optimal solution of scale factors provides a theoretical explanation for why we can use {||w
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
||
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
, . . .,||w
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">d</sup>
||
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
} to evaluate the importance of features. Experimental results on eight benchmark data sets show the superior performance of the proposed method."
691,,A Generalized Locally Linear Factorization Machine with Supervised Variational Encoding.,"Xiaoshuang Chen,Yin Zheng,Peilin Zhao,Zhuxi Jiang,Wenye Ma,Junzhou Huang",https://doi.org/10.1109/TKDE.2019.2903403,TKDE,2020,"Frequency modulation,Encoding,Artificial neural networks,Euclidean distance,Optimization,Computational modeling","Factorization Machines (FMs) learn weights for feature interactions, and achieve great success in many data mining tasks. Recently, Locally Linear Factorization Machines (LLFMs) have been proposed to capture the underlying structures of data for better performance. However, one obvious drawback of LLFM is that the local coding is only operated in the original feature space, which limits the model to be applied to high-dimensional and sparse data. In this work, we present a generalized LLFM (GLLFM) which overcomes this limitation by modeling the local coding procedure in a latent space. Moreover, a novel Supervised Variational Encoding (SVE) technique is proposed such that the distance can effectively describe the similarity between data points. Specifically, the proposed GLLFM-SVE trains several local FMs in the original space to model the higher order feature interactions effectively, where each FM associates to an anchor point in the latent space induced by SVE. The prediction for a data point is computed by a weighted sum of several local FMs, where the weights are determined by local coding coordinates with anchor points. Actually, GLLFM-SVE is quite flexible and other Neural Network (NN) based FMs can be easily embedded into this framework. Experimental results show that GLLFM-SVE significantly improves the performance of LLFM. By using NN-based FMs as local predictors, our model outperforms all the state-of-the-art methods on large-scale real-world benchmarks with similar number of parameters and comparable training time."
692,1,Multi-Party High-Dimensional Data Publishing Under Differential Privacy.,"Xiang Cheng 0003,Peng Tang,Sen Su,Rui Chen,Zequn Wu,Binyuan Zhu",https://doi.org/10.1109/TKDE.2019.2906610,TKDE,2020,"Differential privacy,Bayes methods,Publishing,Distributed databases,Correlation,Privacy","In this paper, we study the problem of publishing high-dimensional data in a distributed multi-party environment under differential privacy. In particular, with the assistance of a semi-trusted curator, the parties (i.e., local data owners) collectively generate a synthetic integrated dataset while satisfying ε-differential privacy. To solve this problem, we present a differentially private sequential update of Bayesian network (DP-SUBN) approach. In DP-SUBN, the parties and the curator collaboratively identify the Bayesian network N that best fits the integrated dataset in a sequential manner, from which a synthetic dataset can then be generated. The fundamental advantage of adopting the sequential update manner is that the parties can treat the intermediate results provided by previous parties as their prior knowledge to direct how to learn N. The core of DP-SUBN is the construction of the search frontier, which can be seen as a priori knowledge to guide the parties to update N. By exploiting the correlations of attribute pairs, we propose exact and heuristic methods to construct the search frontier. In particular, to privately quantify the correlations of attribute pairs without introducing too much noise, we first put forward a non-overlapping covering design (NOCD) method, and then devise a dynamic programming method for determining the optimal parameters used in NOCD. Through privacy analysis, we show that DP-SUBN satisfies ε-differential privacy. Extensive experiments on real datasets demonstrate that DP-SUBN offers desirable data utility with low communication cost."
693,,Automated Data Slicing for Model Validation - A Big Data - AI Integration Approach.,"Yeounoh Chung,Tim Kraska,Neoklis Polyzotis,Ki Hyun Tae,Steven Euijong Whang",https://doi.org/10.1109/TKDE.2019.2916074,TKDE,2020,"Data models,Analytical models,Market research ,Data models,Machine learning,Artificial intelligence,Big Data","As machine learning systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose SliceFinder, which is an interactive framework for identifying such slices using statistical techniques. Applications include diagnosing model fairness and fraud detection, where identifying slices that are interpretable to humans is crucial. This research is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research."
694,,MV-RNN - A Multi-View Recurrent Neural Network for Sequential Recommendation.,"Qiang Cui 0002,Shu Wu,Qiang Liu 0006,Wen Zhong,Liang Wang 0001",https://doi.org/10.1109/TKDE.2018.2881260,TKDE,2020,"Markov processes,Recurrent neural networks,Visualization,Task analysis,Collaboration,Logic gates","Sequential recommendation is a fundamental task for network applications, and it usually suffers from the item cold start problem due to the insufficiency of user feedbacks. There are currently three kinds of popular approaches which are respectively based on matrix factorization (MF) of collaborative filtering, Markov chain (MC), and recurrent neural network (RNN). Although widely used, they have some limitations. MF based methods could not capture dynamic user's interest. The strong Markov assumption greatly limits the performance of MC based methods. RNN based methods are still in the early stage of incorporating additional information. Based on these basic models, many methods with additional information only validate incorporating one modality in a separate way. In this work, to make the sequential recommendation and deal with the item cold start problem, we propose a Multi-View Rrecurrent Neural Network (MV-RNN) model. Given the latent feature, MV-RNN can alleviate the item cold start problem by incorporating visual and textual information. First, At the input of MV-RNN, three different combinations of multi-view features are studied, like concatenation, fusion by addition and fusion by reconstructing the original multi-modal data. MV-RNN applies the recurrent structure to dynamically capture the user's interest. Second, we design a separate structure and a united structure on the hidden state of MV-RNN to explore a more effective way to handle multi-view features. Experiments on two real-world datasets show that MV-RNN can effectively generate the personalized ranking list, tackle the missing modalities problem, and significantly alleviate the item cold start problem."
695,,Online Knowledge Level Tracking with Data-Driven Student Models and Collaborative Filtering.,"Antoine Cully,Yiannis Demiris",https://doi.org/10.1109/TKDE.2019.2912367,TKDE,2020,"Hidden Markov models,Predictive models,Data models,Task analysis,Adaptation models,Knowledge engineering,Production facilities","Intelligent Tutoring Systems are promising tools for delivering optimal and personalized learning experiences to students. A key component for their personalization is the student model, which infers the knowledge level of the students to balance the difficulty of the exercises. While important advances have been achieved, several challenges remain. In particular, the models should be able to track in real-time the evolution of the students' knowledge levels. These evolutions are likely to follow different profiles for each student, while measuring the exact knowledge level remains difficult given the limited and noisy information provided by the interactions. This paper introduces a novel model that addresses these challenges with three contributions:. 1) the model relies on Gaussian Processes to track online the evolution of the student's knowledge level over time, 2) it uses collaborative filtering to rapidly provide long-term predictions by leveraging the information from previous users, and 3) it automatically generates abstract representations of knowledge components via automatic relevance determination of covariance matrices. The model is evaluated on three datasets, including real users. The results demonstrate that the model converges to accurate predictions in average four times faster than the compared methods."
696,,Semi-Supervised Deep Learning Approach for Transportation Mode Identification Using GPS Trajectory Data.,"Sina Dabiri,Chang-Tien Lu,Kevin Heaslip,Chandan K. Reddy",https://doi.org/10.1109/TKDE.2019.2896985,TKDE,2020,"Global Positioning System,Transportation,Trajectory,Feature extraction,Deep learning,Task analysis,Acceleration","Identification of travelers' transportation modes is a fundamental step for various problems that arise in the domain of transportation such as travel demand analysis, transport planning, and traffic management. In this paper, we aim to identify travelers' transportation modes purely based on their GPS trajectories. First, a segmentation process is developed to partition a user's trip into GPS segments with only one transportation mode. A majority of studies have proposed mode inference models based on hand-crafted features, which might be vulnerable to traffic and environmental conditions. Furthermore, the classification task in almost all models have been performed in a supervised fashion while a large amount of unlabeled GPS trajectories has remained unused. Accordingly, we propose a deep SEmi-Supervised Convolutional Autoencoder (SECA) architecture that can not only automatically extract relevant features from GPS segments but also exploit useful information in unlabeled data. The SECA integrates a convolutional-deconvolutional autoencoder and a convolutional neural network into a unified framework to concurrently perform supervised and unsupervised learning. The two components are simultaneously trained using both labeled and unlabeled GPS segments, which have already been converted into an efficient representation for the convolutional operation. An optimum schedule for varying the balancing parameters between reconstruction and classification errors are also implemented. The performance of the proposed SECA model, trip segmentation, the method for converting a raw trajectory into a new representation, the hyperparameter schedule, and the model configuration are evaluated by comparing to several baselines and alternatives for various amounts of labeled and unlabeled data. Our experimental results demonstrate the superiority of the proposed model over the state-of-the-art semi-supervised and supervised methods with respect to metrics such as accuracy and F-measure."
697,,Boosting with Lexicographic Programming - Addressing Class Imbalance without Cost Tuning.,"Shounak Datta,Sayak Nag,Swagatam Das",https://doi.org/10.1109/TKDE.2019.2894148,TKDE,2020,"Boosting,Tuning,Fasteners,Games,Training,Linear programming,Proposals","A large amount of research effort has been dedicated to adapting boosting for imbalanced classification. However, boosting methods are yet to be satisfactorily immune to class imbalance, especially for multi-class problems. This is because most of the existing solutions for handling class imbalance rely on expensive cost set tuning for determining the proper level of compensation. We show that the assignment of weights to the component classifiers of a boosted ensemble can be thought of as a game of Tug of War between the classes in the margin space. We then demonstrate how this insight can be used to attain a good compromise between the rare and abundant classes without having to resort to cost set tuning, which has long been the norm for imbalanced classification. The solution is based on a lexicographic linear programming framework which requires two stages. Initially, class-specific component weight combinations are found so as to minimize a hinge loss individually for each of the classes. Subsequently, the final component weights are assigned so that the maximum deviation from the class-specific minimum loss values (obtained in the previous stage) is minimized. Hence, the proposal is not only restricted to two-class situations, but is also readily applicable to multi-class problems. Additionally, we also derive the dual formulation corresponding to the proposed framework. Experiments conducted on artificial and real-world imbalanced datasets as well as on challenging applications such as hyperspectral image classification and ImageNet classification establish the efficacy of the proposal."
698,,A Novel Trust Model Based Overlapping Community Detection Algorithm for Social Networks.,"Shuai Ding 0001,Zijie Yue,Shanlin Yang,Feng Niu,Youtao Zhang",https://doi.org/10.1109/TKDE.2019.2914201,TKDE,2020,"Social networking (online),Detection algorithms,Computational modeling,Network topology,Peer-to-peer computing,Topology,Clustering algorithms","With the fast advances in Internet technologies, social networks have become a major platform for social interaction, lifestyle demonstration, and message dissemination. Effective community detection in social networks helps to assess public sentiment, identify community leaders, and produce personalized recommendation. While different community detection approaches have been proposed in the literature, the trust model based detection schemes model user interactions as trust transfer, which helps to capture the implicit relation in the network. Unfortunately, trust model based detection schemes face a cold start problem, i.e., they cannot accurately model newly joined users as these users have few interactions for a duration after joining the network. In this paper, we propose TLCDA, a novel trust model based community detection algorithm. By enhancing the traditional trust computation with inter-node relation strength and similarity in social networks, TLCDA detects communities through coarse-grained K-Mediods clustering. Our evaluation on real social networks shows that the communities detected by TLCDA exhibit superior preference cohesion while satisfying the topology cohesion."
699,,GAT - A Unified GPU-Accelerated Framework for Processing Batch Trajectory Queries.,"Kaixing Dong,Bowen Zhang 0006,Yanyan Shen,Yanmin Zhu,Jiadi Yu",https://doi.org/10.1109/TKDE.2018.2879862,TKDE,2020,"Trajectory,Graphics processing units,Query processing,Indexes,Throughput,Public transportation,Instruction sets","The increasing amount of trajectory data facilitates a wide spectrum of practical applications in which large numbers of trajectory range and similarity queries are issued continuously. This calls for high-throughput trajectory query processing. Traditional in-memory databases lack considerations of the unique features of trajectories, while specialized trajectory query processing systems are typically designed for only one type of trajectory queries. This paper introduces GAT, a unified GPU-accelerated framework to process batch trajectory queries with the objective of high throughput. GAT follows the filtering-and-verification paradigm where we develop a novel index GTIDX for effectively filtering invalid trajectories on the CPU, and exploit the massive parallelism of the GPU for verification. To optimize the performance of GAT, we first greedily partition batch queries to reduce the amortized query processing latency. We then apply the Morton-based encoding method to coalesce data access requests from the GPU cores, and maintain a hash table to avoid redundant data transfer between CPU and GPU. To achieve load balance, we group size-varying cells into balanced blocks with similar numbers of trajectory points. Extensive experiments have been conducted over real-life trajectory datasets. The results show that GAT is efficient, scalable, and achieves high throughput with acceptable indexing cost."
700,,CVTM - A Content-Venue-Aware Topic Model for Group Event Recommendation.,"Yulu Du,Xiangwu Meng,Yujie Zhang",https://doi.org/10.1109/TKDE.2019.2904066,TKDE,2020,"Correlation,Games,Context modeling,Social networking (online),Probabilistic logic,Recommender systems,Task analysis","Event recommendation is essential to help people find attractive events to attend, but it intrinsically faces cold-start problem. The previous studies exploit multiple contextual factors to overcome the cold-start problem in event recommendation. However, they do not consider the correlation among different contextual factors. Moreover, suggesting events for a group of users also has not been well studied. In this paper, we first discover the correlation between organizer and textual content, i.e., the events held by the same organizer tend to have more similar content. Based on this observation, we present a content-venue-aware topic model (CVTM) to capture group interests on an event from two perspectives: content and venue. The correlation between organizer and content is modeled in CVTM to alleviate the sparsity of textual content, and then we can further extract group interests on content of an event more accurately. Finally, a group event recommendation method using CVTM is proposed. We conduct comprehensive experiments to evaluate the recommendation performance of our model on two real-world datasets. The results demonstrate that the proposed model outperforms the state-of-the-art methods that suggest upcoming events for groups. Besides, CVTM can learn semantically coherent latent topics which are useful to explain recommendations."
701,,GERF - A Group Event Recommendation Framework Based on Learning-to-Rank.,"Yulu Du,Xiangwu Meng,Yujie Zhang,Pengtao Lv",https://doi.org/10.1109/TKDE.2019.2893361,TKDE,2020,"Feature extraction,Task analysis,Bayes methods,Recommender systems,Prediction algorithms,Context modeling,Training","Event recommendation is an essential means to enable people to find attractive upcoming social events, such as party, exhibition, and concert. While growing line of research has focused on suggesting events to individuals, making event recommendation for a group of users has not been well studied. In this paper, we aim to recommend upcoming events for a group of users. We formalize group recommendation as a ranking problem and propose a group event recommendation framework GERF based on learning-to-rank technique. Specifically, we first analyze different contextual influences on user's event attendance, and extract preference of user to event considering each contextual influence. Then, the preference scores of the users in a group are taken as the features for learningto-rank to model the preference of the group. Moreover, a fast pairwise learning-to-rank algorithm, Bayesian group ranking, is proposed to learn ranking model for each group. Our framework is easily to incorporate additional contextual influences, and can be applied to other group recommendation scenarios. Extensive experiments have been conducted to evaluate the performance of GERF on two real-world datasets and demonstrate the appealing performance of our method on both accuracy and time efficiency."
702,,Personalized Video Recommendation Using Rich Contents from Videos.,"Xingzhong Du,Hongzhi Yin,Ling Chen 0006,Yang Wang 0023,Yi Yang 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2018.2885520,TKDE,2020,"Collaboration,Training,Feature extraction,Recommender systems,Visualization,Streaming media,Task analysis","Video recommendation has become an essential way of helping people explore the massive videos and discover the ones that may be of interest to them. In the existing video recommender systems, the models make the recommendations based on the user-video interactions and single specific content features. When the specific content features are unavailable, the performance of the existing models will seriously deteriorate. Inspired by the fact that rich contents (e.g., text, audio, motion, and so on) exist in videos, in this paper, we explore how to use these rich contents to overcome the limitations caused by the unavailability of the specific ones. Specifically, we propose a novel general framework that incorporates arbitrary single content feature with user-video interactions, named as collaborative embedding regression (CER) model, to make effective video recommendation in both in-matrix and out-of-matrix scenarios. Our extensive experiments on two real-world large-scale datasets show that CER beats the existing recommender models with any single content feature and is more time efficient. In addition, we propose a priority-based late fusion (PRI) method to gain the benefit brought by the integrating the multiple content features. The corresponding experiment shows that PRI brings real performance improvement to the baseline and outperforms the existing fusion methods."
703,,Generalized Translation-Based Embedding of Knowledge Graph.,"Takuma Ebisu,Ryutaro Ichise",https://doi.org/10.1109/TKDE.2019.2893920,TKDE,2020,"Task analysis,Predictive models,Linear programming,Measurement,Neural networks,Knowledge engineering,Standards","Knowledge graphs are useful for many AI tasks but often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. TransE is one of such models and the first translation-based method. TransE is well known because the principle of TransE can effectively capture the rules of a knowledge graph although it seems very simple. However, TransE has problems with its regularization and an unchangeable ratio of negative sampling. In this paper, we generalize TransE to solve these problems by proposing knowledge graph embedding on a Lie group (KGLG) and the Weighted Negative Part (WNP) method for the objective function of translation-based models. KGLG is the novel translation-based method which embeds entities and relations of a knowledge graph on any Lie group. It allows us not to employ regularization during training of the model if we choose a compact lie group for the embedding space. The WNP method is for changing the ratio of negative sampling, which enhances translation-based models. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult, and ComplEx on a standard link prediction task. We show that TorusE, KGLG on a torus, is scalable to large-size knowledge graphs and faster than the original TransE."
704,,Scheduling Resources to Multiple Pipelines of One Query in a Main Memory Database Cluster.,"Zhuhe Fang,Chuliang Weng,Li Wang,Huiqi Hu,Aoying Zhou",https://doi.org/10.1109/TKDE.2018.2884724,TKDE,2020,"Pipelines,Servers,Task analysis,Parallel processing,Optimal scheduling,Job shop scheduling","To fully utilize the resources of a main memory database cluster, we additionally take the independent parallelism into account to parallelize multiple pipelines of one query. However, scheduling resources to multiple pipelines is an intractable problem. Traditional static approaches to this problem may lead to a serious waste of resources and suboptimal execution order of pipelines, because it is hard to predict the actual data distribution and fluctuating workloads at compile time. In response, we propose a dynamic scheduling algorithm, List with Filling and Preemption (LFPS), based on two novel techniques. (1) Adaptive filling improves resource utilization by issuing more extra pipelines to adaptively fill idle resource “holes” during execution. (2) Rank-based preemption strictly guarantees scheduling the pipelines on the critical path first at run time. Interestingly, the latter facilitates the former filling idle “holes” with best efforts to finish multiple pipelines as soon as possible. We implement LFPS in our prototype database system. Under the workloads of TPC-H, experiments show our work improves the finish time of parallelizable pipelines from one query up to 2.5X than a static approach and 2.1X than a serialized execution."
705,,Discovery and Recognition of Emerging Human Activities Using a Hierarchical Mixture of Directional Statistical Models.,"Lei Fang 0001,Juan Ye,Simon Dobson",https://doi.org/10.1109/TKDE.2019.2905207,TKDE,2020,"Data models,Activity recognition,Training,Training data,Kernel,Mixture models,Smart homes","Human activity recognition plays a significant role in enabling pervasive applications as it abstracts low-level noisy sensor data into high-level human activities, which applications can respond to. With more and more activity-aware applications deployed in real-world environments, a research challenge emerges-discovering and learning new activities that have not been pre-defined or observed in the training phase. This paper tackles this challenge by proposing a hierarchical mixture of directional statistical models. The model supports incrementally, continuously updating the activity model over time with the reduced annotation effort and without the need for storing historical sensor data. We have validated this solution on four publicly available, third-party smart home datasets, and have demonstrated up to 91.5 percent accuracies of detecting and recognising new activities."
706,,SURGE - Continuous Detection of Bursty Regions Over a Stream of Spatial Objects.,"Kaiyu Feng,Tao Guo,Gao Cong,Sourav S. Bhowmick,Shuai Ma 0001",https://doi.org/10.1109/TKDE.2019.2915654,TKDE,2020,"Surges,Real-time systems,Vehicles,Search problems,Monitoring,Mobile handsets","With the proliferation of mobile devices and location-based services, continuous generation of massive volume of streaming spatial objects (i.e., geo-tagged data) opens up new opportunities to address real-world problems by analyzing them. In this paper, we present a novel continuous bursty region detection (SURGE) problem that aims to continuously detect a burstyregion of a given size in a specified geographical area from a stream of spatial objects. Specifically, a bursty region shows maximum spike in the number of spatial objects in a given time window. The SURGE problem is useful in addressing several real-world challenges such as surge pricing problem in online transportation and disease outbreak detection. To solve the problem, we propose an exact solution and two approximate solutions, and the approximation ratio is 1-α/4 in terms of the burst score, where α is a parameter to control the burst score. We further extend these solutions to support detection of top-k bursty regions. Extensive experiments with real-world data are conducted to demonstrate the efficiency and effectiveness of our solutions."
707,,Ensemble of Classifiers Based on Multiobjective Genetic Sampling for Imbalanced Data.,"Everlandio R. Q. Fernandes,André C. P. L. F. de Carvalho,Xin Yao 0001",https://doi.org/10.1109/TKDE.2019.2898861,TKDE,2020,"Prediction algorithms,Task analysis,Training,Genetics,Boosting,Machine learning algorithms","Imbalanced datasets may negatively impact the predictive performance of most classical classification algorithms. This problem, commonly found in real-world, is known in machine learning domain as imbalanced learning. Most techniques proposed to deal with imbalanced learning have been proposed and applied only to binary classification. When applied to multiclass tasks, their efficiency usually decreases and negative side effects may appear. This paper addresses these limitations by presenting a novel adaptive approach, E-MOSAIC (Ensemble of Classifiers based on MultiObjective Genetic Sampling for Imbalanced Classification). E-MOSAIC evolves a selection of samples extracted from training dataset, which are treated as individuals of a MOEA. The multiobjective process looks for the best combinations of instances capable of producing classifiers with high predictive accuracy in all classes. E-MOSAIC also incorporates two mechanisms to promote the diversity of these classifiers, which are combined into an ensemble specifically designed for imbalanced learning. Experiments using twenty imbalanced multi-class datasets were carried out. In these experiments, the predictive performance of E-MOSAIC is compared with state-of-the-art methods, including methods based on presampling, active-learning, cost-sensitive, and boosting. According to the experimental results, the proposed method obtained the best predictive performance for the multiclass accuracy measures mAUC and G-mean."
708,,Memory Augmented Deep Generative Models for Forecasting the Next Shot Location in Tennis.,"Tharindu Fernando,Simon Denman,Sridha Sridharan,Clinton Fookes",https://doi.org/10.1109/TKDE.2019.2911507,TKDE,2020,"Sports,Generative adversarial networks,Semantics,Trajectory,Memory modules,Adaptation models,Predictive models","This paper presents a novel framework for predicting shot location and type in tennis. Inspired by recent neuroscience discoveries, we incorporate neural memory modules to model the episodic and semantic memory components of a tennis player. We propose a Semi-Supervised Generative Adversarial Network architecture that couples these memory models with the automatic feature learning power of deep neural networks, and demonstrate methodologies for learning player level behavioral patterns with the proposed framework. We evaluate the effectiveness of the proposed model on tennis tracking data from the 2012 Australian Tennis Open and exhibit applications of the proposed method in discovering how players adapt their style depending on the match context."
709,,"Control-Flow Modeling with Declare - Behavioral Properties, Computational Complexity, and Tools.","Valeria Fionda,Antonella Guzzo",https://doi.org/10.1109/TKDE.2019.2897309,TKDE,2020,"Lenses,Computational modeling,Process control,Tools,Analytical models,NP-hard problem,Computational complexity","Declarative approaches to control-flow modeling use logic-based languages to formalize a number of constraints that valid traces must satisfy. The most noticeable example is the DECLARE framework based on linear temporal logic. Despite the interest that DECLARE has been attracting, the current knowledge about its formal properties was rather limited. The goal of this paper is to fill this gap by: (i) analyzing the behavioral properties of DECLARE by comparing it with the modeling capabilities of traditional procedural design approaches, in particular, block-structured processes; (ii) analyzing DECLARE from the computational point of view. As for the former point, we identify both the block-structured processes constructs that can be simulated in DECLARE and the features of DECLARE that can be encoded in block-structured processes. As for the latter point, we show that checking whether a given set of DECLARE patterns admits a satisfying trace is an NP-hard problem. In particular, we identify some DECLARE specifications whose satisfying traces are all of exponential length and some useful DECLARE fragments where a satisfying trace whose length is polynomially bounded is guaranteed to exist. The paper also discusses the declare2sat prototype system and the results of a thorough experimental validation."
710,,Jointly Learning Topics in Sentence Embedding for Document Summarization.,"Yang Gao 0016,Yue Xu 0001,Heyan Huang,Qian Liu,Linjing Wei,Luyang Liu",https://doi.org/10.1109/TKDE.2019.2892430,TKDE,2020,"Semantics,Predictive models,Feature extraction,Computational modeling,Task analysis,Context modeling,Training","Summarization systems for various applications, such as opinion mining, online news services, and answering questions, have attracted increasing attention in recent years. These tasks are complicated, and a classic representation using bag-of-words does not adequately meet the comprehensive needs of applications that rely on sentence extraction. In this paper, we focus on representing sentences as continuous vectors as a basis for measuring relevance between user needs and candidate sentences in source documents. Embedding models based on distributed vector representations are often used in the summarization community because, through cosine similarity, they simplify sentence relevance when comparing two sentences or a sentence/query and a document. However, the vector-based embedding models do not typically account for the salience of a sentence, and this is a very necessary part of document summarization. To incorporate sentence salience, we developed a model, called CCTSenEmb, that learns latent discriminative Gaussian topics in the embedding space and extended the new framework by seamlessly incorporating both topic and sentence embedding into one summarization system. To facilitate the semantic coherence between sentences in the framework of prediction-based tasks for sentence embedding, the CCTSenEmb further considers the associations between neighboring sentences. As a result, this novel sentence embedding framework combines sentence representations, word-based content, and topic assignments to predict the representation of the next sentence. A series of experiments with the DUC datasets validate CCTSenEmb's efficacy in document summarization in a query-focused extraction-based setting and an unsupervised ILP-based setting."
711,,LN-SNE - Log-Normal Distributed Stochastic Neighbor Embedding for Anomaly Detection.,"Zahra Ghafoori,Sarah M. Erfani,James C. Bezdek,Shanika Karunasekera,Christopher Leckie",https://doi.org/10.1109/TKDE.2019.2934450,TKDE,2020,"Dimensionality reduction,Anomaly detection,Training,Neurons,Optimization,Density measurement,Data models","We present a new unsupervised dimensionality reduction technique, called LN-SNE, for anomaly detection. LN-SNE generates a parametric embedding by means of Restricted Boltzmann Machines and uses a heavy-tail distribution to project data to a lower dimensional space such that dissimilarities between normal data and anomalies are preserved or strengthened. We compare LN-SNE to several benchmark dimensionality reduction methods on real datasets. The results suggest that LN-SNE for anomaly detection is less sensitive to the dimension of the latent space than the other methods and outperforms them in terms of accuracy. We empirically show that our technique scales near-linearly with respect to the number of dimensions and data size."
712,,An Attribute-Specific Ranking Method Based on Language Models for Keyword Search over Graphs.,"Asieh Ghanbarpour,Hassan Naderi",https://doi.org/10.1109/TKDE.2018.2879863,TKDE,2020,"Keyword search,Semantics,Urban areas,Africa,Rivers","Many real-world networks such as Facebook, LinkedIn, and Wikipedia exhibit rich connectivity patterns along with worthwhile content nodes often labeled with meaningful attributes. Keyword search is an effective method to retrieve information from such useful networks. The aim of keyword search is to find a set of answers (subgraphs) covering all or part of the queried keywords. A challenge in keyword search systems is to rank answers according to their relevance to the query. This relevance lies in the textual content and structural compactness of the answers. In this paper, an attribute-specific ranking method is proposed based on language models to rank candidate answers according to their semantic information up to the attribute level. This method scores answers using a model enriched with attribute-specific preferences and integrating both the structure and content of answers. The proposed model is directly estimated on the sub-graphs (answers) and is defined such that it can preserve the local importance of keywords in nodes. Extensive experiments conducted on a standard evaluation framework with three real-world datasets illustrate the superior effectiveness of the proposed ranking method to that of the state-of-the-art methods."
713,,Evaluating Overfit and Underfit in Models of Network Community Structure.,"Amir Ghasemian,Homa Hosseinmardi,Aaron Clauset",https://doi.org/10.1109/TKDE.2019.2911585,TKDE,2020,"Task analysis,Clustering algorithms,Probabilistic logic,Bayes methods,Detection algorithms,Prediction algorithms,Partitioning algorithms","A common graph mining task is community detection, which seeks an unsupervised decomposition of a network into groups based on statistical regularities in network connectivity. Although many such algorithms exist, community detection's No Free Lunch theorem implies that no algorithm can be optimal across all inputs. However, little is known in practice about how different algorithms over or underfit to real networks, or how to reliably assess such behavior across algorithms. Here, we present a broad investigation of over and underfitting across 16 state-of-the-art community detection algorithms applied to a novel benchmark corpus of 572 structurally diverse real-world networks. We find that (i) algorithms vary widely in the number and composition of communities they find, given the same input; (ii) algorithms can be clustered into distinct high-level groups based on similarities of their outputs on real-world networks; (iii) algorithmic differences induce wide variation in accuracy on link-based learning tasks; and, (iv) no algorithm is always the best at such tasks across all inputs. Finally, we quantify each algorithm's overall tendency to over or underfit to network data using a theoretically principled diagnostic, and discuss the implications for future advances in community detection."
714,,A Semi-Supervised Approach to Message Stance Classification.,"Georgios Giasemidis,Nikolaos Kaplis,Ioannis Agrafiotis,Jason R. C. Nurse",https://doi.org/10.1109/TKDE.2018.2880192,TKDE,2020,"Machine learning,Twitter,Graph theory,Pattern classification,Social networking (online),Semisupervised learning","Social media communications are becoming increasingly prevalent; some useful, some false, whether unwittingly or maliciously. An increasing number of rumours daily flood the social networks. Determining their veracity in an autonomous way is a very active and challenging field of research, with a variety of methods proposed. However, most of the models rely on determining the constituent messages' stance towards the rumour, a feature known as the “wisdom of the crowd.” Although several supervised machine-learning approaches have been proposed to tackle the message stance classification problem, these have numerous shortcomings. In this paper, we argue that semi-supervised learning is more effective than supervised models and use two graphbased methods to demonstrate it. This is not only in terms of classification accuracy, but equally important, in terms of speed and scalability. We use the Label Propagation and Label Spreading algorithms and run experiments on a dataset of 72 rumours and hundreds of thousands messages collected from Twitter. We compare our results on two available datasets to the state-of-the-art to demonstrate our algorithms' performance regarding accuracy, speed, and scalability for real-time applications."
715,,Adaptive Self-Paced Deep Clustering with Data Augmentation.,"Xifeng Guo 0001,Xinwang Liu,En Zhu,Xinzhong Zhu,Miaomiao Li,Xin Xu 0001,Jianping Yin",https://doi.org/10.1109/TKDE.2019.2911833,TKDE,2020,"Clustering algorithms,Training,Task analysis,Gallium nitride,Gaussian mixture model,Feature extraction","Deep clustering gains superior performance than conventional clustering by jointly performing feature learning and cluster assignment. Although numerous deep clustering algorithms have emerged in various applications, most of them fail to learn robust cluster-oriented features which in turn hurts the final clustering performance. To solve this problem, we propose a two-stage deep clustering algorithm by incorporating data augmentation and self-paced learning. Specifically, in the first stage, we learn robust features by training an autoencoder with examples that are augmented by random shifting and rotating the given clean examples. Then, in the second stage, we encourage the learned features to be cluster-oriented by alternatively finetuning the encoder with the augmented examples and updating the cluster assignments of the clean examples. During finetuning the encoder, the target of each augmented example in the loss function is the center of the cluster to which the clean example is assigned. The targets may be computed incorrectly, and the examples with incorrect targets could mislead the encoder network. To stabilize the network training, we select most confident examples in each iteration by utilizing the adaptive self-paced learning. Extensive experiments validate that our algorithm outperforms the state of the arts on four image datasets."
716,,Best Bang for the Buck - Cost-Effective Seed Selection for Online Social Networks.,"Kai Han 0003,Yuntian He,Keke Huang,Xiaokui Xiao,Shaojie Tang,Jingxin Xu,Liusheng Huang",https://doi.org/10.1109/TKDE.2019.2922271,TKDE,2020,"Approximation algorithms,Integrated circuit modeling,Social networking (online),Sampling methods,Time complexity,Greedy algorithms","We study the min-cost seed selection problem in online social networks for viral marketing, where the goal is to select a set of seed nodes with the minimum total cost such that the expected number of influenced nodes in the network exceeds a predefined threshold. We propose several algorithms that outperform the previous studies both on the theoretical approximation ratio and on the experimental performance. In the case where the nodes have heterogeneous costs, our algorithms are the first bi-criteria approximation algorithms with polynomial running time and provable approximation ratio. In the case where the users have uniform costs, our algorithms achieve logarithmic approximation ratio and provable time complexity which is smaller than that of the existing algorithms in orders of magnitude. We conduct extensive experiments using real social networks. The experimental results show that, our algorithms significantly outperform the existing algorithms both on the total cost and on the running time, and also scale well to billion-scale networks."
717,,User Interface Derivation for Business Processes.,"Lei Han,Jian Yang 0001,Weiliang Zhao,Quan Z. Sheng",https://doi.org/10.1109/TKDE.2019.2891655,TKDE,2020,"Task analysis,Interviews,Data models,Containers,Proposals,Data mining","User Interfaces (UI) are the bridge to connect Business Processes (BPs) and end users. The implementation of UIs normally needs a lot of manual efforts of developers. Aiming to resolve this issue, this work proposes a UI derivation method with a role-enriched BP (REBP) model as its foundation. This process model has the capability to present the details of task control flow and data operations in tasks. A set of control flow patterns and data operation patterns is identified. For each participant role, tasks of a process are abstracted and aggregated, then data relationships are extracted according to the identified control flow patterns and data operation patterns. A set of mandatory and recommended rules has been developed for deriving the UI logic from a BP. The solution for the UI derivation has been provided and implemented in the prototype. This proposed UI derivation method can provide help for the analysis, design, and maintenance of UI components of BPs."
718,,Cross-Domain Sentiment Encoding through Stochastic Word Embedding.,"Yanbin Hao,Tingting Mu,Richang Hong,Meng Wang 0001,Xueliang Liu,John Yannis Goulermas",https://doi.org/10.1109/TKDE.2019.2913379,TKDE,2020,"Training,Task analysis,Computational modeling,Stochastic processes,Standards,Buildings,Computer science","Sentiment analysis is an important topic concerning identification of feelings, attitudes, emotions and opinions from text. To automate such analysis, a large amount of example text needs to be manually annotated for model training. This is laborious and expensive, but the cross-domain technique is a key solution to reducing the cost by reusing annotated reviews across domains. However, its success largely relies on the learning of a robust common representation space across domains. In the recent years, significant effort has been invested to improve the cross-domain representation learning by designing increasingly more complex and elaborate model inputs and architectures. We support that it is not necessary to increase design complexity as this inevitably consumes more time in model training. Instead, we propose to explore the word polarity and occurrence information through a simple mapping and encode such information more accurately whilst managing lower computational costs. The proposed approach is unique and takes advantage of the stochastic embedding technique to tackle cross-domain sentiment alignment. Its effectiveness is benchmarked with over ten data tasks constructed from two review corpora and it is compared against ten classical and state-of-the-art methods."
719,,Interactive Bike Lane Planning Using Sharing Bikes&apos; Trajectories.,"Tianfu He,Jie Bao 0003,Sijie Ruan,Ruiyuan Li,Yanhua Li,Hui He,Yu Zheng 0004",https://doi.org/10.1109/TKDE.2019.2907091,TKDE,2020,"Trajectory,Roads,Planning,Government,Indexes,Silicon,Parallel processing","Cycling as a green transportation mode has been promoted by many governments all over the world. As a result, constructing effective bike lanes has become a crucial task to promote the cycling life style, as well-planned bike lanes can reduce traffic congestions and safety risks. Unfortunately, existing trajectory mining approaches for bike lane planning do not consider one or more key realistic government constraints: 1) budget limitations, 2) construction convenience, and 3) bike lane utilization. In this paper, we propose a data-driven approach to develop bike lane construction plans based on the large-scale real world bike trajectory data collected from Mobike, a station-less bike sharing system. We enforce these constraints to formulate our problem and introduce a flexible objective function to tune the benefit between coverage of users and the length of their trajectories. We prove the NP-hardness of the problem and propose greedy-based heuristics to address it. To improve the efficiency of the bike lane planning system for the urban planner, we propose a novel trajectory indexing structure and deploy the system based on a parallel computing framework (Storm) to improve the system's efficiency. Finally, extensive experiments and case studies are provided to demonstrate the system efficiency and effectiveness."
720,,Scalable Detection of Crowd Motion Patterns.,"Stijn Heldens,Nelly Litvak,Maarten van Steen",https://doi.org/10.1109/TKDE.2018.2879079,TKDE,2020,"Sensors,Trajectory,Pipelines,Data collection,Cameras,Target tracking","Studying the movements of crowds is important for understanding and predicting the behavior of large groups of people. When analyzing crowds, one is often interested in the long-term macro-level motions of the crowd as a whole, as opposed to the micro-level short-term movements of individuals. A high-level representation of these motions is thus desirable. In this work, we present a scalable method for detection of crowd motion patterns, i.e., spatial areas describing the dominant motions within crowds. For measuring crowd movements, we propose a fast, scalable, and low-cost method based on proximity graphs. For analyzing crowd movements, we utilize a three-stage pipeline: (1) represents the behavior of each person at each moment in time as a low-dimensional data point, (2) cluster these data points based on spatial relations, and (3) concatenate these clusters based on temporal relations. Experiments on synthetic datasets reveals our method can handle various scenarios including curved lanes and diverging flows. Evaluation on real-world datasets shows our method is able to extract useful motion patterns which could not be properly detected by existing methods. Overall, we see our work as an initial step towards rich pattern recognition."
721,,Recurrent Poisson Factorization for Temporal Recommendation.,"Seyed Abbas Hosseini,Ali Khodadadi,Keivan Alizadeh,Ali Arabzadeh,Mehrdad Farajtabar,Hongyuan Zha,Hamid R. Rabiee",https://doi.org/10.1109/TKDE.2018.2879796,TKDE,2020,"Correlation,Metadata,Social network services,History,Gold,Standards,Task analysis","Poisson Factorization (PF) is the gold standard framework for recommendation systems with implicit feedback whose variants show state-of-the-art performance on real-world recommendation tasks. However, they do not explicitly take into account the temporal behavior of users which is essential to recommend the right item to the right user at the right time. In this paper, we introduce Recurrent Poisson Factorization (RPF) framework that generalizes the classical PF methods by utilizing a Poisson process for modeling the implicit feedback. RPF treats time as a natural constituent of the model, and takes important factors for recommendation into consideration to provide a rich family of time-sensitive factorization models. They include Hierarchical RPFthat captures the consumption heterogeneity among users and items, Dynamic RPF that handles dynamic user preferences and item specifications, Social RPF that models the social-aspect of product adoption, Item-Item RPFthat considers the inter-item correlations, and eXtended Item-Item RPF that utilizes items' metadata to better infer the correlation among engagement patterns of users with items. We also develop an efficient variational algorithm for approximate inference that scales up to massive datasets. We demonstrate RPF's superior performance over many state-of-the-art methods on synthetic dataset, and wide variety of large scale real-world datasets."
722,,r-HUMO - A Risk-Aware Human-Machine Cooperation Framework for Entity Resolution with Quality Guarantees.,"Boyi Hou,Qun Chen 0001,Zhaoqiang Chen,Youcef Nafa,Zhanhuai Li",https://doi.org/10.1109/TKDE.2018.2883532,TKDE,2020,"Erbium,Measurement,Crowdsourcing,Inspection,Risk analysis,Man-machine systems,Task analysis","Even though many approaches have been proposed for entity resolution (ER), it remains very challenging to enforce quality guarantees. To this end, we propose a risk-aware HUman-Machine cOoperation framework for ER, denoted by r-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both precision and recall guarantees by partitioning an ER workload between the human and the machine. However, r-HUMO is the first solution that optimizes the process of human workload selection from a risk perspective. It iteratively selects human workload by real-time risk analysis based on the human-labeled results as well as the prespecified machine metric. In this paper, we first introduce the r-HUMO framework and then present the risk model to prioritize the instances for manual inspection. Finally, we empirically evaluate r-HUMO's performance on real data. Our extensive experiments show that r-HUMO is effective in enforcing quality guarantees, and compared with the state-of-the-art alternatives, it can achieve desired quality control with reduced human cost."
723,,A Variational Bayesian Framework for Cluster Analysis in a Complex Network.,"Lun Hu,Keith C. C. Chan,Xiaohui Yuan,Shengwu Xiong",https://doi.org/10.1109/TKDE.2019.2914200,TKDE,2020,"Complex networks,Clustering algorithms,Bayes methods,Task analysis,Analytical models,Social networking (online),Stochastic processes","A complex network is a network with non-trivial topological structures. It contains not just topological information but also attribute information available in the rich content of nodes. Concerning the task of cluster analysis in a complex network, model-based algorithms are preferred over distance-based ones, as they avoid designing specific distance measures. However, their models are only applicable to complex networks where the attribute information is composed of attributes in binary form. To overcome this disadvantage, we introduce a three-layer node-attribute-value hierarchical structure to describe the attribute information in a flexible and interpretable manner. Then, a new Bayesian model is proposed to simulate the generative process of a complex network. In this model, the attribute information is generated by following the hierarchical structure while the links between pairwise nodes are generated by a stochastic blockmodel. To solve the corresponding inference problem, we develop a variational Bayesian algorithm called TARA, which allows us to identify functionally meaningful clusters through an iterative procedure. Our extensive experiment results show that TARA can be an effective algorithm for cluster analysis in a complex network. Moreover, the parallelized version of TARA makes it possible to perform efficiently at its tasks when applied to large complex networks."
724,,Anomaly Detection Using Local Kernel Density Estimation and Context-Based Regression.,"Weiming Hu,Jun Gao,Bing Li 0001,Ou Wu,Junping Du,Stephen J. Maybank",https://doi.org/10.1109/TKDE.2018.2882404,TKDE,2020,"Anomaly detection,Kernel,Estimation,Saliency detection,Visualization,Data models,Computational modeling","Current local density-based anomaly detection methods are limited in that the local density estimation and the neighborhood density estimation are not accurate enough for complex and large databases, and the detection performance depends on the size parameter of the neighborhood. In this paper, we propose a new kernel function to estimate samples' local densities and propose a weighted neighborhood density estimation to increase the robustness to changes in the neighborhood size. We further propose a local kernel regression estimator and a hierarchical strategy for combining information from the multiple scale neighborhoods to refine anomaly factors of samples. We apply our general anomaly detection method to image saliency detection by regarding salient pixels in objects as anomalies to the background regions. Local density estimation in the visual feature space and kernel-based saliency score propagation in the image enable the assignment of similar saliency values to homogenous object regions. Experimental results on several benchmark datasets demonstrate that our anomaly detection methods overall outperform several state-of-art anomaly detection methods. The effectiveness of our image saliency detection method is validated by comparison with several state-of-art saliency detection methods."
725,,Quality Control in Crowdsourcing Using Sequential Zero-Determinant Strategies.,"Qin Hu 0001,Shengling Wang 0001,Peizi Ma,Xiuzhen Cheng,Weifeng Lv,Rongfang Bie",https://doi.org/10.1109/TKDE.2019.2896926,TKDE,2020,"Crowdsourcing,Games,Quality control,Task analysis,Process control,Gold,Economics","Quality control in crowdsourcing is challenging due to the heterogeneous nature of the workers. The state-of-the-art solutions attempt to address the issue from the technical perspective, which may be costly because they function as an additional procedure in crowdsourcing. In this paper, an economics based idea is adopted to embed quality control into the crowdsourcing process, where the requestor can take advantage of the market power to stimulate the workers for submitting high-quality jobs. Specifically, we employ two sequential games to model the interactions between the requestor and the workers, with one considering binary strategies while the other taking continuous strategies. Accordingly, two incentive algorithms for improving the job quality are proposed to tackle the sequential crowdsourcing dilemma problem. Both algorithms are based on a sequential zero-determinant (ZD) strategy modified from the classical ZD strategy. Such a revision not only provides a theoretical basis for designing our incentive algorithms, but also enlarges the application space of the classical ZD strategy itself. Our incentive algorithms have the following desired features: 1) they do not depend on any specific crowdsourcing scenario; 2) they leverage economics theory to train the workers to behave nicely for better job quality instead of filtering out the unprofessional workers; 3) no extra costs are incurred in a long run of crowdsourcing; and 4) fairness is realized as even the requestor (the ZD player), who dominates the game, cannot increase her utility by arbitrarily penalizing any innocent worker."
726,,On Combining Biclustering Mining and AdaBoost for Breast Tumor Classification.,"Qinghua Huang,Yongdong Chen,Longzhong Liu,Dacheng Tao,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2891622,TKDE,2020,"Breast tumors,Feature extraction,Breast cancer,Ultrasonic imaging","Breast cancer is now considered as one of the leading causes of deaths among women all over the world. Aiming to assist clinicians in improving the accuracy of diagnostic decisions, computer-aided diagnosis (CAD) system is of increasing interest in breast cancer detection and analysis nowadays. In this paper, a novel computer-aided diagnosis scheme with human-in-the-loop is proposed to help clinicians identify the benign and malignant breast tumors in ultrasound. In this framework, feature acquisition is performed by a user-participated feature scoring scheme that is based on Breast Imaging Reporting and Data System (BI-RADS) lexicon and experience of doctors. Biclustering mining is then used as a useful tool to discover the column consistency patterns on the training data. The patterns frequently appearing in the tumors with the same label can be regarded as a potential diagnostic rule. Subsequently, the diagnostic rules are utilized to construct component classifiers of the Adaboost algorithm via a novel rules combination strategy which resolves the problem of classification in different feature spaces (PC-DFS). Finally, the AdaBoost learning is performed to discover effective combinations and integrate them into a strong classifier. The proposed approach has been validated using a large ultrasounic dataset of 1,062 breast tumor instances (including 418 benign cases and 644 malignant cases) and its performance was compared with several conventional approaches. The experimental results show that the proposed method yielded the best prediction performance, indicating a good potential in clinical applications."
727,,Ultra-Scalable Spectral Clustering and Ensemble Clustering.,"Dong Huang,Chang-Dong Wang,Jian-Sheng Wu,Jian-Huang Lai,Chee-Keong Kwoh 0001",https://doi.org/10.1109/TKDE.2019.2903410,TKDE,2020,"Clustering algorithms,Sparse matrices,Complexity theory,Robustness,Bipartite graph,Scalability,Approximation algorithms","This paper focuses on scalability and robustness of spectral clustering for extremely large-scale datasets with limited resources. Two novel algorithms are proposed, namely, ultra-scalable spectral clustering (U-SPEC) and ultra-scalable ensemble clustering (U-SENC). In U-SPEC, a hybrid representative selection strategy and a fast approximation method for K-nearest representatives are proposed for the construction of a sparse affinity sub-matrix. By interpreting the sparse sub-matrix as a bipartite graph, the transfer cut is then utilized to efficiently partition the graph and obtain the clustering result. In U-SENC, multiple U-SPEC clusterers are further integrated into an ensemble clustering framework to enhance the robustness of U-SPEC while maintaining high efficiency. Based on the ensemble generation via multiple U-SEPC's, a new bipartite graph is constructed between objects and base clusters and then efficiently partitioned to achieve the consensus clustering result. It is noteworthy that both U-SPEC and U-SENC have nearly linear time and space complexity, and are capable of robustly and efficiently partitioning 10-million-level nonlinearly-separable datasets on a PC with 64 GB memory. Experiments on various large-scale datasets have demonstrated the scalability and robustness of our algorithms. The MATLAB code and experimental data are available at https://www.researchgate.net/publication/330760669."
728,,LERI - Local Exploration for Rare-Category Identification.,"Hao Huang 0001,Qian Yan 0001,Wei Lu 0015,Huaizhong Lin,Yunjun Gao,Lei Chen 0002",https://doi.org/10.1109/TKDE.2019.2911941,TKDE,2020,"Shape,Labeling,Clustering algorithms,Search problems,Bicycles,Computer science,Data mining","To identify the data examples of rare categories that form small compact clusters in large data sets, existing approaches mostly require enough labeled data examples as a training set to learn a classifier, assuming that the rare-category clusters are spherical or nearly spherical. Nonetheless, a large enough training set is usually difficult to obtain in practice, and rare categories in many real-world applications often form small compact clusters with arbitrary shapes. In this paper, we investigate how to identify all data examples of a rare category with an arbitrary shape based on only one seed (i.e., a labeled rare-category data example). Instead of finding a compact and spherical local region around the seed, we locally explore the data set from the seed by continuously searching and visiting the k-nearest neighbors of each newly visited data example. The local exploration connects the data examples in the objective rare category by the relationship of k-nearest neighbors, and meanwhile, suspected external data examples are filtered out if they are not close enough to any visited data example. Experimental results on both synthetic and real-world data sets are conducted, and the results verify the effectiveness and efficiency of our approach."
729,,Real-Time Ambulance Redeployment - A Data-Driven Approach.,"Shenggong Ji,Yu Zheng 0004,Wenjun Wang,Tianrui Li 0001",https://doi.org/10.1109/TKDE.2019.2914206,TKDE,2020,"Real-time systems,Hospitals,Safety,Indexes,Optimal matching,Urban areas","Emergency Medical Services (EMS) are of great importance to saving people's lives from emergent accidents and diseases by efficiently picking up patients using ambulances. The transporting capability of an EMS system (e.g., defined as the average pickup time of patients) significantly depends on the real-time redeployment strategy of ambulances. That is, which station should an ambulance be redeployed to, after it becomes available (after it transports a patient to a hospital or after it finishes the in-site treatment for a patient)? However, it is a challenging task concerning with the multiple data D1-D5 as detailed in Introduction. To this end, in this paper, we propose a data-driven real-time ambulance redeployment approach that redeploys an ambulance to a proper station after it becomes available, so as to optimize the transporting capability of an EMS system, considering the aforementioned multiple data D1-D5. Specifically, the proposed approach is comprised of two stages to well consider the D1-D5. First, we propose a method (a safety time-based urgency index) to incorporate D1, D2, and D3 into each ambulance station's urgency degree (D*). Second, we propose an optimal matching algorithm to combine D*, D4, and D5 into the redeployment of the current available ambulance. Experimental results using data collected in real world demonstrate the significant advantages of our approach over many baselines. Comparing with baselines, our approach can save ~4 minutes (~35 percent) of the average pickup time for each patient, improve the ratio of patients picked up within 10 minutes from 0.684 and 0.803 (~17 percent), and largely enhance the survival rate of patients (~12 percent for patients in category A1 and ~17 percent for patients in A2)."
730,,Corrections to &quot;NATERGM - A Model for Examining the Role of Nodal Attributes in Dynamic Social Media Networks&quot;.,"Shan Jiang,Hsinchun Chen",https://doi.org/10.1109/TKDE.2020.3005579,TKDE,2020,"Social networking (online),Management information systems,Data models,Computational modeling,Libraries",Presents corrections to affiliation information in the above named paper.
731,,QuickPoint - Efficiently Identifying Densest Sub-Graphs in Online Social Networks for Event Stream Dissemination.,"Hai Jin 0001,Changfu Lin,Hanhua Chen,Jiangchuan Liu",https://doi.org/10.1109/TKDE.2018.2881435,TKDE,2020,"Feeds,Production,Twitter,Upper bound,Convergence,Servers","Efficient event stream dissemination is a challenging problem in large-scale Online Social Network (OSN) systems due to the costly inter-server communications caused by the per-user view data storage. To solve the problem, previous schemes mainly explore the structures of social graphs to reduce the inter-server traffic. Based on the observation of high cluster coefficients in OSNs, a state-of-the-art social piggyback scheme can save redundant messages by exploiting an intrinsic hub-structure in an OSN graph for message piggybacking. Essentially, finding the best hub-structure for piggybacking is equivalent to finding a variation of the densest sub-graph. The existing scheme computes the best hub-structure by iteratively removing the node with the minimum weighted degree. Such a scheme incurs a worst computation cost of O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
), making it not scalable to large-scale OSN graphs. Using alternative hubstructure instead of the best hub-structure can speed up the piggyback assignment. However, they greatly sacrifice the communication efficiency of the assignment schedule. Different from the existing designs, in this work, we propose a QuickPoint algorithm, which removes a fraction of nodes in each iteration in finding the best hub-structure. We mathematically prove that QuickPoint converges in O(log
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">α</sub>
n)(α > 1) iterations in finding the best hub-structure for efficient piggyback. We implement QuickPoint in parallel atop Pregel, a vertex-centric distributed graph processing platform. Comprehensive experiments using large-scale data from Twitter and Flickr show that our scheme is 38.8× more efficient compared to existing schemes."
732,,"Detecting Communities with Multiplex Semantics by Distinguishing Background, General, and Specialized Topics.","Di Jin,Kunzeng Wang,Ge Zhang,Pengfei Jiao,Dongxiao He,Françoise Fogelman-Soulié,Xin Huang 0001",https://doi.org/10.1109/TKDE.2019.2937298,TKDE,2020,"Semantics,Bayes methods,Multiplexing,Inference algorithms,Probabilistic logic,Complex networks","Finding semantic communities using network topology and contents together is a hot topic in community detection. Existing methods often use word attributes in an indiscriminate way to help finding communities. Through analysis we find that, words in networked contents often embody a hierarchical semantic structure. Some words reflect a background topic of the whole network with all communities, some imply the high-level general topic covering several topic-related communities, and some imply the high-resolution specialized topic to describe each community. Ignoring such semantic structures often leads to defects in depicting networked contents where deep semantics are not fully utilized. To solve this problem, we propose a new Bayesian probabilistic model. By distinguishing words from either a background topic or some two-level topics (i.e., general and specialized topics), this model not only better utilizes the networked contents to help finding communities, but also provides a clearer multiplex semantic community interpretation. We then give an efficient variational algorithm for model inference. The superiority of this new approach is demonstrated by comparing with ten state-of-the-art methods on nine real networks and an artificial benchmark. A case study is further provided to show its strong ability in deep semantic interpretation of communities."
733,,SAL-Hashing - A Self-Adaptive Linear Hashing Index for SSDs.,"Peiquan Jin,Chengcheng Yang,Xiaoliang Wang,Lihua Yue,Dezhi Zhang",https://doi.org/10.1109/TKDE.2018.2884714,TKDE,2020,"Indexes,Parallel processing,Proposals,Bandwidth,Terminology,Drives","Flash memory based solid state drives (SSDs) have emerged as a new alternative to replace magnetic disks due to their high performance and low power consumption. However, random writes on SSDs are much slower than SSD reads. Therefore, traditional index structures, which are designed based on the symmetrical I/O property of magnetic disks, cannot completely exert the high performance of SSDs. In this paper, we propose an SSD-optimized linear hashing index called Self-Adaptive Linear Hashing (SALhashing) to reduce small random-writes to SSDs that are caused by index operations. The contributions of our work are manifold. First, we propose to organize the buckets of a linear hashing index into groups and sets to facilitate coarse-grained writes and adaptivity to access patterns. A group consisting of a fixed number of buckets is proposed to transform small random writes to buckets into coarsegrained writes and in turn improve write performance of the index. A set consists of a number of groups, and we propose to employ different split strategies for each set. With this mechanism, SAL-hashing is able to adapt to the changes of access patterns. Second, we attach a log region to each set, and amortize the cost of reads and writes by committing updates to the log region in batch. Third, in order to reduce search cost, each log region is equipped with Bloom filters to index update logs. We devise a cost-based online algorithm to adaptively merge the log region with the corresponding set when the set becomes search-intensive. Fourth, we propose a new technique called virtual split to optimize the search performance of SAL-hashing. Finally, we propose a new scheme for the management of the log buffer. We conduct extensive experiments on real SSDs. The results suggest that our proposal is self-adaptive according to the change of access patterns, and outperforms several competitors under various workloads."
734,,T-PCCE - Twitter Personality based Communicative Communities Extraction System for Big Data.,"Eleanna Kafeza,Andreas Kanavos,Christos Makris,Georgios Pispirigos,Pantelis Vikatos",https://doi.org/10.1109/TKDE.2019.2906197,TKDE,2020,"Twitter,Data mining,Big Data,Detection algorithms,Feature extraction,Facebook","The identification of social media communities has recently been of major concern, since users participating in such communities can contribute to viral marketing campaigns. In this work, we focus on users' communication considering personality as a key characteristic for identifying communicative networks i.e., networks with high information flows. We describe the Twitter Personality based Communicative Communities Extraction (T-PCCE) system that identifies the most communicative communities in a Twitter network graph considering users' personality. We then expand existing approaches in users' personality extraction by aggregating data that represent several aspects of user behavior using machine learning techniques. We use an existing modularity based community detection algorithm and we extend it by inserting a post-processing step that eliminates graph edges based on users' personality. The effectiveness of our approach is demonstrated by sampling the Twitter graph and comparing the communication strength of the extracted communities with and without considering the personality factor. We define several metrics to count the strength of communication within each community. Our algorithmic framework and the subsequent implementation employ the cloud infrastructure and use the MapReduce Programming Environment. Our results show that the T-PCCE system creates the most communicative communities."
735,,A Constrained Randomization Approach to Interactive Visual Data Exploration with Subjective Feedback.,"Bo Kang,Kai Puolamäki,Jefrey Lijffijt,Tijl De Bie",https://doi.org/10.1109/TKDE.2019.2907082,TKDE,2020,"Data visualization,Data models,Computational modeling,Data mining,Reactive power,Visualization,Tools","Data visualization and iterative/interactive data mining are growing rapidly in attention, both in research as well as in industry. However, while there are a plethora of advanced data mining methods and lots of works in the field of visualization, integrated methods that combine advanced visualization and/or interaction with data mining techniques in a principled way are rare. We present a framework based on constrained randomization which lets users explore high-dimensional data via `subjectively informative' two-dimensional data visualizations. The user is presented with `interesting' projections, allowing users to express their observations using visual interactions that update a background model representing the user's belief state. This background model is then considered by a projection-finding algorithm employing data randomization to compute a new `interesting' projection. By providing users with information that contrasts with the background model, we maximize the chance that the user encounters striking new information present in the data. This process can be iterated until the user runs out of time or until the difference between the randomized and the real data is insignificant. We present two case studies, one controlled study on synthetic data and another on census data, using the proof-of-concept tool SIDE that demonstrates the presented framework."
736,,Variable Weighting in Fuzzy k-Means Clustering to Determine the Number of Clusters.,"Imran Khan 0003,Zongwei Luo,Joshua Zhexue Huang,Waseem Shahzad",https://doi.org/10.1109/TKDE.2019.2911582,TKDE,2020,"Clustering algorithms,Data models,Bayes methods,Partitioning algorithms,Adaptation models,Data mining,Mixture models","One of the most significant problems in cluster analysis is to determine the number of clusters in unlabeled data, which is the input for most clustering algorithms. Some methods have been developed to address this problem. However, little attention has been paid on algorithms that are insensitive to the initialization of cluster centers and utilize variable weights to recover the number of clusters. To fill this gap, we extend the standard fuzzy k-means clustering algorithm. It can automatically determine the number of clusters by iteratively calculating the weights of all variables and the membership value of each object in all clusters. Two new steps are added to the fuzzy k-means clustering process. One of them is to introduce a penalty term to make the clustering process insensitive to the initial cluster centers. The other one is to utilize a formula for iterative updating of variable weights in each cluster based on the current partition of data. Experimental results on real-world and synthetic datasets have shown that the proposed algorithm effectively determined the correct number of clusters while initializing the different number of cluster centroids. We also tested the proposed algorithm on gene data to determine a subset of important genes."
737,1,Learning New Words from Keystroke Data with Local Differential Privacy.,"Sungwook Kim,Hyejin Shin,Chung Hun Baek,Soohyung Kim,Junbum Shin",https://doi.org/10.1109/TKDE.2018.2885749,TKDE,2020,"Privacy,Servers,Dictionaries,Indexes,Frequency estimation","Keystroke data collected from smart devices includes various sensitive information about users. Collecting and analyzing such data raise serious privacy concerns. Google and Apple have recently applied local differential privacy (LDP) to address privacy issue on learning new words from users' keystroke data. However, these solutions require multiple LDP reports for a single word, which result in inefficient use of privacy budget and high computational cost. In this paper, we develop a novel algorithm for learning new words under LDP. Unlike the existing solutions, the proposed method generates only one LDP report for a single word. This enables the proposed method to use full privacy budget for generating a report and brings the benefit that the proposed method provides better utility at the same privacy degree than the existing methods. In our algorithm, each user appends a hash value to new word and sends only one LDP report of an n-gram selected randomly from the string packed by each new word and its hash value. The server then decodes frequent n-grams at each position of the string and discovers the candidate words by exploring graph-theoretic links between n-grams and checking integrity of candidates with hash values. Frequencies of frequent new words discovered are estimated from distribution estimates of n-grams by robust regression. We theoretically show that our algorithm can recover popular new words even though the server does not know the domain of the raw data. In addition, we theoretically and empirically demonstrate that our algorithm achieves higher accuracy compared to the existing solutions."
738,,Equilibrium of Redundancy in Relational Model for Optimized Data Retrieval.,"Nemanja Kojic,Dragan Milicev",https://doi.org/10.1109/TKDE.2019.2911580,TKDE,2020,"Optimization,Data models,Redundancy,Unified modeling language,Distributed databases,Indexes","Conceptual and relational data models of online transaction processing (OLTP) applications are usually created and maintained following the principle of normalization, which implies avoidance of redundancy. Data retrieval from a disk-based normalized relational database often requires complex and inefficient queries that may cause noticeable performance issues when executed on larger volumes of data. Computer professionals sometimes intentionally trade off the strict normal form to optimize data retrieval queries through error-prone manual tuning and denormalization. We propose a fully automatic optimization approach, based on data redundancy, that relies on a formal cost-benefit model. We prove that finding the optimal level of data redundancy, for given workload statistics, is an NP-Complete optimization problem. A detailed reduction of the problem to binary linear programming is presented in the paper. The proposed optimization approach was evaluated using the TPCE benchmark for OLTP systems. The evaluation has shown that the proposed optimization approach is highly scalable, and that it can be efficiently applied to real-life relational data models."
739,,Parallel Simulated Annealing with a Greedy Algorithm for Bayesian Network Structure Learning.,"Sang Min Lee,Seoung Bum Kim",https://doi.org/10.1109/TKDE.2019.2899096,TKDE,2020,"Simulated annealing,Markov processes,Greedy algorithms,Bayes methods,Search problems,Convergence,Instruction sets","We present a hybrid algorithm called parallel simulated annealing with a greedy algorithm (PSAGA) to learn Bayesian network structures. This work focuses on simulated annealing and its parallelization with memoization to accelerate the search process. At each step of the local search, a hybrid search method combining simulated annealing with a greedy algorithm was adopted. The proposed PSAGA aims to achieve both the efficiency of parallel search and the effectiveness of a more exhaustive search. The Bayesian Dirichlet equivalence metric was used to determine an optimal structure for PSAGA. The proposed PSAGA was evaluated on seven well-known Bayesian network benchmarks generated at random. We first conducted experiments to evaluate the computational time performance of the proposed parallel search. We then compared PSAGA with existing variants of simulated annealing-based algorithms to evaluate the quality of the learned structure. Overall, the experimental results demonstrate that the proposed PSAGA shows better performance than the alternatives in terms of computational time and accuracy."
740,,Adaptive Consistency Propagation Method for Graph Clustering.,"Xuelong Li,Mulin Chen,Qi Wang 0009",https://doi.org/10.1109/TKDE.2019.2936195,TKDE,2020,"Manifolds,Optimization,Clustering methods,Task analysis,Eigenvalues and eigenfunctions,Data mining,Adaptation models","Graph clustering plays an important role in data mining. Based on an input data graph, data points are partitioned into clusters. However, most existing methods keep the data graph fixed during the clustering procedure, so they are limited to exploit the implied data manifold and highly dependent on the initial graph construction. Inspired by the recent development on manifold learning, this paper proposes an Adaptive Consistency Propagation (ACP) method for graph clustering. In order to utilize the features captured from different perspectives, we further put forward the Multi-view version of the ACP model (MACP). The main contributions are threefold: (1) the manifold structure of input data is sufficiently exploited by propagating the topological connectivities between data points from near to far; (2) the optimal graph for clustering is learned by taking graph learning as a part of the optimization procedure; and (3) the negotiation among the heterogeneous features is captured by the multi-view clustering model. Extensive experiments on real-world datasets validate the effectiveness of the proposed methods on both single-and multi-view clustering, and show their superior performance over the state-of-the-arts."
741,,Discrimination-Aware Projected Matrix Factorization.,"Xuelong Li,Mulin Chen,Qi Wang 0009",https://doi.org/10.1109/TKDE.2019.2936855,TKDE,2020,"Integrated circuits,Manifolds,Linear programming,Data structures,Linear discriminant analysis,Optimization,Robustness","Non-negative Matrix Factorization (NMF) has been one of the most popular clustering techniques in machine leaning, and involves various real-world applications. Most existing works perform matrix factorization on high-dimensional data directly. However, the intrinsic data structure is always hidden within the low-dimensional subspace. And, the redundant features within the input space may affect the final result adversely. In this paper, a new unsupervised matrix factorization method, Discrimination-aware Projected Matrix Factorization (DPMF), is proposed for data clustering. The main contributions are threefold: (1) The linear discriminant analysis is jointly incorporated into the unsupervised matrix factorization framework, so the clustering can be accomplished in the discriminant subspace. (2) The manifold regularization is introduced to perceive the geometric information, and the ℓ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,1</sub>
-norm is utilized to improve the robustness. (3) An efficient optimization algorithm is designed to solve the proposed problem with proved convergence. Experimental results on one toy dataset and eight real-world benchmarks show the effectiveness of the proposed method."
742,,Entropy-based Sampling Approaches for Multi-Class Imbalanced Problems.,"Lusi Li,Haibo He,Jie Li 0024",https://doi.org/10.1109/TKDE.2019.2913859,TKDE,2020,"Entropy,Earth Observing System,Information entropy,Uncertainty,Measurement uncertainty,Data mining","In data mining, large differences between multi-class distributions regarded as class imbalance issues have been known to hinder the classification performance. Unfortunately, existing sampling methods have shown their deficiencies such as causing the problems of over-generation and over-lapping by oversampling techniques, or the excessive loss of significant information by undersampling techniques. This paper presents three proposed sampling approaches for imbalanced learning: the first one is the entropy-based oversampling (EOS) approach; the second one is the entropy-based undersampling (EUS) approach; the third one is the entropy-based hybrid sampling (EHS) approach combined by both oversampling and undersampling approaches. These three approaches are based on a new class imbalance metric, termed entropy-based imbalance degree (EID), considering the differences of information contents between classes instead of traditional imbalance-ratio. Specifically, to balance a data set after evaluating the information influence degree of each instance, EOS generates new instances around difficult-to-learn instances and only remains the informative ones. EUS removes easy-to-learn instances. While EHS can do both simultaneously. Finally, we use all the generated and remaining instances to train several classifiers. Extensive experiments over synthetic and real-world data sets demonstrate the effectiveness of our approaches."
743,,Translation-Based Sequential Recommendation for Complex Users on Sparse Data.,"Hui Li 0057,Ye Liu 0002,Nikos Mamoulis,David S. Rosenblum",https://doi.org/10.1109/TKDE.2019.2906180,TKDE,2020,"Recommender systems,Task analysis,Data models,Bayes methods,Training,Recurrent neural networks","Sequential recommendation is one of the main tasks in recommender systems, where the next action (e.g., purchase, visit, and click) of the user is predicted based on his/her past sequence of actions. Translating Embeddings is a knowledge graph completion approach which was recently adapted to a translation-based sequential recommendation (TransRec) method. We observe a flaw of TransRec when handling complex translations, which hinders it from generating accurate suggestions. In view of this, we propose a translation-based recommender for complex users (CTransRec), which utilizes category-specific projection and temporal dynamic relaxation. Using our proposed Margin-based Pairwise Bayesian Personalized Ranking and Time-Aware Negative Sampling, CTransRec outperforms state-of-the-art methods for sequential recommendation on extremely sparse data. The superiority of CTransRec, which is confirmed by our extensive experiments on both public data and real data obtained from the industry, comes from not only the additional information used in training but also the fact that CTransRec makes good use of this additional information to model the complex translations."
744,,ASCENT - Active Supervision for Semi-Supervised Learning.,"Yanchao Li,Yongli Wang,Dong-Jun Yu,Ning Ye,Peng Hu,Ruxin Zhao",https://doi.org/10.1109/TKDE.2019.2897307,TKDE,2020,"Task analysis,Clustering algorithms,Data models,Redundancy,Uncertainty,Semisupervised learning","Active learning algorithms attempt to overcome the labeling bottleneck by asking queries from large collection of unlabeled examples. Existing batch mode active learning algorithms sufferfrom three limitations: (1) The methods that are based on similarityfunction or optimizing certain diversity measurement, in which may lead to suboptimal performance and produce the selected set with redundant examples. (2) The models with assumption on data are hard in finding images that are both informative and representative. (3) The problem of noise labels has been an obstacle for algorithms. In this paper, we propose a novel active learning method that makes embeddings of labeled examples to those of unlabeled ones and back via deep neural networks. The active scheme makes correct association cycles that end up at the same class from that the association was started, which considers both the informativeness and representativeness of examples, as well as being robust to the noise labels. We apply our active learning method to semi-supervised classification and clustering. The submodular function is designed to reduce the redundancy of the selected examples. Specifically, we incorporate our batch mode active scheme into the classification approaches, in which the generalization ability is improved. For semi-supervised clustering, we try to use our active scheme for constraints to make fast convergence and perform better than unsupervised clustering. Finally, we apply our active learning method to data filtering. To validate the effectiveness of the proposed algorithms, extensive experiments are conducted on diversity benchmark datasets for different tasks, i.e., classification, clustering, and data filtering, and the experimental results demonstrate consistent and substantial improvements over the state-of-the-art approaches."
745,,Citywide Bike Usage Prediction in a Bike-Sharing System.,"Yexin Li,Yu Zheng 0004",https://doi.org/10.1109/TKDE.2019.2898831,TKDE,2020,"Urban areas,Predictive models,Clustering algorithms,Meteorology,Time series analysis,Adaptation models,Prediction algorithms","To operate a bike-sharing system efficiently, system operators need to accurately predict how many bikes are to be rented and returned throughout the city. In this paper, we propose a Hierarchical Consistency Prediction (HCP) model to predict the citywide bike usage in the next period. First, an Adaptive Transition Constraint (AdaTC) clustering algorithm is proposed to cluster stations into groups, making the rent and transition at each cluster more regular than those at each single station. Second, a Similarity-based efficient Gaussian Process Regressor (SGPR) is proposed to respectively predict how many bikes are to be rented at different-scale locations, i.e., at each station, each cluster, and in the entire city. Besides largely improving the training and online prediction efficiency, our regressor considers external impacted factors, addresses the data unbalance issue, and better captures the non-linearity in spatio-temporal data. Third, we design a General Least Square (GLS) formulation to collectively improve those obtained predictions via a mutual reinforcement way. GLS makes the final predictions for rent more reasonable. Considering the causality between rent and return, a Transition based Inference (TINF) method is designed to infer the citywide bike return demand based on the predicted rent demands. Experiments on real-world data are conducted to confirm the effectiveness of our model."
746,,"Approximate Nearest Neighbor Search on High Dimensional Data - Experiments, Analyses, and Improvement.","Wen Li,Ying Zhang 0001,Yifang Sun,Wei Wang 0011,Mingjie Li,Wenjie Zhang 0001,Xuemin Lin 0001",https://doi.org/10.1109/TKDE.2019.2909204,TKDE,2020,"Machine learning,Performance evaluation,Data models,Nearest  neighbor methods","Nearest neighbor search is a fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Because exact searching results are not efficient for a high-dimensional space, a lot of efforts have turned to approximate nearest neighbor search. Although many algorithms have been continuously proposed in the literature each year, there is no comprehensive evaluation and analysis of their performance. In this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 19 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings."
747,,Scalable Spectral Clustering for Overlapping Community Detection in Large-Scale Networks.,"Hadrien Van Lierde,Tommy W. S. Chow,Guanrong Chen",https://doi.org/10.1109/TKDE.2019.2892096,TKDE,2020,"Clustering algorithms,Image edge detection,Benchmark testing,Partitioning algorithms,Social network services,Detection algorithms,Optimization","While the majority of methods for community detection produce disjoint communities of nodes, most real-world networks naturally involve overlapping communities. In this paper, a scalable method for the detection of overlapping communities in large networks is proposed. The method is based on an extension of the notion of normalized cut to cope with overlapping communities. A spectral clustering algorithm is formulated to solve the related cut minimization problem. When available, the algorithm may take into account prior information about the likelihood for each node to belong to several communities. This information can either be extracted from the available metadata or from node centrality measures. We also introduce a hierarchical version of the algorithm to automatically detect the number of communities. In addition, a new benchmark model extending the stochastic blockmodel for graphs with overlapping communities is formulated. Our experiments show that the proposed spectral method outperforms the state-of-the-art algorithms in terms of computational complexity and accuracy on our benchmark graph model and on five real-world networks, including a lexical network and large-scale social networks. The scalability of the proposed algorithm is also demonstrated on large synthetic graphs with millions of nodes and edges."
748,,Fraud Detection in Dynamic Interaction Network.,"Hao Lin 0002,Guannan Liu,Junjie Wu 0002,Yuan Zuo,Xin Wan,Hong Li",https://doi.org/10.1109/TKDE.2019.2912817,TKDE,2020,"Hidden Markov models,Anomaly detection,Data models,Probabilistic logic,Correlation,Telecommunications","Fraud detection from massive user behaviors is often regarded as trying to find a needle in a haystack. In this paper, we suggest abnormal behavioral patterns can be better revealed if both sequential and interaction behaviors of users can be modeled simultaneously, which however has rarely been addressed in prior work. Along this line, we propose a COllective Sequence and INteraction (COSIN) model, in which the behavioral sequences and interactions between source and target users in a dynamic interaction network are modeled uniformly in a probabilistic graphical model. More specifically, the sequential schema is modeled with a hierarchical Hidden Markov Model, and meanwhile it is shifted to the interaction schema to generate the interaction counts through Poisson factorization. A hybrid Gibbs-Variational algorithm is then proposed for efficient parameter estimation of the COSIN model. We conduct extensive experiments on both synthetic and real-world telecom datasets in different scales, and the results show that the proposed model outperforms some competitive baseline methods and is scalable. A case is further presented to show the precious explainability of the model."
749,,Explainable Outfit Recommendation with Joint Outfit Matching and Comment Generation.,"Yujie Lin,Pengjie Ren,Zhumin Chen,Zhaochun Ren,Jun Ma 0001,Maarten de Rijke",https://doi.org/10.1109/TKDE.2019.2906190,TKDE,2020,"Visualization,Feature extraction,Task analysis,Clothing,Computer science,Recurrent neural networks,Recommender systems","Most previous work on outfit recommendation focuses on designing visual features to enhance recommendations. Existing work neglects user comments of fashion items, which have been proven to be effective in generating explanations along with better recommendation results. We propose a novel neural network framework, neural outfit recommendation (NOR), that simultaneously provides outfit recommendations and generates abstractive comments. Neural outfit recommendation (NOR) consists of two parts: outfit matching and comment generation. For outfit matching, we propose a convolutional neural network with a mutual attention mechanism to extract visual features. The visual features are then decoded into a rating score for the matching prediction. For abstractive comment generation, we propose a gated recurrent neural network with a cross-modality attention mechanism to transform visual features into a concise sentence. The two parts are jointly trained based on a multi-task learning framework in an end-to-end back-propagation paradigm. Extensive experiments conducted on an existing dataset and a collected real-world dataset show NOR achieves significant improvements over state-of-the-art baselines for outfit recommendation. Meanwhile, our generated comments achieve impressive ROUGE and BLEU scores in comparison to human-written comments. The generated comments can be regarded as explanations for the recommendation results. We release the dataset and code to facilitate future research."
750,,Efficient Entity Resolution on Heterogeneous Records.,"Yiming Lin,Hongzhi Wang 0001,Jianzhong Li 0001,Hong Gao 0001",https://doi.org/10.1109/TKDE.2019.2898191,TKDE,2020,"Erbium,Indexes,Measurement,Merging,Companies,Data integration,Transforms","Entity resolution (ER) is the problem of identifying and merging records that refer to the same real-world entity. In many scenarios, raw records are stored under heterogeneous environment. Specifically, the schemas of records may differ from each other. To leverage such records better, most existing work assume that schema matching and data exchange have been done to convert records under different schemas to those under a predefined schema. However, we observe that schema matching would lose information in some cases, which could be useful or even crucial to ER. To leverage sufficient information from heterogeneous sources, in this paper, we address several challenges of ER on heterogeneous records and show that none of existing similarity metrics or their transformations could be applied to find similar records under heterogeneous settings. Motivated by this, we design the similarity function and propose a novel framework to iteratively find records which refer to the same entity. Regarding efficiency, we build an index to generate candidates and accelerate similarity computation. Evaluations on real-world datasets show the effectiveness and efficiency of our methods."
751,,Evaluation of Community Detection Methods.,"Xin Liu,Hui-Min Cheng,Zhong-Yuan Zhang",https://doi.org/10.1109/TKDE.2019.2911943,TKDE,2020,"Indexes,Mutual information,Entropy,Clustering methods,Measurement,Complex networks","Community structures are critical towards understanding not only the network topology but also how the network functions. However, how to evaluate the quality of detected community structures is still challenging and remains unsolved. The most widely used metric, normalized mutual information (NMI), was proven to have finite size effect, and its improved form relative normalized mutual information (rNMI) has reverse finite size effect. Corrected normalized mutual information (cNMI) was thus proposed and has neither finite size effect nor reverse finite size effect. However, in this paper, we show that cNMI violates the so-called proportionality assumption. In addition, NMI-type metrics have the problem of ignoring importance of small communities. Finally, they cannot be used to evaluate a single community of interest. In this paper, we map the computed community labels to the ground-truth ones through integer linear programming, and then use kappa index and F-score to evaluate the detected community structures. Experimental results demonstrate the advantages of our method."
752,,Learning Distilled Graph for Large-Scale Social Network Data Clustering.,"Wenhe Liu,Dong Gong,Mingkui Tan,Qinfeng (Javen) Shi,Yi Yang 0001,Alexander G. Hauptmann",https://doi.org/10.1109/TKDE.2019.2904068,TKDE,2020,"Social networking (online),Feature extraction,Spectral analysis,Clustering algorithms,Noise measurement,Sparse matrices,Laplace equations","Spectral analysis is critical in social network analysis. As a vital step of the spectral analysis, the graph construction in many existing works utilizes content data only. Unfortunately, the content data often consists of noisy, sparse, and redundant features, which makes the resulting graph unstable and unreliable. In practice, besides the content data, social network data also contain link information, which provides additional information for graph construction. Some of previous works utilize the link data. However, the link data is often incomplete, which makes the resulting graph incomplete. To address these issues, we propose a novel Distilled Graph Clustering (DGC) method. It pursuits a distilled graph based on both the content data and the link data. The proposed algorithm alternates between two steps: in the feature selection step, it finds the most representative feature subset w.r.t. an intermediate graph initialized with link data; in graph distillation step, the proposed method updates and refines the graph based on only the selected features. The final resulting graph, which is referred to as the distilled graph, is then utilized for spectral clustering on the large-scale social network data. Extensive experiments demonstrate the superiority of the proposed method."
753,,Model-Based Synthetic Sampling for Imbalanced Data.,"Chien-Liang Liu,Po-Yen Hsieh",https://doi.org/10.1109/TKDE.2019.2905559,TKDE,2020,"Data models,Machine learning,Training,Sampling methods,Manufacturing,Kernel,Data mining","Imbalanced data is characterized by the severe difference in observation frequency between classes and has received a lot of attention in data mining research. The prediction performances usually deteriorate as classifiers learn from imbalanced data, as most classifiers assume the class distribution is balanced or the costs for different types of classification errors are equal. Although several methods have been devised to deal with imbalance problems, it is still difficult to generalize those methods to achieve stable improvement in most cases. In this study, we propose a novel framework called model-based synthetic sampling (MBS) to cope with imbalance problems, in which we integrate modeling and sampling techniques to generate synthetic data. The key idea behind the proposed method is to use regression models to capture the relationship between features and to consider data diversity in the process of data generation. We conduct experiments on 13 datasets and compare the proposed method with 10 methods. The experimental results indicate that the proposed method is not only comparative but also stable. We also provide detailed investigations and visualizations of the proposed method to empirically demonstrate why it could generate good data samples."
754,,Structural Representation Learning for User Alignment Across Social Networks.,"Li Liu 0030,Xin Li 0033,William K. Cheung,Lejian Liao",https://doi.org/10.1109/TKDE.2019.2911516,TKDE,2020,"Social networking (online),Task analysis,Computational modeling,Learning systems,Context modeling,Optimization,Manifolds","Aligning users across different social networks has become increasingly studied as an important task to social network analysis. In this paper, we propose a novel representation learning method that mainly exploits social structures for the network alignment. In particular, the proposed network embedding framework models the follower-ship and followee-ship of each user explicitly as input and output context vectors, while preserving the proximity of users with “similar” followers and followees in the embedded space. We incorporate both known and predicted user anchors across the networks as constraints to facilitate the transfer of context information to achieve accurate user alignment. Both network embedding and user alignment are inferred under a unified optimization framework with negative sampling adopted to ensure scalability. Also, variants of the proposed framework, including the incorporation of higher-order structural features, are also explored for further boosting the alignment accuracy. Extensive experiments on large-scale social and academia network datasets demonstrate the efficacy of our proposed model compared with state-of-the-art methods."
755,,Complication Risk Profiling in Diabetes Care - A Bayesian Multi-Task and Feature Relationship Learning Approach.,"Bin Liu 0045,Ying Li,Soumya Ghosh,Zhaonan Sun,Kenney Ng,Jianying Hu",https://doi.org/10.1109/TKDE.2019.2904060,TKDE,2020,"Diabetes,Task analysis,Diseases,Medical diagnostic imaging,Predictive models,Analytical models","Diabetes mellitus, commonly known as diabetes, is a chronic disease that often results in multiple complications. Risk prediction of diabetes complications is critical for healthcare professionals to design personalized treatment plans for patients in diabetes care for improved outcomes. In this paper, focusing on Type 2 diabetes mellitus (T2DM), we study the risk of developing complications after the initial T2DM diagnosis from longitudinal patient records. We propose a novel multi-task learning approach to simultaneously model multiple complications where each task corresponds to the risk modeling of one complication. Specifically, the proposed method strategically captures the relationships (1) between the risks of multiple T2DM complications, (2) between different risk factors, and (3) between the risk factor selection patterns, which assumes similar complications have similar contributing risk factors. The method uses coefficient shrinkage to identify an informative subset of risk factors from high-dimensional data, and uses a hierarchical Bayesian framework to allow domain knowledge to be incorporated as priors. The proposed method is favorable for healthcare applications because in addition to improved prediction performance, relationships among the different risks and among risk factors are also identified. Extensive experimental results on a large electronic medical claims database show that the proposed method outperforms state-of-the-art models by a significant margin. Furthermore, we show that the risk associations learned and the risk factors identified lead to meaningful clinical insights."
756,,NewMCOS - Towards a Practical Multi-Cloud Oblivious Storage Scheme.,"Zheli Liu,Bo Li 0062,Yanyu Huang,Jin Li 0002,Yang Xiang 0001,Witold Pedrycz",https://doi.org/10.1109/TKDE.2019.2891581,TKDE,2020,"Time factors,Cloud computing,Servers,Encryption,Random access memory,Bandwidth","Encryption alone is not enough to protect data privacy, because access pattern leaks some sensitive information. Oblivious RAM (ORAM), the solution to this problem, is still far from practical deployment for heavy storage and communication/computation overhead. To reduce them, an insightful idea was proposed to utilize non-colluding clouds to shift client computation and client-cloud communication to the clouds. The proposed multi-cloud ORAM achieved O(1) client-cloud bandwidth cost and removed most of client computation. In this paper, we exploit “disconnected ORAMoperation” and design “two-layerencryption” to further reduce these overheads. Experiments show that our proposed scheme, NewMCOS, significantly reduces evict cache size from GB/MB to KB level with about 2-3 times lower response time and 20 percent savings in bandwidth for clouds, compared to other schemes. Theoretically speaking, we reduce evict cache size from O(√N) to O(ZK), where N is the number of real data blocks, K is the number of clouds (2 <; K <; <; √N ), and Z is the number of real blocks uploaded from the client for eviction. By employing “lazy eviction operation”, the write frequency is reduced by O(Z), the shuffling bandwidth cost is reduced by Ω(Z log Z). Meanwhile, NewMCOS is proved to be secure."
757,,Generative Adversarial Active Learning for Unsupervised Outlier Detection.,"Ye-Zheng Liu 0001,Zhe Li,Chong Zhou,Yuanchun Jiang,Jianshan Sun,Meng Wang 0001,Xiangnan He 0001",https://doi.org/10.1109/TKDE.2019.2905606,TKDE,2020,"Anomaly detection,Generators,Computational modeling,Data models,Training,Generative adversarial networks,Gallium nitride","Outlier detection is an important topic in machine learning and has been used in a wide range of applications. In this paper, we approach outlier detection as a binary-classification issue by sampling potential outliers from a uniform reference distribution. However, due to the sparsity of data in high-dimensional space, a limited number of potential outliers may fail to provide sufficient information to assist the classifier in describing a boundary that can separate outliers from normal data effectively. To address this, we propose a novel Single-Objective Generative Adversarial Active Learning (SO-GAAL) method for outlier detection, which can directly generate informative potential outliers based on the mini-max game between a generator and a discriminator. Moreover, to prevent the generator from falling into the mode collapsing problem, the stop node of training should be determined when SO-GAAL is able to provide sufficient information. But without any prior information, it is extremely difficult for SO-GAAL. Therefore, we expand the network structure of SO-GAAL from a single generator to multiple generators with different objectives (MO-GAAL), which can generate a reasonable reference distribution for the whole dataset. We empirically compare the proposed approach with several state-of-the-art outlier detection methods on both synthetic and real-world datasets. The results show that MO-GAAL outperforms its competitors in the majority of cases, especially for datasets with various cluster types or high irrelevant variable ratio. The experiment codes are available at: https://github.com/leibinghe/GAAL-based-outlier-detection."
758,,VA-Store - A Virtual Approximate Store Approach to Supporting Repetitive Big Data in Genome Sequence Analyses.,"Xianying Liu,Qiang Zhu 0001,Sakti Pramanik,C. Titus Brown,Gang Qian",https://doi.org/10.1109/TKDE.2018.2885952,TKDE,2020,"Bioinformatics,Genomics,Sequences,Sequential analysis,Search problems,Big Data,Query processing","In recent years, we have witnessed an increasing demand to process big data in numerous applications. It is observed that there often exist substantial amounts of repetitive data in different portions of a big data repository/dataset for applications such as genome sequence analyses. In this paper, we present a novel method, called the VA-Store, to reduce the large space requirement for repetitive data in prevailing genome sequence analysis tasks using k-mers (i.e., subsequences of length k) with multiple k values. The VA-Store maintains a physical store for one portion of the input dataset (i.e., k
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-mers) and supports multiple virtual stores for other portions of the dataset (i.e., k-mers with k ≠ k
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
). Utilizing important relationships among repetitive data, the VA-Store transforms a given query on a virtual store into one or more queries on the physical store for execution. Both precise and approximate transformations are considered. Accuracy estimation models for approximate solutions are derived. Query optimization strategies are suggested to improve query performance. Our experiments using real and synthetic datasets demonstrate that the VA-Store is quite promising in providing effective storage and efficient query processing for solving a kernel database problem on repetitive big data for genome sequence analysis applications."
759,,The Disruptions of 5G on Data-Driven Technologies and Applications.,"Dumitrel Loghin,Shaofeng Cai,Gang Chen 0001,Tien Tuan Anh Dinh,Feiyi Fan,Qian Lin,Janice Ng,Beng Chin Ooi,Xutao Sun,Quang-Trung Ta,Wei Wang 0059,Xiaokui Xiao,Yang Yang 0002,Meihui Zhang,Zhonghua Zhang",https://doi.org/10.1109/TKDE.2020.2967670,TKDE,2020,"5G mobile communication,Virtualization,Bandwidth,Security,Network slicing,Wireless fidelity,Frequency measurement","With 5G on the verge of being adopted as the next mobile network, there is a need to analyze its impact on the landscape of computing and data management. In this paper, we analyze the impact of 5G on both traditional and emerging technologies and project our view on future research challenges and opportunities. With a predicted increase of 10-100× in bandwidth and 5-10x decrease in latency, 5G is expected to be the main enabler for smart cities, smart IoT and efficient healthcare, where machine learning is conducted at the edge. In this context, we investigate how 5G can help the development of federated learning. Network slicing, another key feature of 5G, allows running multiple isolated networks on the same physical infrastructure. However, security remains the main concern in the context of virtualization, multi-tenancy and high device density. Formal verification of 5G networks can be applied to detect security issues in massive virtualized environments. In summary, 5G will make the world even more densely and closely connected. What we have experienced in 4G connectivity will pale in comparison to the vast amounts of possibilities engendered by 5G."
760,,ROAM - A Fundamental Routing Query on Road Networks with Efficiency.,"Siqiang Luo,Reynold Cheng,Ben Kao,Xiaokui Xiao,Shuigeng Zhou,Jiafeng Hu",https://doi.org/10.1109/TKDE.2019.2906188,TKDE,2020,"Public transportation,Roads,Task analysis,Crowdsourcing,Euclidean distance,Indexes,Computer science","Novel road-network applications often recommend a moving object (e.g., a vehicle) about interesting services or tasks on its way to a destination. A taxi-sharing system, for instance, suggests a new passenger to a taxi while it is serving another one. The traveling cost is then shared among these passengers. A fundamental query is: given two nodes s and t, and an area A on road network graph , is there a “good” route (e.g., short enough path) P from s to t that crosses A in G? In a taxi-sharing system, s and t can be a taxi's current and destined locations, and A contains all the places to which a person waiting for a taxi is willing to walk. Answering this Route and Area Matching (ROAM) Query allows the application involved to recommend appropriate services to users efficiently. In this paper, we examine efficient ROAM query algorithms. Particularly, we develop solutions for finding a ρ-route, which is an s-t path that passes A, with a length of at most (1 + ρ) times the shortest distance between s and t. The existence of a ρ-route implies that a service or task located at A can be found for a given moving object m, and that m only deviates slightly from its current route. We present comprehensive studies on index-free and index-based algorithms for answering ROAM queries. Comprehensive experiments show that our algorithm runs up to 30 times faster than baseline algorithms."
761,,BATON - Batch One-Hop Personalized PageRanks with Efficiency and Accuracy.,"Siqiang Luo,Xiaokui Xiao,Wenqing Lin,Ben Kao",https://doi.org/10.1109/TKDE.2019.2912606,TKDE,2020,"Twitter,Art,Monte Carlo methods,Heuristic algorithms,Pins,Benchmark testing","Personalized PageRank (PPR) is a classic measure of the relevance among different nodes in a graph, and has been applied in numerous systems, such as Twitter's Who-To-Follow and Pinterest's Related Pins. Existing work on PPR has mainly focused on three general types of queries, namely, single-pair PPR, single-source PPR, and all-pair PPR. However, we observe that there are applications that rely on a new query type (referred to as batch one-hop PPR), which takes as input a set S of source nodes and, for each nodes E ϵ S and each of s's neighbor v, asks for the PPR value of v with respect to s. None of the existing PPR algorithms is able to efficiently process batch one-hop queries, due to the inherent differences between batch one-hop PPR and the three general query types. To address the limitations of existing algorithms, this paper presents Baton, an algorithm for batch one-hop PPR that offers both strong theoretical guarantees and practical efficiency. Baton leverages the characteristics of one-hop PPR to avoid unnecessary computation, and it incorporates advanced mechanisms to improve the cost-effectiveness of PPR derivations. Extensive experiments on benchmark datasets show that Baton is up to three orders of magnitude faster than the state of the art, while offering the same accuracy."
762,,An Efficient Approach to Finding Dense Temporal Subgraphs.,"Shuai Ma 0001,Renjun Hu,Luoshu Wang,Xuelian Lin,Jinpeng Huai",https://doi.org/10.1109/TKDE.2019.2891604,TKDE,2020,"Aggregates,Approximation algorithms,Roads,Heuristic algorithms,Steiner trees,Semantics,Urban areas","Dense subgraph discovery has proven useful in various applications of temporal networks. We focus on a special class of temporal networks whose nodes and edges are kept fixed, but edge weights regularly vary with timestamps. However, finding dense subgraphs in temporal networks is non-trivial, and its state of the art solution uses a filter-and-verification framework that is not scalable on large temporal networks. In this study, we propose a highly efficient approach to finding dense subgraphs in large temporal networks with T timestamps. (1) We first develop a statistics-driven approach that employs hidden statistics to identifying k time intervals, instead of T(T + 1)/2 ones (k is typically much smallerthan T), which strikes a balance between quality and efficiency. (2) After proving that the problem has no constant factor approximation algorithms, we design better heuristic algorithms to attack the problem, by connecting finding dense subgraphs with a variant of the Prize Collecting Steiner Tree problem. (3) Finally, we have conducted an extensive experimental study to verify that our approach is both effective and efficient."
763,,A Hybrid Discriminative Mixture Model for Cumulative Citation Recommendation.,"Lerong Ma,Dandan Song,Lejian Liao,Jingang Wang",https://doi.org/10.1109/TKDE.2019.2893328,TKDE,2020,"Mixture models,Task analysis,Training data,Knowledge based systems,Correlation,Encyclopedias","This paper explores Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to deal with unseen entities without annotation. A compromised solution is to build a global entity-unspecific model for all entities without respect to the relationship information among entities, which cannot guarantee achieving a satisfactory result for each entity. Moreover, most previous methods can not adequately exploit prior knowledge embedded in entities or documents due to considering all kinds of features indifferently. In this paper, we propose a novel entity and document class-dependent discriminative mixture model by introducing one intermediate layer to model the correlation between entity-document pairs and hybrid latent entity-document classes. The model can better adjust to different types of entities and documents, and achieve better performance when dealing with a broad range of entity and document classes. An extensive set of experiments has been conducted on two offical datasets, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance."
764,,"Nonparametric Density Estimation Using Copula Transform, Bayesian Sequential Partitioning, and Diffusion-Based Kernel Estimator.","Aref Majdara,Saeid Nooshabadi",https://doi.org/10.1109/TKDE.2019.2930052,TKDE,2020,"Estimation,Kernel,Bandwidth,Histograms,Transforms,Bayes methods,Diffusion processes","Non-parametric density estimation methods are more flexible than parametric methods, due to the fact that they do not assume any specific shape or structure for the data. Most non-parametric methods, like Kernel estimation, require tuning of parameters to achieve good data smoothing, a non-trivial task, even in low dimensions. In higher dimensions, sparsity of data in local neighborhoods becomes a challenge even for non-parametric methods. In this paper, we use the copula transform and two efficient non-parametric methods to develop a new method for improved non-parametric density estimation in multivariate domain. After separation of marginal and joint densities using copula transform, a diffusion-based kernel estimator is employed to estimate the marginals. Next, Bayesian sequential partitioning (BSP) is used in the joint density estimation."
765,,A General Centrality Framework-Based on Node Navigability.,"Pasquale De Meo,Mark Levene,Fabrizio Messina,Alessandro Provetti",https://doi.org/10.1109/TKDE.2019.2947035,TKDE,2020,"Measurement,Navigation,Eigenvalues and eigenfunctions,Task analysis,Indexes,Social networking (online),Computer science","Centrality metrics are a popular tool in Network Science to identify important nodes within a graph. We introduce the Potential Gain as a centrality measure that unifies many walk-based centrality metrics in graphs and captures the notion of node navigability, interpreted as the property of being reachable from anywhere else (in the graph) through short walks. Two instances of the Potential Gain (called the Geometric and the Exponential Potential Gain) are presented and we describe scalable algorithms for computing them on large graphs. We also give a proof of the relationship between the new measures and established centralities. The geometric potential gain of a node can thus be characterized as the product of its Degree centrality by its Katz centrality scores. At the same time, the exponential potential gain of a node is proved to be the product of Degree centrality by its Communicability index. These formal results connect potential gain to both the “popularity” and “similarity” properties that are captured by the above centralities."
766,,Sticky Policies - A Survey.,"Daniele Miorandi,Alessandra Rizzardi,Sabrina Sicari,Alberto Coen-Porisini",https://doi.org/10.1109/TKDE.2019.2936353,TKDE,2020,"Data privacy,Privacy,Access control,Data security","In the digital age, where the Internet connects things across the globe and individuals are constantly online, data security and privacy are becoming key drivers (and barriers) of change for adoption of innovative solutions. Traditional approaches, whereby communication links are secured by means of encryption, and access control is run in a static way by a centralized authority, are showing their limits when applied to massive-scale, interconnected and distributed systems. Regulations, while still fragmented, are moving to adapt to changes in technology and society, with the aim to protect confidential information by governments, businesses, and individual citizens. In this landscape, proper mechanisms should be defined to allow a strict control over the data life-cycle and to guarantee the privacy and the application of specific regulations on personal information's disclosure, usage and access. Sticky policies represent one approach to improve owners' control over their data. In such an approach, machine-readable policies are attached to data. They are called 'sticky' in that they travel together with data, as data travels across multiple administrative domains. In this article we survey the state-of-the-art in sticky policies, discussing limitations, open issues, applications and research challenges, with a specific focus on their applicability to Internet of Things, cloud computing, and Content Centric Networking."
767,,Learning to Weight for Text Classification.,"Alejandro Moreo,Andrea Esuli,Fabrizio Sebastiani 0001",https://doi.org/10.1109/TKDE.2018.2883446,TKDE,2020,"Training data,Task analysis,Training,Neural networks,Feature extraction,Time-frequency analysis,Information retrieval","In information retrieval (IR) and related tasks, term weighting approaches typically consider the frequency of the term in the document and in the collection in order to compute a score reflecting the importance of the term for the document. In tasks characterized by the presence of training data (such as text classification) it seems logical that the term weighting function should take into account the distribution (as estimated from training data) of the term across the classes of interest. Although “supervised term weighting” approaches that use this intuition have been described before, they have failed to show consistent improvements. In this article, we analyze the possible reasons for this failure, and call consolidated assumptions into question. Following this criticism, we propose a novel supervised term weighting approach that, instead of relying on any predefined formula, learns a term weighting function optimized on the training set of interest; we dub this approach Learning to Weight (LTW). The experiments that we run on several well-known benchmarks, and using different learning methods, show that our method outperforms previous term weighting approaches in text classification."
768,,Scalable Multiway Stream Joins in Hardware.,"Mohammadreza Najafi,Mohammad Sadoghi,Hans-Arno Jacobsen",https://doi.org/10.1109/TKDE.2019.2916860,TKDE,2020,"Pipelines,Real-time systems,Computer architecture,Acceleration,Microsoft Windows,Buffer storage,Computational efficiency,Internet of Things","Efficient real-time analytics are an integral part of an increasing number of data management applications, such as computational targeted advertising, algorithmic trading, and Internet of Things. In this paper, we focus primarily on accelerating stream joins, which are arguably one of the most commonly used and resource-intensive operators in stream processing. We propose a scalable circular pipeline design (Circular-MJ) in hardware to orchestrate a multiway join while minimizing data flow disruption. In this circular design, each new tuple (given its origin stream) starts its processing from a specific join core and passes through all respective join cores in a pipeline sequence to produce the final results. We also present a novel two-stage pipeline stream join (Stashed-MJ) that uses a best-effort buffering technique (referred to as stash) to maintain intermediate results. If an overwrite is detected in the stash, our design automatically resorts to recomputing intermediate results. Finally, we present a parallelized version of our multiway stream join by integrating our proposed pipelines into a parallel unidirectional flow-based architecture (Parallel-MJ). Our experimental results demonstrate a linear throughput scaling with respect to the numbers of streams and processing cores."
769,,Blocking Self-Avoiding Walks Stops Cyber-Epidemics - A Scalable GPU-Based Approach.,"Hung T. Nguyen 0003,Alberto Cano 0001,Tam Vu,Thang N. Dinh",https://doi.org/10.1109/TKDE.2019.2904969,TKDE,2020,"Graphics processing units,Instruction sets,Approximation algorithms,Twitter,Partitioning algorithms,Heuristic algorithms","Cyber-epidemics, the widespread of fake news or propaganda through social media, can cause devastating economic and political consequences. A common countermeasure against cyber-epidemics is to disable a small subset of suspected social connections or accounts to effectively contain the epidemics. An example is the recent shutdown of 125,000 ISIS-related Twitter accounts. Despite many proposed methods to identify such a subset, none are scalable enough to provide high-quality solutions in nowadays' billion-size networks. To this end, we investigate the Spread Interdiction problems that seek the most effective links (or nodes) for removal under the well-known Linear Threshold model. We propose novel CPU-GPU methods that scale to networks with billions of edges, yet possess rigorous theoretical guarantee on the solution quality. At the core of our methods is an O(1)-space out-of-core algorithm to generate a new type of random walks, called Hitting Self-avoiding Walks (HSAWs). Such a low memory requirement enables handling of big networks and, more importantly, hiding latency via scheduling of millions of threads on GPUs. Comprehensive experiments on real-world networks show that our algorithms provide much higher quality solutions and are several orders of magnitude faster than the state-of-the art. Comparing to the (single-core) CPU counterpart, our GPU implementations achieve significant speedup factors up to 177× on a single GPU and 338× on a GPU pair."
770,,Joint Multi-View Hashing for Large-Scale Near-Duplicate Video Retrieval.,"Xiushan Nie,Weizhen Jing,Chaoran Cui,Chen Jason Zhang,Lei Zhu 0002,Yilong Yin",https://doi.org/10.1109/TKDE.2019.2913383,TKDE,2020,"Feature extraction,Hash functions,Time complexity,Indexing,Error analysis,Computer science,Economics","Multi-view hashing can well support large-scale near-duplicate video retrieval, due to its desirable advantages of mutual reinforcement of multiple features, low storage cost, and fast retrieval speed. However, there are still two limitations that impede its performance. First, existing methods only consider local structures in multiple features. They ignore the global structure that is important for near-duplicate video retrieval, and cannot fully exploit the dependence and complementarity of multiple features. Second, existing works always learn hashing functions bit by bit, which unfortunately increases the time complexity of hash function learning. In this paper, we propose a supervised hashing scheme, termed as joint multi-view hashing (JMVH), to address the aforementioned problems. It jointly preserves the global and local structures of multiple features while learning hashing functions efficiently. Specially, JMVH considers features of video as items, based on which an underlying Hamming space is learned by simultaneously preserving their local and global structures. In addition, a simple but efficient multi-bit hash function learning based on generalized eigenvalue decomposition is devised to learn multiple hash functions within a single step. It can significantly reduce the time complexity of conventional hash function learning processes that sequentially learn multiple hash functions bit by bit. The proposed JMVH is evaluated on two public databases: CC_WEB_VIDEO and UQ_VIDEO. Experimental results demonstrate that the proposed JMVH achieves more than a 5 percent improvement compared to several state-of-the-art methods which indicates the superior performance of JMVH."
771,,Semi-Supervised Learning with Auto-Weighting Feature and Adaptive Graph.,"Feiping Nie 0001,Shaojun Shi,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2901853,TKDE,2020,"Semisupervised learning,Feature extraction,Data models,Sparse matrices,Adaptation models,Correlation,Kernel","Traditional graph-based Semi-Supervised Learning (SSL) methods usually contain two separate steps. First, constructing an affinity matrix. Second, inferring the unknown labels. While such a two-step method has been successful, it cannot take full advantage of the correlation between affinity matrix and label information. In order to address the above problem, we propose a novel graph-based SSL method. It can learn the affinity matrix and infer the unknown labels simultaneously. Moreover, feature selection with auto-weighting is introduced to extract the effective and robust features. Further, the proposed method learns the data similarity matrix by assigning the adaptive neighbors for each data point based on the local distance. We solve the unified problem via an alternative minimization algorithm. Extensive experimental results on synthetic data and benchmark data show that the proposed method consistently outperforms the state-of-the-art approaches."
772,,Multiview Semi-Supervised Learning Model for Image Classification.,"Feiping Nie 0001,Lai Tian,Rong Wang 0001,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2920985,TKDE,2020,"Semisupervised learning,Data models,Adaptation models,Optimization,Computational modeling,Feature extraction,Laplace equations","Semi-supervised learning models for multiview data are important in image classification tasks, since heterogeneous features are easy to obtain and semi-supervised schemes are economical and effective. To model the view importance, conventional graph-based multiview learning models learn a linear combination of views while assuming a priori weights distribution. In this paper, we present a novel structural regularized semi-supervised model for multiview data, termed Adaptive MUltiview SEmi-supervised model (AMUSE). Our new model learns weights from a priori graph structure, which is more reasonable than weight regularization. Theoretical analysis reveals the significant difference between AMUSE and the prior arts. An efficient optimization algorithm is provided to solve the new model. Experimental results on six real-world data sets demonstrate the effectiveness of the structural regularized weights learning scheme."
773,,Sequence Pattern Mining with Variables.,"James S. Okolica,Gilbert L. Peterson,Robert F. Mills,Michael R. Grimaila",https://doi.org/10.1109/TKDE.2018.2881675,TKDE,2020,"Data mining,Digital forensics,Printers,Production,Databases,Printing,Government","Sequence pattern mining (SPM) seeks to find multiple items that commonly occur together in a specific order. One common assumption is that the relevant differences between items are captured through creating distinct items. In some domains, this leads to an exponential increase in the number of items. This paper presents a new SPM, Sequence Mining of Temporal Clusters (SMTC), that allows item differentiation through attribute variables for domains with large numbers of items. It also provides a new technique for addressing interleaving, a phenomena that occurs when two sequences occur simultaneously resulting in their items alternating. By first clustering items temporally and only focusing on sequences after the temporal clusters are established, it sidesteps the traditional interleaving issues. SMTC is evaluated on a digital forensics dataset, a domain with a large number of items and frequent interleaving. Its results are compared with Discontinuous Varied Order Sequence Mining (DVSM) with variables added (DVSM-V). By adding variables, both algorithms reduce the data by 96 percent, and identify 100 percent of the events while keeping the false positive rate below 0.03 percent. SMTC mines the data in 20 percent of the time it takes DVSM-V and provides a lower false positive rate even at higher similarity thresholds."
774,,User Group Analytics Survey and Research Opportunities.,"Behrooz Omidvar-Tehrani,Sihem Amer-Yahia",https://doi.org/10.1109/TKDE.2019.2913651,TKDE,2020,"Data visualization,Motion pictures,Sociology,Statistics,Task analysis,Data analysis,Databases","User data can be acquired from various domains and is characterized by a combination of demographics such as age and occupation, and user actions such as rating a movie or recording one's blood pressure. User data is appealing to analysts in their role as data scientists who seek to conduct large-scale population studies, and gain insights on various population segments. It is also appealing to users in their role as information consumers who use the social Web for routine tasks such as finding a book club or choosing a physical activity. User data analytics usually relies on identifying group-level behaviors such as “Asian women who publish regularly in databases”. Group analytics addresses peculiarities of user data such as noise and sparsity to enable insights. In this survey, we discuss different approaches for each component of user group analytics, i.e., discovery, exploration, and visualization. We focus on related work which arises from combining those components. We also discuss challenges and future directions of having an all-in-one system, where all those components are combined. This survey has been presented in the form of two tutorials [1] , [2]."
775,,Deep Private-Feature Extraction.,"Seyed Ali Osia,Ali Taheri,Ali Shahin Shamsabadi,Kleomenis Katevas,Hamed Haddadi,Hamid R. Rabiee",https://doi.org/10.1109/TKDE.2018.2878698,TKDE,2020,"Data privacy,Feature extraction,Privacy,Data models,Task analysis,Training","We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFEon smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information."
776,,On Optimally Partitioning Variable-Byte Codes.,"Giulio Ermanno Pibiri,Rossano Venturini",https://doi.org/10.1109/TKDE.2019.2911288,TKDE,2020,"Indexes,Encoding,Partitioning algorithms,Decoding,Query processing,Optimization","The ubiquitous Variable-Byte encoding is one of the fastest compressed representation for integer sequences. However, its compression ratio is usually not competitive with other more sophisticated encoders, especially when the integers to be compressed are small which is the typical case for inverted indexes. This paper shows that the compression ratio of Variable-Byte can be improved by 2× by adopting a partitioned representation of the inverted lists. This makes Variable-Byte surprisingly competitive in space with the best bit-aligned encoders, hence disproving the folklore belief that Variable-Byte is space-inefficient for inverted index compression. Despite the significant space savings, we show that our optimization almost comes for free, given that: we introduce an optimal partitioning algorithm that does not affect indexing time because of its linear-time complexity; we show that the query processing speed of Variable-Byte is preserved, with an extensive experimental analysis and comparison with several other state-of-the-art encoders."
777,,Active Online Learning for Social Media Analysis to Support Crisis Management.,"Daniela Pohl,Abdelhamid Bouchachia,Hermann Hellwagner",https://doi.org/10.1109/TKDE.2019.2906173,TKDE,2020,"Prototypes,Social networking (online),Crisis management,Uncertainty,Measurement,Labeling,Knowledge engineering","People use social media (SM) to describe and discuss different situations they are involved in, like crises. It is therefore worthwhile to exploit SM contents to support crisis management, in particular by revealing useful and unknown information about the crises in real-time. Hence, we propose a novel active online multiple-prototype classifier, called AOMPC. It identifies relevant data related to a crisis. AOMPC is an online learning algorithm that operates on data streams and which is equipped with active learning mechanisms to actively query the label of ambiguous unlabeled data. The number of queries is controlled by a fixed budget strategy. Typically, AOMPC accommodates partly labeled data streams. AOMPC was evaluated using two types of data: (1) synthetic data and (2) SM data from Twitter related to two crises, Colorado Floods and Australia Bushfires. To provide a thorough evaluation, a whole set of known metrics was used to study the quality of the results. Moreover, a sensitivity analysis was conducted to show the effect of AOMPC's parameters on the accuracy of the results. A comparative study of AOMPC against other available online learning algorithms was performed. The experiments showed very good behavior of AOMPC for dealing with evolving, partly-labeled data streams."
778,,Deep Learning Driven Venue Recommender for Event-Based Social Networks.,"Soumajit Pramanik,Rajarshi Haldar,Anand Kumar,Sayan Pathak,Bivas Mitra",https://doi.org/10.1109/TKDE.2019.2915523,TKDE,2020,"Deep learning,History,Heterogeneous networks,Social networking (online),Metadata,Computational modeling,Numerical models","Event-based online social platforms, such as Meetup and Plancast, have experienced increased popularity and rapid growth in recent years. In EBSN setup, selecting suitable venues for hosting events, which can attract a great turnout, is a key challenge. In this paper, we present a deep learning based venue recommendation system DeepVenue which provides context driven venue recommendations for the Meetup event-hosts to host their events. The crux of the proposed model relies on the notion of similarity between multiple Meetup entities such as events, venues, groups, etc. We develop deep learning techniques to compute a compact descriptor for each entity, such that two entities (say, venues) can be compared numerically. Notably, to mitigate the scarcity of venue related information in Meetup, we leverage on the cross domain knowledge transfer from popular LBSN service Yelp to extract rich venue related content. For hosting an event, the proposed DeepVenue model computes a success score for each candidate venue and ranks those venues according to the scores and finally recommend the top k venues. Our rigorous evaluation on the Meetup data collected for the city of Chicago shows that DeepVenue significantly outperforms the baselines algorithms. Precisely, for 84 percent of events, the correct hosting venue appears in the top 5 of the DeepVenue recommended list."
779,,Dynamic Connection-Based Social Group Recommendation.,"Dong Qin,Xiangmin Zhou,Lei Chen 0002,Guangyan Huang,Yanchun Zhang",https://doi.org/10.1109/TKDE.2018.2879658,TKDE,2020,"Recommender systems,Collaboration,Social groups,Motion pictures,Australia,Data mining,Media","Group recommendation has become highly demanded when users communicate in the forms of group activities in online sharing communities. These group activities include student group study, family TV program watching, friends travel decision, etc. Existing group recommendation techniques mainly focus on the small user groups. However, online sharing communities have enabled group activities among thousands of users. Accordingly, recommendation over large groups has become urgent. In this paper, we propose a new framework to accomplish this goal by exploring the group interests and the connections between group users. We first divide a big group into different interest subgroups, each of which contains users closely connected with each other and sharing the similar interests. Then, for each interest subgroup, our framework exploits the connections between group users to collect a comparably compact potential candidate set of media-user pairs, on which the collaborative filtering is performed to generate an interest subgroup-based recommendation list. After that, a novel aggregation function is proposed to integrate the recommended media lists of all interest subgroups as the final group recommendation results. Extensive experiments have been conducted on two real social media datasets to demonstrate the effectiveness and efficiency of our proposed approach."
780,,Similarity Join and Similarity Self-Join Size Estimation in a Streaming Environment.,"Davood Rafiei,Fan Deng",https://doi.org/10.1109/TKDE.2019.2893175,TKDE,2020,"Hamming distance,Pattern matching,Cleaning,Distributed databases,Benchmark testing,Estimation error","We study the problem of similarity self-join and similarity join size estimation in a streaming setting where the goal is to estimate, in one scan of the input and with sublinear space in the input size, the number of record pairs that have a similarity within a given threshold. The problem has many applications in data cleaning and query plan generation, where the cost of a similarity join may be estimated before actually doing the join. On unary input where two records either match or don't match, the problem becomes join and self-join size estimation for which one-pass algorithms are readily available. Our work addresses the problem for d-ary input, for d ≥ 1, where the degree of similarity can vary from 1 to d. We show that our proposed algorithm gives an accurate estimate and scales well with the input size. We provide error bounds and time and space costs, and conduct an extensive experimental evaluation of our algorithm, comparing its estimation accuracy to a few competitors, including some multi-pass algorithms. Our results show that given the same space, the proposed algorithm has an order of magnitude less error for a large range of similarity thresholds."
781,,Cleaning Data with Forbidden Itemsets.,"Joeri Rammelaere,Floris Geerts",https://doi.org/10.1109/TKDE.2019.2905548,TKDE,2020,"Itemsets,Maintenance engineering,Cleaning,Data integrity,Data mining,Heuristic algorithms","Methods for cleaning dirty data typically employ additional information about the data such as user-provided constraints specifying when data is dirty, e.g., domain restrictions, illegal value combinations, or logical rules. However, real-world scenarios usually only have dirty data available, without known constraints. In such settings, constraints are automatically discovered on dirty data and discovered constraints are used to detect and repair errors. Typical repairing processes stop there. Yet, when constraint discovery algorithms are re-run on the repaired data (assumed to be clean), new constraints and thus errors are often found. The repairing process then introduces new constraint violations. We present a different type of repairing method, which prevents introducing new constraint violations, according to a discovery algorithm. Summarily, our repairs guarantee that all errors identified by constraints discovered on the dirty data are fixed; and the constraint discovery process cannot identify new constraint violations. We do this for a new kind of constraints, called forbidden itemsets (FBIs), capturing unlikely value co-occurrences. We show that FBIs detect errors with high precision. Evaluation on real-world data shows that our repair method obtains high-quality repairs without introducing new FBIs. Optional user interaction is readily integrated, with users deciding how much effort to invest."
782,,Ghost Imputation - Accurately Reconstructing Missing Data of the Off Period.,"Reza Rawassizadeh,Hamidreza Keshavarz,Michael J. Pazzani",https://doi.org/10.1109/TKDE.2019.2914653,TKDE,2020,"Sensors,Indexes,Uncertainty,Data mining,Computational complexity,Computer science,Batteries","Noise and missing data are intrinsic characteristics of real-world data, leading to uncertainty that negatively affects the quality of knowledge extracted from the data. The burden imposed by missing data is often severe in sensors that collect data from the physical world, where large gaps of missing data may occur when the system is temporarily off or disconnected. How can we reconstruct missing data for these periods? We introduce an accurate and efficient algorithm for missing data reconstruction (imputation), that is specifically designed to recover off-period segments of missing data. This algorithm, Ghost, searches the sequential dataset to find data segments that have a prior and posterior segment that matches those of the missing data. If there is a similar segment that also satisfies the constraint - such as location or time of day - then it is substituted for the missing data. A baseline approach results in quadratic computational complexity, therefore we introduce a caching approach that reduces the search space and improves the computational complexity to linear in the common case. Experimental evaluations on five real-world datasets show that our algorithm significantly outperforms four state-of-the-art algorithms with an average of 18 percent higher F-score."
783,,Deep Inductive Graph Representation Learning.,"Ryan A. Rossi,Rong Zhou 0001,Nesreen K. Ahmed",https://doi.org/10.1109/TKDE.2018.2878247,TKDE,2020,"Graph theory,Learning (artificial intelligence),Runtime","This paper presents a general inductive graph representation learning framework called 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 for learning deep node 
<i>and</i>
 edge features that generalize across-networks. In particular, 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 begins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 learns 
<i>relational functions</i>
 (each representing a feature) that naturally generalize across-networks and are therefore useful for graph-based transfer learning tasks. Moreover, 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 naturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition, 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 is expressive, flexible with many interchangeable components, efficient with a time complexity of 
<inline-formula><tex-math notation=""LaTeX"">$\mathcal {O}(|E|)$</tex-math></inline-formula>
, and scalable for large networks via an efficient parallel implementation. Compared with recent methods, 
<inline-formula><tex-math notation=""LaTeX"">$\text{DeepGL}$</tex-math></inline-formula>
 is (1) 
<i>effective</i>
 for across-network transfer learning tasks 
<i>and</i>
 large (attributed) graphs, (2) 
<i>space-efficient</i>
 requiring up to 6x less memory, (3) 
<i>fast</i>
 with up to 106x speedup in runtime performance, and (4) 
<i>accurate</i>
 with an average improvement in AUC of 20 percent or more on many learning tasks and across a wide variety of networks."
784,,Artifact-Based Workflows for Supporting Simulation Studies.,"Andreas Ruscheinski,Tom Warnke,Adelinde M. Uhrmacher",https://doi.org/10.1109/TKDE.2019.2899840,TKDE,2020,"Data models,Analytical models,Computational modeling,Mathematical model,Adaptation models,Biological system modeling,Context modeling","Valid models are central for credible simulation studies. If those models do not exist, they need to be developed. In fact, entire simulation studies are often aimed at developing valid models. Thereby, successive model refinement and execution of diverse simulation experiments are closely intertwined. Whereas software-based support for individual simulation experiments exists, the intricate interdependencies and the diversity of tasks that govern simulation studies have prevented a more comprehensive support. To achieve the required flexibility while adhering to the constraints that apply between individual tasks, we adopt a declarative, artifact-based workflow approach. Therefore, central products of these simulation studies are identified and specified as artifacts: the conceptual model (with a focus on formally defined requirements), the simulation model, and the experiment. Each artifact is characterized by stages the artifact moves through to reach certain milestones and which are guarded by conditions. Thereby, the relations and constraints between artifacts become explicit. This is instrumental to check and ensure the consistency between conceptual model and simulation model, to automatically execute simulation experiments to probe the specified requirements, and to develop plans to provide goal-directed guidance to the user. We demonstrate the approach by using it to repeat an existing simulation study."
785,,An Effective Clustering Method over CF$+$+ Tree Using Multiple Range Queries.,"Hyeong-Cheol Ryu,Sungwon Jung,Sakti Pramanik",https://doi.org/10.1109/TKDE.2019.2911520,TKDE,2020,"Clustering methods,Vegetation,Shape,Scalability,Time complexity,Computer science,Buildings","Many existing clustering methods usually compute clusters from the reduced data sets obtained by summarizing the original very large data sets. BIRCH is a popular summary-based clustering method that first builds a CF tree, and then performs a global clustering using the leaf entries of the tree. However, to the best of our knowledge, no prior studies have proposed a global clustering method that uses the structure of a CF tree. Therefore, we propose a novel global clustering method ERC (effective multiple range queries-based clustering), which takes advantage of the structure of a CF tree. We further propose a CF
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 tree, which optimizes the node split scheme used in the CF tree. As a result, the CF
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
-ERC (CF
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 tree-based ERC) method effectively computes clusters over large data sets. Furthermore, it does not require a predefined number of clusters to compute the clusters. We present in-depth theoretical and experimental analyses of our method. Experimental results on very large synthetic data sets demonstrate that the proposed approach is effective in terms of cluster quality and robustness and is significantly faster than existing clustering methods. In addition, we apply our clustering method to real data sets and achieve promising results."
786,,BRIGHT - Drift-Aware Demand Predictions for Taxi Networks.,"Amal Saadallah,Luís Moreira-Matias,Ricardo Sousa,Jihed Khiari,Erik Jenelius,João Gama 0001",https://doi.org/10.1109/TKDE.2018.2883616,TKDE,2020,"Public transportation,Adaptation models,Time series analysis,Training,Forecasting,Random processes,Vehicles","Massive data broadcast by GPS-equipped vehicles provide unprecedented opportunities. One of the main tasks in order to optimize our transportation networks is to build data-driven real-time decision support systems. However, the dynamic environments where the networks operate disallow the traditional assumptions required to put in practice many off-the-shelf supervised learning algorithms, such as finite training sets or stationary distributions. In this paper, we propose BRIGHT: a drift-aware supervised learning framework to predict demand quantities. BRIGHT aims to provide accurate predictions for short-term horizons through a creative ensemble of time series analysis methods that handles distinct types of concept drift. By selecting neighborhoods dynamically, BRIGHT reduces the likelihood of overfitting. By ensuring diversity among the base learners, BRIGHT ensures a high reduction of variance while keeping bias stable. Experiments were conducted using three large-scale heterogeneous real-world transportation networks in Porto (Portugal), Shanghai (China), and Stockholm (Sweden), as well as with controlled experiments using synthetic data where multiple distinct drifts were artificially induced. The obtained results illustrate the advantages of BRIGHT in relation to state-of-the-art methods for this task."
787,,A Crowdsourcing Framework for Collecting Tabular Data.,"Caihua Shan,Nikos Mamoulis,Guoliang Li 0001,Reynold Cheng,Zhipeng Huang 0001,Yudian Zheng",https://doi.org/10.1109/TKDE.2019.2914903,TKDE,2020,"Task analysis,Crowdsourcing,Computers,Cleaning,Data models,Inference algorithms,Estimation","In crowdsourcing, human workers are employed to tackle problems that are traditionally difficult for computers (e.g., data cleaning, missing value filling, and sentiment analysis). In this paper, we study the effective use of crowdsourcing in filling missing values in a given relation (e.g., a table containing different attributes of celebrity stars, such as nationality and age). A task given to a worker typically consists of questions about the missing attribute values (e.g., What is the age of Jet Li?). Although this problem has been studied before, existing work often treats related attributes independently, leading to suboptimal performance. In this paper, we present T-Crowd, which is a crowdsourcing system that considers attribute relationships. Particularly, T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. The attribute relationship information is used to guide task allocation to workers. Our solution seamlessly supports categorical and continuous attributes. Our extensive experiments on real and synthetic datasets show that T-Crowd outperforms state-of-the-art methods, improving the quality of truth inference and reducing the monetary cost of crowdsourcing."
788,,VR-SGD - A Simple Stochastic Variance Reduction Method for Machine Learning.,"Fanhua Shang,Kaiwen Zhou,Hongying Liu,James Cheng,Ivor W. Tsang,Lijun Zhang 0005,Dacheng Tao,Licheng Jiao",https://doi.org/10.1109/TKDE.2018.2878765,TKDE,2020,"Convergence,Acceleration,Complexity theory,Stochastic processes,Optimization,Machine learning,Risk management","In this paper, we propose a simple variant of the original SVRG, called variance reduced stochastic gradient descent (VR-SGD). Unlike the choices of snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the two vectors of VR-SGD are set to the average and last iterate of the previous epoch, respectively. The settings allow us to use much larger learning rates, and also make our convergence analysis more challenging. We also design two different update rules for smooth and nonsmooth objective functions, respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly convex problems directly without any reduction techniques. Moreover, we analyze the convergence properties of VR-SGD for strongly convex problems, which show that VR-SGD attains linear convergence. Different from most algorithms that have no convergence guarantees for nonstrongly convex problems, we also provide the convergence guarantees of VR-SGD for this case, and empirically verify that VR-SGD with varying learning rates achieves similar performance to its momentum accelerated variant that has the optimal convergence rate O(1=T
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
). Finally, we apply VR-SGD to solve various machine learning problems, such as convex and non-convex empirical risk minimization, and leading eigenvalue computation. Experimental results show that VR-SGD converges significantly faster than SVRG and Prox-SVRG, and usually outperforms state-of-the-art accelerated methods, e.g., Katyusha."
789,,Mining Behavioral Sequence Constraints for Classification.,"Johannes De Smedt,Galina Deeva,Jochen De Weerdt",https://doi.org/10.1109/TKDE.2019.2897311,TKDE,2020,"Databases,Data mining,Feature extraction,Distance measurement,Knowledge based systems,Task analysis","Sequence classification deals with the task of finding discriminative and concise sequential patterns. To this purpose, many techniques have been proposed, which mainly resort to the use of partial orders to capture the underlying sequences in a database according to the labels. Partial orders, however, pose many limitations, especially on expressiveness, i.e., the aptitude towards capturing certain behavior, and on conciseness, i.e., doing so in a compact and informative way. These limitations can be addressed by using a better representation. In this paper, we present the interesting Behavioral Constraint Miner (iBCM), a sequence classification technique that discovers patterns using behavioral constraint templates. The templates comprise a variety of constraints and can express patterns ranging from simple occurrence, to looping and position-based behavior over a sequence. Furthermore, iBCM also captures negative constraints, i.e., absence of particular behavior. The constraints can be discovered by using simple string operations in an efficient way. Finally, deriving the constraints with a window-based approach allows to pinpoint where the constraints hold in a string, and to detect whether patterns are subject to concept drift. Through empirical evaluation, it is shown that iBCM is better capable of classifying sequences more accurately and concisely in a scalable manner."
790,,Design and Implementation of SSD-Assisted Backup and Recovery for Database Systems.,"Yongseok Son,Moonsub Kim,Sunggon Kim,Heon Young Yeom,Nam Sung Kim,Hyuck Han",https://doi.org/10.1109/TKDE.2018.2884466,TKDE,2020,"Database systems,Tools,Performance evaluation,Reliability,Random access memory,Arrays","As flash-based solid-state drive (SSD) becomes more prevalent because of the rapid fall in price and the significant increase in capacity, customers expect better data services than traditional disk-based systems. However, the order of magnitude performance provided and new characteristics of flash require a rethinking of data services. For example, backup and recovery is an important service in a database system since it protects data against unexpected hardware and software failures. To provide backup and recovery, backup/recovery tools or backup/recovery methods by operating systems can be used. However, the tools perform time-consuming jobs, and the methods may negatively affect run-time performance during normal operation even though high-performance SSDs are used. To handle these issues, we propose an SSD-assisted backup/recovery scheme for database systems. Our scheme is to utilize the characteristics (e.g., out-of-place update) of flash-based SSD for backup/recovery operations. To this end, we exploit the resources (e.g., flash translation layer and DRAM cache with supercapacitors) inside SSD, and we call our SSD with new backup/ recovery functionality BR-SSD. We design and implement the functionality in the Samsung enterprise-class SSD (i.e., SM843Tn) for more realistic systems. Furthermore, we exploit and integrate BR-SSDs into database systems (i.e., MySQL) in replication and redundant array of independent disks (RAID) environments, as well as a database system in a single BR-SSD. The experimental result demonstrates that our scheme provides fast backup and recovery but does not negatively affect the run-time performance during normal operation."
791,,Haery - A Hadoop Based Query System on Accumulative and High-Dimensional Data Model for Big Data.,"Jie Song 0001,HongYan He,Richard Thomas 0002,Yubin Bao,Ge Yu 0001",https://doi.org/10.1109/TKDE.2019.2904056,TKDE,2020,"Data models,Big Data,Partitioning algorithms,NoSQL databases,Indexing","Column-oriented stores, known for their scalability and flexibility, are a common NoSQL database implementation and are increasingly used in big data management. In column-oriented stores, a “full-scan” query strategy is inefficient and the search space can be reduced if data is well partitioned or indexed; however, there is no pre-defined schema for building and maintaining partitions and indexes at lower cost. We leverage an accumulative and high-dimensional data model, a sophisticated linearization algorithm, and an efficient query algorithm, to solve the challenge of how a pre-defined and well-partitioned data model can be applied to flexible and time-varied key-value data. We adapt a high-dimensional array as the data model to partition the key-value data without additional storage and massive calculation; improve the Z-order linearization algorithm, which map multidimensional data to one dimension while preserving locality of the data points, for flexibility; efficiently build an expansion mechanism for the data model to support time-varied data. The result is Haery, a column-oriented store, based on a distributed file system and computing framework. In experiments, Haery is compared with Hive, HBase, Cassandra, MongoDB, PostgresXL, and HyperDex in terms of query performance. With results indicating Haery on average performs 4.57x, 4.23x, 3.55x, 1.79x, 1.82x, and 120.6x faster, respectively."
792,,Scientific Workflow Protocol Discovery from Public Event Logs in Clouds.,"Wei Song 0003,Hans-Arno Jacobsen,Fangfei Chen",https://doi.org/10.1109/TKDE.2019.2922183,TKDE,2020,"Protocols,Cloud computing,Collaboration,PROM,Privacy,Jacobian matrices,Big Data","With the advancement of cloud computing, many challenging scientific problems can be solved using scientific workflow technology which integrates geo-distributed instruments, applications, and big data effectively and efficiently. For workflow collaboration, the workflow protocols of all participants are needed. However, workflow protocols are not always available and are often outdated as the workflow evolve frequently. To address this problem, we propose a novel workflow discovery approach which can extract up-to-date scientific workflow protocols from public event logs in clouds, without the need to access the full-fledged event logs involving private events. Our approach leverages transitive precedence relations between events to achieve this. We implement our approach as a ProM plug-in, and evaluate it through extensive experiments on event logs of real-world scientific workflows. The experimental results demonstrate that our approach requires a weaker completeness notion of event logs than the state-of-the-art do, and our approach derives the same workflow protocol from the public event log as that discovered from the original event log, and thus the private events can be protected."
793,,Enriching Data Imputation under Similarity Rule Constraints.,"Shaoxu Song,Yu Sun,Aoqian Zhang,Lei Chen 0002,Jianmin Wang 0001",https://doi.org/10.1109/TKDE.2018.2883103,TKDE,2020,"Bayes methods,Data mining,Computational complexity,Databases,Approximation algorithms,Data integration","Incomplete information often occurs along with many database applications, e.g., in data integration, data cleaning, or data exchange. The idea of data imputation is often to fill the missing data with the values of its neighbors who share the same/similar information. Such neighbors could either be identified certainly by editing rules or extensively by similarity relationships. Owing to data sparsity, the number of neighbors identified by editing rules w.r.t. value equality is rather limited, especially in the presence of data values with variances. To enrich the imputation candidates, a natural idea is to extensively consider the neighbors with similarity relationship. However, the candidates suggested by these (heterogenous) similarity neighbors may conflict with each other. In this paper, we propose to utilize the similarity rules with tolerance to small variations (instead of the aforesaid editing rules with strict equality constraints) to rule out the invalid candidates provided by similarity neighbors. To enrich the data imputation, i.e., imputing the missing values more, we study the problem of maximizing the missing data imputation. Our major contributions include (1) the NP-hardness analysis on solving as well as approximating the problem, (2) exact algorithms for tackling the problem, and (3) efficient approximation with performance guarantees. Experiments on real and synthetic data sets demonstrate the superiority of our proposal in filling accuracy. We also demonstrate that the record matching application is indeed improved, after applying the proposed imputation."
794,,Impacts of Fractional Hot-Deck Imputation on Learning and Prediction of Engineering Data.,"Ikkyun Song,Yicheng Yang,Jongho Im,Tong Tong,Halil Ceylan,In Ho Cho",https://doi.org/10.1109/TKDE.2019.2922638,TKDE,2020,"Mercury (metals),Machine learning,Learning systems,Data analysis,Support vector machines,Sensitivity analysis,Predicitive models","In broad engineering fields, missing data is a common issue which often causes undesired bias and sparseness impeding rigorous data analyses. To tackle this problem, many imputation theories have been proposed and widely used. However, prior methods often require distributional assumptions and prior knowledge regarding data which may cause some difficulty for engineering research. Essentially, the fractional hot-deck imputation (FHDI) is an assumption-free imputation method, holding broad applicability in the engineering domains. FHDIs internal parameters and impact on statistical and machine learning methods, however, have been rarely understood. Thus, this study investigates the behavior and impacts of FHDI on prediction methods including generalized additive model, support vector machine, extremely randomized trees, and artificial neural network, for which four practical datasets (appliance energy, air quality, phenotypes, and weather) are used. Results show that FHDI performs better for improving the prediction accuracy compared to a simple naive method which cures missing data using the mean value of attributes, and FHDI has an asymptotically positive effect on prediction accuracy with decreasing response rates. Regarding an optimal setting, 30 to 35 is recommended for the FHDIs internal categorization number while 5 is recommended for the FHDI donors, which is aligned with Rubins recommendation."
795,,Training Simplification and Model Simplification for Deep Learning - A Minimal Effort Back Propagation Method.,"Xu Sun 0001,Xuancheng Ren,Shuming Ma,Bingzhen Wei,Wei Li 0101,Jingjing Xu,Houfeng Wang,Yi Zhang 0050",https://doi.org/10.1109/TKDE.2018.2883613,TKDE,2020,"Backpropagation,Computational modeling,Training,Adaptation models,Neurons,Computational efficiency,Decoding","We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of the time we only need to update fewer than 5 percent of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy."
796,,Joint Learning of Question Answering and Question Generation.,"Yibo Sun,Duyu Tang,Nan Duan,Tao Qin,Shujie Liu 0001,Zhao Yan,Ming Zhou 0001,Yuanhua Lv,Wenpeng Yin 0001,Xiaocheng Feng,Bing Qin 0001,Ting Liu 0001",https://doi.org/10.1109/TKDE.2019.2897773,TKDE,2020,"Task analysis,Training,Mathematical model,Knowledge discovery,Data models,Computational modeling,Collaboration","Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in the literature. In this paper, we present two training algorithms for learning better QA and QG models through leveraging one another. The first algorithm extends Generative Adversarial Network (GAN), which selectively incorporates artificially generated instances as additional QA training data. The second algorithm is an extension of dual learning, which incorporates the probabilistic correlation of QA and QG as additional regularization in training objectives. To test the scalability of our algorithms, we conduct experiments on both document based and table based question answering tasks. Results show that both algorithms improve a QA model in terms of accuracy and QG model in terms of BLEU score. Moreover, we find that the performance of a QG model could be easily improved by a QA model via policy gradient, however, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Our algorithm that selectively assigns labels to generated questions would bring a performance boost."
797,,Enhanced Graph Transforming V2 Algorithm for Non-Simple Graph in Big Data Pre-Processing.,"Sutedi Sutedi,Noor Akhmad Setiawan,Teguh Bharata Adji",https://doi.org/10.1109/TKDE.2018.2880971,TKDE,2020,"NoSQL databases,Relational databases,Big Data,Redundancy,Transforms,Ecosystems","Incapability of relational database in handling large-scale data triggers the development of NoSQL database that becomes part of a big data ecosystem. NoSQL database has different characteristics compared to the relational database. However, NoSQL database requires data from the relational database as one of the structured data sources. Therefore, data pre-processing is required to ensure proper data migration from a relational database to NoSQL database. This data pre-processing is normally called data transformation. One of the simple and understandable transformation algorithms is graph transforming algorithm. However, the algorithm has a problem in solving a non-simple graph (multigraph). This research proposes an algorithm to overcome several multigraph problems. The experimental work confirms that the algorithm proposed in this research is able to transform data from a relational database to NoSQL schema that has a minimum number of redundant attributes while the data completeness is still maintained."
798,,Adversarial Training Towards Robust Multimedia Recommender System.,"Jinhui Tang,Xiaoyu Du,Xiangnan He 0001,Fajie Yuan,Qi Tian 0001,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2019.2893638,TKDE,2020,"Perturbation methods,Predictive models,Multimedia systems,Visualization,Feature extraction,Recommender systems,Robustness","With the prevalence of multimedia content on the Web, developing recommender solutions that can effectively leverage the rich signal in multimedia data is in urgent need. Owing to the success of deep neural networks in representation learning, recent advances on multimedia recommendation has largely focused on exploring deep learning methods to improve the recommendation accuracy. To date, however, there has been little effort to investigate the robustness of multimedia representation and its impact on the performance of multimedia recommendation. In this paper, we shed light on the robustness of multimedia recommender system. Using the state-of-the-art recommendation framework and deep image features, we demonstrate that the overall system is not robust, such that a small (but purposeful) perturbation on the input image will severely decrease the recommendation accuracy. This implies the possible weakness of multimedia recommender system in predicting user preference, and more importantly, the potential of improvement by enhancing its robustness. To this end, we propose a novel solution named Adversarial Multimedia Recommendation (AMR), which can lead to a more robust multimedia recommender model by using adversarial learning. The idea is to train the model to defend an adversary, which adds perturbations to the target image with the purpose of decreasing the model's accuracy. We conduct experiments on two representative multimedia recommendation tasks, namely, image recommendation and visually-aware product recommendation. Extensive results verify the positive effect of adversarial learning and demonstrate the effectiveness of our AMR method. Source codes are available in https://github.com/duxy-me/AMR."
799,,Feature Selective Projection with Low-Rank Embedding and Dual Laplacian Regularization.,"Chang Tang,Xinwang Liu,Xinzhong Zhu,Jian Xiong,Miaomiao Li,Jingyuan Xia,Xiangke Wang,Lizhe Wang 0001",https://doi.org/10.1109/TKDE.2019.2911946,TKDE,2020,"Feature extraction,Manifolds,Laplace equations,Task analysis,Principal component analysis,Learning systems,Dimensionality reduction","Feature extraction and feature selection have been regarded as two independent dimensionality reduction methods in most of the existing literature. In this paper, we propose to integrate both approaches into a unified framework and design an unsupervised linear feature selective projection (FSP) for feature extraction with low-rank embedding and dual Laplacian regularization, with the aim to exploit the intrinsic relationship among data and suppress the impact of noise. Specifically, a projection matrix with an l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,1</sub>
-norm regularization is introduced to project original high dimensional data points into a new subspace with lower dimension, where the l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,1</sub>
-norm regularization can endow the projection with good interpretability. We deploy a coefficient matrix with low rank constraint to reconstruct the data points and the l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,1</sub>
-norm is imposed to regularize the data reconstruction errors in the low-dimensional subspace and make FSP robust to noise. Furthermore, a dual graph Laplacian regularization term is imposed on the low dimensional data and data reconstruction matrix for preserving the local manifold geometrical structure of data. Finally, an alternatively iterative algorithm is carefully designed for solving the proposed optimization model. Theoretical convergence and computational complexity analysis of the algorithm are also provided. Comprehensive experiments on various benchmark datasets have been carried out to evaluate the performance of the proposed FSP. As indicated, our algorithm significantly outperforms other state-of-the-art methods for feature extraction."
800,,DAG - A General Model for Privacy-Preserving Data Mining.,"Sin G. Teo,Jianneng Cao,Vincent C. S. Lee",https://doi.org/10.1109/TKDE.2018.2880743,TKDE,2020,"Data models,Protocols,Cryptography,Task analysis,Data mining,Computational modeling","Secure multi-party computation (SMC) allows parties to jointly compute a function over their inputs, while keeping every input confidential. It has been extensively applied in tasks with privacy requirements, such as privacy-preserving data mining (PPDM), to learn task output and at the same time protect input data privacy. However, existing SMC-based solutions are ad-hoc - they are proposed for specific applications, and thus cannot be applied to other applications directly. To address this issue, we propose a privacy model DAG (Directed Acyclic Graph) that consists of a set of fundamental secure operators (e.g., +, -, ×, /, and power). Our model is general - its operators, if pipelined together, can implement various functions, even complicated ones like Naı̈ve Bayes classifier. It is also extendable - new secure operators can be defined to expand the functions that the model supports. For case study, we have applied our DAG model to two data mining tasks: kernel regression and Naı̈ve Bayes. Experimental results show that DAG generates outputs that are almost the same as those by non-private setting, where multiple parties simply disclose their data. The experimental results also show that our DAG model runs in acceptable time, e.g., in kernel regression, when training data size is 683,093, one prediction in non-private setting takes 5.93 sec, and that by our DAG model takes 12.38 sec."
801,,Enabling Ternary Hash Tree Based Integrity Verification for Secure Cloud Data Storage.,"M. Thangavel,P. Varalakshmi",https://doi.org/10.1109/TKDE.2019.2922357,TKDE,2020,"Cloud computing,Data integrity,Computational efficiency,Memory,Data security","Cloud Computing enables the remote users to access data, services, and applications in on-demand from the shared pool of configurable computing resources, without the consideration of storage, hardware, and software management. On the other hand, it is not easy for cloud users to identify whether Cloud Service Provider's (CSP) tag along with the data security legal expectations. So, cloud users could not rely on CSP's in terms of trust. So, it is significant to build a secure and efficient data auditing framework for increasing and maintaining cloud users trust with CSP. Researchers suggested introducing Third Party Auditor (TPA) on behalf of cloud user for verifying the outsourced data integrity, which may reduce the computation overhead of cloud users. In this work, we proposed a novel integrity verification framework for securing cloud storage based on Ternary Hash Tree (THT) and Replica based Ternary Hash Tree (R-THT), which will be used by TPA to perform data auditing. Differing from existing work, the proposed framework performs Block-level, File-level and Replica-level auditing with tree block ordering, storage block ordering for verifying the data integrity and ensuring data availability in the cloud. We further extend our framework to support error localization with data correctness, dynamic updates with block update, insert, and delete operations in the cloud. The structure of THT and R-THT will reduce the computation cost and provide efficiency in data updates compared to the existing schemes. The security analysis of the proposed public auditing framework indicates the achievement of desired properties and performance has been evaluated with the detailed experiment set. The results show that the proposed secure cloud auditing framework is highly secure and efficient in storage, communication, and computation costs."
802,,Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences.,"Marcel Trotzek,Sven Koitka,Christoph M. Friedrich",https://doi.org/10.1109/TKDE.2018.2885515,TKDE,2020,"Task analysis,Natural language processing,Social network services,Machine learning,Linguistics,Metadata,Europe","Depression is ranked as the largest contributor to global disability and is also a major reason for suicide. Still, many individuals suffering from forms of depression are not treated for various reasons. Previous studies have shown that depression also has an effect on language usage and that many depressed individuals use social media platforms or the internet in general to get information or discuss their problems. This paper addresses the early detection of depression using machine learning models based on messages on a social platform. In particular, a convolutional neural network based on different word embeddings is evaluated and compared to a classification based on user-level linguistic metadata. An ensemble of both approaches is shown to achieve state-of-the-art results in a current early detection task. Furthermore, the currently popular ERDE score as metric for early detection systems is examined in detail and its drawbacks in the context of shared tasks are illustrated. A slightly modified metric is proposed and compared to the original score. Finally, a new word embedding was trained on a large corpus of the same domain as the described task and is evaluated as well."
803,,Scalable Dynamic Graph Summarization.,"Ioanna Tsalouchidou,Francesco Bonchi,Gianmarco De Francisci Morales,Ricardo Baeza-Yates",https://doi.org/10.1109/TKDE.2018.2884471,TKDE,2020,"Heuristic algorithms,Silicon,Microsoft Windows,Clustering algorithms,Tensile stress,Scalability","Large-scale dynamic interaction graphs can be challenging to process and store, due to their size and the continuous change of communication patterns between nodes. In this work, we address the problem of summarizing large-scale dynamic graphs, while maintaining the evolution of their structure and interactions. Our approach is based on grouping the nodes of the graph in supernodes according to their connectivity and communication patterns. The resulting summary graph preserves the information about the evolution of the graph within a time window. We propose two online algorithms for summarizing this type of graphs. Our baseline algorithm kC based on clustering is fast but rather memory expensive. The second method we propose, named /LC, reduces the memory requirements by introducing an intermediate step that keeps statistics of the clustering of the previous rounds. Our algorithms are distributed by design, and we implement them over the Apache Spark framework, so as to address the problem of scalability for large-scale graphs and massive streams. We apply our methods to several dynamic graphs, and show that we can efficiently use the summary graphs to answer temporal and probabilistic graph queries."
804,,Automatic Classification of Algorithm Citation Functions in Scientific Literature.,"Suppawong Tuarob,Sung Woo Kang,Poom Wettayakorn,Chanatip Pornprasit,Tanakitti Sachati,Saeed-Ul Hassan,Peter Haddawy",https://doi.org/10.1109/TKDE.2019.2913376,TKDE,2020,"Feature extraction,Machine learning algorithms,Metadata,Clustering algorithms,Approximation algorithms,Machine learning,Computer science","Computer sciences and related disciplines evolve around developing, evaluating, and applying algorithms. Typically, an algorithm is not developed from scratch, but uses and builds upon existing ones, which often are proposed and published in scholarly articles. The ability to capture this evolution relationship among these algorithms in scientific literature would not only allow us to understand how a particular algorithm is composed, but also shed light on large-scale analysis of algorithmic evolution through different temporal spans and thematic scales. We propose to capture such evolution relationship between two algorithms by investigating the knowledge represented in citation contexts, where authors explain how cited algorithms are used in their works. A set of heterogeneous ensemble machine-learning methods is proposed, where the combination of two base classifiers trained with heterogeneous feature types is used to automatically identify the algorithm usage relationship. The proposed heterogeneous ensemble methods achieve the best average F1 of 0.749 and 0.905 for fine-grained and binary algorithm citation function classification, respectively. The success of this study will allow us to generate a large-scale algorithm citation network from a collection of scholarly documents representing multiple time spans, venues, and fields of study. Such a network will be used as an instrument not only to answer critical questions in algorithm search, such as identifying the most influential and generalizable algorithms, but also to study the evolution of algorithmic development and trends over time."
805,,A Framework for Supervised Classification Performance Analysis with Information-Theoretic Methods.,"Francisco J. Valverde-Albacete,Carmen Peláez-Moreno",https://doi.org/10.1109/TKDE.2019.2915643,TKDE,2020,"Task analysis,Entropy,Mutual information,Proposals,Tools,Performance analysis","We introduce a framework for the evaluation of multiclass classifiers by exploring their confusion matrices. Instead of using error-counting measures of performance, we concentrate in quantifying the information transfer from true to estimated labels using information-theoretic measures. First, the Entropy Triangle allows us to visualize the balance of mutual information, variation of information, and the deviation from uniformity in the true and estimated label distributions. Next, the Entropy-Modified Accuracy allows us to rank classifiers by performance while the Normalized Information Transfer rate allows us to evaluate classifiers by the amount of information accrued during learning. Finally, if the question rises to elucidate which errors are systematically committed by the classifier, we use a generalization of Formal Concept Analysis to elicit such knowledge. All such techniques can be applied either to artificially or biologically embodied classifiers-e.g., human performance on perceptual tasks. We instantiate the framework in a number of examples to provide guidelines for the use of these tools in the case of assessing single classifiers or populations of them-whether induced with the same technique or not-either on single tasks or in a set of them. These include well-known UCI tasks and the more complex KDD cup 99 competition on Intrusion Detection."
806,,Simple Statistics Are Sometime Too Simple - A Case Study in Social Media Data.,Dan Vilenchik,https://doi.org/10.1109/TKDE.2019.2899355,TKDE,2020,"Semantics,Twitter,Principal component analysis,YouTube,LinkedIn,Correlation","In this work we ask to which extent are simple statistics useful to make sense of social media data. By simple statistics we mean counting and bookkeeping type features such as the number of likes given to a user's post, a user's number of friends, etc. We find that relying solely on simple statistics is not always a good approach. Specifically, we develop a statistical framework that we term semantic shattering which allows to detect semantic inconsistencies in the data that may occur due to relying solely on simple statistics. We apply our framework to simple-statistics data collected from six online social media platforms and arrive at a surprising counter-intuitive finding in three of them, Twitter, Instagram and YouTube. We find that overall, the activity of the user is not correlated with the feedback that the user receives on that activity. A hint to understand this phenomenon may be found in the fact that the activity-feedback shattering did not occur in LinkedIn, Steam and Flickr. A possible explanation for this separation is the amount of effort required to produce content. The lesser the effort the lesser the correlation between activity and feedback. The amount of effort may be a proxy to the level of commitment that the users feel towards each other in the network, and indeed sociologists claim that commitment explains consistent human behavior, or lack thereof. However, the amount of effort or the level of commitment are by no means a simple statistic."
807,,A Hybrid E-Learning Recommendation Approach Based on Learners&apos; Influence Propagation.,"Shanshan Wan,Zhendong Niu",https://doi.org/10.1109/TKDE.2019.2895033,TKDE,2020,"Electronic learning,Recommender systems,Uncertainty,Collaboration,Hafnium,Computational modeling,Data models","In e-learning recommender systems, interpersonal information between learners is very scarce, which makes it difficult to apply collaborative filtering (CF) techniques to achieve recommendations. In this study, we propose a hybrid filtering recommendation approach (SI - IFL) combining learner influence model (LIM), self-organization based (SOB) recommendation strategy, and sequential pattern mining (SPM) together for recommending learning objects (LOs) to learners. The method works as follows: (i) LIM is applied to acquire the interpersonal information by computing the influence that a learner exerts on others. LIM consists of learner similarity, knowledge credibility, and learner aggregation, meanwhile, LIM is independent of ratings. Furthermore, to address the uncertainty and fuzzy natures of learners, intuitionistic fuzzy logic (IFL) is applied to optimize the LIM. (ii) A SOB recommendation strategy is applied to recommend the optimal learner cliques for active learners by simulating the influence propagation among learners. Influence propagation means that a learner can move towards active learners, and such behaviors can stimulate the moving behaviors of his/her neighbors. This SOB recommendation approach achieves a stable structure based on distributed and bottom-up behaviors of individuals. (iii) SPM is applied to decide the final learning objects (LOs) and navigational paths based on the recommended learner cliques. The experimental results demonstrate that SI - IFL can provide personalized and diversified recommendations, and it shows promising efficiency and adaptability in e-learning scenarios."
808,,Locally Balanced Inductive Matrix Completion for Demand-Supply Inference in Stationless Bike-Sharing Systems.,"Senzhang Wang,Hao Chen,Jiannong Cao 0001,Jiawei Zhang 0001,Philip S. Yu",https://doi.org/10.1109/TKDE.2019.2922636,TKDE,2020,"Urban areas,Correlation,Smart phones,Predictive models,Optimization,Global Positioning System,Computer science","Stationless bike-sharing systems such as Mobike are currently becoming extremely popular in China as well as some other big cities in the world. Compared to traditional bicycle-sharing systems, stationless bike-sharing systems do not need bike stations. Users can rent and return bikes at arbitrary locations through an App installed on their smart phones. Such a convenient and flexible bike-sharing mode greatly solves the last mile issue of the commuters, and better meets their real bike usage demand. However, it also poses new challenges for operators to manage the system. The first primary challenge is how to accurately estimate the real bike usage demand in different areas of a city and in different time intervals, which is crucial for the system planning and operation. This paper for the first time proposes a data driven approach for bike usage demand inference in stationless bike-sharing systems. The idea is that we first estimate the demands in some regions and time intervals from a small number of observed bike check-out/in data directly, and then use them as seeds to infer the region-level bike usage demands of an entire city. Specifically, we formulate this problem as a matrix completion task by modeling the bike usage demand as a matrix whose two dimensions are time intervals of a day and regions of a city, respectively. With the observation that POI distribution of a region is an important indicator to bike demand, we propose to utilize inductive matrix factorization by considering POIs as side information. As the bike usage data are highly correlated in both spatial and temporal dimensions, we also incorporate the spatial-temporal correlations as well as the balanced bike usage constraint into a joint optimization framework. We evaluate the proposed model on a large Mobike trip dataset collected from Beijing, and the experimental results show its superior performance by comparison with various baseline methods."
809,,Parameter-Free Weighted Multi-View Projected Clustering with Structured Graph Learning.,"Rong Wang 0001,Feiping Nie 0001,Zhen Wang 0004,Haojie Hu,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2913377,TKDE,2020,"Clustering methods,Task analysis,Dimensionality reduction,Laplace equations,Visualization,Clustering algorithms,Biomedical optical imaging","In many real-world applications, we are often confronted with high dimensional data which are represented by various heterogeneous views. How to cluster this kind of data is still a challenging problem due to the curse of dimensionality and effectively integration of different views. To address this problem, we propose two parameter-free weighted multi-view projected clustering methods which perform structured graph learning and dimensionality reduction simultaneously. We can use the obtained structured graph directly to extract the clustering indicators, without performing other discretization procedures as previous graph-based clustering methods have to do. Moreover, two parameter-free strategies are adopted to learn an optimal weight for each view automatically, without introducing a regularization parameter as previous methods do. Extensive experiments on several public datasets demonstrate that the proposed methods outperform other state-of-the-art approaches and can be used more practically."
810,,SentiDiff - Combining Textual Information and Sentiment Diffusion Patterns for Twitter Sentiment Analysis.,"Lei Wang 0037,Jianwei Niu 0002,Shui Yu",https://doi.org/10.1109/TKDE.2019.2913641,TKDE,2020,"Twitter,Sentiment analysis,Prediction algorithms,Fuses,Task analysis,Data mining,Noise measurement","Twitter sentiment analysis has become a hot research topic in recent years. Most of existing solutions to Twitter sentiment analysis basically only consider textual information of Twitter messages, and struggle to perform well when facing short and ambiguous Twitter messages. Recent studies show that sentiment diffusion patterns on Twitter have close relationships with sentiment polarities of Twitter messages. Therefore, in this paper, we focus on how to fuse textual information of Twitter messages and sentiment diffusion patterns to obtain better performance of sentiment analysis on Twitter data. To this end, we first analyze sentiment diffusion by investigating a phenomenon called sentiment reversal, and find some interesting properties of sentiment reversals. Then, we consider the inter-relationships between textual information of Twitter messages and sentiment diffusion patterns, and propose an iterative algorithm called SentiDiff to predict sentiment polarities expressed in Twitter messages. To the best of our knowledge, this work is the first to utilize sentiment diffusion patterns to help improve Twitter sentiment analysis. Extensive experiments on real-world dataset demonstrate that compared with state-of-the-art textual information based sentiment analysis algorithms, our proposed algorithm yields PR-AUC improvements between 5.09 and 8.38 percent on Twitter sentiment classification tasks."
811,,Capturing Joint Label Distribution for Multi-Label Classification Through Adversarial Learning.,"Shangfei Wang,Guozhu Peng,Zhuangqiang Zheng",https://doi.org/10.1109/TKDE.2019.2922603,TKDE,2020,"Learning systems,Benchmark testing,Databases,Classification","Label correlations are important for multi-label learning. Although current multi-label learning approaches can exploit first-order, second-order, and high-order label dependencies, they fail to exploit complete label correlations, which are included in the joint label distribution of the ground truth labels. However, directly modeling the complex and unknown joint label distribution is very challenging, if not impossible. In this paper, we propose an adversarial learning framework to enforce similarity between joint distribution of the ground truth multi-labels and the predicted multiple labels. Specifically, the proposed multi-label learning method includes a multi-label classifier and a label discriminator. The classifier minimizes error between predicted labels and corresponding ground truth labels and gives the discriminator room for error. The object of the discriminator is to distinguish the predicted labels from the ground truth labels. The classifier and discriminator are trained simultaneously through an alternate process. By adversarial learning, the joint label distribution of the predicted multi-labels converges to the joint distribution inherent in the ground truth multi-labels, and thus boosts the performance of multi-label learning as demonstrated in the experiments on 11 benchmark databases."
812,,A Nodes&apos; Evolution Diversity Inspired Method to Detect Anomalies in Dynamic Social Networks.,"Huan Wang 0005,Chunming Qiao",https://doi.org/10.1109/TKDE.2019.2912574,TKDE,2020,"Social networking (online),Evolution (biology),Heuristic algorithms,Cultural differences,Prediction algorithms,Birds,Anomaly detection","Recently dynamic social networks witnessed a massive surge in popularity, especially in the area of anomaly detection. Although the text-based methods have achieved impressive detection performances, their applications are limited to the social text provided by users. This research focuses on graph-based methods and proposes a universal method for generalized social networks. Different from the existing graph-based methods that summarize a number of structural features, the proposed nodes' evolution diversity inspired method (NEDM) detects anomalies in dynamic social networks from the perspective of diverse evolution mechanisms. More specifically, NEDM applies link prediction algorithms at the micro-level to fit evolution mechanisms followed by the behaviors of nodes, and designs indices to evaluate their fitting degrees in edge removal and generation processes. In addition, the behavior of a node is represented as a quantum superposition state where such behavior follows different evolution mechanisms with uncertain probabilities. We propose a quantum mechanism based particle swarm optimization algorithm (QMPSO) in NEDM. QMPSO determines the optimal observation states of the behaviors of different nodes, and maximally reflects the evolutional fluctuations in the evolution processes of social networks. As a result, NEDM can quantify the evolutional fluctuations in different periods, and detect anomalies in dynamic social networks. Comparing with art-of-the-state methods and real social data in extensive experiments on disparate real-world social networks, we verify the outstanding performance of NEDM in terms of both accuracy and universality."
813,,Understanding Urban Dynamics via Context-Aware Tensor Factorization with Neighboring Regularization.,"Jingyuan Wang,Junjie Wu 0002,Ze Wang,Fei Gao,Zhang Xiong",https://doi.org/10.1109/TKDE.2019.2915231,TKDE,2020,"Urban areas,Data models,Global Positioning System,Trajectory,Public transportation,Sociology","Recent years have witnessed the world-wide emergence of mega-metropolises with incredibly huge populations. Understanding residents mobility patterns, or urban dynamics, thus becomes crucial for building modern smart cities. In this paper, we propose a Neighbor-Regularized and context-aware Non-negative Tensor Factorization model (NR-cNTF) to discover interpretable urban dynamics from urban heterogeneous data. Different from many existing studies concerned with prediction tasks via tensor completion, NR-cNTF focuses on gaining urban managerial insights from spatial, temporal, and spatio-temporal patterns. This is enabled by high-quality Tucker factorizations regularized by both POI-based urban contexts and geographically neighboring relations. NR-cNTF is also capable of unveiling long-term evolutions of urban dynamics via a pipeline initialization approach. We apply NR-cNTF to a real-life data set containing rich taxi GPS trajectories and POI records of Beijing. The results indicate: 1) NR-cNTF accurately captures four kinds of city rhythms and seventeen spatial communities; 2) the rapid development of Beijing, epitomized by the CBD area, indeed intensifies the job-housing imbalance; 3) the southern areas with recent government investments have shown more healthy development tendency. Finally, NR-cNTF is compared with some baselines on traffic prediction, which further justifies the importance of urban contexts awareness and neighboring regulations."
814,,GMC - Graph-Based Multi-View Clustering.,"Hao Wang 0068,Yan Yang 0001,Bing Liu 0001",https://doi.org/10.1109/TKDE.2019.2903810,TKDE,2020,"Matrix converters,Clustering algorithms,Clustering methods,Laplace equations,Computational modeling,Computational complexity,Kernel","Multi-view graph-based clustering aims to provide clustering solutions to multi-view data. However, most existing methods do not give sufficient consideration to weights of different views and require an additional clustering step to produce the final clusters. They also usually optimize their objectives based on fixed graph similarity matrices of all views. In this paper, we propose a general Graph-based Multi-view Clustering (GMC) to tackle these problems. GMC takes the data graph matrices of all views and fuses them to generate a unified graph matrix. The unified graph matrix in turn improves the data graph matrix of each view, and also gives the final clusters directly. The key novelty of GMC is its learning method, which can help the learning of each view graph matrix and the learning of the unified graph matrix in a mutual reinforcement manner. A novel multi-view fusion technique can automatically weight each data graph matrix to derive the unified graph matrix. A rank constraint without introducing a tuning parameter is also imposed on the graph Laplacian matrix of the unified matrix, which helps partition the data points naturally into the required number of clusters. An alternating iterative optimization algorithm is presented to optimize the objective function. Experimental results using both toy data and real-world data demonstrate that the proposed method outperforms state-of-the-art baselines markedly."
815,,On Nearby-Fit Spatial Keyword Queries.,"Victor Junqiu Wei,Raymond Chi-Wing Wong,Cheng Long,Pan Hui 0001",https://doi.org/10.1109/TKDE.2019.2915295,TKDE,2020,"Approximation algorithms,Spatial databases,Games,Sports,Indexes,History,Shape","Geo-textual data is ubiquitous nowadays, where each object has a location and is associated with some keywords. Many types of queries based on geo-textual data, termed as spatial keyword queries, have been proposed, and are to find optimal object(s) in terms of both its (their) location(s) and keywords. In this paper, we propose a new type of query called nearby-fit spatial keyword query (NSKQ), where an optimal object is defined based not only on the location and the keywords of the object itself, but also on those of the objects nearby. For example, in an application of finding a hotel, not only the location of a hotel but also the objects near the hotel (e.g., shopping malls, restaurants, and bus stops nearby) might need to be taken into consideration. The query is proved to be NP-hard, and in order to perform the query efficiently, we developed two approximate algorithms with small constant approximation factors equal to 1.155 and 1.79. We conducted extensive experiments based on both real and synthetic datasets, which verified our algorithms."
816,,Reliable Accuracy Estimates from k-Fold Cross Validation.,"Tzu-Tsung Wong,Po-Yang Yeh",https://doi.org/10.1109/TKDE.2019.2912815,TKDE,2020,"Reliability,Classification algorithms,Correlation,Statistical analysis,Testing,Roads,Urban areas","It is popular to evaluate the performance of classification algorithms by k-fold cross validation. A reliable accuracy estimate will have a relatively small variance, and several studies therefore suggested to repeatedly perform k-fold cross validation. Most of them did not consider the correlation among the replications of k-fold cross validation, and hence the variance could be underestimated. The purpose of this study is to explore whether k-fold cross validation should be repeatedly performed for obtaining reliable accuracy estimates. The dependency relationships between the predictions of the same instance in two replications of k-fold cross validation are first analyzed for k-nearest neighbors with k = 1. Then, statistical methods are proposed to test the strength of the dependency level between the accuracy estimates resulting from two replications of k-fold cross validation. The experimental results on 20 data sets show that the accuracy estimates obtained from various replications of k-fold cross validation are generally highly correlated, and the correlation will be higher as the number of folds increases. The k-fold cross validation with a large number of folds and a small number of replications should be adopted for performance evaluation of classification algorithms."
817,,A Hierarchical Attention Model for Social Contextual Image Recommendation.,"Le Wu,Lei Chen 0051,Richang Hong,Yanjie Fu,Xing Xie 0001,Meng Wang 0001",https://doi.org/10.1109/TKDE.2019.2913394,TKDE,2020,"Visualization,Social networking (online),Context modeling,Recommender systems,Data models,Task analysis,History","Image based social networks are among the most popular social networking services in recent years. With a tremendous amount of images uploaded everyday, understanding users' preferences on user-generated images and making recommendations have become an urgent need. In fact, many hybrid models have been proposed to fuse various kinds of side information (e.g., image visual representation, social network) and user-item historical behavior for enhancing recommendation performance. However, due to the unique characteristics of the user generated images in social image platforms, the previous studies failed to capture the complex aspects that influence users' preferences in a unified framework. Moreover, most of these hybrid models relied on predefined weights in combining different kinds of information, which usually resulted in sub-optimal recommendation performance. To this end, in this paper, we develop a hierarchical attention model for social contextual image recommendation. In addition to basic latent user interest modeling in the popular matrix factorization based recommendation, we identify three key aspects (i.e., upload history, social influence, and owner admiration) that affect each user's latent preferences, where each aspect summarizes a contextual factor from the complex relationships between users and images. After that, we design a hierarchical attention network that naturally mirrors the hierarchical relationship (elements in each aspects level, and the aspect level) of users' latent interests with the identified key aspects. Specifically, by taking embeddings from state-of-the-art deep learning models that are tailored for each kind of data, the hierarchical attention network could learn to attend differently to more or less content. Finally, extensive experimental results on real-world datasets clearly show the superiority of our proposed model."
818,,Unlocking Author Power - On the Exploitation of Auxiliary Author-Retweeter Relations for Predicting Key Retweeters.,"Bo Wu 0018,Wen-Huang Cheng,Yongdong Zhang 0001,Juan Cao,Jintao Li,Tao Mei 0001",https://doi.org/10.1109/TKDE.2018.2889664,TKDE,2020,"Social network services,Predictive models,Prediction algorithms,Computers,Technological innovation,Information processing,Task analysis","Retweeting is a powerful driving force in information propagation on microblogging sites. However, identifying the most effective retweeters of a message (called the ”key retweeter prediction” problem) has become a significant research topic. Conventional approaches have addressed this topic from two main aspects: by analyzing either the personal attributes of microblogging users or the structures of user graph networks. However, according to sociological findings, author-retweeter dependencies also play a crucial role in influencing message propagation. In this paper, we propose a novel model to solve the key retweeter prediction problem by incorporating the auxiliary relations between a tweet author and potential retweeters. Without loss of generality, we formulate the relations from four relational factors: status relation, temporal relation, locational relation, and interactive relation. In addition, we propose a novel method, called “Relation-based Learning to Rank (RL2R),” to determine the key retweeters for a given tweet by ranking the potential retweeters in terms of their spreadability. The experimental results show that our method outperforms the state-of-the-art algorithms at top-k retweeter prediction, achieving a significant relative average improvement of 19.7-29.4 percent. These findings provide new insights for understanding user behaviors on social media for key retweeter prediction purposes."
819,,SRA - Secure Reverse Auction for Task Assignment in Spatial Crowdsourcing.,"Mingjun Xiao,Kai Ma,An Liu 0002,Hui Zhao,Zhixu Li,Kai Zheng 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2893240,TKDE,2020,"Task analysis,Crowdsourcing,Protocols,Bipartite graph,Security,Approximation algorithms,Computer science","In this paper, we study a new type of spatial crowdsourcing, namely competitive detour tasking, where workers can make detours from their original travel paths to perform multiple tasks, and each worker is allowed to compete for preferred tasks by strategically claiming his/her detour costs. The objective is to make suitable task assignment by maximizing the social welfare of crowdsourcing systems and protecting workers' private sensitive information. We first model the task assignment problem as a reverse auction process. We formalize the winning bid selection of reverse auction as an n-to-one weighted bipartite graph matching problem with multiple 0-1 knapsack constraints. Since this problem is NP-hard, we design an approximation algorithm to select winning bids and determine corresponding payments. Based on this, a Secure Reverse Auction (SRA) protocol is proposed for this novel spatial crowdsourcing. We analyze the approximation performance of the proposed protocol and prove that it has some desired properties, including truthfulness, individual rationality, computational efficiency, and security. To the best of our knowledge, this is the first theoretically provable secure auction protocol for spatial crowdsourcing systems. In addition, we also conduct extensive simulations on a real trace to verify the performance of the proposed protocol."
820,,Multi-View Support Vector Machines with the Consensus and Complementarity Information.,"Xijiong Xie,Shiliang Sun",https://doi.org/10.1109/TKDE.2019.2933511,TKDE,2020,"Support vector machines,Machine learning,Quadratic programming,Training data,Linear programming,Quadratic programming","Multi-view learning (MVL) is an active direction in machine learning that aims at exploiting the consensus and complementarity information among multiple distinct feature sets to boost the generalization performance of the counterpart algorithm. So far, two classical SVM-based MVL methods are SVM-2K and multi-view twin support vector machine (MvTSVM). They are designed only for two-view classification and cannot tackle the general multi-view classification problem. They also cannot effectively leverage the complementarity information among different feature views. In this paper, we propose two novel multi-view support vector machines with the consensus and complementarity information for MVL that not only can deal with the two-view classification problem but also the general multi-view classification problem by jointly learning multiple different views in a non-pairwise way. The disagreement among different views is regarded as a constraint or a regularization term in the objective function which plays an important role in exploring the consensus information. Combination weights for the reconstruction of each view in regularization terms are learned to explore complementarity information among different views. Finally, an efficient iteration algorithm with the classical convex quadratic programming is developed for optimization. Experimental results validate the effectiveness of our proposed methods."
821,,OLAP over Probabilistic Data Cubes II - Parallel Materialization and Extended Aggregates.,"Xike Xie,Kai Zou,Xingjun Hao,Torben Bach Pedersen,Peiquan Jin,Wei Yang 0011",https://doi.org/10.1109/TKDE.2019.2913420,TKDE,2020,"Probabilistic logic,Aggregates,Sensors,Temperature measurement,Query processing,Convolution,Time measurement","On-Line Analytical Processing (OLAP) enables powerful analytics by quickly computing aggregate values of numerical measures over multiple hierarchical dimensions for massive datasets. However, many types of source data, e.g., from GPS, sensors, and other measurement devices, are intrinsically inaccurate (imprecise and/or uncertain) and thus OLAP cannot be readily applied. In this paper, we address the resulting data veracityproblem in OLAP by proposing the concept of probabilistic data cubes. Such a cube is comprised of a set of probabilistic cuboids which summarize the aggregated values in the form of probability mass functions (pmfs in short) and thus offer insights into the underlying data quality and enable confidence-aware query evaluation and analysis. However, the probabilistic nature of data poses computational challenges, since a probabilistic database can have exponential number of possible worlds under the possible world semantics. Even worse, it is hard to share computations among different cuboids, as aggregation functions that are distributive for traditional data cubes, e.g., SUM, become holistic in probabilistic settings. In this paper, we propose a complete set of techniques for probabilistic data cubes, from cuboid aggregation, over cube materialization, to query evaluation. We study two types of aggregation: convolution and sketch-based, which take polynomial time complexities for aggregation and jointly enable efficient query processing. Also, our proposal is versatile in terms of: 1) its capability of supporting common aggregation functions, i.e., SUM, COUNT, MAX, and AVG; 2) its adaptivity to different materialization strategies, e.g., full versus partial materialization, with support of our devised cost models and parallelization framework; 3) its coverage of common OLAP operations, i.e., probabilistic slicing and dicing queries. Extensive experiments over real and synthetic datasets show that our techniques are effective and scalable."
822,,Network Embedding via Coupled Kernelized Multi-Dimensional Array Factorization.,"Linchuan Xu,Jiannong Cao 0001,Xiaokai Wei,Philip S. Yu",https://doi.org/10.1109/TKDE.2019.2931833,TKDE,2020,"Kernel,Arrays,Data mining,Proteins,Social networking (online),Data models,Context modeling","Network embedding has been widely employed in networked data mining applications as it can learn low-dimensional and dense node representations from the high-dimensional and sparse network structure. While most existing network embedding methods only model the proximity between two nodes regardless of the order of the proximity, this paper proposes to explicitly model multi-node proximities which can be widely observed in practice, e.g., multiple researchers coauthor a paper, and multiple genes co-express a protein. Explicitly modeling multi-node proximities is important because some two-node interactions may not come into existence without a third node. By proving that LINE(1st), a recent network embedding method, is equivalent to kernelized matrix factorization, this paper proposes coupled kernelized multi-dimensional array factorization (Cetera) which jointly factorizes multiple multi-dimensional arrays by enforcing a consensus representation for each node. In this way, node representations can be more comprehensive and effective, which is demonstrated on three real-world networks through link prediction and multi-label classification."
823,,Multi-View Scaling Support Vector Machines for Classification and Feature Selection.,"Jinglin Xu,Junwei Han,Feiping Nie 0001,Xuelong Li",https://doi.org/10.1109/TKDE.2019.2904256,TKDE,2020,"Support vector machines,Feature extraction,Collaboration,Optimization,Training,Data mining,Machine learning","With the explosive growth of data, the multi-view data is widely used in many fields, such as data mining, machine learning, computer vision, and so on. Because such data always has a complex structure, i.e., many categories, many perspectives of description and high dimension, how to formulate an accurate and reliable framework for the multi-view classification is a very challenging task. In this paper, we propose a novel multi-view classification method by using multiple multi-class Support Vector Machines (SVMs) with a novel collaborative strategy. Here, each multi-class SVM embeds the scaling factor to renewedly adjust the weight allocation of all features, which is beneficial to highlight more important and discriminative features. Furthermore, we adopt the decision function values to integrate multiple multi-class learners and introduce the confidence score across multiple classes to determine the final classification result. In addition, through a series of the mathematical deduction, we bridge the proposed model with the solvable problem and solve it through an alternating iteration optimization method. We evaluate the proposed method on several image and face datasets, and the experimental results demonstrate that our proposed method performs better than other state-of-the-art learning algorithms."
824,,Robust Multi-Label Learning with PRO Loss.,"Miao Xu,Yu-Feng Li,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2019.2908898,TKDE,2020,"Optimization,Roads,Proposals,Focusing,Correlation,Learning systems,Prediction algorithms","Multi-label learning methods assign multiple labels to one object. In practice, in addition to differentiating relevant labels from irrelevant ones, it is often desired to rank relevant labels for an object, whereas the ranking of irrelevant labels is not important. Thus, we require an algorithm to do classification and ranking of relevant labels simultaneously. Such a requirement, however, cannot be met because most existing methods were designed to optimize existing criteria, yet there is no criterion which encodes the aforementioned requirement. In this paper, we present a new criterion, PRO LOSS, concerning the prediction of all labels as well as the ranking of only relevant labels. We then propose ProSVM which optimizes PRO LOSS efficiently using alternating direction method of multipliers. We further improve its efficiency with an upper approximation that reduces the number of constraints from O(T
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) to O(T), where T is the number of labels. We then notice that in real applications, it is difficult to get full supervised information for multi-label data. To make the proposed algorithm more robust to supervised information, we adapt ProSVM to deal with the multi-label learning with partial labels problem. Experiments show that our proposal is not only superior on PRO LOSS, but also highly competitive on existing evaluation criteria."
825,,Skia - Scalable and Efficient In-Memory Analytics for Big Spatial-Textual Data.,"Yang Xu 0031,Bin Yao 0002,Zhi-Jie Wang 0009,Xiaofeng Gao,Jiong Xie,Minyi Guo",https://doi.org/10.1109/TKDE.2019.2915828,TKDE,2020,"Indexes,Programming,Distributed databases,Location awareness,Time factors,Spatial databases,Keyword search","In recent years, spatial-keyword queries have attracted much attention with the fast development of location-based services. However, current spatial-keyword techniques are disk-based, which cannot fulfill the requirements of high throughput and low response time. With the surging data size, people tend to process data in distributed in-memory environments to achieve low latency. In this paper, we present the distributed solution, i.e., Skia (Spatial-Keyword In-memory Analytics), to provide a scalable backend for spatial-textual analytics. Skia introduces a two-level index framework for big spatial-textual data including: (1) efficient and scalable global index, which prunes the candidate partitions a lot while achieving small space budget; and (2) four novel local indexes, that further support low latency services for exact and approximate spatial-keyword queries. Skia can support common spatial-keyword queries via traditional SQL programming interfaces. The experiments conducted on large-scale real datasets have demonstrated the promising performance of the proposed indexes and our distributed solution."
826,,Massively Distributed Time Series Indexing and Querying.,"Djamel Edine Yagoubi,Reza Akbarinia,Florent Masseglia,Themis Palpanas",https://doi.org/10.1109/TKDE.2018.2880215,TKDE,2020,"Time series analysis,Indexing,Query processing,Euclidean distance,Task analysis","Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on four billion time series in less than five hours, while the state of the art centralized algorithms do not scale and have their limit on 1 billion time series, where they need more than five days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism."
827,,Using Latent Knowledge to Improve Real-Time Activity Recognition for Smart IoT.,"Surong Yan,Kwei-Jay Lin,Xiaolin Zheng,Wenyu Zhang 0001",https://doi.org/10.1109/TKDE.2019.2891659,TKDE,2020,"Hidden Markov models,Probability distribution,Real-time systems,Windows,Deep learning,Microsoft Windows","Real-time/online activity recognition (AR) is an important technology in smart Internet of Things (IoT) systems where users are assisted by smart devices in their daily activities. How to generate appropriate feature representation from sensor event streaming is a challenging issue for accurate and efficient real-time AR. Previous AR models that rely on explicit domain knowledge are not appropriate for online recognition of complex human activities. We propose to use unsupervised learning to learn about the latent knowledge and embed the activity probability distribution prediction as high-level features to boost real-time AR performance. The proposed approach first learns the latent knowledge from explicit-activity window sequences using unsupervised learning, and derives the probability distribution prediction of activity classes for a given sliding window. Our approach then feeds the prediction with other basic features of the sliding window into a classifier to produce the final class result on each event-count sliding window. Experiments on five smart home datasets show that the proposed method achieves a higher accuracy by at least 20 percent improvement on F1_score than previous traditional algorithms, while maintaining a lower time cost than deep learning based methods. An analysis on the feature importance shows that the addition of probability distribution prediction about activity classes leads to a promising direction for real-time AR."
828,,"Fast and Low Memory Cost Matrix Factorization - Algorithm, Analysis, and Case Study.","Yan Yan 0006,Mingkui Tan,Ivor W. Tsang,Yi Yang 0001,Qinfeng Shi,Chengqi Zhang",https://doi.org/10.1109/TKDE.2018.2882197,TKDE,2020,"Imaging,Optimization,Recommender systems,Manifolds,Convergence,Minimization,Dynamic range","Matrix factorization has been widely applied to various applications. With the fast development of storage and internet technologies, we have been witnessing a rapid increase of data. In this paper, we propose new algorithms for matrix factorization with the emphasis on efficiency. In addition, most existing methods of matrix factorization only consider a general smooth least square loss. Differently, many real-world applications have distinctive characteristics. As a result, different losses should be used accordingly. Therefore, it is beneficial to design new matrix factorization algorithms that are able to deal with both smooth and non-smooth losses. To this end, one needs to analyze the characteristics of target data and use the most appropriate loss based on the analysis. We particularly study two representative cases of low-rank matrix recovery, i.e., collaborative filtering for recommendation and high dynamic range imaging. To solve these two problems, we respectively propose a stage-wise matrix factorization algorithm by exploiting manifold optimization techniques. From our theoretical analysis, they are both are provably guaranteed to converge to a stationary point. Extensive experiments on recommender systems and high dynamic range imaging demonstrate the satisfactory performance and efficiency of our proposed method on large-scale real data."
829,,An Efficient Destination Prediction Approach Based on Future Trajectory Prediction and Transition Matrix Optimization.,"Zhou Yang,Heli Sun,Jianbin Huang,Zhongbin Sun,Hui Xiong,Shaojie Qiao,Ziyu Guan,Xiaolin Jia",https://doi.org/10.1109/TKDE.2018.2883938,TKDE,2020,"Dynamic programming,Markov processes,Bayes methods,Computational complexity","Destination prediction is an essential task in various mobile applications and up to now many methods have been proposed. However, existing methods usually suffer from the problems of heavy computational burden, data sparsity, and low coverage. Therefore, a novel approach named DestPD is proposed to tackle the aforementioned problems. Differing from an earlier approach that only considers the starting and current location of a partial trip, DestPD first determines the most likely future location and then predicts the destination. It comprises two phases, the offline training and the online prediction. During the offline training, transition probabilities between two locations are obtained via Markov transition matrix multiplication. In order to improve the efficiency of matrix multiplication, we propose two data constructs, Efficient Transition Probability (ETP) and Transition Probabilities with Detours (TPD). They are capable of pinpointing the minimum amount of needed computation. During the online prediction, we design Obligatory Update Point (OUP) and Transition Affected Area (TAA) to accelerate the frequent update of ETP and TPD for recomputing the transition probabilities. Moreover, a new future trajectory prediction approach is devised. It captures the most recent movement based on a query trajectory. It consists of two components: similarity finding through Best Path Notation (BPN) and best node selection. Our novel BPN similarity finding scheme keeps track of the nodes that induces inefficiency and then finds similarity fast based on these nodes. It is particularly suitable for trajectories with overlapping segments. Finally, the destination is predicted by combining transition probabilities and the most probable future location through Bayesian reasoning. The DestPD method is proved to achieve one order of cut in both time and space complexity. Furthermore, the experimental results on real-world and synthetic datasets have shown that DestPD consistently surpasses the state-of-the-art methods in terms of both efficiency (approximately over 100 times faster) and accuracy."
830,,A Scalable Multi-Data Sources Based Recursive Approximation Approach for Fast Error Recovery in Big Sensing Data on Cloud.,"Chi Yang,Xianghua Xu,Kotagiri Ramamohanarao,Jinjun Chen",https://doi.org/10.1109/TKDE.2019.2895612,TKDE,2020,"Sensors,Big Data,Cloud computing,Reliability,Complex networks,Time series analysis","Big sensing data is commonly encountered from various surveillance or sensing systems. Sampling and transferring errors are commonly encountered during each stage of sensing data processing. How to recover from these errors with accuracy and efficiency is quite challenging because of high sensing data volume and unrepeatable wireless communication environment. While Cloud provides a promising platform for processing big sensing data, however scalable and accurate error recovery solutions are still need. In this paper, we propose a novel approach to achieve fast error recovery in a scalable manner on cloud. This approach is based on the prediction of a recovery replacement data by making multiple data sources based approximation. The approximation process will use coverage information carried by data units to limit the algorithm in a small cluster of sensing data instead of a whole data spectrum. Specifically, in each sensing data cluster, a Euclidean distance based approximation is proposed to calculate a time series prediction. With the calculated time series, a detected error can be recovered with a predicted data value. Through the experiment with real world meteorological data sets on cloud, we demonstrate that the proposed error recovery approach can achieve high accuracy in data approximation to replace the original data error. At the same time, with MapReduce based implementation for scalability, the experimental results also show significant efficiency on time saving."
831,,"Towards Automatic Construction of Diverse, High-Quality Image Datasets.","Yazhou Yao,Jian Zhang 0002,Fumin Shen,Li Liu 0004,Fan Zhu 0001,Dongxiang Zhang,Heng Tao Shen",https://doi.org/10.1109/TKDE.2019.2903036,TKDE,2020,"Noise measurement,Search engines,Manuals,Visualization,Scalability,Data models,Task analysis","The availability of labeled image datasets has been shown critical for high-level image understanding, which continuously drives the progress of feature designing and models developing. However, constructing labeled image datasets is laborious and monotonous. To eliminate manual annotation, in this work, we propose a novel image dataset construction framework by employing multiple textual queries. We aim at collecting diverse and accurate images for given queries from the Web. Specifically, we formulate noisy textual queries removing and noisy images filtering as a multi-view and multi-instance learning problem separately. Our proposed approach not only improves the accuracy but also enhances the diversity of the selected images. To verify the effectiveness of our proposed approach, we construct an image dataset with 100 categories. The experiments show significant performance gains by using the generated data of our approach on several tasks, such as image classification, cross-dataset generalization, and object detection. The proposed method also consistently outperforms existing weakly supervised and web-supervised approaches."
832,,Conversion Prediction from Clickstream - Modeling Market Prediction and Customer Predictability.,"Jinyoung Yeo,Seung-won Hwang,Sungchul Kim,Eunyee Koh,Nedim Lipka",https://doi.org/10.1109/TKDE.2018.2884467,TKDE,2020,"Predictive models,Data models,Measurement,Microsoft Windows,Advertising,Tools,Computer science","As 98 percent of shoppers do not make a purchase on the first visit, we study the problem of predicting whether they would come back for a purchase later (i.e., conversion prediction). This problem is important for strategizing “retargeting”, for example, by sending coupons for customers who are likely to convert. For this goal, we study the following two problems, prediction of market and predictability of customer. First, prediction of market aims at identifying a conversion rate for a given product and its customer behavior modeling, which is an important analytics metric for retargeting process. Compared to existing approaches using either of customer or product-level conversion pattern, we propose a joint modeling of both patterns based on the well-studied buying decision process. Second, we can observe customer-specific behaviors after showing retargeting ads, to predict whether this specific customer follows the market model (high predictability) or not (low predictability). For the former, we apply the market model, and for the latter, we propose a new customer-specific prediction based on dynamic ad behavior features. To evaluate the effectiveness of our methods, we perform extensive experiments on the simulated dataset generated based on a set of real-world web logs and retargeting campaign logs. The evaluation results show that conversion predictions and predictability by our approach are consistently more accurate and robust than those by existing baselines in dynamic market environment."
833,,XINA - Explainable Instance Alignment Using Dominance Relationship.,"Jinyoung Yeo,Haeju Park,Sanghoon Lee,Eric Wonhee Lee,Seung-won Hwang",https://doi.org/10.1109/TKDE.2018.2881956,TKDE,2020,"Data mining,Decision making,Information retrieval,Feature extraction,Knowledge based systems,Information retrieval,Inference mechanisms","Over the past few years, knowledge bases (KBs) like DBPedia, Freebase, and YAGO have accumulated a massive amount of knowledge from web data. Despite their seemingly large size, however, individual KBs often lack comprehensive information on any given domain. For example, over 70 percent of people on Freebase lack information on place of birth. For this reason, the complementary nature across different KBs motivates their integration through a process of aligning instances. Meanwhile, since application-level machine systems, such as medical diagnosis, have heavily relied on KBs, it is necessary to provide users with trustworthy reasons why the alignment decisions are made. To address this problem, we propose a new paradigm, explainable instance alignment (XINA), which provides user-understandable explanations for alignment decisions. Specifically, given an alignment candidate, XINA replaces existing scalar representation of an aggregated score, by decision and explanation-vector spaces for machine decision and user understanding, respectively. To validate XINA, we perform extensive experiments on real-world KBs and show that XINA achieves comparable performance with state-of-the-arts, even with far less human effort."
834,,Privacy-Preserving User Profile Matching in Social Networks.,"Xun Yi,Elisa Bertino,Fang-Yu Rao,Kwok-Yan Lam,Surya Nepal,Athman Bouguettaya",https://doi.org/10.1109/TKDE.2019.2912748,TKDE,2020,"Encryption,Servers,Social networking (online),Databases,Privacy","In this paper, we consider a scenario where a user queries a user profile database, maintained by a social networking service provider, to identify users whose profiles match the profile specified by the querying user. A typical example of this application is online dating. Most recently, an online dating website, Ashley Madison, was hacked, which resulted in a disclosure of a large number of dating user profiles. This data breach has urged researchers to explore practical privacy protection for user profiles in a social network. In this paper, we propose a privacy-preserving solution for profile matching in social networks by using multiple servers. Our solution is built on homomorphic encryption and allows a user to find out matching users with the help of multiple servers without revealing to anyone the query and the queried user profiles in clear. Our solution achieves user profile privacy and user query privacy as long as at least one of the multiple servers is honest. Our experiments demonstrate that our solution is practical."
835,,Efficient Contour Computation of Group-Based Skyline.,"Wenhui Yu,Jinfei Liu,Jian Pei,Li Xiong 0001,Xu Chen 0017,Zheng Qin",https://doi.org/10.1109/TKDE.2019.2905239,TKDE,2020,"Clustering algorithms,Aggregates,Pareto optimization,Decision making,Computational geometry,Databases,Time complexity","Skyline, aiming at finding a Pareto optimal subset of points in a multi-dimensional dataset, has gained great interest due to its extensive use for multi-criteria analysis and decision making. The skyline consists of all points that are not dominated by any other points. It is a candidate set of the optimal solution, which depends on a specific evaluation criterion for optimum. However, conventional skyline queries, which return individual points, are inadequate in group querying case since optimal combinations are required. To address this gap, we study the skyline computation in the group level and propose efficient methods to find the Group-based skyline (G-skyline). For computing the front l skyline layers, we lay out an efficient approach that does the search concurrently on each dimension and investigates each point in the subspace. After that, we present a novel structure to construct the G-skyline with a queue of combinations of the first-layer points. We further demonstrate that the G-skyline is a complete candidate set of top-l solutions, which is the main superiority over previous group-based skyline definitions. However, as G-skyline is complete, it contains a large number of groups which can make it impractical. To represent the “contour” of the G-skyline, we define the Representative G-skyline (RG-skyline). Then, we propose a Group-based clustering (G-clustering) algorithm to find out RG-skyline groups. Experimental results show that our algorithms are several orders of magnitude faster than the previous work."
836,,Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy Rates.,"Chen Jason Zhang,Lei Chen 0002,H. V. Jagadish,Mengchen Zhang,Yongxin Tong",https://doi.org/10.1109/TKDE.2018.2881185,TKDE,2020,"Uncertainty,Entropy,Tools,Crowdsourcing,Databases,Computer science,Task analysis","Schema matching is a central challenge for data integration systems. Inspired by the popularity and the success of crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching. Since crowdsourcing platforms are most effective for simple questions, we assume that each Correspondence Correctness Question (CCQ) asks the crowd to decide whether a given correspondence should exist in the correct matching. Furthermore, members of a crowd may sometimes return incorrect answers with different probabilities. Accuracy rates of individual crowd workers can be attributes of CCQs as well as evaluations of individual workers. We prove that uncertainty reduction equals to entropy of answers minus entropy of crowds and show how to obtain lower and upper bounds for it. We propose frameworks and efficient algorithms to dynamically manage the CCQs to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely “Single CCQ” and “Multiple CCQ”, which adaptively select, publish, and manage questions. We verify the value of our solutions with simulation and real implementation."
837,,Robust Low-Rank Kernel Subspace Clustering based on the Schatten p-norm and Correntropy.,"Xiaoqian Zhang,Beijia Chen,Huaijiang Sun,Zhigui Liu,Zhenwen Ren,Yanmeng Li",https://doi.org/10.1109/TKDE.2019.2922637,TKDE,2020,"Kernel,Data models,Robustness,Sparse matrices,Clustering methods,Optimization,Closed-form solutions","Subspace clustering plays an important role in the tasks such as data processing and pattern recognition. Since the high-dimensional data may contain complex noise, as well as non-linear structure, learning low-dimensional subspace structures is a challenging task. However, the existing methods to deal with both problems relax the original problem convexly. The results of solving by these methods deviate from the solution of the original problem. In this paper, to overcome this deficiency, we propose a robust low-rank kernel subspace clustering model, which coalesces the non-convex Schatten p-norm (0 <; p ≤ 1) regularizer with “kernel trick” and correntropy. Our “kernel trick” extends linear subspace clustering to non-linear counterparts, the Schatten p-norm regularizer can approximate the rank of the data in feature space effectively, and the correntropy is a robust measure to large corruptions. Furthermore, an efficient iterative algorithm (HQ-ADMM) is designed to solve the formulated problem, which coalesces the half-quadratic technique and Alternating Direction Method of Multipliers. This algorithm can ensure the closed form solutions at each iteration, which improves the computation speed of the algorithm. Extensive experiments on face/object clustering and motion segmentation clearly attest the ascendancy of the proposed method over several state-of-the-art methods."
838,,Modeling and Computing Probabilistic Skyline on Incomplete Data.,"Kaiqi Zhang,Hong Gao 0001,Xixian Han,Zhipeng Cai 0001,Jianzhong Li 0001",https://doi.org/10.1109/TKDE.2019.2904967,TKDE,2020,"Probabilistic logic,Computational modeling,Data models,Partitioning algorithms,Indexes,Motion pictures,Sorting","The skyline query is important in the database community. In recent years, the researches on incomplete data have been increasingly considered, especially for the skyline query. However, the existing skyline definition on incomplete data cannot provide users with valuable references. In this paper, we propose a novel skyline definition utilizing probabilistic model on incomplete data where each point has a probability to be in the skyline. In particular, it returns K points with the highest skyline probabilities. In addition, we propose incomplete models and estimate probability density functions of missing values on independent, correlated, and anti-correlated distributions, respectively. Meanwhile, it is a big challenge to compute probabilistic skyline on incomplete data. We propose three efficient algorithms SPISkyline, SPCSkyline, and SPASkyline for probabilistic skyline computation on incomplete data complying with independent, correlated, and anti-correlated distributions, respectively. They employ pruning strategy, optimization of the process of probability computation, and sorting technique to improve the efficiency of probabilistic skyline computation on incomplete data. Our experimental results demonstrate that our proposed concept of probabilistic skyline is an effective method to tackle skyline query on incomplete data and our algorithms are tens of times faster than the naive algorithm on both synthetic and real datasets."
839,,"Re-Revisiting Learning on Hypergraphs - Confidence Interval, Subgradient Method, and Extension to Multiclass.","Chenzi Zhang,Shuguang Hu,Zhihao Gavin Tang,T.-H. Hubert Chan",https://doi.org/10.1109/TKDE.2018.2880448,TKDE,2020,"Linear programming,Semisupervised learning,Laplace equations,Task analysis,Image edge detection,Standards,Machine learning","We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our subgradient method gives faster running times when the number of vertices is much larger than the number of edges. Our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy. Furthermore, our model can be readily extended to capture multiclass learning."
840,,Finding Critical Users in Social Communities - The Collapsed Core and Truss Problems.,"Fan Zhang 0036,Conggai Li,Ying Zhang 0001,Lu Qin,Wenjie Zhang 0001",https://doi.org/10.1109/TKDE.2018.2880976,TKDE,2020,"Social network services,Computational modeling,Heuristic algorithms,Complexity theory,Proteins,Prediction algorithms,Fans","In social networks, the leave of critical users may significantly break network engagement, i.e., lead a large number of other users to drop out. A popular model to measure social network engagement is k-core, the maximal subgraph in which every vertex has at least k neighbors. To identify critical users, we propose the collapsed k-core problem: given a graph G, a positive integer k and a budget b, we aim to find b vertices in G such that the deletion of the b vertices leads to the smallest k-core. We prove the problem is NP-hard and in approximate. An efficient algorithm is proposed, which significantly reduces the number of candidate vertices. We also study the user leave towards the model of k-truss which further considers tie strength by conducting additional computation w.r.t. k-core. We prove the corresponding collapsed k-truss problem is also NP-hard and in approximate. An efficient algorithm is proposed to solve the problem. The advantages and disadvantages of the two proposed models are experimentally compared. Comprehensive experiments on nine real-life social networks demonstrate the effectiveness and efficiency of our proposed methods."
841,,Evaluation of the Sample Clustering Process on Graphs.,"Jianpeng Zhang,Yulong Pei,George Fletcher,Mykola Pechenizkiy",https://doi.org/10.1109/TKDE.2019.2904682,TKDE,2020,"Measurement,Clustering algorithms,Task analysis,Memory management,Social networking (online),Knowledge engineering,Indexes","An increasing number of networks are becoming large-scale and continuously growing in nature, such that clustering on them in their entirety could be intractable. A feasible way to overcome this problem is to sample a representative subgraph and exploit its clustering structure (namely, sample clustering process). However, there are two issues that we should address in current studies. One underlying question is how to evaluate the clustering quality of the entire sample clustering process. Another non-trivial issue is that multiple ground-truths exist in networks, thus evaluating the clustering results in such scenario is also a challenging task. In this paper, first we utilize the set-matching methodology to quantitatively evaluate how differently the clusters of the sampled counterpart correspond to the ground-truth(s) in the original graph, and propose several new quality metrics to capture the differences of clustering structure in various aspects. Second, we put forward an evaluation framework for the general problems of evaluating the clustering quality on graph samples. Extensive experiments on various synthetic and real-world graphs demonstrate that our new quality metrics are more accurate and insightful for the sample clustering evaluation than conventional metrics (e.g., NMI). Thus the evaluation framework is effective and practical to assess the clustering quality of the sample clustering process on massive graphs."
842,,Bayesian Networks for Data Integration in the Absence of Foreign Keys.,"Bohan Zhang,Scott Sanner,Mohamed Reda Bouadjenek,Shagun Gupta",https://doi.org/10.1109/TKDE.2019.2940019,TKDE,2020,"Bayes methods,Probabilistic logic,Data integration,Periodic structures,Education,Data models,Random variables","In the era of open data, a single data source rarely contains all of the attributes we need for inference in specific applications. For example, a marketing department may aim to integrate retailer-specific purchase data with separate demographic data for purposes of targeted advertising - a capability not possible with either dataset alone. In this work, we address two key desiderata of an automated framework for probabilistic data integration over multiple data sources: (1) we require that each relational data source share at least one attribute with another relational data source, but we do not require these attributes to be foreign keys (e.g., attributes such as gender, age, and postal code are not foreign keys because they do not uniquely identify individuals in a data source) and (2) we require inference to be probabilistic to reflect inherent uncertainty in population-level predictions given the absence of foreign keys. While some frameworks such as Probabilistic Relational Models (PRMs) address point (2), they do not address point (1) since they rely on foreign keys to link tables. To achieve both desiderata simultaneously, we develop an automated framework to construct Bayesian networks for data integration capable of answering any probabilistic query spanning the attributes of multiple relational data sources. We demonstrate that our framework is able to closely approximate the inference of a global Bayesian network over a single relation that has been projected onto multiple local relations and further investigate properties of local relations such as the number of shared attributes and their cardinality to understand how these properties affect the quality of inference."
843,,Combine Topic Modeling with Semantic Embedding - Embedding Enhanced Topic Model.,"Peng Zhang 0064,Suge Wang,Deyu Li,Xiaoli Li 0001,Zhikang Xu",https://doi.org/10.1109/TKDE.2019.2922179,TKDE,2020,"Semantics,Task analysis,Data models,Context modeling,Syntactics,Text analysis,Correlation","Topic model and word embedding reflect two perspectives of text semantics. Topic model maps documents into topic distribution space by utilizing word collocation patterns within and across documents, while word embedding represents words within a continuous embedding space by exploiting the local word collocation patterns in context windows. Clearly, these two types of patterns are complementary. In this paper, we propose a novel integration framework to combine the two representation methods, where topic information can be transmitted into corresponding semantic embedding structure. Based on this framework, we construct a Embedding Enhanced Topic Model (EETM), which can improve topic modeling and generate topic embeddings by leveraging the word embedding. Extensive experimental results show that EETM can learn high-quality document representations for common text analysis tasks across multiple data sets, indicating it is very effective for merging topic models with word embeddings."
844,,Feature Selection for Neural Networks Using Group Lasso Regularization.,"Huaqing Zhang 0002,Jian Wang 0010,Zhanquan Sun,Jacek M. Zurada,Nikhil R. Pal",https://doi.org/10.1109/TKDE.2019.2893266,TKDE,2020,"Feature extraction,Task analysis,Optimization,Multilayer perceptrons,Input variables","We propose an embedded/integrated feature selection method based on neural networks with Group Lasso penalty. Group Lasso regularization is considered to produce sparsity on the inputs to the network, i.e., for selection of useful features. Lasso based feature selection using a multi-layer perceptron usually requires an additional set of weights, while our Group Lasso formulation does not require that. However, Group Lasso penalty is non-differentiable at the origin. This may lead to oscillations in numerical simulations and make it difficult to analyze theoretically. To address this issue, four smoothing Group Lasso penalties are introduced. A rigorous proof for the convergence of the proposed algorithm is presented under suitable assumptions. To verify the effectiveness, a three-step algorithmic architecture is adopted in implementation. Experimental results on several datasets validate the theoretical results and demonstrate the competitive performance of the proposed method."
845,,A Transformation-Based Framework for KNN Set Similarity Search.,"Yong Zhang 0002,Jiacheng Wu,Jin Wang 0007,Chunxiao Xing",https://doi.org/10.1109/TKDE.2018.2886189,TKDE,2020,"Search problems,Indexes,Approximation algorithms,Measurement,Transforms,Estimation,Computer science","Set similarity search is a fundamental operation in a variety of applications. While many previous studies focus on threshold based set similarity search and join, few efforts have been paid for KNN set similarity search. In this paper, we propose a transformation based framework to solve the problem of KNN set similarity search, which given a collection of set records and a query set, returns k results with the largest similarity to the query. We devise an effective transformation mechanism to transform sets with various lengths to fixed length vectors which can map similar sets closer to each other. Then, we index such vectors with a tiny tree structure. Next, we propose efficient search algorithms and pruning strategies to perform exact KNN set similarity search. We also design an estimation technique by leveraging the data distribution to support approximate KNN search, which can speed up the search while retaining high recall. Experimental results on real world datasets show that our framework significantly outperforms state-of-the-art methods in both memory and disk based settings."
846,,Optimal Margin Distribution Machine.,"Teng Zhang 0001,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2019.2897662,TKDE,2020,"Support vector machines,Kernel,Training,Biological system modeling,Quadratic programming,History","Support Vector Machine (SVM) has always been one of the most successful learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. However, recent theoretical results disclosed that maximizing the minimum margin does not necessarily lead to better generalization performance, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose the Optimal margin Distribution Machine (ODM), which can achieve a better generalization performance by optimizing the margin distribution explicitly. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be applied in any place where SVMs are used, and its superiority is verified both theoretically and empirically in this paper."
847,,Joint Label Prediction Based Semi-Supervised Adaptive Concept Factorization for Robust Data Representation.,"Zhao Zhang 0001,Yan Zhang 0053,Guangcan Liu,Jinhui Tang,Shuicheng Yan,Meng Wang 0001",https://doi.org/10.1109/TKDE.2019.2893956,TKDE,2020,"Manifolds,Image reconstruction,Kernel,Adaptation models,Data models,Data mining,Principal component analysis","Constrained Concept Factorization (CCF) yields the enhanced representation ability over CF by incorporating label information as additional constraints, but it cannot classify and group unlabeled data appropriately. Minimizing the difference between the original data and its reconstruction directly can enable CCF to model a small noisy perturbation, but is not robust to gross sparse errors. Besides, CCF cannot preserve the manifold structures in new representation space explicitly, especially in an adaptive manner. In this paper, we propose a joint label prediction based Robust Semi-Supervised Adaptive Concept Factorization (RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF) framework. To obtain robust representation, RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF relaxes the factorization to make it simultaneously stable to small entrywise noise and robust to sparse errors. To enrich prior knowledge to enhance the discrimination, RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF clearly uses class information of labeled data and more importantly propagates it to unlabeled data by jointly learning an explicit label indicator for unlabeled data. By the label indicator, RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF can ensure the unlabeled data of the same predicted label to be mapped into the same class in feature space. Besides, RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF incorporates the joint neighborhood reconstruction error over the new representations and predicted labels of both labeled and unlabeled data, so the manifold structures can be preserved explicitly and adaptively in the representation space and label space at the same time. Owing to the adaptive manner, the tricky process of determining the neighborhood size or kernel width can be avoided. Extensive results on public databases verify that our RS
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
ACF can deliver state-of-the-art data representation, compared with other related methods."
848,,Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning.,"Junbo Zhang,Yu Zheng 0004,Junkai Sun,Dekang Qi",https://doi.org/10.1109/TKDE.2019.2891537,TKDE,2020,"Correlation,Predictive models,Urban areas,Matrix converters,Sparse matrices,Deep learning","Predicting flows (e.g., the traffic of vehicles, crowds, and bikes), consisting of the in-out traffic at a node and transitions between different nodes, in a spatio-temporal network plays an important role in transportation systems. However, this is a very challenging problem, affected by multiple complex factors, such as the spatial correlation between different locations, temporal correlation among different time intervals, and external factors (like events and weather). In addition, the flow at a node (called node flow) and transitions between nodes (edge flow) mutually influence each other. To address these issues, we propose a multitask deep-learning framework that simultaneously predicts the node flow and edge flow throughout a spatio-temporal network. Based on fully convolutional networks, our approach designs two sophisticated models for predicting node flow and edge flow, respectively. These two models are connected by coupling their latent representations of middle layers, and trained together. The external factor is also integrated into the framework through a gating fusion mechanism. In the edge flow prediction model, we employ an embedding component to deal with the sparse transitions between nodes. We evaluate our method based on the taxicab data in Beijing and New York City. Experimental results show the advantages of our method beyond 11 baselines, such as ConvLSTM, CNN, and Markov Random Field."
849,,Efficient Learning with Exponentially-Many Conjunctive Precursors for Interpretable Spatial Event Forecasting.,"Liang Zhao 0002,Feng Chen 0001,Yanfang Ye",https://doi.org/10.1109/TKDE.2019.2912187,TKDE,2020,"Forecasting,Computational modeling,Predictive models,Numerical models,Kernel,Task analysis,Complexity theory","Forecasting spatial societal events in social media is significant and challenging. Most existing methods consider the frequencies of keywords or n-grams to be features, but have not explored the exponentially large space of the conjunctions of those features, such as keyword co-occurrence in messages, which can serve as crucial precursor rules. Due to the inherent exponential complexity of ensemble rule learning, existing work typically adopts greedy/heuristic strategies. This means that they cannot guarantee the solution's optimality, which would require a considerably more sophisticated model for spatial event forecasting, while still suffering from major challenges: 1) Exponentially-dimensional feature learning with distant supervision, 2) Numerical values of conjunctive features, and 3) Spatially heterogeneous conjunction patterns. To concurrently address all these challenges with a theoretical guarantee, we propose a novel spatial event forecasting model which learns numerical conjunctive features efficiently. Specifically, to consider their magnitude, traditional Boolean rules are innovatively generalized to deal with numerical conjunctive features with amenable computational properties. To handle the geographical similarity and heterogeneity in numerical conjunctive feature learning, we propose a new model that implements through a new bi-space hierarchical sparsity regularization for locations and features. Moreover, we propose a new algorithm to optimize the model parameters and prove that it enjoys theoretical guarantees for both the error bounds and time efficiency. Extensive experiments on multiple datasets demonstrate the effectiveness and efficiency of the proposed method."
850,,Discerning Influence Patterns with Beta-Poisson Factorization in Microblogging Environments.,"Wei Zhao 0019,Ziyu Guan,Yuhui Huang,Tingting Xi,Huan Sun,Zhiheng Wang 0001,Xiaofei He 0001",https://doi.org/10.1109/TKDE.2019.2897932,TKDE,2020,"Twitter,Predictive models,Bayes methods,Computer science,Probabilistic logic,Sun","Social influence analysis in microblogging services has attracted much attention in recent years. However, most previous studies were focused on measuring users' (topical) influence. Little effort has been made to discern and quantify how a user is influenced. Specifically, the fact that user i retweets a tweet from author j could be either because i is influenced by j (i.e., j is a topical authority), or simply because he is “influenced” by the content (interested in the content). To mine such influence patterns, we propose a novel Bayesian factorization model, dubbed Influence Beta-Poisson Factorization (IBPF). IBPF jointly factorizes the retweet data and tweet content to quantify latent topical factors of user preference, author influence and content influence. It generates every retweet record according to the sum of two causing terms: one representing author influence, and the other one derived from content influence. To control the impact of the two terms, for each user IBPF generates a probability for each latent topic by Beta distribution, indicating how strongly the user cares about the topical authority of the author. We develop an efficient variational inference algorithm for IBPF. We demonstrate the efficacy of IBPF on two public microblogging datasets."
851,,Voice of Charity - Prospecting the Donation Recurrence &amp; Donor Retention in Crowdfunding.,"Hongke Zhao,Binbin Jin,Qi Liu 0003,Yong Ge,Enhong Chen,Xi Zhang,Tong Xu 0001",https://doi.org/10.1109/TKDE.2019.2906199,TKDE,2020,"Predictive models,Task analysis,Deep learning,Optimization,Data models,Analytical models,Collaboration","Online donation-based crowdfunding has brought new life to charity by soliciting small monetary contributions from crowd donors to help others in trouble or with dreams. However, a crucial issue for crowdfunding platforms as well as traditional charities is the problem of high donor attrition, i.e., many donors donate only once or very few times within a rather short lifecycle and then leave. Thus, it is an urgent task to analyze the factors of and then further predict the donors behaviors. Especially, we focus on two types of behavioral events, e.g., donation recurrence (whether one donor will make donations at some time slices in the future) and donor retention (whether she will remain on the crowdfunding platform until a future time). However, this problem has not been well explored due to many domain and technical challenges, such as the heterogeneous influence, the relevance of the two types of events, and the censoring phenomenon of retention records. In this paper, we present a focused study on donation recurrence and donor retention with the help of large-scale behavioral data collected from crowdfunding. Specifically, we propose a Joint Deep Survival model, i.e., JDS, which can integrate heterogeneous features, e.g., donor motives, projects recently donated to, social contacts, to jointly model the donation recurrence and donor retention since these two types of behavioral events are highly relevant. In addition, we model the censoring phenomenon and dependence relations of different behaviors from the survival analysis view by designing multiple innovative constraints and incorporating them into the objective functions. Finally, we conduct extensive analysis and validation experiments with large-scale data collected from Kiva.org. The experimental results clearly demonstrate the effectiveness of our proposed models for analyzing and predicting the donation recurrence and donor retention in crowdfunding."
852,,Destination-Aware Task Assignment in Spatial Crowdsourcing - A Worker Decomposition Approach.,"Yan Zhao 0008,Kai Zheng 0001,Yang Li,Han Su,Jiajun Liu,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2019.2922604,TKDE,2020,"Wireless networks,Crowdsourcing,Optimization,Smart devices,Performance evaluation,Partitioning algorithms,Clustering algorithms","With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper, we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. In order to make our proposed framework applicable to more scenarios, we further optimize the original framework by proposing strategies to reduce the overall travel cost and allow each task to be assigned to multiple workers. Extensive empirical studies verify that the proposed technique and optimization strategies perform effectively and settle the problem nicely."
853,,Fast Discrete Collaborative Multi-Modal Hashing for Large-Scale Multimedia Retrieval.,"Chaoqun Zheng,Lei Zhu 0002,Xu Lu,Jingjing Li 0001,Zhiyong Cheng,Hanwang Zhang",https://doi.org/10.1109/TKDE.2019.2913388,TKDE,2020,"Semantics,Optimization,Collaboration,Correlation,Training,Binary codes,Complexity theory","Many achievements have been made on learning to hash for uni-modal and cross-modal retrieval. However, it is still an unsolved problem that how to directly and efficiently learn discriminative discrete hash codes for the multimedia retrieval, where both query and database samples are represented with heterogeneous multi-modal features. With this motivation, we propose a Fast Discrete Collaborative Multi-modal Hashing (FDCMH) method in this paper. We first propose an efficient collaborative multi-modal mapping that first transforms heterogeneous multi-modal features into the unified factors to exploit the complementarity of multi-modal features and preserve the semantic correlations in multiple modalities with linear computation and space complexity. Such shared factors also bridge the heterogeneous modality gap and remove the inter-modality redundancy. Further, we develop an asymmetric hashing learning module to simultaneously correlate the learned hash codes with low-level data distribution and high-level semantics. In particular, this design could avoid the challenging symmetric semantic matrix factorization and O(n2) memory cost (n is the number of training samples). It can support both computation and memory efficient discrete hash optimization. Experiments on several public multimedia retrieval datasets demonstrate the superiority of the proposed approach compared with state-of-the-art hashing techniques, in terms of both model learning efficiency and retrieval accuracy."
854,,Multi-Campaign Oriented Spatial Crowdsourcing.,"Libin Zheng,Lei Chen 0002",https://doi.org/10.1109/TKDE.2019.2893293,TKDE,2020,"Task analysis,Crowdsourcing,Throughput,Diversity reception,Linear programming,Companies,Advertising","Recently, spatial crowdsourcing has been drawing increasing attention with its great potential in collecting geographical knowledge. The system throughput (number of assigned tasks) and workers' travel distance are two of many important factors in spatial crowdsourcing, and the improvement to one of them usually means the sacrifice of the other. However, most existing works resolve the trade-off between these two factors by simply targeting tasks within a bounding circle of each worker. In this paper, we compromise between the throughput and the distance by formulating these two factors as score terms in the objective function. This flexible formulation has the advantages of abandoning distant tasks and minimizing workers' travel distance for reachable tasks. Aside from that, we study the multi-campaign scenario of spatial crowdsourcing, which is not uncommon in practical applications while not yet discussed in existing works. The worker diversity of the campaigns is considered to be another goal and formulated as another score term in the objective function. Subsequently, the problem of multi-campaign oriented spatial crowdsourcing is to maximize the objective function comprised by the aforementioned score terms. We prove that the problem is NP-hard, thus, we propose several approximation solutions. Extensive experiments have been conducted to confirm the effectiveness and the efficiency of the devised solutions."
855,,Answering Why-Not Group Spatial Keyword Queries.,"Bolong Zheng,Kai Zheng 0001,Christian S. Jensen,Nguyen Quoc Viet Hung,Han Su,Guohui Li 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2018.2879819,TKDE,2020,"Usability,Learning (artificial intelligence),Query processing","With the proliferation of geo-textual objects on the web, extensive efforts have been devoted to improving the efficiency of top-k spatial keyword queries in different settings. However, comparatively much less work has been reported on enhancing the quality and usability of such queries. In this context, we propose means of enhancing the usability of a top-k group spatial keyword query, where a group of users aim to find k objects that contain given query keywords and are nearest to the users. Specifically, when users receive the result of such a query, they may find that one or more objects that they expect to be in the result are in fact missing, and they may wonder why. To address this situation, we develop a so-called why-notquery that is able to minimally modifythe original query into a query that returns the expected, but missing, objects, in addition to other objects. Specifically, we formalize the why-not query in relation to the top-k group spatial keyword query, called the Why-not Group Spatial Keyword Query (WGSK) that is able to provide a group of users with a more satisfactory query result. We propose a three-phase framework for efficiently computing the WGSK. The first phase substantially reduces the search space for the subsequent phases by retrieving a set of objects that may affect the ranking of the user-expected objects. The second phase provides an incremental sampling algorithm that generates candidate weightings of more promising queries. The third phase determines the penalty of each refined query and returns the querywith minimal penalty, i.e., the minimally modified query. Extensive experiments with real and synthetic data offer evidence that the proposed solution excels over baselines with respect to both effectiveness and efficiency."
856,,Addressing the Item Cold-Start Problem by Attribute-Driven Active Learning.,"Yu Zhu,Jinghao Lin,Shibi He,Beidou Wang,Ziyu Guan,Haifeng Liu,Deng Cai 0001",https://doi.org/10.1109/TKDE.2019.2891530,TKDE,2020,"Collaboration,Recommender systems,Computer science,Task analysis,Learning systems,Uncertainty,Matrix decomposition","In recommender systems, cold-start issues are situations where no previous events, e.g., ratings, are known for certain users or items. In this paper, we focus on the item cold-start problem. Both content information (e.g., item attributes) and initial user ratings are valuable for seizing users' preferences on a new item. However, previous methods for the item cold-start problem either (1) incorporate content information into collaborative filtering to perform hybrid recommendation, or (2) actively select users to rate the new item without considering content information and then do collaborative filtering. In this paper, we propose a novel recommendation scheme for the item cold-start problem by leveraging both active learning and items' attribute information. Specifically, we design useful user selection criteria based on items' attributes and users' rating history, and combine the criteria in an optimization framework for selecting users. By exploiting the feedback ratings, users' previous ratings and items' attributes, we then generate accurate rating predictions for the other unselected users. Experimental results on two real-world datasets show the superiority of our proposed method over traditional methods."
857,,Top-k Dominating Queries on Skyline Groups.,"Haoyang Zhu,Xiaoyong Li 0002,Qiang Liu 0004,Zichen Xu",https://doi.org/10.1109/TKDE.2019.2904065,TKDE,2020,"Indexes,Aggregates,Decision making,Economics,Systematics,Query processing","The top-k dominating (TKD) query on skyline groups returns k skyline groups that dominate the maximum number of points in a given data set. The TKD query combines the advantages of skyline groups and top-k dominating queries, thus has been frequently used in decision making, recommendation systems, and quantitative economics. Traditional skylines are inadequate to answer queries from both individual and groups of points. The group size could be too large to be processed in a reasonable time as a single operator (i.e., the skyline group operator). In this paper, we address the performance problem of grouping for TKD queries in skyline database. We formulate the problem of grouping, define the group operator in skyline, and propose several efficient algorithms to find top-k skyline groups. Thus, we provide a systematic study of TKD queries on skyline groups and validate our algorithms with extensive empirical results on synthetic and realworld data."
858,,Answering Top-$k$ k Graph Similarity Queries in Graph Databases.,"Yuanyuan Zhu,Lu Qin,Jeffrey Xu Yu,Hong Cheng 0001",https://doi.org/10.1109/TKDE.2019.2906608,TKDE,2020,"Indexes,Upper bound,Drugs,Chemical compounds,Computational modeling,Chemicals","Searching similar graphs in graph databases for a query graph has attracted extensive attention recently. Existing works on graph similarity queries are threshold based approaches which return graphs with distances to the query smaller than a given threshold. However, in many applications the number of answer graphs for the same threshold can vary significantly for different queries. In this paper, we study the problem of finding top-k most similar graphs for a query under the distance measure based on maximum common subgraph (MCS). Since computing MCS is NP-hard, we devise a novel framework to prune unqualified graphs based on the lower bounds of graph distance, and accordingly derive four lower bounds with different tightness and computational cost for pruning. To further reduce the number of MCS computations, we also propose an improved framework based on both lower and upper bounds, and derive three new upper bounds. To support efficient pruning, we design three indexes with different tradeoffs between pruning power and construction cost. To accelerate the index construction, we explore bound relaxation techniques, based on which approximate indexes can be efficiently built. We conducted extensive performance studies on real-life graph datasets to validate the effectiveness and efficiency of our approaches."
42,,Analytical Models for the Scalability of Dynamic Group-key Agreement Protocols and Secure File Sharing Systems.,"Gokcan Cantali,Orhan Ermis,Mehmet Ufuk Çaglayan,Cem Ersoy",https://doi.org/10.1145/3342998,TOPS,2019,[],
43,,Skype &amp; Type - Keyboard Eavesdropping in Voice-over-IP.,"Stefano Cecconello,Alberto Compagno,Mauro Conti,Daniele Lain,Gene Tsudik",https://doi.org/10.1145/3365366,TOPS,2019,[],
44,,KIST - Kernel-Informed Socket Transport for Tor.,"Rob Jansen,Matthew Traudt,John Geddes,Chris Wacek,Micah Sherr,Paul Syverson",https://doi.org/10.1145/3278121,TOPS,2019,[],
45,,Resilient Privacy Protection for Location-Based Services through Decentralization.,"Hongyu Jin 0001,Panos Papadimitratos",https://doi.org/10.1145/3319401,TOPS,2019,[],
46,,ANCHOR - Logically Centralized Security for Software-Defined Networks.,"Diego Kreutz,Jiangshan Yu,Fernando M. V. Ramos,Paulo Jorge Esteves Veríssimo",https://doi.org/10.1145/3301305,TOPS,2019,[],
47,,Safe and Efficient Implementation of a Security System on ARM using Intra-level Privilege Separation.,"Donghyun Kwon,Hayoon Yi,Yeongpil Cho,Yunheung Paek",https://doi.org/10.1145/3309698,TOPS,2019,[],
48,,ISOTOP - Auditing Virtual Networks Isolation Across Cloud Layers in OpenStack.,"Taous Madi,Yosr Jarraya,Amir Alimohammadifar,Suryadipta Majumdar,Yushun Wang,Makan Pourzandi,Lingyu Wang 0001,Mourad Debbabi",https://doi.org/10.1145/3267339,TOPS,2019,[],
49,,Alpha-Beta Privacy.,"Sebastian Mödersheim,Luca Viganò 0001",https://doi.org/10.1145/3289255,TOPS,2019,[],
50,,Tractor Beam - Safe-hijacking of Consumer Drones with Adaptive GPS Spoofing.,"Juhwan Noh,Yujin Kwon,Yunmok Son,Hocheol Shin,Dohyun Kim,Jaeyeong Choi,Yongdae Kim",https://doi.org/10.1145/3309735,TOPS,2019,[],
51,,MaMaDroid - Detecting Android Malware by Building Markov Chains of Behavioral Models (Extended Version).,"Lucky Onwuzurike,Enrico Mariconti,Panagiotis Andriotis,Emiliano De Cristofaro,Gordon J. Ross,Gianluca Stringhini",https://doi.org/10.1145/3313391,TOPS,2019,[],
52,,GPLADD - Quantifying Trust in Government and Commercial Systems A Game-Theoretic Approach.,"Alexander V. Outkin,Brandon K. Eames,Meghan A. Galiardi,Sarah Walsh,Eric D. Vugrin,Byron Heersink,Jacob Hobbs,Gregory D. Wyss",https://doi.org/10.1145/3326283,TOPS,2019,[],
53,,Introducing the Temporal Dimension to Memory Forensics.,"Fabio Pagani,Oleksii Fedorov,Davide Balzarotti",https://doi.org/10.1145/3310355,TOPS,2019,[],
54,,Kernel Protection Against Just-In-Time Code Reuse.,"Marios Pomonis,Theofilos Petsios,Angelos D. Keromytis,Michalis Polychronakis,Vasileios P. Kemerlis",https://doi.org/10.1145/3277592,TOPS,2019,[],
55,,Malicious Overtones - Hunting Data Theft in the Frequency Domain with One-class Learning.,Brian A. Powell,https://doi.org/10.1145/3360469,TOPS,2019,[],
56,1,Hybrid Private Record Linkage - Separating Differentially Private Synopses from Matching Records.,"Fang-Yu Rao,Jianneng Cao,Elisa Bertino,Murat Kantarcioglu",https://doi.org/10.1145/3318462,TOPS,2019,[],
57,,Characterizing the Security of the SMS Ecosystem with Public Gateways.,"Bradley Reaves,Luis Vargas,Nolen Scaife,Dave Tian,Logan Blue,Patrick Traynor,Kevin R. B. Butler",https://doi.org/10.1145/3268932,TOPS,2019,[],
58,,A Usability Study of Four Secure Email Tools Using Paired Participants.,"Scott Ruoti,Jeff Andersen,Luke Dickinson,Scott Heidbrink,Tyler Monson,Mark O&apos;Neill,Ken Reese,Brad Spendlove,Elham Vaziripour,Justin Wu,Daniel Zappala,Kent E. Seamons",https://doi.org/10.1145/3313761,TOPS,2019,[],
59,,Will They Use It or Not? Investigating Software Developers&apos; Intention to Follow Privacy Engineering Methodologies.,"Awanthika Senarath,Marthie Grobler,Nalin Asanka Gamagedara Arachchilage",https://doi.org/10.1145/3364224,TOPS,2019,[],
60,,A General Framework for Adversarial Examples with Objectives.,"Mahmood Sharif,Sruti Bhagavatula,Lujo Bauer,Michael K. Reiter",https://doi.org/10.1145/3317611,TOPS,2019,[],
61,,Analysis of Reflexive Eye Movements for Fast Replay-Resistant Biometric Authentication.,"Ivo Sluganovic,Marc Roeschlin,Kasper Bonne Rasmussen,Ivan Martinovic",https://doi.org/10.1145/3281745,TOPS,2019,[],
62,,A Close Look at a Daily Dataset of Malware Samples.,"Xabier Ugarte-Pedrero,Mariano Graziano,Davide Balzarotti",https://doi.org/10.1145/3291061,TOPS,2019,[],
63,,DADS - Decentralized Attestation for Device Swarms.,"Samuel Wedaj,Kolin Paul,Vinay J. Ribeiro",https://doi.org/10.1145/3325822,TOPS,2019,[],
64,,Using Episodic Memory for User Authentication.,"Simon S. Woo,Ron Artstein,Elsi Kaiser,Xiao Le,Jelena Mirkovic",https://doi.org/10.1145/3308992,TOPS,2019,[],
65,,Database Audit Workload Prioritization via Game Theory.,"Chao Yan 0004,Bo Li 0026,Yevgeniy Vorobeychik,Aron Laszka,Daniel Fabbri,Bradley A. Malin",https://doi.org/10.1145/3323924,TOPS,2019,[],
859,,A Survey on Spatial Prediction Methods.,Zhe Jiang 0001,https://doi.org/10.1109/TKDE.2018.2866809,TKDE,2019,"Spatial databases,Predictive models,Media,Earth,Public healthcare,Data mining,Diseases","With the advancement of GPS and remote sensing technologies, large amounts of geospatial data are being collected from various domains, driving the need for effective and efficient prediction methods. Given spatial data samples with explanatory features and targeted responses (categorical or continuous) at a set of locations, the spatial prediction problem aims to learn a model that can predict the response variable based on explanatory features. The problem is important with broad applications in earth science, urban informatics, geosocial media analytics, and public health, but is challenging due to the unique characteristics of spatial data, including spatial autocorrelation, heterogeneity, limited ground truth, and multiple scales and resolutions. This paper provides a systematic review on principles and methods in spatial prediction. We provide a taxonomy of methods categorized by the key challenge they address. For each method, we introduce its underlying assumption, theoretical foundation, and discuss its advantages and disadvantages. We also discuss spatiotemporal extensions of methods. Our goal is to help interdisciplinary domain scientists choose techniques to solve their problems, and more importantly, to help data mining researchers to understand the main principles and methods in spatial prediction and identify future research opportunities."
860,,Utilization-Aware Trip Advisor in Bike-Sharing Systems Based on User Behavior Analysis.,"Peng Cheng 0001,Ji Hu,Zidong Yang,Yuanchao Shu,Jiming Chen 0001",https://doi.org/10.1109/TKDE.2018.2867197,TKDE,2019,"Bicycles,Public transportation,Predictive models,Behavorial sciences,Recommender systems","The rapid development of bike-sharing systems has brought people enormous convenience during the past decade. On the other hand, high transport flexibility gives rise to problems for both users and operators. For users, dynamic distribution of shared bikes caused by uneven user demand often leads to the check in or check out service unavailable at some stations. For operators, unbalanced bike usage comes with more bike broken and growing maintenance cost. In this paper, we consider enhancing user experiences and rebalance bicycle utilization by directing users to different stations with a higher success rate of rental and return. For the first time, we devise a trip advisor that recommends bike check-in and check-out stations with joint consideration of service quality and bicycle utilization. To ensure service quality, we firstly predict the user demand of each station to obtain the success rate of rental and return in the future. Experiments indicate that the precision of our method is as much as 0.826, which has raised by 25.9 percent as compared with that of the historical average method. To rebalance bike usage, from historical data, we identify that biased bike usage is rooted from circumscribed bicycle circulation among few active stations. Therefore, with defined station activeness, we optimize the bike circulation by leading users to shift bikes between highly active stations and inactive ones. We extensively evaluate the performance of our design through real-world datasets. Evaluation results show that the percentage of frequently used bikes decreases by 33.6 percent on usage number and 28.6 percent on usage time."
861,,Fog-enabled Event Processing Based on IoT Resource Models.,"Yang Zhang 0015,Victor S. Sheng",https://doi.org/10.1109/TKDE.2018.2867504,TKDE,2019,"Semantics,Data centers,Internet of Things,Data models,Event detection,Sensors,Error analysis","Complex Event Processing (CEP) systems extract interest situations from event flows based on event detection patterns. However, local event processing for distributed Internet of Things (IoT) has not been discussed yet. Besides, it is complex or impossible to discover such patterns in some applications of IoT. In this article, we design a complex event service to process event flows based on IoT resource models, which does not depend on existing patterns, and deals with both discrete events and continuous variables. To improve the CEP performance, local IoT resources are used for local event processing, and a lazy exchange method is designed to realize the collaborated event processing between network edges and a data center. Our evaluation shows that our solution is feasible and effective."
862,1,Differentially Private Mixture of Generative Neural Networks.,"Gergely Ács,Luca Melis,Claude Castelluccia,Emiliano De Cristofaro",https://doi.org/10.1109/TKDE.2018.2855136,TKDE,2019,"Data models,Privacy,Data privacy,Training,Neural networks,Kernel,Machine learning","Generative models are used in a wide range of applications building on large amounts of contextually rich information. Due to possible privacy violations of the individuals whose data is used to train these models, however, publishing or sharing generative models is not always viable. In this paper, we present a novel technique for privately releasing generative models and entire high-dimensional datasets produced by these models. We model the generator distribution of the training data with a mixture of k generative neural networks. These are trained together and collectively learn the generator distribution of a dataset. Data is divided into k clusters, using a novel differentially private kernel k-means, then each cluster is given to separate generative neural networks, such as Restricted Boltzmann Machines or Variational Autoencoders, which are trained only on their own cluster using differentially private gradient descent. We evaluate our approach using the MNIST dataset, as well as call detail records and transit datasets, showing that it produces realistic synthetic samples, which can also be used to accurately compute arbitrary number of counting queries."
863,,An Efficient Semi-Supervised Multi-label Classifier Capable of Handling Missing Labels.,"Amirhossein Akbarnejad,Mahdieh Soleymani Baghshah",https://doi.org/10.1109/TKDE.2018.2833850,TKDE,2019,"Training,Gaussian processes,Correlation,Probabilistic logic,Transforms,Dimensionality reduction","Multi-label classification has received considerable interest in recent years. Multi-label classifiers usually need to address many issues including: handling large-scale datasets with many instances and a large set of labels, compensating missing label assignments in the training set, considering correlations between labels, as well as exploiting unlabeled data to improve prediction performance. To tackle datasets with a large set of labels, embedding-based methods represent the label assignments in a low-dimensional space. Many state-of-the-art embedding-based methods use a linear dimensionality reduction to map the label assignments to a low-dimensional space. However, by doing so, these methods actually neglect the tail labels - labels that are infrequently assigned to instances. In this paper, we propose an embedding-based method that non-linearly embeds the label vectors using a stochastic approach, thereby predicting the tail labels more accurately. Moreover, the proposed method has excellent mechanisms for handling missing labels, dealing with large-scale datasets, as well as exploiting unlabeled data. Experiments on real-world datasets show that our method outperforms state-of-the-art multi-label classifiers by a large margin, in terms of prediction performance, as well as training time. Our implementation of the proposed method is available online at:https://github.com/Akbarnejad/ESMC_ Implementation."
864,,Real-Time Change Point Detection with Application to Smart Home Time Series Data.,"Samaneh Aminikhanghahi,Tinghui Wang,Diane J. Cook",https://doi.org/10.1109/TKDE.2018.2850347,TKDE,2019,"Time series analysis,Detection algorithms,Real-time systems,Smart homes,Prediction algorithms,Change detection algorithms,Measurement","Change Point Detection (CPD) is the problem of discovering time points at which the behavior of a time series changes abruptly. In this paper, we present a novel real-time nonparametric change point detection algorithm called SEP, which uses Separation distance as a divergence measure to detect change points in high-dimensional time series. Through experiments on artificial and real-world datasets, we demonstrate the usefulness of the proposed method in comparison with existing methods."
865,,Automated Discovery of Process Models from Event Logs - Review and Benchmark.,"Adriano Augusto,Raffaele Conforti,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Andrea Marrella,Massimo Mecella,Allar Soo",https://doi.org/10.1109/TKDE.2018.2841877,TKDE,2019,"Benchmark testing,Data mining,Process control,Systematics,Data models,Task analysis","Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy, and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures, and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use of closed datasets. This article provides a systematic review and comparative evaluation of automated process discovery methods, using an open-source benchmark and covering 12 publicly-available real-life event logs, 12 proprietary real-life event logs, and nine quality metrics. The results highlight gaps and unexplored tradeoffs in the field, including the lack of scalability of some methods and a strong divergence in their performance with respect to the different quality metrics used."
866,,HiTR - Hierarchical Topic Model Re-Estimation for Measuring Topical Diversity of Documents.,"Hosein Azarbonyad,Mostafa Dehghani 0001,Tom Kenter,Maarten Marx,Jaap Kamps,Maarten de Rijke",https://doi.org/10.1109/TKDE.2018.2874246,TKDE,2019,"Impurities,Task analysis,Sociology,Statistics,Mathematical model,Feature extraction,Data models","A high degree of topical diversity is often considered to be an important characteristic of interesting text documents. A recent proposal for measuring topical diversity identifies three distributions for assessing the diversity of documents: distributions of words within documents, words within topics, and topics within documents. Topic models play a central role in this approach and, hence, their quality is crucial to the efficacy of measuring topical diversity. The quality of topic models is affected by two causes: generality and impurity of topics. General topics only include common information of a background corpus and are assigned to most of the documents. Impure topics contain words that are not related to the topic. Impurity lowers the interpretability of topic models. Impure topics are likely to get assigned to documents erroneously. We propose a hierarchical re-estimation process aimed at removing generality and impurity. Our approach has three re-estimation components: (1) document re-estimation, which removes general words from the documents; (2) topic re-estimation, which re-estimates the distribution over words of each topic; and (3) topic assignment re-estimation, which re-estimates for each document its distributions over topics. For measuring topical diversity of text documents, our HiTR approach improves over the state-of-the-art measured on PubMed dataset."
867,,An Information-Theoretical Framework for Cluster Ensemble.,"Liang Bai,Jiye Liang,Hangyuan Du,Yike Guo",https://doi.org/10.1109/TKDE.2018.2865954,TKDE,2019,"Clustering algorithms,Information entropy,Robustness,Uncertainty,Task analysis,Aggregates","Cluster ensemble is a very important tool that aggregates several base clusterings to generate a single output clustering with improved robustness and stability. However, the quality of the final clustering is often affected by uncertainties on the generation and integration of base clusterings. In this paper, we develop an information-theoretical framework which makes an effort to obtain a final clustering with high consensus on both the original data set and the base clustering set by minimizing the two uncertainties of cluster ensemble. In this framework, we provide a weighted consensus measure based on information entropy to evaluate the quality of a clustering, the similarity between clusters and the similarity between objects. Based on the measure, we propose three weighted cluster ensemble algorithms with different ensemble strategies in the framework, including the weighted feature consensus algorithm, the weighted relabeling consensus algorithm and the weighted pairwise-similarity consensus algorithm. In the experimental analysis, we compare the proposed algorithms with other existing clustering ensemble algorithms on several data sets. The comparison results illustrate the proposed algorithms are very effective and robust."
868,,Correction to &quot;Characterizing and Predicting Early Reviewers for Effective Product Marketing on E-Commerce Websites&quot;.,"Ting Bai,Wayne Xin Zhao,Yulan He,Jian-Yun Nie,Ji-Rong Wen",https://doi.org/10.1109/TKDE.2019.2894055,TKDE,2019,,"Presents corrections to author information from the paper, “Characterizing and predicting early reviewers for effective product marketing on e-commerce websites,” (Bai, T., et al), IEEE Trans. Knowl. Data Eng., vol. 30, no. 12, pp. 2271–2284, Dec. 2018. "
869,,A Cost Model for SPARK SQL.,"Lorenzo Baldacci,Matteo Golfarelli",https://doi.org/10.1109/TKDE.2018.2850339,TKDE,2019,"Sparks,Computational modeling,Task analysis,Engines,Data models,Analytical models,Delays","In this paper, we propose a novel cost model for Spark SQL. The cost model covers the class of Generalized Projection, Selection, Join (GPSJ) queries. The cost model keeps into account the network and IO costs as well as the most relevant CPU costs. The execution cost is computed starting from a physical plan produced by Spark. The set of operations adopted by Spark when executing a GPSJ query are analytically modeled based on the cluster and application parameters, together with a set of database statistics. Experimental results carried out on three benchmarks and on two clusters of different sizes and with different computation features show that our model can estimate the actual execution time with about the 20 percent of errors on the average. Such an accuracy is good enough to let the system choose the most effective plan even when the execution time differences are limited. The error can be reduced to 14 percent, if the analytic model is coupled with our straggler handling strategy."
870,,Co-Clustering via Information-Theoretic Markov Aggregation.,"Clemens Blöchl,Rana Ali Amjad,Bernhard C. Geiger",https://doi.org/10.1109/TKDE.2018.2846252,TKDE,2019,"Cost function,Markov processes,Mutual information,Bipartite graph,Task analysis,Noise measurement,Probability distribution","We present an information-theoretic cost function for co-clustering, i.e., for simultaneous clustering of two sets based on similarities between their elements. By constructing a simple random walk on the corresponding bipartite graph, our cost function is derived from a recently proposed generalized framework for information-theoretic Markov chain aggregation. The goal of our cost function is to minimize relevant information loss, hence it connects to the information bottleneck formalism. Moreover, via the connection to Markov aggregation, our cost function is not ad hoc, but inherits its justification from the operational qualities associated with the corresponding Markov aggregation problem. We furthermore show that, for appropriate parameter settings, our cost function is identical to well-known approaches from the literature, such as “Information-Theoretic Co-Clustering” by Dhillon et al. Hence, understanding the influence of this parameter admits a deeper understanding of the relationship between previously proposed information-theoretic cost functions. We highlight some strengths and weaknesses of the cost function for different parameters. We also illustrate the performance of our cost function, optimized with a simple sequential heuristic, on several synthetic and real-world data sets, including the Newsgroup20 and the MovieLens100k data sets."
871,,Efficient Evaluation of Multi-Column Selection Predicates in Main-Memory.,"David Broneske,Veit Köppen,Gunter Saake,Martin Schäler",https://doi.org/10.1109/TKDE.2018.2825349,TKDE,2019,"Indexes,Layout,Redundancy,Acceleration,Windows,Benchmark testing","Efficient evaluation of selection predicates is a performance-critical task, for instance to reduce intermediate result sizes being the input for further operations. With analytical queries getting more and more complex, the number of evaluated selection predicates per query and table rises, too. This leads to numerous multi-column selection predicates. Recent approaches to increase the performance of main-memory databases for selection-predicate evaluation aim at optimally exploiting the speed of the CPU by using accelerated scans. However, scanning each column one by one leaves tuning opportunities open that arise if all predicates are considered together. To this end, we introduce Elf, an index structure that is able to exploit the relation between several selection predicates. Elf features cache sensitivity, an optimized storage layout, fixed search paths, and slight data compression. In a large-scale evaluation, we compare its query performance to state-of-the-art approaches and a sequential scan using SIMD capabilities. Our results indicate a clear superiority of our approach for queries returning less than 10 percent of all tuples - a selectivity almost one order of magnitude larger than observed for related indexing approaches. For TPC-H queries with multi-column selection predicates, we achieve a speedup between factor five and two orders of magnitude, mainly depending on the selectivity of the predicates. Further scaling experiments reveal that for large data sets, these speedup factors are expected to increase, due to more densely populated data spaces. Finally, our results indicate that using a delta-store like concept to support periodic insertions results in virtually no performance penalty for reasonable sizes of a write-optimized Elf as delta store."
872,1,Nonintrusive Smartphone User Verification Using Anonymized Multimodal Data.,"Hong Cao,Kevin Chen-Chuan Chang",https://doi.org/10.1109/TKDE.2018.2828309,TKDE,2019,"Data models,Hidden Markov models,Authentication,Computational modeling,Usability,Training","Smartphone user verification is important as personal daily activities are increasingly conducted on the phone and sensitive information is constantly logged. The commonly adopted user verification methods are typically active, i.e., they require a user's cooperative input of a security token to gain access permission. Though popular, these methods impose heavy burden to smartphone users to memorize, maintain, and input the token at a high frequency. To alleviate this imposition onto the users and to provide additional security, we propose a new nonintrusive and continuous mobile user verification framework that can reduce the frequency required for a user to input his/her security token. Using tailored Hidden Markov Models and sequential likelihood ratio test, our verification is built on low-cost, readily available, anonymized, and multimodal smartphone data without additional effort of data collection and risk of privacy leakage. With extensive evaluation, we achieve a high rate of about 94 percent for detecting illegitimate smartphone uses and a rate of 74 percent for confirming legitimate uses. In a practical setting, this can translate into 74 percent of frequency reduction of inputting a security token using an active authentication method with only about 6 percent risk of miss detection of a random intruder, which is highly desirable."
873,1,Quantifying Differential Privacy in Continuous Data Release Under Temporal Correlations.,"Yang Cao 0011,Masatoshi Yoshikawa,Yonghui Xiao,Li Xiong 0001",https://doi.org/10.1109/TKDE.2018.2824328,TKDE,2019,"Correlation,Privacy,Data privacy,Databases,Markov processes,Probabilistic logic,Sensitivity","Differential Privacy (DP) has received increasing attention as a rigorous privacy framework. Many existing studies employ traditional DP mechanisms (e.g., the Laplace mechanism) as primitives to continuously release private data for protecting privacy at each time point (i.e., event-level privacy), which assume that the data at different time points are independent, or that adversaries do not have knowledge of correlation between data. However, continuously generated data tend to be temporally correlated, and such correlations can be acquired by adversaries. In this paper, we investigate the potential privacy loss of a traditional DP mechanism under temporal correlations. First, we analyze the privacy leakage of a DP mechanism under temporal correlation that can be modeled using Markov Chain. Our analysis reveals that, the event-level privacy loss of a DP mechanism may increase over time. We call the unexpected privacy loss temporal privacy leakage (TPL). Although TPL may increase over time, we find that its supremum may exist in some cases. Second, we design efficient algorithms for calculating TPL. Third, we propose data releasing mechanisms that convert any existing DP mechanism into one against TPL. Experiments confirm that our approach is efficient and effective."
874,1,Errata on &quot;Quantifying Differential Privacy in Continuous Data Release under Temporal Correlations&quot;.,"Yang Cao 0011,Masatoshi Yoshikawa,Yonghui Xiao,Li Xiong 0001",https://doi.org/10.1109/TKDE.2019.2937245,TKDE,2019,"Privacy,Correlation,Differential privacy,Real-time systems,Computational complexity,Task analysis",Presents revisions to the above named paper.
875,,Introducing Cuts Into a Top-Down Process for Checking Tree Inclusion.,"Yangjun Chen,Yibin Chen",https://doi.org/10.1109/TKDE.2018.2854797,TKDE,2019,"Vegetation,Time complexity,Data mining,XML,RNA,Forestry","By the ordered tree inclusion we will check whether a pattern tree P can be included in a target tree T, where the order of siblings in both Pand T matters. This problem has many applications in practice, such as retrieval of documents, data mining, and RNA structure matching. In this paper, we propose an efficient algorithm for this problem. Its time complexity is bounded by O(|T| · min{h
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">P</sub>
; |leaves(P)|}), with O(|T| + |P|) space being used, where h
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">P</sub>
 (h
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">T</sub>
) represents the height of P (resp., T) and leaves (P) stands for the set of the leaves of P. Up to now the best algorithm for this problem needs Θ(|T| · |leaves(P)|) time and O(|P| + |T|) space. Extensive experiments have been done, which show that the new algorithm can perform much better than the existing ones in practice."
876,,Exploring Communities in Large Profiled Graphs.,"Yankai Chen,Yixiang Fang,Reynold Cheng,Yun Li,Xiaojun Chen 0006,Jie Zhang 0002",https://doi.org/10.1109/TKDE.2018.2882837,TKDE,2019,"Hardware,Information systems,Machine learning,Computer science,Collaboration,Software","Given a graph G and a vertex q∈G, the community search (CS) problem aims to efficiently find a subgraph of G whose vertices are closely related to q. Communities are prevalent in social and biological networks, and can be used in product advertisement and social event recommendation. In this paper, we study profiled community search (PCS), where CS is performed on a profiled graph. This is a graph in which each vertex has labels arranged in a hierarchical manner. Extensive experiments show that PCS can identify communities with themes that are common to their vertices, and is more effective than existing CS approaches. As a naive solution for PCS is highly expensive, we have also developed a tree index, which facilitates efficient and online solutions for PCS."
877,,Pricing Average Price Advertising Options When Underlying Spot Market Prices Are Discontinuous.,"Bowei Chen,Mohan S. Kankanhalli",https://doi.org/10.1109/TKDE.2018.2867027,TKDE,2019,"Advertising,Pricing,Stochastic processes,Contracts,Real-time systems,Biological system modeling,Monte Carlo methods","Advertising options have been recently studied as a special type of guaranteed contracts in online advertising, which are an alternative sales mechanism to real-time auctions. An advertising option is a contract which gives its buyer a right but not obligation to enter into transactions to purchase page views or link clicks at one or multiple pre-specified prices in a specific future period. Different from typical guaranteed contracts, the option buyer pays a lower upfront fee but can have greater flexibility and more control of advertising. Many studies on advertising options so far have been restricted to the situations where the option payoff is determined by the underlying spot market price at a specific time point and the price evolution over time is assumed to be continuous. The former leads to a biased calculation of option payoff and the latter is invalid empirically for many online advertising slots. This paper addresses these two limitations by proposing a new advertising option pricing framework. First, the option payoff is calculated based on an average price over a specific future period. Therefore, the option becomes path-dependent. The average price is measured by the power mean, which contains several existing option payoff functions as its special cases. Second, jump-diffusion stochastic models are used to describe the movement of the underlying spot market price, which incorporate several important statistical properties including jumps and spikes, non-normality, and absence of autocorrelations. A general option pricing algorithm is obtained based on Monte Carlo simulation. In addition, an explicit pricing formula is derived for the case when the option payoff is based on the geometric mean. This pricing formula is also a generalized version of several other option pricing models discussed in related studies [1], [2], [3], [4], [5], [6]."
878,,On Efficient Processing of Group and Subsequent Queries for Social Activity Planning.,"Yi-Ling Chen 0002,De-Nian Yang,Chih-Ya Shen,Wang-Chien Lee,Ming-Syan Chen",https://doi.org/10.1109/TKDE.2018.2875911,TKDE,2019,"Social groups,Query processing,Search problems,Social networking (online),Computational complexity","Three essential criteria are important for social activity planning: (1) finding attendees familiar with the initiator, (2) ensuring most attendees have tight social relations with each other, and (3) selecting an activity period available to all. In this paper, we propose the Social-Temporal Group Query (STGQ) to find suitable time and attendees with minimum total social distance. We first prove that the problem is NP-hard and inapproximable within any ratio. Next, we design two algorithms, SGSelect and STGSelect, which include effective pruning techniques to substantially reduce running time. Moreover, as users may iteratively adjust query parameters to fine tune the results, we study the problem of Subsequent Social Group Query (SSGQ). We propose the Accumulative Search Tree and Social Boundary, to cache and index intermediate results of previous queries in order to accelerate subsequent query processing. Experimental results indicate that SGSelect and STGSelect are significantly more efficient than baseline approaches. With the caching mechanisms, processing time of subsequent queries can be further reduced by 50-75 percent. We conduct a user study to compare the proposed approach with manual activity coordination. The results show that our approach obtains higher quality solutions with lower coordination effort, thereby increasing the users' willingness to organize activities."
879,,Efficient Mining of Frequent Patterns on Uncertain Graphs.,"Yifan Chen,Xiang Zhao 0002,Xuemin Lin 0001,Yang Wang 0023,Deke Guo",https://doi.org/10.1109/TKDE.2018.2830336,TKDE,2019,"Approximation methods,Probability,Uncertainty,Data mining,Computational complexity,Graph theory","Uncertainty is intrinsic to a wide spectrum of real-life applications, which inevitably applies to graph data. Representative uncertain graphs are seen in bio-informatics, social networks, etc. This paper motivates the problem of frequent subgraph mining on single uncertain graphs, and investigates two different - probabilistic and expected - semantics in terms of support definitions. First, we present an enumeration-evaluation algorithm to solve the problem under probabilistic semantics. By showing the support computation under probabilistic semantics is #P-complete, we develop an approximation algorithm with accuracy guarantee for efficient problem-solving. To enhance the solution, we devise computation sharing techniques to achieve better mining performance. Afterwards, the algorithm is extended in a similar flavor to handle the problem under expected semantics, where checkpoint-based pruning and validation techniques are integrated. Experiment results on real-life datasets confirm the practical usability of the mining algorithms."
880,,FROG - A Fast and Reliable Crowdsourcing Framework.,"Peng Cheng 0003,Xiang Lian,Xun Jian,Lei Chen 0002",https://doi.org/10.1109/TKDE.2018.2849394,TKDE,2019,"Greedy algorithms,Crowdsourcing,Scheduling algorithms,Expectation-maximization algorithms","For decades, the crowdsourcing has gained much attention from both academia and industry, which outsources a number of tasks to human workers. Typically, existing crowdsourcing platforms include CrowdFlower, Amazon Mechanical Turk (AMT), and so on, in which workers can autonomously select tasks to do. However, due to the unreliability of workers or the difficulties of tasks, workers may sometimes finish doing tasks either with incorrect/incomplete answers or with significant time delays. Existing studies considered improving the task accuracy through voting or learning methods, they usually did not fully take into account reducing the latency of the task completion. This is especially critical, when a task requester posts a group of tasks (e.g., sentiment analysis), and one can only obtain answers of all tasks after the last task is accomplished. As a consequence, the time delay of even one task in this group could delay the next step of the task requester's work from minutes to days, which is quite undesirable for the task requester. Inspired by the importance of the task accuracy and latency, in this paper, we will propose a novel crowdsourcing framework, namely Fast and Reliable crOwdsourcinG framework (FROG), which intelligently assigns tasks to workers, such that the latencies of tasks are reduced and the expected accuracies of tasks are met. Specifically, our FROG framework consists of two important components, task scheduler and notification modules. For the task scheduler module, we formalize a FROG task scheduling (FROG-TS) problem, in which the server actively assigns workers to tasks to achieve high task reliability and low task latency. We prove that the FROG-TS problem is NP-hard. Thus, we design two heuristic approaches, request-based and batch-based scheduling. For the notification module, we define an efficient worker notifying (EWN) problem, which only sends task invitations to those workers with high probabilities of accepting the tasks. To tackle the EWN problem, we propose a smooth kernel density estimation approach to estimate the probability that a worker accepts the task invitation. Through extensive experiments, we demonstrate the effectiveness and efficiency of our proposed FROG platform on both real and synthetic data sets."
881,,Adversarial Deep Learning Models with Multiple Adversaries.,"Aneesh Sreevallabh Chivukula,Wei Liu 0007",https://doi.org/10.1109/TKDE.2018.2851247,TKDE,2019,"Games,Training,Machine learning,Stochastic processes,Testing,Machine learning algorithms,Game theory","We develop an adversarial learning algorithm for supervised classification in general and Convolutional Neural Networks (CNN) in particular. The algorithm's objective is to produce small changes to the data distribution defined over positive and negative class labels so that the resulting data distribution is misclassified by the CNN. The theoretical goal is to determine a manipulating change on the input data that finds learner decision boundaries where many positive labels become negative labels. Then we propose a CNN which is secure against such unforeseen changes in data. The algorithm generates adversarial manipulations by formulating a multiplayer stochastic game targeting the classification performance of the CNN. The multiplayer stochastic game is expressed in terms of multiple two-player sequential games. Each game consists of interactions between two players-an intelligent adversary and the learner CNN-such that a player's payoff function increases with interactions. Following the convergence of a sequential noncooperative Stackelberg game, each two-player game is solved for the Nash equilibrium. The Nash equilibrium finds a pair of strategies (learner weights and evolutionary operations) from which there is no incentive for either learner or adversary to deviate. We then retrain the learner over all the adversarial manipulations generated by multiple players to propose a secure CNN which is robust to subsequent adversarial data manipulations. The adversarial data and corresponding CNN performance is evaluated on MNIST handwritten digits data. The results suggest that game theory and evolutionary algorithms are very effective in securing deep learning models against performance vulnerabilities simulated as attack scenarios from multiple adversaries."
882,,Precise and Fast Cryptanalysis for Bloom Filter Based Privacy-Preserving Record Linkage.,"Peter Christen,Thilina Ranbaduge,Dinusha Vatsalan,Rainer Schnell",https://doi.org/10.1109/TKDE.2018.2874004,TKDE,2019,"Database systems,Cryptography,Data privacy,Data analysis,Data structures","Being able to identify records that correspond to the same entity across diverse databases is an increasingly important step in many data analytics projects. Research into privacy-preserving record linkage (PPRL) aims to develop techniques that can link records across databases such that besides the record pairs classified as matches no sensitive information about the entities in these databases is revealed. A popular technique used in PPRL is to encode sensitive values into Bloom filters (bit vectors), which has the advantage of allowing approximate matching using character q-grams. PPRL based on Bloom filter encoding has been shown to be accurate and scalable to large databases, and is thus now being used in real-world PPRL systems in Australia, Canada, and the UK. However, recent studies have shown that Bloom filters used for PPRL are vulnerable to cryptanalysis attacks that can re-identify some of the sensitive values encoded in these Bloom filters. While previous such attack methods were slow and required knowledge of various encoding parameters, we present a novel efficient attack which exploits how attribute values are encoded into Bloom filters. Our attack method does not require knowledge of the encoding function or its parameter settings used. It is able to correctly re-identify with high precision q-grams that could not have been hashed to certain Bloom filter bit positions, and using these re-identified q-grams it can then re-identify attribute values with high precision. Our method is significantly faster than earlier PPRL cryptanalysis attacks, and in our experimental evaluation, it is able to successfully re-identify attribute values from large real-world databases in a few minutes."
883,,A Survey on Network Embedding.,"Peng Cui 0001,Xiao Wang 0017,Jian Pei,Wenwu Zhu 0001",https://doi.org/10.1109/TKDE.2018.2849727,TKDE,2019,"Task analysis,Machine learning,Network topology,Computational complexity,Image reconstruction,Social network services,Distributed databases","Network embedding assigns nodes in a network to low-dimensional representations and effectively preserves the network structure. Recently, a significant amount of progresses have been made toward this emerging network analysis paradigm. In this survey, we focus on categorizing and then reviewing the current development on network embedding methods, and point out its future research directions. We first summarize the motivation of network embedding. We discuss the classical graph embedding algorithms and their relationship with network embedding. Afterwards and primarily, we provide a comprehensive overview of a large number of network embedding methods in a systematic manner, covering the structure- and property-preserving network embedding methods, the network embedding methods with side information, and the advanced information preserving network embedding methods. Moreover, several evaluation approaches for network embedding and some useful online resources, including the network data sets and softwares, are reviewed, too. Finally, we discuss the framework of exploiting these network embedding methods to build an effective system and point out some potential future directions."
884,,On Efficient External-Memory Triangle Listing.,"Yi Cui,Di Xiao,Dmitri Loguinov",https://doi.org/10.1109/TKDE.2018.2858820,TKDE,2019,"Complexity theory,Random access memory,Nickel,Computational modeling,Computer science,Runtime,Redundancy","Discovering triangles in large graphs is a well-studied area; however, both external-memory performance of existing methods and our understanding of the complexity involved leave much room for improvement. To shed light on this problem, we first generalize the existing in-memory algorithms into a single framework of 18 triangle-search techniques. We then develop a novel external-memory approach, which we call Pruned Companion Files (PCF), that supports operation of all 18 algorithms, while significantly reducing I/O compared to the common methods in this area. After finding the best node-traversal order, we build an implementation around it using SIMD instructions for list intersection and PCF for I/O. This method runs 5-10 times faster than the available implementations and exhibits orders of magnitude less I/O. In one of our graphs, the program finds 1 trillion triangles in 237 seconds using a desktop CPU."
885,,User Preference Analysis for Most Frequent Peer/Dominator.,"Ke Deng,Yanhua Li,Jia Zeng,Mingxuan Yuan,Jun Luo 0007,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2018.2857484,TKDE,2019,"Approximation algorithms,Search problems,Information systems,Object recognition,Databases,Task analysis,Robustness","Given a set of objects O (such as hotels), each can be represented as a point in a multi-dimensional feature space where each dimension corresponds to one attribute of the objects (such as price). Given the preference of a customer, the objects in O not dominated by any other object (i.e., beat in all dimensions) are those worthy to be further considered. Such objects are known as skyline objects in database community. Suppose we have an object o ∈ O. If o is a skyline point, other skyline objects are called peers of o. If o is not a skyline object, it must be dominated by some skyline objects which are called dominators of o. Given a large number of user preferences, an interesting problem is to identify the most frequent peer/dominator (MFP/MFD) of o. The MFP/MFD search has unique values in competitor analysis of various information systems. However, it is a challenging task because of the complexity to process a large number of user preferences. In this work, we provide robust solutions including exact and approximate methods. While the exact solutions explore the dominance relationship in the feature space, the approximate solutions are based on sampling techniques with theoretical bounds. We did extensive tests on large data sets which are up to 100 million user preferences generated from commercial surveys. The test resutls demonstrate the exact algorithms outperform various baseline algorithms significantly, and the approximate algorithms make further improvement by one order of magnitude with 90-98 percent accuracy."
886,,Normalization of Duplicate Records from Multiple Sources.,"Yongquan Dong,Eduard C. Dragut,Weiyi Meng",https://doi.org/10.1109/TKDE.2018.2844176,TKDE,2019,"Data integration,Standards,Task analysis,Databases,Google,Data mining,Terminology","Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice."
887,,"Analysis of University Fitness Center Data Uncovers Interesting Patterns, Enables Prediction.","Yunshu Du,Assefaw H. Gebremedhin,Matthew E. Taylor",https://doi.org/10.1109/TKDE.2018.2863705,TKDE,2019,"Predictive models,Springs,Analytical models,Data models,Radio frequency,Data science","Data is increasingly being used to make everyday life easier and better. Applications such as waiting time estimation, traffic prediction, and parking search are good examples of how data from different sources can be used to facilitate our daily life. In this study, we consider an under-utilized data source: university ID cards. Such cards are used on many campuses to purchase food, allow access to different areas, and even take attendance in classes. In this article, we use data from our university to analyze usage of the university fitness center and build a predictor for future visit volume. The work makes several contributions: it demonstrates the richness of the data source, shows how the data can be leveraged to improve student services, discovers interesting trends and behavior, and serves as a case study illustrating the entire data science process."
888,,Spell - Online Streaming Parsing of Large Unstructured System Logs.,"Min Du,Feifei Li 0001",https://doi.org/10.1109/TKDE.2018.2875442,TKDE,2019,"Printing,Data mining,Analytical models,Semantics,Stability criteria,Machine learning","System event logs have been frequently used as a valuable resource in data-driven approaches to enhance system health and stability. A typical procedure in system log analytics is to first parse unstructured logs to structured data, and then apply data mining and machine learning techniques and/or build workflow models from the resulting structured data. Previous work on parsing system event logs focused on offline, batch processing of raw log files. But increasingly, applications demand online monitoring and processing. As a result, a streaming method to parse unstructured logs is needed. We propose an online streaming method Spell, which utilizes a longest common subsequence based approach, to parse system event logs. We show how to dynamically extract log patterns from incoming logs and how to maintain a set of discovered message types in streaming fashion. An enhancement to find more accurate message types is also proposed. We also propose and evaluate a method to automatically discover semantic meanings for parameter fields identified by Spell. We compare Spell against state-of-the-art methods to extract patterns from system event logs on large real data. The results demonstrate that, compared with other log parsing alternatives, Spell shows its superiority in terms of both efficiency and effectiveness."
889,,Detecting Pickpocket Suspects from Large-Scale Public Transit Records.,"Bowen Du,Chuanren Liu,Wenjun Zhou 0001,Zhenshan Hou,Hui Xiong 0001",https://doi.org/10.1109/TKDE.2018.2834909,TKDE,2019,"Feature extraction,Pattern classification,Urban areas,Anomaly detection,Public transportation,Security,Customer satisfaction","Massive data collected by automated fare collection (AFC) systems provide opportunities for studying both personal traveling behaviors and collective mobility patterns in urban areas. Existing studies on AFC data have primarily focused on identifying passengers' movement patterns. However, we creatively leveraged such data for identifying pickpocket suspects. Stopping pickpockets in the public transit system has been crucial for improving passenger satisfaction and public safety. Nonetheless, in practice, it is challenging to discern thieves from regular passengers. In this paper, we developed a suspect detection and surveillance system, which can identify pickpocket suspects based on their daily transit records. Specifically, we first extracted a number of useful features from each passenger's daily activities in the transit system. Then, we took a two-step approach that exploits the strengths of unsupervised outlier detection and supervised classification models to identify thieves, who typically exhibit abnormal traveling behaviors. Experimental results demonstrated the effectiveness of our method. We also developed a prototype system for potential uses by security personnel."
890,,Fast Cosine Similarity Search in Binary Space with Angular Multi-Index Hashing.,"Sepehr Eghbali,Ladan Tahvildari",https://doi.org/10.1109/TKDE.2018.2828095,TKDE,2019,"Information retrieval,Nearest  neighbor methods,Indexing,Approximation methods,Binary codes","Given a large dataset of binary codes and a binary query point, we address how to efficiently find 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula>
 codes in the dataset that yield the largest cosine similarities to the query. The straightforward answer to this problem is to compare the query with all items in the dataset, but this is practical only for small datasets. One potential solution to enhance the search time and achieve sublinear cost is to use a hash table populated with binary codes of the dataset and then look up the nearby buckets to the query to retrieve the nearest neighbors. However, if codes are compared in terms of cosine similarity rather than the Hamming distance, then the main issue is that the order of buckets to probe is not evident. To examine this issue, we first elaborate on the connection between the Hamming distance and the cosine similarity. Doing this allows us to systematically find the probing sequence in the hash table. However, solving the nearest neighbor search with a single table is only practical for short binary codes. To address this issue, we propose the angular multi-index hashing search algorithm which relies on building multiple hash tables on binary code substrings. The proposed search algorithm solves the exact angular 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula>
 nearest neighbor problem in a time that is often orders of magnitude faster than the linear scan baseline and even approximation methods."
891,,Distribution-Aware Crowdsourced Entity Collection.,"Ju Fan,Zhewei Wei,Dongxiang Zhang,Jingru Yang,Xiaoyong Du 0001",https://doi.org/10.1109/TKDE.2016.2611509,TKDE,2019,"Crowdsourcing,Data collection,Databases,Estimation,Search problems,Fans,Knowledge based systems","The problem of crowdsourced entity collection solicits people (a.k.a. workers) to complete missing data in a database and has witnessed many applications in knowledge base completion and enterprise data collection. Although previous studies have attempted to address the “open world” challenge of crowdsourced entity collection, they do not pay much attention to the “distribution” of the collected entities. Evidently, in many real applications, users may have distribution requirements on the collected entities, e.g., even spatial distribution when collecting points-of-interest. In this paper, we study a new research problem, distribution-aware crowdsourced entity collection (CrowdDEC): Given an expected distribution w.r.t. an attribute (e.g., region or year), it aims to collect a set of entities via crowdsourcing and minimize the difference of the entity distribution from the expected distribution. Due to the openness of crowdsourcing, the CrowdDEC problem calls for effective crowdsourcing quality control. We propose an adaptive worker selection approach to address this problem. The approach estimates underlying entity distribution of workers on-the-fly based on the collected entities. Then, it adaptively selects the best set of workers that minimizes the difference from the expected distribution. Once workers submit their answers, it adjusts the estimation of workers' underlying distributions for subsequent adaptive worker selections. We prove the hardness of the problem, and develop effective estimation techniques as well as efficient worker selection algorithms to support this approach. We deployed the proposed approach on Amazon Mechanical Turk and the experimental results on two real datasets show that the approach achieves superiority on both effectiveness and efficiency."
892,,On Spatial-Aware Community Search.,"Yixiang Fang,Zheng Wang,Reynold Cheng,Xiaodong Li 0009,Siqiang Luo,Jiafeng Hu,Xiaojun Chen 0006",https://doi.org/10.1109/TKDE.2018.2845414,TKDE,2019,"Approximation algorithms,Search problems,Measurement,Social network services,Indexes,Lifting equipment,Urban areas","Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. The CS problem aims to look for a dense subgraph that contains a query vertex. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate setting social events (e.g., finding conference attendees to join a dinner), it is important to find groups of people who are physically close to each other, so it is desirable to have a 
<i>spatial-aware community</i>
 (or SAC), whose vertices are close structurally and spatially. Given a graph 
<inline-formula><tex-math notation=""LaTeX"">$G$</tex-math></inline-formula>
 and a query vertex 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
, we develop an exact solution to find the SAC containing 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
, but it cannot scale to large datasets, so we design three approximation algorithms. We further study the problem of continuous SAC search on a “dynamic spatial graph,” whose vertices’ locations change with time, and propose three fast solutions. We evaluate the solutions on both real and synthetic datasets, and the results show that SACs are better than communities returned by existing solutions. Moreover, our approximation solutions perform accurately and efficiently."
893,,Effective and Efficient Community Search Over Large Directed Graphs.,"Yixiang Fang,Zhongran Wang,Reynold Cheng,Hongzhi Wang 0001,Jiafeng Hu",https://doi.org/10.1109/TKDE.2018.2872982,TKDE,2019,"Indexes,Twitter,Measurement,Lifting equipment,Image edge detection,Biology","Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS), extracting a dense subgraph containing a query vertex q from a graph, has received great attention. However, existing CS solutions are designed for undirected graphs, and overlook directions of edges which potentially lose useful information carried on directions. In many applications (e.g., Twitter), users' relationships are often modeled as directed graphs (e.g., if a user a follows another user b, then there is an edge from a to b). In this paper, we study the problem of CS on directed graph. Given a vertex q of a graph G, we aim to find a densely connected subgraph containing q from G, in which vertices have strong interactions and high similarities, by using the minimum in/out-degrees metric. We first develop a baseline algorithm based on the concept of D-core. We further propose three index structures and corresponding query algorithms. Our experimental results on seven real graphs show that our solutions are very effective and efficient. For example, on a graph with over 1 billion of edges, we only need around 40mins to index it and 1~2sec to answer a query."
894,,An Extension to the Noisy-OR Function to Resolve the &apos;Explaining Away&apos; Deficiency for Practical Bayesian Network Problems.,"Norman E. Fenton,Takao Noguchi,Martin Neil",https://doi.org/10.1109/TKDE.2019.2891680,TKDE,2019,"Noise measurement,Probabilistic logic,Bayes methods,History,Tools,Standards,Heart","The “leaky noisy-OR” function is a common and popular method used to simplify the elicitation of complex conditional probability tables in Bayesian networks involving Boolean variables. It has proven to be useful for approximating the required relationship in many real-world situations where there is a set of two or more variables that are potential causes of a single effect variable. However, one of the properties of leaky noisy-OR is Conditional Inter-causal Independence (CII). This property means that the `explaining away' behavior-one of the most powerful benefits of BN inference-is not present when the effect variable is observed as false. Yet, for many real-world problems where the leaky noisy-OR has been considered, this behavior would be expected, meaning that leaky noisy-OR is deficient as an approximation of the required relationship in such cases. There have been previous attempts to adapt the noisy-OR to resolve this problem. However, they require too many additional parameters to be elicited. We describe a simple but powerful extension to leaky noisy-OR that requires only a single additional parameter. While it does not solve the CII problem in all cases, it does resolve most of the explaining away deficiencies that occur in practice. The problem and solution is illustrated using an example from intelligence analysis."
895,,A General Theory of IR Evaluation Measures.,"Marco Ferrante,Nicola Ferro 0001,Silvia Pontarollo",https://doi.org/10.1109/TKDE.2018.2840708,TKDE,2019,"Atmospheric measurements,Particle measurements,Statistical analysis,Correlation,Systematics,Indexes,Information retrieval","Interval scales are assumed by several basic descriptive statistics, such as mean and variance, and by many statistical significance tests which are daily used in IR to compare systems. Unfortunately, so far, there has not been any systematic and formal study to discover the actual scale properties of IR measures. Therefore, in this paper, we develop a theory of 
<i>Information Retrieval (IR)</i>
 evaluation measures, based on the representational theory of measurements, to determine whether and when IR measures are interval scales. We found that common set-based retrieval measures—namely Precision, Recall, and F-measure—always are interval scales in the case of binary relevance while this happens also in the case of multi-graded relevance only when the relevance degrees themselves are on a ratio scale and we define a specific partial order among systems. In the case of rank-based retrieval measures—namely AP, gRBP, DCG, and ERR—only gRPB is an interval scale when we choose a specific value of the parameter 
<inline-formula><tex-math notation=""LaTeX"">$p$</tex-math></inline-formula>
 and define a specific total order among systems while all the other IR measures are not interval scales. Besides the formal framework itself and the proof of the scale properties of several commonly used IR measures, the paper also defines some brand new set-based and rank-based IR evaluation measures which ensure to be interval scales."
896,,Efficient and Distributed Generalized Canonical Correlation Analysis for Big Multiview Data.,"Xiao Fu 0001,Kejun Huang,Evangelos E. Papalexakis,Hyun Ah Song,Partha P. Talukdar,Nicholas D. Sidiropoulos,Christos Faloutsos,Tom M. Mitchell",https://doi.org/10.1109/TKDE.2018.2875908,TKDE,2019,"Distributed algorithms,Data analysis,Correlation,Machine learning algorithms,Data mining,Neural networks","Generalized canonical correlation analysis (GCCA) integrates information from data samples that are acquired at multiple feature spaces (or `views') to produce low-dimensional representations-which is an extension of classical two-view CCA. Since the 1960s, (G)CCA has attracted much attention in statistics, machine learning, and data mining because of its importance in data analytics. Despite these efforts, the existing GCCA algorithms have serious complexity issues. The memory and computational complexities of the existing algorithms usually grow as a quadratic and cubic function of the problem dimension (the number of samples / features), respectively-e.g., handling views with ≈1,000 features using such algorithms already occupies ≈10
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">6</sup>
 memory and the periteration complexity is ≈10
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">9</sup>
 flops-which makes it hard to push these methods much further. To circumvent such difficulties, we first propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively. Consequently, the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100,000. Our second contribution lies in proposing two distributed algorithms for GCCA, which compute the canonical components of different views in parallel and thus can further reduce the runtime significantly if multiple computing agents are available. We provide detailed convergence analyses of the proposed algorithms and show that all the largescale GCCA algorithms converge to a Karush-Kuhn-Tucker (KKT) point at least sublinearly. Judiciously designed synthetic and realdata experiments are employed to showcase the effectiveness of the proposed algorithms."
897,,Real-Time Data Retrieval in Cyber-Physical Systems with Temporal Validity and Data Availability Constraints.,"Chenchen Fu,Qiangqiang Liu,Peng Wu 0009,Minming Li,Chun Jason Xue,Yingchao Zhao,Jingtong Hu,Song Han 0002",https://doi.org/10.1109/TKDE.2018.2866842,TKDE,2019,"Task analysis,Real-time systems,Schedules,Heuristic algorithms,Sensors,Cyber-physical systems,Complexity theory","Maintaining the temporal validity of real-time data in cyber-physical systems is of critical importance to ensure the correct decision making and appropriate system operation. Most existing work on real-time data retrieval assume that the real-time data under study are always available for retrieval, and the developed scheduling algorithms mainly focus on making real-time decisions while meeting the temporal validity constraints. This assumption, however does not hold in many real-time applications with intermittent data availability. In this paper, we study the Availability-constrained Fresh Data Retrieval (AFDR) problem, which aims to retrieve all required real-time data for a given set of decision tasks on time while taking both the temporal validity and data availability constraints into consideration. We formulate the AFDR problem as an ILP problem and study its complexity under different settings. Given the general case of the AFDR problem is proved to be NP-hard, we focus on the cases that data items have unit-size retrieval time. For the single decision task scenario, we propose a polynomial-time optimal data retrieval algorithm, which consists of a task finish time selection phase and an optimal retrieval schedule construction phase, to solve the AFDR problem. For the multiple decision task scenario, we propose an efficient heuristic algorithm by transforming the temporal validity constraint of a real-time data item to the availability constraint. The effectiveness of the proposed algorithms has been validated through extensive experiments. Our results show that the heuristic algorithm outputs around 1.5× feasible cases compared to that of the state-of-the-art scheme."
898,,Representing Urban Forms - A Collective Learning Model with Heterogeneous Human Mobility Data.,"Yanjie Fu,Guannan Liu,Yong Ge,Pengyang Wang,Hengshu Zhu,Chunxiao Li 0003,Hui Xiong 0001",https://doi.org/10.1109/TKDE.2018.2837027,TKDE,2019,"Data models,Behavioral sciences,Public transportation,Data mining","Human mobility data refers to records of human movements, such as cellphone traces, vehicle GPS trajectories, geo-tagged posts, and photos. While successfully mining human mobility data can benefit many applications such as city planning, transportation, urban economics, and public safety, it is very challenging to model large-scale Heterogeneous Human Mobility Data (HHMD) that are generated from different resources. In this paper, we develop a general collective learning approach to model HHMD at an individual level towards identifying and quantifying the urban forms of residential communities. Specifically, our proposed method exploits two geographic regularities among HHMD. First, we jointly capture the correlations among residential communities, urban functions, temporal effects, and user mobility patterns by analogizing communities as documents and mobility patterns as words. Also, we further combine explicit LASSO analysis and significant testing into latent representation learning as a regularization term by analogizing compatible Point-of-Interests (POIs) as the meta-data of communities. In this way, we can learn the urban forms, including a mix of functions and corresponding portfolios, of residential communities from HHDM and POIs. We further leverage these learned results to address two application problems: real estate ranking and restaurant popularity prediction. Finally, we conduct intensive evaluations with a variety of real-world data, where experimental results demonstrate the effectiveness of our proposed modeling method and its successful applications for other problems."
899,,Declarative Parameterizations of User-Defined Functions for Large-Scale Machine Learning and Optimization.,"Zekai J. Gao,Niketan Pansare,Christopher M. Jermaine",https://doi.org/10.1109/TKDE.2018.2873325,TKDE,2019,"Optimization,Data models,Machine learning,Task analysis,Sparks,Computational modeling,Servers","Large-scale optimization has become an important application for data management systems, particularly in the context of statistical machine learning. In this paper, we consider how one might implement the join-and-co-group pattern in the context of a fully declarative data processing system. The join-and-co-group pattern is ubiquitous in iterative, large-scale optimization. In the join-and-co-group pattern, a user-defined function g is parameterized with a data object x as well as the subset of the statistical model Θ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">x</sub>
 that applies to that object, so that g(x|Θ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">x</sub>
) can be used to compute a partial update of the model. This is repeated for every x in the full data set X. All partial updates are then aggregated and used to perform a complete update of the model. The join-and-co-group pattern has several implementation challenges, including the potential for a massive blow-up in the size of a fully parameterized model. Thus, unless the correct physical execution plan be chosen for implementing the join-and-co-group pattern, it is easily possible to have an execution that takes a very long time or even fails to complete. In this paper, we carefully consider the alternatives for implementing the join-and-co-group pattern on top of a declarative system, as well as how the best alternative can be selected automatically. Our focus is on the SimSQL database system, which is an SQL-based system with special facilities for large-scale, iterative optimization. Since it is an SQL-based system with a query optimizer, those choices can be made automatically."
900,,Guest Editors&apos; Introduction to the Special Section on the 33rd International Conference on Data Engineering (ICDE 2017).,"Wolfgang Gatterbauer,Arun Kumar 0001",https://doi.org/10.1109/TKDE.2019.2912043,TKDE,2019,"Special issues and sections,Meetings,Data engineering","The papers in this special section were presented at the 33rd International Conference on Data Engineering, ICDE 2017, that was held in San Diego, California, during April 19-22, 2017. "
901,,Personalized Market Basket Prediction with Temporal Annotated Recurring Sequences.,"Riccardo Guidotti,Giulio Rossetti,Luca Pappalardo,Fosca Giannotti,Dino Pedreschi",https://doi.org/10.1109/TKDE.2018.2872587,TKDE,2019,"History,Predictive models,Data mining,Adaptation models,Markov processes,Analytical models,Data models","Nowadays, a hot challenge for supermarket chains is to offer personalized services to their customers. Market basket prediction, i.e., supplying the customer a shopping list for the next purchase according to her current needs, is one of these services. Current approaches are not capable of capturing at the same time the different factors influencing the customer's decision process: co-occurrence, sequentuality, periodicity, and recurrency of the purchased items. To this aim, we define a pattern TemporalAnnotated Recurring Sequence (TARS) able to capture simultaneously and adaptively all these factors. We define the method to extract TARS and develop a predictor for next basket named TBP (TARS Based Predictor) that, on top of TARS, is able to understand the level of the customer's stocks and recommend the set of most necessary items. By adopting the TBP the supermarket chains could crop tailored suggestions for each individual customer which in turn could effectively speed up their shopping sessions. A deep experimentation shows that TARS are able to explain the customer purchase behavior, and that TBP outperforms the state-of-the-art competitors."
902,,CFOND - Consensus Factorization for Co-Clustering Networked Data.,"Ting Guo,Shirui Pan,Xingquan Zhu 0001,Chengqi Zhang",https://doi.org/10.1109/TKDE.2018.2846555,TKDE,2019,"Network topology,Couplings,Topology,Merging,Linear programming,Manifolds,Data mining","Networked data are common in domains where instances are characterized by both feature values and inter-dependency relationships. Finding cluster structures for networked instances and discovering representative features for each cluster represent a special co-clustering task usefully for many real-world applications, such as automatic categorization of scientific publications and finding representative key-words for each cluster. To date, although co-clustering has been commonly used for finding clusters for both instances and features, all existing methods are focused on instance-feature values, without leveraging valuable topology relationships between instances to help boost co-clustering performance. In this paper, we propose CFOND, a consensus factorization based framework for co-clustering networked data. We argue that feature values and linkages provide useful information from different perspectives, but they are not always consistent and therefore need to be carefully aligned for best clustering results. In the paper, we advocate a consensus factorization principle, which simultaneously factorizes information from three aspects: network topology structures, instance-feature content relationships, and feature-feature correlations. The consensus factorization ensures that the final cluster structures are consistent across information from the three aspects with minimum errors. Experiments on real-life networks validate the performance of our algorithm."
903,,Structured Manifold Broad Learning System - A Manifold Perspective for Large-Scale Chaotic Time Series Analysis and Prediction.,"Min Han,Shoubo Feng,C. L. Philip Chen,Meiling Xu,Tie Qiu 0001",https://doi.org/10.1109/TKDE.2018.2866149,TKDE,2019,"Manifolds,Time series analysis,Learning systems,Feature extraction,Laplace equations,Spectral analysis,Redundancy","High-dimensional and large-scale time series processing has aroused considerable research interests during decades. It is difficult for traditional methods to reveal the evolution state in dynamical systems and discover the relationship among variables automatically. In this paper, we propose a unified framework for nonuniform embedding, dynamical system revealing, and time series prediction, termed as Structured Manifold Broad Learning System (SM-BLS). The structured manifold learning is introduced for nonuniform embedding and unsupervised manifold learning simultaneously. Graph embedding and feature selection are both considered to depict the intrinsic structure connections between chaotic time series and its low-dimensional manifold. Compared with traditional methods, the proposed framework could discover potential deterministic evolution information of dynamical systems and make the modeling more interpretable. It provides us a homogeneous way to recover the chaotic attractor from multivariate and heterogeneous time series. Simulation analysis and results show that SM-BLS has advantages in dynamic discovery and feature extraction of large-scale chaotic time series prediction."
904,,Organizing an Influential Social Event Under a Budget Constraint.,"Kai Han 0003,Yuntian He,Xiaokui Xiao,Shaojie Tang,Fei Gui,Chaoting Xu,Jun Luo 0001",https://doi.org/10.1109/TKDE.2018.2875914,TKDE,2019,"Approximation algorithms,Computational complexity,Social networking (online),Time complexity,Optimization","Recently, the proliferation of event-based social services has made it possible for organizing personalized offline events through the users' information shared online. In this paper, we study the budget-constrained influential social event organization problem, where the goal is to select a group of influential users with required features to organize a social event under a budget B. We show that our problem is NP-hard and can be formulated as a submodular maximization problem with mixed packing and covering constraints. We then propose several polynomial time algorithms for our problem with provable approximation ratios, which adopt a novel “surrogate optimization” approach and the method of reverse-reachable set sampling. Moreover, we also consider the case where the influence spread function is unknown and can be arbitrarily selected from a set of candidate submodular functions, and extend our algorithms to address a “robust influential event organization” problem under this case. Finally, we conduct extensive experiments using real social networks to test the performance of our algorithms, and the experimental results demonstrate that our algorithms significantly outperform the prior studies both on the running time and on the influence spread."
905,,Correlated Matrix Factorization for Recommendation with Implicit Feedback.,"Yuan He 0006,Cheng Wang 0001,Changjun Jiang",https://doi.org/10.1109/TKDE.2018.2840993,TKDE,2019,"Correlation,Gaussian distribution,Computational modeling,Data models,Recommender systems,Semantics,Prediction algorithms","As a typical latent factor model, Matrix Factorization (MF) has demonstrated its great effectiveness in recommender systems. Users and items are represented in a shared low-dimensional space so that the user preference can be modeled by linearly combining the item factor vector 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
 using the user-specific coefficients 
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
. From a generative model perspective, 
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
 are drawn from two 
<i>independent</i>
 Gaussian distributions, which is not so faithful to the reality. Items are produced to maximally meet users’ requirements, which makes 
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
 strongly correlated. Meanwhile, the linear combination between 
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
 forces a bijection (one-to-one mapping), which thereby neglects the mutual correlation between the latent factors. In this paper, we address the upper drawbacks, and propose a new model, named Correlated Matrix Factorization (CMF). Technically, we apply Canonical Correlation Analysis (CCA) to map 
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
 into a new semantic space. Besides achieving the optimal fitting on the rating matrix, one component in each vector (
<inline-formula><tex-math notation=""LaTeX"">$U$</tex-math></inline-formula>
 or 
<inline-formula><tex-math notation=""LaTeX"">$V$</tex-math></inline-formula>
) is also tightly correlated with every single component in the other. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on four public datasets. Experimental results show that our approach achieves competitive performance on both prediction accuracy and efficiency compared with the current state of the art."
906,,A Note on the Behavior of Majority Voting in Multi-Class Domains with Biased Annotators.,"Jerónimo Hernández-González,Iñaki Inza,José Antonio Lozano",https://doi.org/10.1109/TKDE.2018.2845400,TKDE,2019,"Training,Labeling,Robustness,Standards,Noise measurement,Aggregates","Majority voting is a popular and robust strategy to aggregate different opinions in learning from crowds, where each worker labels examples according to their own criteria. Although it has been extensively studied in the binary case, its behavior with multiple classes is not completely clear, specifically when annotations are biased. This paper attempts to fill that gap. The behavior of the majority voting strategy is studied in-depth in multi-class domains, emphasizing the effect of annotation bias. By means of a complete experimental setting, we show the limitations of the standard majority voting strategy. The use of three simple techniques that infer global information from the annotations and annotators allows us to put the performance of the majority voting strategy in context."
907,,Special Section on the International Conference on Data Engineering 2016.,"Meichun Hsu,Alfons Kemper,Timos Sellis",https://doi.org/10.1109/TKDE.2018.2876580,TKDE,2019,"Special issues and sections,Meetings,Data engineering","The papers in this special section were presented at the 32nd International Conference on Data Engineering that was held in Helsinki, Finland, May 16- May 20, 2016. "
908,,A Novel Feature Incremental Learning Method for Sensor-Based Activity Recognition.,"Chunyu Hu,Yiqiang Chen,Xiaohui Peng 0002,Han Yu 0001,Chenlong Gao,Lisha Hu",https://doi.org/10.1109/TKDE.2018.2855159,TKDE,2019,"Activity recognition,Adaptation models,Data models,Forestry,Learning systems,Monitoring,Vegetation","Recognizing activities of daily living is an important research topic for health monitoring and elderly care. However, most existing activity recognition models only work with static and pre-defined sensor configurations. Enabling an existing activity recognition model to adapt to the emergence of new sensors in a dynamic environment is a significant challenge. In this paper, we propose a novel feature incremental learning method, namely the Feature Incremental Random Forest (FIRF), to improve the performance of an existing model with a small amount of data on newly appeared features. It consists of two important components - 1) a mutual information based diversity generation strategy (MIDGS) and 2) a feature incremental tree growing mechanism (FITGM). MIDGS enhances the internal diversity of random forests, while FITGM improves the accuracy of individual decision trees. To evaluate the performance of FIRF, we conduct extensive experiments on three well-known public datasets for activity recognition. Experimental results demonstrate that FIRF is significantly more accurate and efficient compared with other state-of-the-art methods. It has the potential to allow the dynamic exploitation of new sensors in changing environments."
909,,A New Robust Approach for Reversible Database Watermarking with Distortion Control.,"Donghui Hu,Dan Zhao,Shuli Zheng",https://doi.org/10.1109/TKDE.2018.2851517,TKDE,2019,"Watermarking,Databases,Robustness,Genetic algorithms,Distortion,Histograms,Data integrity","Nowadays information is crucial in many fields such as medicine, science and business, where databases are used effectively for information sharing. However, the databases face the risk of being pirated, stolen or misused, which may result in a lot of security threats concerning ownership rights, data tampering and privacy protection. Watermarking is utilized to enforce ownership rights on shared relational databases. Many reversible watermarking methods are proposed recently to protect rights of owners along with recovering original data. Most state-of-the-art methods modify the original data to a large extent, result in data quality degradation, and cannot achieve good balance between robustness against malicious attacks and data recovery. In this paper, we propose a robust and reversible database watermarking technique, Genetic Algorithm and Histogram Shifting Watermarking (GAHSW), for numerical relational database. The genetic algorithm is used to select the best secret key for grouping database, where the watermarking can be embedded with balanced distortion and capacity. The histogram of the prediction error is shifted to embed the watermark with good robustness. Experimental results demonstrate the effectiveness of GAHSW and show that it outperforms state-of-the-art approaches in terms of robustness against malicious attacks and preservation of data quality."
910,,Enhanced Clients for Data Stores and Cloud Services.,Arun Iyengar,https://doi.org/10.1109/TKDE.2018.2873321,TKDE,2019,"Servers,Encryption,Software,Monitoring,Java,Generators","Data stores and cloud services are typically accessed using a client-server paradigm wherein the client runs as part of an application process which is trying to access the data store or cloud service. This paper presents the design and implementation of enhanced clients for improving both the functionality and performance of applications accessing data stores or cloud services. Our enhanced clients can improve performance via multiple types of caches, encrypt data for providing confidentiality before sending information to a server, and compress data for reducing the size of data transfers. Our clients can perform data analysis to allow applications to more effectively use cloud services. They also provide both synchronous and asynchronous interfaces. An asynchronous interface allows an application program to access a data store or cloud service and continue execution before receiving a response which can significantly improve performance. We present a Universal Data Store Manager (UDSM) which allows an application to access multiple different data stores and provides a common interface to each data store. The UDSM also can monitor the performance of different data stores. A workload generator allows users to easily determine and compare the performance of different data stores. We also present NLU-SA, an application for performing natural language understanding and sentiment analysis on text documents. NLU-SA is implemented on top of our enhanced clients and integrates text analysis with Web searching. We present results from NLU-SA on sentiment on the Web towards major companies and countries. We also present a performance analysis of our enhanced clients."
911,,Next Generation Indexing for Genomic Intervals.,"Vahid Jalili,Matteo Matteucci,Jeremy Goecks,Yashar Deldjoo,Stefano Ceri",https://doi.org/10.1109/TKDE.2018.2871031,TKDE,2019,"Bioinformatics,Genomics,Tools,DNA,Indexing,Calculus","One-dimensional intervals incremental inverted index (Di4) is a multi-resolution, single-dimension indexing framework for efficient, scalable, and extensible computation of genomic interval expressions. The framework has a tri-layer architecture: the semantic layer provides orthogonal and generic means (including the support of user-defined function) of sense-making and higher-lever reasoning from region-based datasets; the logical layer provides building blocks for region calculus and topological relations between intervals; the physical layer abstracts from persistence technology and makes the model adaptable to variety of persistence technologies, spanning from small-scale (e.g., B+tree) to large-scale (e.g., LevelDB). The extensibility of Di4 to application scenarios is shown with an example of comparative evaluation of ChIP-seq and DNase-Seq replicates. Performance of Di4 is benchmarked for small and large scale scenarios under common bioinformatics application scenarios. Di4 is freely available from https://genometric.github.io/Di4."
912,,CURE - Flexible Categorical Data Representation by Hierarchical Coupling Learning.,"Songlei Jian,Guansong Pang,Longbing Cao,Kai Lu,Hang Gao",https://doi.org/10.1109/TKDE.2018.2848902,TKDE,2019,"Couplings,Encoding,Anomaly detection,Task analysis,Semantics,Unsupervised learning,Data models","The representation of categorical data with hierarchical value coupling relationships (i.e., various value-to-value cluster interactions) is very critical yet challenging for capturing complex data characteristics in learning tasks. This paper proposes a novel and flexible coupled unsupervised categorical data representation (CURE) framework, which not only captures the hierarchical couplings but is also flexible enough to be instantiated for contrastive learning tasks. CURE first learns the value clusters of different granularities based on multiple value coupling functions and then learns the value representation from the couplings between the obtained value clusters. With two complementary value coupling functions, CURE is instantiated into two models: coupled data embedding (CDE) for clustering and coupled outlier scoring of high-dimensional data (COSH) for outlier detection. These show that CURE is flexible for value clustering and coupling learning between value clusters for different learning tasks. CDE embeds categorical data into a new space in which features are independent and semantics are rich. COSH represents data w.r.t. an outlying vector to capture complex outlying behaviors of objects in high-dimensional data. Substantial experiments show that CDE significantly outperforms three popular unsupervised encoding methods and three state-of-the-art similarity measures, and COSH performs significantly better than five state-of-the-art outlier detection methods on high-dimensional data. CDE and COSH are scalable and stable, linear to data size and quadratic to the number of features, and are insensitive to their parameters."
913,,Stacked Robust Adaptively Regularized Auto-Regressions for Domain Adaptation.,"Wenhao Jiang,Hongchang Gao,Wei Lu 0006,Wei Liu 0005,Fu-Lai Chung,Heng Huang",https://doi.org/10.1109/TKDE.2018.2837085,TKDE,2019,"Machine learning,Robustness,Training,Noise reduction,Adaptation models,Learning systems,Training data","Domain adaptation is the situation for supervised learning in which the training data are sampled from the source domain while the test data are sampled from the target domain that follows a different distribution. The key to solving such a problem is to reduce effects of the discrepancy between the training data and test data. Recently, deep learning methods that employ stacked denoising auto-encoders (SDAs) to learn new representations for both domains have been successfully applied in domain adaptation. And, remarkable performance on multi-domain sentiment analysis datasets has been reported, making deep learning a promising approach to domain adaptation problems. In this paper, a deep learning method called Stacked Robust Adaptively Regularized Auto-regressions (SRARAs) is proposed to learn useful representations for domain adaptation problems. Each layer of SRARAs contains two steps: a linear transformation step, which is based on robust adaptively regularized auto-regression, and a non-linear squashing transformation step. The first step aims at reducing the discrepancy between the training data and test data, and the second step is to introduce non-linearity and control the range of the elements in the outputs. The experimental results on text and image datasets demonstrate that the proposed method is very effective."
914,,Relative Pairwise Relationship Constrained Non-Negative Matrix Factorisation.,"Shuai Jiang,Kan Li 0001,Richard Yi Da Xu",https://doi.org/10.1109/TKDE.2018.2859223,TKDE,2019,"Loss measurement,Recommender systems,Convergence,Motion pictures,Clustering algorithms,Symmetric matrices,Euclidean distance","Non-negative Matrix Factorisation (NMF) has been extensively used in machine learning and data analytics applications. Most existing variations of NMF only consider how each row/column vector of factorised matrices should be shaped, and ignore the relationship among pairwise rows or columns. In many cases, such pairwise relationship enables better factorisation, for example, image clustering and recommender systems. In this paper, we propose an algorithm named, Relative Pairwise Relationship constrained Non-negative Matrix Factorisation (RPR-NMF), which places constraints over relative pairwise distances amongst features by imposing penalties in a triplet form. Two distance measures, squared Euclidean distance and Symmetric divergence, are used, and exponential and hinge loss penalties are adopted for the two measures, respectively. It is well known that the so-called “multiplicative update rules” result in a much faster convergence than gradient descend for matrix factorisation. However, applying such update rules to RPR-NMF and also proving its convergence is not straightforward. Thus, we use reasonable approximations to relax the complexity brought by the penalties, which are practically verified. Experiments on both synthetic datasets and real datasets demonstrate that our algorithms have advantages on gaining close approximation, satisfying a high proportion of expected constraints, and achieving superior performance compared with other algorithms."
915,,HyperX - A Scalable Hypergraph Framework.,"Wenkai Jiang,Jianzhong Qi 0001,Jeffrey Xu Yu,Jin Huang 0003,Rui Zhang 0003",https://doi.org/10.1109/TKDE.2018.2848257,TKDE,2019,"Partitioning algorithms,Optimization,Tools,Knowledge engineering,Data engineering,Approximation algorithms,Modeling","Hypergraphs are generalizations of graphs where the (hyper)edges can connect any number of vertices. They are powerful tools for representing complex and non-pairwise relationships. However, existing graph computation frameworks cannot accommodate hypergraphs without converting them into graphs, because they do not offer APIs that support (hyper)edges directly. This graph conversion may create excessive replicas and result in very large graphs, causing difficulties in workload balancing. A few tools have been developed for hypergraph partitioning, but they are not general-purpose frameworks for hypergraph processing. In this paper, we propose HyperX, a general-purpose distributed hypergraph processing framework built on top of Spark. HyperX is based on the computation paradigm “Pregel”, which is user-friendly and has been widely adopted by popular graph computation frameworks. To help create balanced workloads for distributed hypergraph processing, we further investigate the hypergraph partitioning problem and propose a novel label propagation partitioning (LPP) algorithm. We conduct extensive experiments using both real and synthetic data. The result shows that HyperX achieves an order of magnitude improvement for running hypergraph learning algorithms compared with graph conversion based approaches in terms of running time, network communication costs, and memory consumption. For hypergraph partitioning, LPP outperforms the baseline algorithms significantly in these measures as well."
916,,A Correlation-Based Feature Weighting Filter for Naive Bayes.,"Liangxiao Jiang,Lungan Zhang,Chaoqun Li,Jia Wu 0001",https://doi.org/10.1109/TKDE.2018.2836440,TKDE,2019,"Pattern classification,Feature extraction,Decision trees,Data mining,Bayes methods,Mutual information","Due to its simplicity, efficiency, and efficacy, naive Bayes (NB) has continued to be one of the top 10 algorithms in the data mining and machine learning community. Of numerous approaches to alleviating its conditional independence assumption, feature weighting has placed more emphasis on highly predictive features than those that are less predictive. In this paper, we argue that for NB highly predictive features should be highly correlated with the class (maximum mutual relevance), yet uncorrelated with other features (minimum mutual redundancy). Based on this premise, we propose a correlation-based feature weighting (CFW) filter for NB. In CFW, the weight for a feature is a sigmoid transformation of the difference between the feature-class correlation (mutual relevance) and the average feature-feature intercorrelation (average mutual redundancy). Experimental results show that NB with CFW significantly outperforms NB and all the other existing state-of-the-art feature weighting filters used to compare. Compared to feature weighting wrappers for improving NB, the main advantages of CFW are its low computational complexity (no search involved) and the fact that it maintains the simplicity of the final model. Besides, we apply CFW to text classification and have achieved remarkable improvements."
917,,Collective Keyword Query on a Spatial Knowledge Base.,"Xiongnan Jin,Sangjin Shin,Eunju Jo,Kyong-Ho Lee",https://doi.org/10.1109/TKDE.2018.2873376,TKDE,2019,"Semantics,Knowledge based systems,Resource description framework,Query processing,Data models,Standards","The conventional works on spatial keyword queries for a knowledge base focus on finding a subtree to cover all the query keywords. The retrieved subtree is rooted at a place vertex, spatially close to a query location and compact in terms of the query keywords. However, user requirements may not be satisfied by a single subtree in some application scenarios. A group of subtrees should be combined together to collectively cover the query keywords. In this paper, we propose and study a novel way of searching on a spatial knowledge, namely collective spatial keyword query on a knowledge base (CoSKQ-KB). We formalize the problem of CoSKQKB and design a baseline method for CoSKQ-KB (BCK). To further speed up the query processing, an improved scalable method for CoSKQ-KB (iSCK) is proposed based on a set of efficient pruning and early termination techniques. In addition, we conduct empirical experiments on two real-world datasets to show the efficiency and effectiveness of our proposed algorithms."
918,,Interactive Data Exploration with Smart Drill-Down.,"Manas Joglekar,Hector Garcia-Molina,Aditya G. Parameswaran",https://doi.org/10.1109/TKDE.2017.2685998,TKDE,2019,"Bicycles,Heuristic algorithms,Optimization,Algorithm design and analysis,Prototypes,Aggregates,Databases","We present 
<i>smart drill-down</i>
, an operator for interactively exploring a relational table to discover and summarize “interesting” groups of tuples. Each group of tuples is described by a 
<i>rule</i>
 . For instance, the rule 
<inline-formula><tex-math notation=""LaTeX"">$(a, b, \star, 1000)$</tex-math></inline-formula>
 tells us that there are 1,000 tuples with value 
<inline-formula><tex-math notation=""LaTeX"">$a$</tex-math></inline-formula>
 in the first column and 
<inline-formula><tex-math notation=""LaTeX"">$b$</tex-math></inline-formula>
 in the second column (and any value in the third column). Smart drill-down presents an analyst with a list of rules that together describe interesting aspects of the table. The analyst can tailor the definition of interesting, and can interactively apply smart drill-down on an existing rule to explore that part of the table. We demonstrate that the underlying optimization problems are 
<small>NP-Hard</small>
, and describe an algorithm for finding the approximately optimal list of rules to display when the user uses a smart drill-down, and a dynamic sampling scheme for efficiently interacting with large tables. Finally, we perform experiments on real datasets on our experimental prototype to demonstrate the usefulness of smart drill-down and study the performance of our algorithms."
919,,Sequential Multi-Class Labeling in Crowdsourcing.,"Qiyu Kang,Wee Peng Tay",https://doi.org/10.1109/TKDE.2018.2874003,TKDE,2019,"Labeling,Crowdsourcing,Task analysis,Games,Dogs,Reliability,Encoding","We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem. As workers may be unreliable, we propose to perform sequential questioning in which the questions posed to the workers are designed based on previous questions and answers. We propose a Partially-Observable Markov Decision Process (POMDP) framework to determine the best questioning strategy, subject to the crowdsourcer's budget constraint. As this POMDP formulation is in general intractable, we develop a suboptimal approach based on a q-ary Ulam-Renyi game. We also propose a sampling heuristic, which can be used in tandem with standard POMDP solvers, using our Ulam-Renyi strategy. We demonstrate through simulations that our approaches outperform a non-sequential strategy based on error correction coding and which does not utilize workers' previous responses."
920,,Machine Learning for the Geosciences - Challenges and Opportunities.,"Anuj Karpatne,Imme Ebert-Uphoff,Sai Ravela,Hassan Ali Babaie,Vipin Kumar",https://doi.org/10.1109/TKDE.2018.2861006,TKDE,2019,"Geology,Earth,Machine learning,Meteorology,Data models,Atmospheric modeling,Sensors","Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)-that has been widely successful in commercial domains-offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines."
921,,Platform-Independent Robust Query Processing.,"Srinivas Karthik,Jayant R. Haritsa,Sreyash Kenkre,Vinayaka Pandit,Lohit Krishnan",https://doi.org/10.1109/TKDE.2017.2664827,TKDE,2019,"Query processing,Robustness,Estimation,Engines,Three-dimensional displays,Benchmark testing","To address the classical selectivity estimation problem for OLAP queries in relational databases, a radically different approach called 
<monospace>PlanBouquet</monospace>
 was recently proposed in 
<xref ref-type=""bibr"" rid=""ref1""> [1]</xref>
 , wherein the estimation process is completely abandoned and replaced with a calibrated discovery mechanism. The beneficial outcome of this new construction is that provable guarantees on worst-case performance, measured as Maximum Sub-Optimality (
<i>MSO</i>
), are obtained thereby facilitating robust query processing. The 
<monospace> PlanBouquet</monospace>
 formulation suffers, however, from a systemic drawback—the MSO bound is a function of not only the query, but also the optimizer's behavioral profile over the underlying database platform. As a result, there are adverse consequences: (i) the bound value becomes highly variable, depending on the specifics of the current operating environment, and (ii) it becomes infeasible to compute the value without substantial investments in preprocessing overheads. In this paper, we first present 
<monospace>SpillBound</monospace>
, a new query processing algorithm that retains the core strength of the 
<monospace>PlanBouquet</monospace>
 discovery process, but reduces the bound dependency to only the query. It does so by incorporating plan termination and selectivity monitoring mechanisms in the database engine. Specifically, 
<monospace>SpillBound</monospace>
 delivers a worst-case multiplicative bound, of 
<inline-formula><tex-math notation=""LaTeX"">$D^2+3D$</tex-math></inline-formula>
, where 
<inline-formula> <tex-math notation=""LaTeX"">$D$</tex-math></inline-formula>
 is simply the number of error-prone predicates in the user query. Consequently, the bound value becomes independent of the optimizer and the database platform, and the guarantee can be issued simply by query inspection. We go on to prove that 
<monospace>SpillBound</monospace>
 is within an 
<inline-formula> <tex-math notation=""LaTeX"">$O(D)$</tex-math></inline-formula>
 factor of the 
<i>best possible</i>
 deterministic selectivity discovery algorithm in its class. We next devise techniques to bridge this quadratic-to-linear MSO gap by introducing the notion of 
<i>contour alignment</i>
, a characterization of the nature of plan structures along the 
<i> boundaries</i>
 of the selectivity space. Specifically, we propose a variant of 
<monospace>SpillBound</monospace>
, called 
<monospace>AlignedBound</monospace>
, which exploits the alignment property and provides a guarantee in the range 
<inline-formula><tex-math notation=""LaTeX"">$\mathbf {[2D+2,D^2+3D]}$</tex-math></inline-formula>
. Finally, a detailed empirical evaluation over the standard decision-support benchmarks indicates that: (i) 
<monospace>SpillBound </monospace>
 provides markedly superior performance w.r.t. MSO as compared to 
<monospace>PlanBouquet</monospace>
, and (ii) 
<monospace>AlignedBound</monospace>
 provides additional benefits for query instances that are challenging for 
<monospace>SpillBound</monospace>
, often coming close to the ideal of MSO linearity in 
<inline-formula> <tex-math notation=""LaTeX"">$D$</tex-math></inline-formula>
. From an absolute perspective, 
<monospace>AlignedBound</monospace>
 evaluates virtually all the benchmark queries considered in our study with MSO of around 
<b>10</b>
 or lesser. Therefore, in an overall sense, 
<monospace>SpillBound</monospace>
 and 
<monospace>AlignedBound</monospace>
 offer a substantive step forward in the long-standing quest for robust query processing."
922,1,Engineering Methods for Differentially Private Histograms - Efficiency Beyond Utility.,"Georgios Kellaris,Stavros Papadopoulos 0001,Dimitris Papadias",https://doi.org/10.1109/TKDE.2018.2827378,TKDE,2019,"Histograms,Privacy,Smoothing methods,Sensitivity,Discrete Fourier transforms,Knowledge engineering,Data engineering","Publishing histograms with 
<inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math></inline-formula>
-
<i>differential privacy</i>
 has been studied extensively in the literature. Existing schemes aim at maximizing the 
<i>utility</i>
 of the published data, while previous experimental evaluations analyze the privacy/utility trade-off. In this paper, we provide the first experimental evaluation of differentially private methods that goes beyond utility, emphasizing also on another important aspect, namely 
<i>efficiency</i>
. Towards this end, we first observe that all existing schemes are comprised of a small set of common blocks. We then optimize and choose the best implementation for each block, determine the combinations of blocks that capture the entire literature, and propose novel block combinations. We qualitatively assess the quality of the schemes based on the skyline of efficiency and utility, i.e., based on whether a method is dominated on both aspects or not. Using exhaustive experiments on four real datasets with different characteristics, we conclude that there are always trade-offs in terms of utility and efficiency. We demonstrate that the schemes derived from our novel block combinations provide the best trade-offs for time critical applications. Our work can serve as a guide to help practitioners 
<i>engineer</i>
 a differentially private histogram scheme depending on their application requirements."
923,,LinkBlackHole** - Robust Overlapping Community Detection Using Link Embedding.,"Jungeun Kim,Sungsu Lim,Jae-Gil Lee 0001,Byung Suk Lee",https://doi.org/10.1109/TKDE.2018.2873750,TKDE,2019,"Clustering algorithms,Robustness,Detection algorithms,Partitioning algorithms,Image edge detection,Computer science,Merging","This paper proposes LinkBlackHole*, a novel algorithm for finding communities that are (i) overlapping in nodes and (ii) mixing (not separating clearly) in links. There has been a small body of work in each category, but this paper is the first one that addresses both. LinkBlackHole* is a merger of our earlier two algorithms, LinkSCAN* and BlackHole, inheriting their advantages in support of highly-mixed overlapping communities. The former is used to handle overlapping nodes, and the latter to handle mixing links in finding communities. Like LinkSCAN and its more efficient variant LinkSCAN*, this paper presents LinkBlackHole and its more efficient variant LinkBlackHole*, which reduces the number of links through random sampling. Thorough experiments show superior quality of the communities detected by LinkBlackHole* and LinkBlackHole to those detected by other state-of-the-art algorithms. In addition, LinkBlackHole* shows high resilience to the link sampling effect, and its running time scales up almost linearly with the number of links in a network."
924,,"A Collective, Probabilistic Approach to Schema Mapping Using Diverse Noisy Evidence.","Angelika Kimmig,Alex Memory,Renée J. Miller,Lise Getoor",https://doi.org/10.1109/TKDE.2018.2865785,TKDE,2019,"Metadata,Task analysis,Probabilistic logic,Cognition,Knowledge engineering,Data engineering,Complexity theory","We propose a probabilistic approach to the problem of schema mapping. Our approach is declarative, scalable, and extensible. It builds upon recent results in both schema mapping and probabilistic reasoning and contributes novel techniques in both fields. We introduce the problem of schema mapping selection, that is, choosing the best mapping from a space of potential mappings, given both metadata constraints and a data example. As selection has to reason holistically about the inputs and the dependencies between the chosen mappings, we define a new schema mapping optimization problem which captures interactions between mappings as well as inconsistencies and incompleteness in the input. We then introduce Collective Mapping Discovery (CMD), our solution to this problem using state-of-the-art probabilistic reasoning techniques. Our evaluation on a wide range of integration scenarios, including several real-world domains, demonstrates that CMD effectively combines data and metadata information to infer highly accurate mappings even with significant levels of noise."
925,,Predicting Consumption Patterns with Repeated and Novel Events.,"Dimitrios Kotzias,Moshe Lichman,Padhraic Smyth",https://doi.org/10.1109/TKDE.2018.2832132,TKDE,2019,"Data models,Predictive models,Music,Mixture models,Social network services,Training data,Motion pictures","There are numerous contexts where individuals typically consume a few items from a large selection of possible items. Examples include purchasing products, listening to music, visiting locations in physical or virtual environments, and so on. There has been significant prior work in such contexts on developing predictive modeling techniques for recommending new items to individuals, often using techniques such as matrix factorization. There are many situations, however, where making predictions for both previously-consumed and new items for an individual is important, rather than just recommending new items. We investigate this problem and find that widely-used matrix factorization methods are limited in their ability to capture important details in historical behavior, resulting in relatively low predictive accuracy for these types of problems. As an alternative we propose an interpretable and scalable mixture model framework that balances individual preferences in terms of exploration and exploitation. We evaluate our model in terms of accuracy in user consumption predictions using several real-world datasets, including location data, social media data, and music listening data. Experimental results show that the mixture model approach is systematically more accurate and more efficient for these problems compared to a variety of state-of-the-art matrix factorization methods."
926,,A Multi-Step Nonlinear Dimension-Reduction Approach with Applications to Big Data.,"Raghavan Krishnan,V. A. Samaranayake,Sarangapani Jagannathan",https://doi.org/10.1109/TKDE.2018.2876848,TKDE,2019,"Covariance matrices,Noise measurement,Correlation,Big Data,Indexes,Organizations,Eigenvalues and eigenfunctions","In this paper, a novel dimension-reduction approach is presented to overcome challenges such as nonlinear relationships, heterogeneity, and noisy dimensions. Initially, the p attributes in the data are first organized into random groups. Next, to systematically remove redundant and noisy dimensions from the data, each group is independently mapped into a low dimensional space via a parametric mapping. The group-wise transformation parameters are estimated using a low-rank approximation of distance covariance. The transformed attributes are reorganized into groups based on the magnitude of their respective eigenvalues. The group-wise organization and reduction process is performed until a user-defined criterion on eigenvalues is satisfied. In addition, novel procedures are introduced to aggregate the transformation parameters when the data is available in batches. Overall performance is demonstrated with extensive simulation analysis on classification by employing 10 data-sets."
927,,l-Injection - Toward Effective Collaborative Filtering Using Uninteresting Items.,"Jongwuk Lee,Won-Seok Hwang,Juan Parc,Youngnam Lee,Sang-Wook Kim,Dongwon Lee 0001",https://doi.org/10.1109/TKDE.2017.2698461,TKDE,2019,"Motion pictures,Collaboration,Recommender systems,Software,Business,Manganese","We develop a novel framework, named as 
<inline-formula><tex-math notation=""LaTeX"">$l$</tex-math></inline-formula>
-injection, to address the sparsity problem of recommender systems. By carefully injecting low values to a selected set of unrated user-item pairs in a user-item matrix, we demonstrate that top-
<i>N</i>
 recommendation accuracies of various collaborative filtering (CF) techniques can be significantly and consistently improved. We first adopt the notion of 
<i>pre-use preferences</i>
 of users toward a vast amount of 
<i>unrated</i>
 items. Using this notion, we identify 
<i>uninteresting</i>
 items that have not been rated yet but are likely to receive low ratings from users, and selectively impute them as low values. As our proposed approach is method-agnostic, it can be easily applied to a variety of CF algorithms. Through comprehensive experiments with three real-life datasets (e.g., Movielens, Ciao, and Watcha), we demonstrate that our solution consistently and universally enhances the accuracies of existing CF algorithms (e.g., item-based CF, SVD-based CF, and SVD++) by 2.5 to 5 times on average. Furthermore, our solution improves the running time of those CF methods by 1.2 to 2.3 times when its setting produces the best accuracy. The datasets and codes that we used in the experiments are available at: 
<uri>https://goo.gl/KUrmip</uri>
."
928,,Automated Influence Maintenance in Social Networks - An Agent-based Approach.,"Weihua Li,Quan Bai,Minjie Zhang,Tung Doan Nguyen",https://doi.org/10.1109/TKDE.2018.2867774,TKDE,2019,"Social network services,Maintenance engineering,Adaptation models,Integrated circuit modeling,Data models,Market research","Social influence modelling and maximization appear significant in various domains, such as e-business, marketing, and social computing. Most existing studies focus on how to maximize positive social impact to promote product adoptions based on static network snapshots. Such approaches can only increase influence in a social network in short-term, but cannot generate sustainable or long-term effects. In this research work, we study how to maintain long-term influence in a social network and propose an agent-based influence maintenance model, which can select influential nodes based on the current status in dynamic social networks in multiple times. Within the context of our investigation, the experimental results indicate that multiple-time seed selection is capable of achieving more constant impact than that of one-shot selection. We claim that influence maintenance is crucial for supporting, enhancing, and assisting long-term goals in business development. The proposed approach can automatically maintain long-lasting impact and achieve influence maintenance."
929,,Multi-Label Learning from Crowds.,"Shao-Yuan Li,Yuan Jiang 0001,Nitesh V. Chawla,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2018.2857766,TKDE,2019,"Labeling,Crowdsourcing,Task analysis,Correlation,Random access memory,Reliability,Tagging","We consider multi-label crowdsourcing learning in two scenarios. In the first scenario, we aim at inferring instances' groundtruth given the crowds' annotations. We propose two approaches NAM/RAM (Neighborhood/Relevance Aware Multi-label crowdsourcing) modeling the crowds' expertise and label correlations from different perspectives. Extended from single-label crowdsourcing methods, NAM models the crowds' expertise on individual labels, but based on the idea that for rational workers, their annotations for instances similar in the feature space should also be similar, NAM utilizes information from the feature space and incorporates the local influence of neighborhoods' annotations. Noting that the crowds tend to act in an effort-saving manner while labeling multiple labels, i.e., rather than carefully annotating every proper label, they would prefer scanning and tagging a few most relevant labels, RAM models the crowds' expertise as their ability to distinguish the relevance between label pairs. In the second scenario, we care about cost-efficient crowdsourcing where the labeling and learning process are conducted in tandem. We extend NAM/RAM to the active paradigm and propose instance, label, and worker selection criteria such that the labeling cost is significantly saved compared to passive learning without labeling control. The proposals' effectiveness are validated on simulated and real data."
930,,Location Inference for Non-Geotagged Tweets in User Timelines.,"Pengfei Li 0005,Hua Lu 0001,Nattiya Kanhabua,Sha Zhao,Gang Pan 0001",https://doi.org/10.1109/TKDE.2018.2852764,TKDE,2019,"Twitter,Urban areas,Adaptation models,Feature extraction,Hidden Markov models,Location awareness","Social media like Twitter have become globally popular in the past decade. Thanks to the high penetration of smartphones, social media users are increasingly going mobile. This trend has contributed to foster various location based services deployed on social media, the success of which heavily depends on the availability and accuracy of users' location information. However, only a very small fraction of tweets in Twitter are geo-tagged. Therefore, it is necessary to infer locations for tweets in order to attain the purpose of those location based services. In this paper, we tackle this problem by scrutinizing Twitter user timelines in a novel fashion. First of all, we split each user's tweet timeline temporally into a number of clusters, each tending to imply a distinct location. Subsequently, we adapt two machine learning models to our setting and design classifiers that classify each tweet cluster into one of the pre-defined location classes at the city level. The Bayes based model focuses on the information gain of words with location implications in the user-generated contents. The convolutional LSTM model treats user-generated contents and their associated locations as sequences and employs bidirectional LSTM and convolution operation to make location inferences. The two models are evaluated on a large set of real Twitter data. The experimental results suggest that our models are effective at inferring locations for non-geotagged tweets and the models outperform the state-of-the-art and alternative approaches significantly in terms of inference accuracy."
931,,Finding Most Popular Indoor Semantic Locations Using Uncertain Mobility Data.,"Huan Li 0003,Hua Lu 0001,Lidan Shou,Gang Chen 0001,Ke Chen 0005",https://doi.org/10.1109/TKDE.2018.2875096,TKDE,2019,"Semantics,Topology,Uncertainty,Wireless sensor networks,Wireless communication,Data structures,Reliability","Knowing popular indoor locations can benefit many applications like exhibition planning and location-based advertising, among others. In this work, we use uncertain historical indoor mobility data to find the top-k popular indoor semantic locations with the highest flow values. In the data we use, an object positioning report contains a set of samples, each consisting of an indoor location and a corresponding probability. The problem is challenging due to the difficulty in obtaining reliable flow values and the heavy computational workload on probabilistic samples for large numbers of objects. To address the first challenge, we propose an indoor flow definition that takes into account both data uncertainty and indoor topology. To efficiently compute flows for individual indoor semantic locations, we design data structures for facilitating accessing the relevant data, a data reduction method that reduces the intermediate data to process, and an overall flow computing algorithm. Furthermore, we design search algorithms for finding the top-k popular indoor semantic locations. All proposals are evaluated extensively on real and synthetic data. The evaluation results show that our data reduction method significantly reduces the data volume in computing, our search algorithms are efficient and scalable, and the top-k popular semantic locations returned are in good accord with ground truth."
932,,C2Net - A Network-Efficient Approach to Collision Counting LSH Similarity Join.,"Hangyu Li,Sarana Nutanong,Hong Xu 0001,Chenyun Yu,Foryu Ha",https://doi.org/10.1109/TKDE.2018.2836464,TKDE,2019,"Task analysis,Distributed databases,Runtime,Schedules,Distributed processing,Scheduling,Aggregates","Similarity join of two datasets 
<inline-formula><tex-math notation=""LaTeX"">$P$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$Q$</tex-math></inline-formula>
 is a primitive operation that is useful in many application domains. The operation involves identifying pairs 
<inline-formula><tex-math notation=""LaTeX"">$(p,q)$</tex-math></inline-formula>
, in the Cartesian product of 
<inline-formula><tex-math notation=""LaTeX"">$P$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$Q$</tex-math></inline-formula>
 such that 
<inline-formula><tex-math notation=""LaTeX"">$(p,q)$</tex-math></inline-formula>
 satisfies a stipulated similarity condition. In a high-dimensional space, an approximate similarity join based on locality-sensitive hashing (LSH) provides a good solution while reducing the processing cost with a predictable loss of accuracy. A distributed processing framework such as MapReduce allows the handling of large and high-dimensional datasets. However, network cost estimation frequently turns into a bottleneck in a distributed processing environment, thus resulting in a challenge of achieving faster and more efficient similarity join. This paper focuses on collision counting LSH-based similarity join in MapReduce and proposes a network-efficient solution called C2Net to improve the utilization of MapReduce combiners. The solution uses two graph partitioning schemes: (i) 
<i>minimum spanning tree</i>
 for organizing LSH buckets replication; and (ii) 
<i>spectral clustering</i>
 for runtime collision counting task scheduling. Experiments have shown that, in comparison to the state of the art, the proposed solution is able to achieve 20 percent data reduction and 50 percent reduction in shuffle time."
933,,ROMIR - Robust Multi-View Image Re-Ranking.,"Jun Li 0033,Chang Xu 0002,Wankou Yang,Changyin Sun,Ramamohanarao Kotagiri,Dacheng Tao",https://doi.org/10.1109/TKDE.2018.2876834,TKDE,2019,"Visualization,Kernel,Encoding,Data models,Databases,Training,Robustness","In multi-view re-ranking, multiple heterogeneous visual features are usually projected onto a low-dimensional subspace, and thus the resulting latent representation can be used for the subsequent similarity-based ranking. Albeit effective, this standard mechanism underplays the intrinsic structure underlying the latent subspace and does not take into account the substantial noise in the original spaces. In this paper, we propose a robust multi-view image re-ranking strategy. Due to the dramatic variability in image visual appearance, it is necessary to uncover the shared components underlying those query-related instances that are visually unlike for improving the re-ranking accuracy. Consequently, it is reasonable to assume the latent subspace enjoys the low-rank property and thus the subspace recovery can be achieved via the low-rank modeling accordingly. In addition, since the real-world data are usually partially contaminated, we employ `2;1-norm based sparsity constraint to appropriately model the sample-specific mapping noise for enhancing the model robustness. In order to produce discriminative representations, we encode a similarity preserving term in our multi-view embedding framework. As a result, the sample separability is maximally maintained in the latent subspace with sufficient discriminative power. The extensive evaluations on public landmark benchmarks demonstrate the efficacy and superiority of the proposed method."
934,,A Survey of Multi-View Representation Learning.,"Yingming Li,Ming Yang 0012,Zhongfei Zhang",https://doi.org/10.1109/TKDE.2018.2872063,TKDE,2019,"Correlation,Learning systems,Machine learning,Markov random fields,Neural networks,Data models,Kernel","Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then, from the perspective of representation fusion, we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications."
935,,An Efficient Method for High Quality and Cohesive Topical Phrase Mining.,"Bing Li 0002,Xiaochun Yang 0001,Rui Zhou 0001,Bin Wang 0015,Chengfei Liu,Yanchun Zhang",https://doi.org/10.1109/TKDE.2018.2823758,TKDE,2019,"Data mining,Gaussian mixture model,Support vector machines,Terminology,Natural language processing,Social network services","A phrase is a natural, meaningful, and essential semantic unit. In topic modeling, visualizing phrases for individual topics is an effective way to explore and understand unstructured text corpora. However, from phrase quality and topical cohesion perspectives, the outcomes of existing approaches remain to be improved. Usually, the process of topical phrase mining is twofold: phrase mining and topic modeling. For phrase mining, existing approaches often suffer from order sensitive and inappropriate segmentation problems, which make them often extract inferior quality phrases. For topic modeling, traditional topic models do not fully consider the constraints induced by phrases, which may weaken the cohesion. Moreover, existing approaches often suffer from losing domain terminologies since they neglect the impact of domain-level topical distribution. In this paper, we propose an efficient method for high quality and cohesive topical phrase mining. A high quality phrase should satisfy frequency, phraseness, completeness, and appropriateness criteria. In our framework, we integrate quality guaranteed phrase mining method, a novel topic model incorporating the constraint of phrases, and a novel document clustering method into an iterative framework to improve both phrase quality and topical cohesion. We also describe efficient algorithmic designs to execute these methods efficiently. The empirical verification demonstrates that our method outperforms the state-of-the-art methods from the aspects of both interpretability and efficiency."
936,,Bounded Approximate Query Processing.,"Kaiyu Li,Yong Zhang 0002,Guoliang Li 0001,Wenbo Tao,Ying Yan",https://doi.org/10.1109/TKDE.2018.2877362,TKDE,2019,"Query processing,Finance,Big Data,Data acquisition,Histograms,Computer science","OLAP is a core functionality in database systems and the performance is crucial to enable on-time decisions. However, OLAP queries are rather time consuming, especially on large datasets, and traditional exact solutions usually cannot meet the high-performance requirement. Recently, approximate query processing (AQP) has been proposed to enable approximate OLAP. However, existing AQP methods have some limitations. First, they may involve unacceptable errors on skewed data (e.g., long-tail distribution). Second, they require to store large amount of data and have no significant performance improvement. Third, they only support a small subset of SQL aggregation queries. To overcome these limitations, we propose a bounded approximate query processing framework BAQ. Given a predefined error bound and a set of queries, BAQ judiciously selects high-quality samples from the data to generate a unified synopsis offline, and then uses the synopsis to answer online queries. Compared with existing methods, BAQ has the following salient features. (1) BAQ does not need to generate a synopsis for each query while it only generates a unified synopsis, and thus BAQ has much smaller synopsis. (2) BAQ achieves much smaller error than existing studies. Specifically, BAQ can provide deterministic approximate results (i.e., the estimated query results must be within the error bound with 100 percent confidence) for SQL aggregation queries that do not contain selection conditions on numerical columns. For queries with selection conditions on numerical columns, we propose effective grouping-based techniques and the estimated results are also within the error bound in practice. Experimental results on both real and synthetic datasets show that BAQ significantly outperforms state-of-the-art approaches. For example, on a Microsoft production dataset (a real dataset with synthetic queries), BAQ has 10-100× improvement on synopsis size and 10-100× improvement on the error compared with state-of-the-art algorithms."
937,,"Read, Watch, Listen, and Summarize - Multi-Modal Summarization for Asynchronous Text, Image, Audio and Video.","Haoran Li 0001,Junnan Zhu,Cong Ma,Jiajun Zhang,Chengqing Zong",https://doi.org/10.1109/TKDE.2018.2848260,TKDE,2019,"Visualization,Multimedia communication,Streaming media,Feature extraction,Natural language processing,Motion pictures,Data mining","Automatic text summarization is a fundamental natural language processing (NLP) application that aims to condense a source text into a shorter version. The rapid increase in multimedia data transmission over the Internet necessitates multi-modal summarization (MMS) from asynchronous collections of text, image, audio, and video. In this work, we propose an extractive MMS method that unites the techniques of NLP, speech processing, and computer vision to explore the rich information contained in multi-modal data and to improve the quality of multimedia news summarization. The key idea is to bridge the semantic gaps between multi-modal content. Audio and visual are main modalities in the video. For audio information, we design an approach to selectively use its transcription and to infer the salience of the transcription with audio signals. For visual information, we learn the joint representations of text and images using a neural network. Then, we capture the coverage of the generated summary for important visual information through text-image matching or multi-modal topic modeling. Finally, all the multi-modal aspects are considered to generate a textual summary by maximizing the salience, non-redundancy, readability, and coverage through the budgeted optimization of submodular functions. We further introduce a publicly available MMS corpus in English and Chinese.1 The experimental results obtained on our dataset demonstrate that our methods based on image matching and image topic framework outperform other competitive baseline methods."
938,,Image Co-Segmentation via Locally Biased Discriminative Clustering.,"Xianpeng Liang,Di Wu 0030,De-Shuang Huang",https://doi.org/10.1109/TKDE.2019.2911942,TKDE,2019,"Image segmentation,Proposals,Feature extraction,Task analysis,Markov random fields,Labeling,Image color analysis","Object co-segmentation aims at simultaneously extracting common objects appeared in multiple images. In this paper, we propose a novel object co-segmentation method in which we formulate the image co-segmentation as a locally biased discriminative clustering problem. Specifically, we add a seed vector and a constraint term into the framework of discriminative clustering to constrain the segmentation result bias to this seed vector. In order to deal with the co-segmentation problem with indefinite number of common foreground objects, we design a Markov Random Field (MRF) based method to extract common objects prior. The extracted common objects prior is then added into the discriminative clustering and treated as the seed vector to constrain the segmentation result biased to it. Under the supervision of this prior, the segmentation result can find more common objects and becomes more complete and meaningful. In addition, we present a new segmentation process where we alternately update the MRF-based model and discriminative clustering to refine the common objects prior and the segmentation result. Finally, we test the proposed method on three benchmark datasets, iCoseg, Coseg-Rep, and THUR15K. The experimental results demonstrate that the proposed method outperforms other state-of-the-art methods."
939,,Collaboratively Tracking Interests for User Clustering in Streams of Short Texts.,"Shangsong Liang,Emine Yilmaz,Evangelos Kanoulas",https://doi.org/10.1109/TKDE.2018.2832211,TKDE,2019,"Collaboration,Clustering algorithms,Heuristic algorithms,Context modeling,Twitter,Analytical models,Discrete cosine transforms","In this paper, we aim at tackling the problem of user clustering in the context of their published short text streams. Clustering users by short text streams is more challenging than in the case of long documents associated with them as it is difficult to track users' dynamic interests in streaming sparse data. To obtain better user clustering performance, we propose two user collaborative interest tracking models that aim at tracking changes of each user's dynamic topic distributions in collaboration with their followees' dynamic topic distributions, based both on the content of current short texts and the previously estimated distributions. Our models can be either short-term or long-term dependency topic models. Short-term dependency model collaboratively tracks users' interests based on users' topic distributions at the previous time period only, whereas long-term dependency model collaboratively tracks users' interests based on users' topic distributions at multiple time periods in the past. We also propose two collapsed Gibbs sampling algorithms for collaboratively inferring users' dynamic interests for their clustering in our short-term and long-term dependency topic models, respectively. We evaluate our proposed models via a benchmark dataset consisting of Twitter users and their tweets. Experimental results validate the effectiveness of our proposed models that integrate both users' and their collaborative interests for user clustering by short text streams."
940,1,Generalized Gaussian Mechanism for Differential Privacy.,Fang Liu 0006,https://doi.org/10.1109/TKDE.2018.2845388,TKDE,2019,"Privacy,Probabilistic logic,Sensitivity,Upper bound,Dispersion","Assessment of disclosure risk is of paramount importance in data privacy research and applications. The concept of differential privacy (DP) formalizes privacy in probabilistic terms and provides a robust concept for privacy protection. Practical applications of DP involve development of DP mechanisms to release data at a pre-specified privacy budget. In this paper, we generalize the widely used Laplace mechanism to the family of generalized Gaussian (GG) mechanism based on the 
<inline-formula><tex-math notation=""LaTeX"">$l_p$</tex-math></inline-formula>
 global sensitivity of statistical queries. We explore the theoretical requirement for the GG mechanism to reach DP at prespecified privacy parameters, and investigate the connections and differences between the GG mechanism and the Exponential mechanism based on the GG distribution. We also present a lower bound on the scale parameter of the Gaussian mechanism of 
<inline-formula><tex-math notation=""LaTeX"">$(\epsilon, \delta)$</tex-math></inline-formula>
-probabilistic DP as a special case of the GG mechanism, and compare the utility of sanitized results in the tail probability and dispersion between the Gaussian and Laplace mechanisms. Lastly, we apply the GG mechanism in three experiments and compare the accuracy of sanitized results in the 
<inline-formula><tex-math notation=""LaTeX"">$l_1$</tex-math></inline-formula>
 distance and Kullback-Leibler divergence, and examine the prediction power of a SVM classifier constructed with the sanitized data relative to the original results."
941,,A Contrast Metric for Fraud Detection in Rich Graphs.,"Shenghua Liu,Bryan Hooi,Christos Faloutsos",https://doi.org/10.1109/TKDE.2018.2876531,TKDE,2019,"Image edge detection,Topology,Measurement,Robustness,Time series analysis,Tensile stress,Manuals","How can we detect fraud in a big graph with rich properties, as online fraudsters invest more resources, including purchasing large pools of fake user accounts and dedicated IPs, to hide their fraudulent attacks? To achieve robustness, existing approaches detected dense sub-graphs as suspicious patterns in an unsupervised way, such as average degree maximization. However, such approaches suffer from the bias of including more nodes than necessary, resulting in lower accuracy and increased need for manual verification. Therefore, we propose HoloScope, which introduces a novel metric “contrast suspiciousness” integrating information from graph topology and spikes to more accurately detect fraudulent users and objects. Contrast suspiciousness dynamically emphasizes the contrasting patterns between fraudsters and normal users, making HoloScope capable of distinguishing the synchronized and strange behaviors of fraudsters by means of topology, bursts and drops, and rating scores. In addition, we provide theoretical bounds for how much this method increases the time cost needed for fraudsters to conduct adversarial attacks. Moreover, HoloScope has a concise framework and sub-quadratic time complexity, making the algorithm reproducible and scalable. In extensive experiments, HoloScope achieved significant accuracy improvements on real data with injected labels and true labels, when compared with state-of-the-art fraud detection methods."
942,,Efficient Scheduling of Scientific Workflows Using Hot Metadata in a Multisite Cloud.,"Ji Liu 0003,Luis Pineda-Morales,Esther Pacitti,Alexandru Costan,Patrick Valduriez,Gabriel Antoniu,Marta Mattoso",https://doi.org/10.1109/TKDE.2018.2867857,TKDE,2019,"Metadata,Cloud computing,Task analysis,Servers,Distributed databases,Scheduling algorithms,Data processing","Large-scale, data-intensive scientific applications are often expressed as scientific workflows (SWfs). In this paper, we consider the problem of efficient scheduling of a large SWf in a multisite cloud, i.e., a cloud with geo-distributed cloud data centers (sites). The reasons for using multiple cloud sites to run a SWf are that data is already distributed, the necessary resources exceed the limits at a single site, or the monetary cost is lower. In a multisite cloud, metadata management has a critical impact on the efficiency of SWf scheduling as it provides a global view of data location and enables task tracking during execution. Thus, it should be readily available to the system at any given time. While it has been shown that efficient metadata handling plays a key role in performance, little research has targeted this issue in multisite cloud. In this paper, we propose to identify and exploit hot metadata (frequently accessed metadata) for efficient SWf scheduling in a multisite cloud, using a distributed approach. We implemented our approach within a scientific workflow management system, which shows that our approach reduces the execution time of highly parallel jobs up to 64 percent and that of the whole SWfs up to 55 percent."
943,,Structure-Preserved Unsupervised Domain Adaptation.,"Hongfu Liu,Ming Shao,Zhengming Ding,Yun Fu 0001",https://doi.org/10.1109/TKDE.2018.2843342,TKDE,2019,"Optimization,Clustering algorithms,Unsupervised learning,Pattern clustering,Data mining","Domain adaptation has been a primal approach to addressing the issues by lack of labels in many data mining tasks. Although considerable efforts have been devoted to domain adaptation with promising results, most existing work learns a classifier on a source domain and then predicts the labels for target data, where only the instances near the boundary determine the hyperplane and the whole structure information is ignored. Moreover, little work has been done regarding to multi-source domain adaptation. To that end, we develop a novel unsupervised domain adaptation framework, which ensures the whole structure of source domains is preserved to guide the target structure learning in a semi-supervised clustering fashion. To our knowledge, this is the first time when the domain adaptation problem is re-formulated as a semi-supervised clustering problem with target labels as missing values. Furthermore, by introducing an augmented matrix, a non-trivial solution is designed, which can be exactly mapped into a K-means-like optimization problem with modified distance function and update rule for centroids in an efficient way. Extensive experiments on several widely-used databases show the substantial improvements of our proposed approach over the state-of-the-art methods."
944,,Feature Selection with Unsupervised Consensus Guidance.,"Hongfu Liu,Ming Shao,Yun Fu 0001",https://doi.org/10.1109/TKDE.2018.2875712,TKDE,2019,"Feature extraction,Data mining,Clustering algorithms,Noise measurement,Optimization,Task analysis,Mutual information","Most of the unsupervised feature selection methods employ pseudo labels generated by clustering to guide the feature selection; however, noisy and irrelevant features degrade the cluster structure, which is ineffective to supervise feature selection. In light of this, we propose the Consensus Guided Unsupervised Feature Selection (CGUFS) framework, which introduces consensus clustering to generate pseudo labels for feature selection. Generally speaking, multiple diverse basic partitions are generated from the data and the consensus clustering is employed to provide the high-quality and robust partition to guide the feature selection in a one-step framework. In addition, complex constraints such as non-negative are removed due to the crisp indicators of consensus clustering. Based on the CGUFS framework, two formulations are put forward by using the utility function and co-association matrix, respectively, and we propose the (weighted) K-means-like optimization solution for efficient solutions with theoretical supports. Moreover, we extend the CGUFS framework to handle multi-view data feature selection. Extensive experiments on several singleview and multi-view data mining data sets in different domains demonstrate that our methods outperform the most recent state-ofthe-art work in terms of effectiveness and efficiency. Some important impact factors and model parameters within CGUFS are thoroughly discussed for practical use."
945,,Social-Aware Sequential Modeling of User Interests - A Deep Learning Approach.,"Chi Harold Liu,Jie Xu,Jian Tang 0008,Jon Crowcroft",https://doi.org/10.1109/TKDE.2018.2875006,TKDE,2019,"Predictive models,Machine learning,Data models,Correlation,Knowledge engineering,Recurrent neural networks,Social network services","In this paper, we propose to leverage the emerging deep learning techniques for sequential modeling of user interests based on big social data, which takes into account influence of their social circles. First, we present a preliminary analysis for two popular big datasets from Yelp and Epinions. We show statistically sequential actions of all users and their friends, and discover both temporal autocorrelation and social influence on decision making, which motivates our design. Then, we present a novel hybrid deep learning model, Social-Aware Long Short-Term Memory (SA-LSTM), for predicting the types of item/PoIs that a user will likely buy/visit next, which features stacked LSTMs for sequential modeling and an autoencoder-based deep model for social influence modeling. Moreover, we show that SA-LSTM supports end-to-end training. We conducted extensive experiments for performance evaluation using the two real datasets from Yelp and Epinions. The experimental results show that (1) the proposed deep model significantly improves prediction accuracy compared to widely used baseline methods; (2) the proposed social influence model works effectively; and (3) going deep does help improve prediction accuracy but a not-so-deep deep structure leads to the best performance."
946,,Secure and Efficient Skyline Queries on Encrypted Data.,"Jinfei Liu,Juncheng Yang,Li Xiong 0001,Jian Pei",https://doi.org/10.1109/TKDE.2018.2857471,TKDE,2019,"Servers,Protocols,Cloud computing,Encryption,Query processing","Outsourcing data and computation to cloud server provides a cost-effective way to support large scale data storage and query processing. However, due to security and privacy concerns, sensitive data (e.g., medical records) need to be protected from the cloud server and other unauthorized users. One approach is to outsource encrypted data to the cloud server and have the cloud server perform query processing on the encrypted data only. It remains a challenging task to support various queries over encrypted data in a secure and efficient way such that the cloud server does not gain any knowledge about the data, query, and query result. In this paper, we study the problem of secure skyline queries over encrypted data. The skyline query is particularly important for multi-criteria decision making but also presents significant challenges due to its complex computations. We propose a fully secure skyline query protocol on data encrypted using semantically-secure encryption. As a key subroutine, we present a new secure dominance protocol, which can be also used as a building block for other queries. Furthermore, we demonstrate two optimizations, data partitioning and lazy merging, to further reduce the computation load. Finally, we provide both serial and parallelized implementations and empirically study the protocols in terms of efficiency and scalability under different parameter settings, verifying the feasibility of our proposed solutions."
947,,A Study on Big Knowledge and Its Engineering Issues.,"Ruqian Lu,Xiaolong Jin,Songmao Zhang,Meikang Qiu,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2018.2866863,TKDE,2019,"Big Data,Knowledge engineering,Data mining,Knowledge based systems,Encyclopedias,Computer science","After entering the big data era, a new term of `big knowledge' has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed."
948,,Learning under Concept Drift - A Review.,"Jie Lu 0001,Anjin Liu,Fan Dong,Feng Gu,João Gama 0001,Guangquan Zhang 0001",https://doi.org/10.1109/TKDE.2018.2876857,TKDE,2019,"Machine learning,Market research,Data analysis,Big Data,Mobile handsets,Data models,Cameras","Concept drift describes unforeseeable changes in the underlying distribution of streaming data overtime. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift."
949,,Fast Failure Recovery in Vertex-Centric Distributed Graph Processing Systems.,"Wei Lu 0015,Yanyan Shen,Tongtong Wang,Meihui Zhang,H. V. Jagadish,Xiaoyong Du 0001",https://doi.org/10.1109/TKDE.2018.2843361,TKDE,2019,"System recovery,Checkpointing,Parallel processing,Big Data,Graph theory","There is a growing need for distributed graph processing systems to have many more compute nodes processing graph-based Big Data applications, which, however, increases the chance of node failures. To address the issue, we propose a novel recovery scheme to accelerate the recovery process by parallelizing the recomputation. Once a failure occurs, all recomputations are confined to subgraphs that originally reside in the failed compute nodes. When the recovery starts, these subgraphs are reassigned to another set of compute nodes, where the recomputation over these subgraphs are conducted in parallel. To minimize the recovery latency, we also develop a reassignment strategy, from these subgraphs to the replaced compute nodes, by properly leveraging the computation and communication cost. We integrate the proposed recovery scheme into Giraph system, a widely used graph processing system. The experimental results over a variety of real graph datasets demonstrate that our proposed recovery scheme outperforms existing recovery methods by up to 30x on a cluster of 40 compute nodes."
950,,TourSense - A Framework for Tourist Identification and Analytics Using Transport Data.,"Yu Lu 0003,Huayu Wu 0001,Xin Liu,Penghe Chen",https://doi.org/10.1109/TKDE.2019.2894131,TKDE,2019,"Public transportation,Social network services,Urban areas,Sociology,Statistics,User interfaces","We advocate for and present TourSense, a framework for tourist identification and preference analytics using city-scale transport data (bus, subway, etc.). Our work is motivated by the observed limitations of utilizing traditional data sources (e.g., social media data and survey data) that commonly suffer from the limited coverage of tourist population and unpredictable information delay. TourSense demonstrates how the transport data can overcome these limitations and provide better insights for different stakeholders, typically including tour agencies, transport operators, and tourists themselves. Specifically, we first propose a graph-based iterative propagation learning algorithm to recognize tourists from public commuters. Taking advantage of the trace data from the identified tourists, we then design a tourist preference analytics model to learn and predict their next tour, where an interactive user interface is implemented to ease the information access and gain the insights from the analytics results. Experiments with real-world datasets (from over 5.1 million commuters and their 462 million trips) show the promise and effectiveness of the proposed framework: the Macro and Micro F1 scores of the tourist identification system achieve 0.8549 and 0.7154, respectively, whereas the tourist preference analytics system improves the baselines by at least 23.53 and 11.44 percent in terms of precision and recall."
951,,LPANNI - Overlapping Community Detection Using Label Propagation in Large-Scale Complex Networks.,"Meilian Lu,Zhenglin Zhang,Zhihe Qu,Yu Kang",https://doi.org/10.1109/TKDE.2018.2866424,TKDE,2019,"Complex networks,Time complexity,Detection algorithms,Clustering algorithms,Measurement,Stability analysis","Overlapping community structure is a significant feature of large-scale complex networks. Some existing community detection algorithms cannot be applied to large-scale complex networks due to their high time or space complexity. Label propagation algorithms were proposed for detecting communities in large-scale networks because of their linear time complexity, however most of which can only detect non-overlapping communities, or the results are inaccurate and unstable. Aimed at the defects, we proposed an improved overlapping community detection algorithm, LPANNI (Label Propagation Algorithm with Neighbor Node Influence), which detects overlapping community structures by adopting fixed label propagation sequence based on the ascending order of node importance and label update strategy based on neighbor node influence and historical label preferred strategy. Extensive experimental results in both real networks and synthetic networks show that, LPANNI can significantly improve the accuracy and stability of community detection algorithms based on label propagation in large-scale complex networks. Meanwhile, LPANNI can detect overlapping community structures in large-scale complex networks under linear time complexity."
952,,Scalable Linear Algebra on a Relational Database System.,"Shangyu Luo,Zekai J. Gao,Michael N. Gubanov,Luis Leopoldo Perez,Christopher M. Jermaine",https://doi.org/10.1109/TKDE.2018.2827988,TKDE,2019,"Linear algebra,Relational databases,Database systems,Buildings,Arrays,Structured Query Language","As data analytics has become an important application for modern data management systems, a new category of data management system has appeared recently: the scalable linear algebra system. In this paper, we argue that a parallel or distributed database system is actually an excellent platform upon which to build such functionality. Most relational systems already have support for cost-based optimization-which is vital to scaling linear algebra computations-and it is well-known how to make relational systems scale. We show that by making just a few changes to a parallel/distributed relational database system, such a system can be a competitive platform for scalable linear algebra. Taken together, our results should at least raise the possibility that brand new systems designed from the ground up to support scalable linear algebra are not absolutely necessary, and that such systems could instead be built on top of existing relational technology. Our results also suggest that if scalable linear algebra is to be added to a modern dataflow platform such as Spark, they should be added on top of the system's more structured (relational) data abstractions, rather than being constructed directly on top of the system's raw dataflow operators."
953,,Near-accurate Multiset Reconciliation.,"Lailong Luo,Deke Guo,Xiang Zhao 0002,Jie Wu 0001,Ori Rottenstreich,Xueshan Luo",https://doi.org/10.1109/TKDE.2018.2849997,TKDE,2019,"Data structures,Proposals,Probabilistic logic,Peer-to-peer computing,Synchronization,Cloud computing,Force","The mission of set reconciliation (also called set synchronization) is to identify those elements which appear only in exactly one of two given sets. In this paper, we extend the set reconciliation problem into three design rationales: (i) multiset support; (ii) near 100 percent reconciliation accuracy; and (iii) communication-friendly and time-saving. These three rationales, if realized, will lead to unprecedented benefits for the set reconciliation paradigm. Generally, prior reconciliation methods are mainly designed for simple sets and thus remain inapplicable for multisets. Methods based on probabilistic data structures, e.g., the Counting Bloom Filter (CBF), support efficient representation, and multiplicity queries. Based on these probabilistic data structures, approximate multiset reconciliation can be enabled. However, they often cannot achieve a statisfying accuracy, due to potential hash collisions. The reconciliations enabled by logs or lists incur high time-complexity and communication overhead. Therefore, existing reconciliation methods, fail to realize the three rationales simultaneously. To this end, we redesign Trie and Fenwick Tree (FT), to near-accurately represent and reconcile two types of multisets that we refer to as unsorted and sorted multisets, respectively. Moreover, to further reduce the communication overhead during the reconciliation process, we design a partial transmission strategy when exchanging two Tries or FTs. Comprehensive evaluations are conducted to quantify the performance of our proposals. The trace-driven evaluations demonstrate that Trie and FT achieve near-accurate multiset reconciliation, with 4.31 and 2.96 times faster than the CBF-based method, respectively. The simulations based on synthetic datasets further indicate that our proposals outperform the CBF-based method in terms of accuracy and communication overhead at most time."
954,,Latent Ability Model - A Generative Probabilistic Learning Framework for Workforce Analytics.,"Zhiling Luo,Ling Liu 0001,Jianwei Yin,Ying Li 0001,Zhaohui Wu 0001",https://doi.org/10.1109/TKDE.2018.2848658,TKDE,2019,"Data models,Analytical models,Probabilistic logic,Task analysis,Government,Statistical learning","As more business workflow systems are being deployed in modern enterprises and organizations, more employee-activity log data are being collected and analyzed. In this paper, we develop a latent ability model (LAM) as a generative probabilistic learning framework for workforce analytics over employee-activity logs. The LAM development is novel in three aspects. First, we introduce the concept of latent ability variables to model hidden relations between employees and activities in terms of job performance, such as the set of skills provided by an employee and the set of skills required by an activity, and how well they matchup in employee-activity assignment. Second, we construct the latent ability model by learning latent ability parameters from the employee-activity log data using expectation-maximization and gradient descent. Finally, we leverage LAM to build inference and prediction models for employee performance prediction, employee ability comparison, and employee-activity matchup quality estimation. We evaluate the accuracy and efficiency of our approach using real log datasets collected from a workflow system deployed in the government of the city of Hangzhou, China, which consists of 5,287,621 log records over two years involving 744 activities and 1,725 employees. We show that LAM approach outperforms existing representative methods in both accuracy and efficiency."
955,,Supergraph Search in Graph Databases via Hierarchical Feature-Tree.,"Bingqing Lyu,Lu Qin,Xuemin Lin 0001,Lijun Chang,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2018.2833124,TKDE,2019,"Search problems,Query processing,Testing,Indexing,Heuristic algorithms,Biology","Supergraph search is a fundamental problem in graph databases that is widely applied in many application scenarios. Given a graph database and a query-graph, supergraph search retrieves all data-graphs contained in the query-graph from the graph database. Most existing solutions for supergraph search follow the pruning-and-verification framework, which prune false answers based on features in the pruning phase and perform subgraph isomorphism testings on the remaining graphs in the verification phase. However, they are not scalable to handle large-sized data-graphs and query-graphs due to three drawbacks. First, they rely on a frequent subgraph mining algorithm to select features which is expensive and cannot generate large features. Second, they require a costly verification phase. Third, they process features in a fixed order without considering their relationships to the query-graph. In this paper, we address the three drawbacks and propose new indexing and query processing algorithms. In indexing, we select features directly from the data-graphs without expensive frequent subgraph mining. The features form a feature-tree that contains all-sized features and both the cost sharing and pruning power of the features are considered. In query processing, we propose a new algorithm, where the order to process features is query-dependent by considering both the cost sharing and the pruning power. We explore two optimization strategies to further improve the algorithm efficiency. The first strategy applies a lightweight graph compression technique and the second strategy optimizes the inclusion of answers. We further introduce how to efficiently maintain the index incrementally when the graph database is updated dynamically. Moreover, we propose an approximation approach to significantly reduce the computational cost for large data-graphs and/or query-graphs while preserving a high result quality. Finally, we conduct extensive performance studies on two real large datasets to demonstrate the efficiency and effectiveness of our algorithms."
956,,Community Detection in Multi-Layer Networks Using Joint Nonnegative Matrix Factorization.,"Xiaoke Ma,Di Dong,Quan Wang 0006",https://doi.org/10.1109/TKDE.2018.2832205,TKDE,2019,"Clustering algorithms,Cancer,Proteins,Social network services,Image edge detection,Complex systems,Kernel","Many complex systems are composed of coupled networks through different layers, where each layer represents one of many possible types of interactions. A fundamental question is how to extract communities in multi-layer networks. The current algorithms either collapses multi-layer networks into a single-layer network or extends the algorithms for single-layer networks by using consensus clustering. However, these approaches have been criticized for ignoring the connection among various layers, thereby resulting in low accuracy. To attack this problem, a quantitative function (multi-layer modularity density) is proposed for community detection in multi-layer networks. Afterward, we prove that the trace optimization of multi-layer modularity density is equivalent to the objective functions of algorithms, such as kernel 
<inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula>
-means, nonnegative matrix factorization (NMF), spectral clustering and multi-view clustering, for multi-layer networks, which serves as the theoretical foundation for designing algorithms for community detection. Furthermore, a 
<u>S</u>
emi-
<u>S</u>
upervised 
<u>j</u>
oint 
<u>N</u>
onnegative 
<u>M</u>
atrix 
<u>F</u>
actorization algorithm (
<i>S2-jNMF</i>
) is developed by simultaneously factorizing matrices that are associated with multi-layer networks. Unlike the traditional semi-supervised algorithms, the partial supervision is integrated into the objective of the S2-jNMF algorithm. Finally, through extensive experiments on both artificial and real world networks, we demonstrate that the proposed method outperforms the state-of-the-art approaches for community detection in multi-layer networks."
957,,Order-Sensitive Imputation for Clustered Missing Values.,"Qian Ma 0003,Yu Gu 0002,Wang-Chien Lee,Ge Yu 0001",https://doi.org/10.1109/TKDE.2018.2822662,TKDE,2019,"Diabetes,Machine learning algorithms,Clustering algorithms,Heart,Approximation algorithms,Air quality,Diseases","The issue of missing values (MVs) has appeared widely in real-world datasets and hindered the use of many statistical or machine learning algorithms for data analytics due to their incompetence in handling incomplete datasets. To address this issue, several MV imputation algorithms have been developed. However, these approaches do not perform well when most of the incomplete tuples are clustered with each other, coined here as the Clustered Missing Values Phenomenon, which attributes to the lack of sufficient complete tuples near an MV for imputation. In this paper, we propose the Order-Sensitive Imputation for Clustered Missing values (OSICM) framework, in which missing values are imputed sequentially such that the values filled earlier in the process are also used for later imputation of other MVs. Obviously, the order of imputations is critical to the effectiveness and efficiency of OSICM framework. We formulate the searching of the optimal imputation order as an optimization problem, and show its NP-hardness. Furthermore, we devise an algorithm to find the exact optimal solution and propose two approximate/heuristic algorithms to trade off effectiveness for efficiency. Finally, we conduct extensive experiments on real and synthetic datasets to demonstrate the superiority of our OSICM framework."
958,,Scalable Interactive Dynamic Graph Clustering on Multicore CPUs.,"Son T. Mai,Sihem Amer-Yahia,Ira Assent,Mathias Skovgaard Birk,Martin Storgaard Dieu,Jon Jacobsen,Jesper Kristensen",https://doi.org/10.1109/TKDE.2018.2828086,TKDE,2019,"Graph theory,Parallel algorithms,Iterative methods,Pattern classification,Microprocessor chips","The structural graph clustering algorithm SCAN is a fundamental technique for managing and analyzing graph data. However, its high runtime remains a computational bottleneck, which limits its applicability. In this paper, we propose a novel interactive approach for tackling this problem on multicore CPUs. Our algorithm, called anySCAN, iteratively processes vertices in blocks. The acquired results are merged into an underlying cluster structure consisting of the so-called super-nodes for building clusters. During its runtime, anySCAN can be suspended for examining intermediate results and resumed for finding better results at arbitrary time points, making it an anytime algorithm which is capable of handling very large graphs in an interactive way and under arbitrary time constraints. Moreover, its block processing scheme allows the design of a scalable parallel algorithm on shared memory architectures such as multicore CPUs for speeding up the algorithm further at each iteration. Consequently, anySCAN uniquely is a both interactive and work-efficient parallel algorithm. We further introduce danySCAN an efficient bulk update scheme for anySCAN on dynamic graphs in which the clusters are updated in bulks and in a parallel interactive scheme. Experiments are conducted on very large real graph datasets for demonstrating the performance of anySCAN. They show its ability to acquire very good approximate results early, leading to orders of magnitude speedup compared to SCAN and its variants. Moreover, it scales very well with the number of threads when dealing with both static and dynamic graphs."
959,,Nonnegative Matrix Factorization with Side Information for Time Series Recovery and Prediction.,"Jiali Mei,Yohann de Castro,Yannig Goude,Jean-Marc Azaïs,Georges Hébrail",https://doi.org/10.1109/TKDE.2018.2839678,TKDE,2019,"Matrix decomposition,Time series analysis,Prediction algorithms,Signal processing algorithms,Aggregates,Estimation,Kernel","Motivated by the recovery and prediction of electricity consumption time series, we extend Nonnegative Matrix Factorization to take into account external features as side information. We consider general linear measurement settings, and propose a framework which models non-linear relationships between external features and the response variable. We extend previous theoretical results to obtain a sufficient condition on the identifiability of NMF with side information. Based on the classical Hierarchical Alternating Least Squares (HALS) algorithm, we propose a new algorithm (HALSX, or Hierarchical Alternating Least Squares with eXogeneous variables) which estimates NMF in this setting. The algorithm is validated on both simulated and real electricity consumption datasets as well as a recommendation system dataset, to show its performance in matrix recovery and prediction for new rows and columns."
960,,An Efficient Algorithm to Compute a Quantum Probability Space.,Massimo Melucci,https://doi.org/10.1109/TKDE.2018.2863709,TKDE,2019,"Quantum computing,Random variables,Extraterrestrial measurements,Uncertainty,Probabilistic logic,Search engines","Learning algorithms based on probability organize the observed data in subsets corresponding to binary variables. In this paper, we address the problem of estimating one probability space given a set of observed data about n variables or properties. One problem with estimating one single probability space is the exponential number of events. Approximation is one approach to addressing the problem of the exponential order of the number of events. Alternatively to approximation, we change paradigm - from classical, set-based probability spaces based on sets to quantum probability spaces based on vector subspaces. By changing paradigm, we leverage quantum probability and present an efficient algorithm to calculate a Quantum Probability Space (QPS) in only O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup>
) steps."
961,,A Comparative Study of Ontology Matching Systems via Inferential Statistics.,"Majid Mohammadi 0001,Wout Hofman,Yao-Hua Tan",https://doi.org/10.1109/TKDE.2018.2842019,TKDE,2019,"Ontologies,Benchmark testing,Task analysis,Robustness,Statistical analysis,Geoscience","Ontology matching systems are typically compared by comparing their average performances over multiple datasets. However, this paper examines the alignment systems using statistical inference since averaging is statistically unsafe and inappropriate. The statistical tests for comparison of two or multiple alignment systems are theoretically and empirically reviewed. For comparison of two systems, the Wilcoxon signed-rank and McNemar's mid-p and asymptotic tests are recommended due to their robustness and statistical safety in different circumstances. The Friedman and Quade tests with their corresponding post-hoc procedures are studied for comparison of multiple systems, and their [dis]advantages are discussed. The statistical methods are then applied to benchmark and multifarm tracks from the ontology matching evaluation initiative (OAEI) 2015 and their results are reported and visualized by critical difference diagrams."
962,,Scaling the Construction of Wavelet Synopses for Maximum Error Metrics.,"Ioannis Mytilinis,Dimitrios Tsoumakos,Nectarios Koziris",https://doi.org/10.1109/TKDE.2018.2867185,TKDE,2019,"Wavelet transforms,Arrays,Heuristic algorithms,Measurement,Query processing,Big Data","Modern analytics involve computations over enormous numbers of data records. The volume of data and the stringent response-time requirements place increasing emphasis on the efficiency of approximate query processing. A major challenge over the past years has been the construction of synopses that provide a deterministic quality guarantee, often expressed in terms of a maximum error metric. By approximating sharp discontinuities, wavelet decomposition has proved to be a very effective tool for data reduction. However, existing wavelet thresholding schemes that minimize maximum error metrics are constrained with impractical complexities for large datasets. Furthermore, they cannot efficiently handle the multi-dimensional version of the problem. In order to provide a practical solution, we develop parallel algorithms that take advantage of key-properties of the wavelet decomposition and allocate tasks to multiple workers. To that end, we present (i) a general framework for the parallelization of existing dynamic programming algorithms, (ii) a parallel version of one such DP algorithm, and (iii) two highly efficient distributed greedy algorithms that can deal with data of arbitrary dimensionality. Our extensive experiments on both real and synthetic datasets over Hadoop show that the proposed algorithms achieve linear scalability and superior running-time performance compared to their centralized counterparts."
963,1,A Utility-Optimized Framework for Personalized Private Histogram Estimation.,"Yiwen Nie,Wei Yang 0011,Liusheng Huang,Xike Xie,Zhenhua Zhao,Shaowei Wang 0003",https://doi.org/10.1109/TKDE.2018.2841360,TKDE,2019,"Privacy,Estimation,Histograms,Optimization,Recycling","Recently, local differential privacy (LDP), as a strong and practical notion, has been applied to deal with privacy issues in data collection. However, existing LDP-based strategies mainly focus on utility optimization at a single privacy level while ignoring various privacy preferences of data providers and multilevel privacy demands for statistics. In this paper, we for the first time propose a framework to optimize the utility of histogram estimation with these two privacy requirements. To clarify the goal of privacy protection, we personalize the traditional definition of LDP. We design two independent approaches to minimize the utility loss: Advanced Combination, which composes multilevel results for utility optimization, and Data Recycle with Personalized Privacy, which enlarges the sample size for an estimation. We demonstrate their effectiveness on privacy and utility, respectively. Moreover, we embed these approaches within a Recycle and Combination Framework and prove that the framework stably achieves the optimal utility by quantifying its error bounds. On real-world datasets, our approaches are experimentally validated and remarkably outperform baseline methods."
964,,Heuristic and Cost-Based Optimization for Diverse Provenance Tasks.,"Xing Niu,Raghav Kapoor,Boris Glavic,Dieter Gawlick,Zhen Hua Liu,Vasudha Krishnaswamy,Venkatesh Radhakrishnan",https://doi.org/10.1109/TKDE.2018.2827074,TKDE,2019,"Instruments,Pipelines,Algebra,Optimization,Encoding,Databases,Semantics","A well-established technique for capturing database provenance as annotations on data is to instrument queries to propagate such annotations. However, even sophisticated query optimizers often fail to produce efficient execution plans for instrumented queries. We develop provenance-aware optimization techniques to address this problem. Specifically, we study algebraic equivalences targeted at instrumented queries and alternative ways of instrumenting queries for provenance capture. Furthermore, we present an extensible heuristic and cost-based optimization framework utilizing these optimizations. Our experiments confirm that these optimizations are highly effective, improving performance by several orders of magnitude for diverse provenance tasks."
965,,Achieving Data Truthfulness and Privacy Preservation in Data Markets.,"Chaoyue Niu,Zhenzhe Zheng,Fan Wu 0006,Xiaofeng Gao,Guihai Chen",https://doi.org/10.1109/TKDE.2018.2822727,TKDE,2019,"Data privacy,Data collection,Data models,Cryptography,Data acquisition","As a significant business paradigm, many online information platforms have emerged to satisfy society's needs for person-specific data, where a service provider collects raw data from data contributors, and then offers value-added data services to data consumers. However, in the data trading layer, the data consumers face a pressing problem, i.e., how to verify whether the service provider has truthfully collected and processed data? Furthermore, the data contributors are usually unwilling to reveal their sensitive personal data and real identities to the data consumers. In this paper, we propose TPDM, which efficiently integrates Truthfulness and Privacy preservation in Data Markets. TPDM is structured internally in an Encrypt-then-Sign fashion, using partially homomorphic encryption and identity-based signature. It simultaneously facilitates batch verification, data processing, and outcome verification, while maintaining identity preservation and data confidentiality. We also instantiate TPDM with a profile matching service and a data distribution service, and extensively evaluate their performances on Yahoo! Music ratings dataset and 2009 RECS dataset, respectively. Our analysis and evaluation results reveal that TPDM achieves several desirable properties, while incurring low computation and communication overheads when supporting large-scale data markets."
966,,Addressing the Practical Limitations of Noisy-OR Using Conditional Inter-Causal Anti-Correlation with Ranked Nodes.,"Takao Noguchi,Norman E. Fenton,Martin Neil",https://doi.org/10.1109/TKDE.2018.2873314,TKDE,2019,"Noise measurement,Gaussian distribution,Covariance matrices,Security,Correlation,Bayes methods,Computer science","Numerous methods have been proposed to simplify the problem of eliciting complex conditional probability tables in Bayesian networks. One of the most popular methods -“Noisy-OR”- approximates the required relationship in many real-world situations between a set of variables that are potential causes of an effect variable. However, the Noisy-OR function has the conditional inter-causal independence (CII) property which means that `explaining away' behavior-one of the most powerful benefits of BN inference-is not present when the effect variable is observed as false. Hence, for many real-world problems where the Noisy-OR has been used or proposed, it may be deficient as an approximation of the required relationship. However, there is a very simple alternative solution, namely to define the variables as ranked nodes and to use the ranked node weighted average function. This does not have the CII property-instead, we prove it has the conditional anti-correlation property required to ensure that explaining away works in all cases. Moreover, ranked node variables are not restricted to binary states, and hence provide a more comprehensive and general solution to Noisy-OR in all cases."
967,1,Privacy Engineering for the Smart Micro-Grid.,"Ranjan Pal,Pan Hui 0001,Viktor K. Prasanna",https://doi.org/10.1109/TKDE.2018.2846640,TKDE,2019,"Data privacy,Privacy,Correlation,Monitoring,Databases,Reliability,Communication networks","In developing countries, reliable electricity access is often undermined by the absence of supply from the national power grid and/or load shedding. To alleviate this problem, smart micro-grid (SMG) networks that are small scale distributed electricity provision networks composed of individual electricity providers and consumers, are being increasingly deployed. To ensure the reliable operation of SMGs, monitoring is necessary for data collection and state estimation processes. However, highly calibrated and trustworthy smart meters that are ideally suited to perform such monitoring tasks are often costly and non-ideally suited to SMGs which operate under unreliable communication network infrastructures. As a result, SMGs are an easy target to an adversary who can very easily gain access to private information by monitoring transmission between nodes in the SMG network, and launch inference-based privacy attacks. These attacks lead to electricity theft and grid instability problems in the SMG. The widely popular differential privacy (DP) technique (a rigorous technique in the family of privacy-preserving data publishing (PPDP) techniques to mathematically guarantee the preservation of data privacy) does not address multi-attribute correlations, that are inherently exploited by an adversary in inference attacks. In this paper, we propose HIDE, an oblivious computationally efficient, and rigorous information-theoretic privacy engineering framework for datasets/databases arising in the SMG environments that robustly accounts for multi-attribute correlations while preserving data privacy in a provably optimal fashion. A salient and powerful advantage of HIDE is its ability to generate optimal utility-privacy tradeoffs (computationally efficiently) when the privacy preserving entity in the worst case might have no prior statistical information that links a user's private data with his public data."
968,,"Efficient Feature Selection via $\ell _{2, 0}$ℓ2, 0-norm Constrained Sparse Regression.","Tianji Pang,Feiping Nie 0001,Junwei Han,Xuelong Li",https://doi.org/10.1109/TKDE.2018.2847685,TKDE,2019,"Feature extraction,Linear programming,Stability criteria,Encoding,Optical filters,Transforms","Sparse regression based feature selection method has been extensively investigated these years. However, because it has a non-convex constraint, i.e., $\ell _{2,0}$ℓ2,0-norm constraint, this problem is very hard to solve. In this paper, unlike most of the other methods which only solve its slack version by introducing sparsity regularization into objective function forcibly, a novel framework is proposed by us to solve the original $\ell _{2,0}$ℓ2,0-norm constrained sparse regression based feature selection problem. We transform our objective function into Linear Discriminant Analysis (LDA) by using a new label coding method, thus enabling our model to calculate the ratio of inter-class scatter to intra-class scatter of features which is the most widely used feature discrimination evaluation metric. According to that ratio, features can be selected by a simple sorting method. The projection gradient descent algorithm is introduced to further improve the performance of our algorithm by using the solution obtained before as its initial solution. This ensures the stability of this iterative algorithm. We prove that the proposed method can get the global optimal solution of this non-convex problem when all features are statistically independent. For the general case where features are statistically dependent, extensive experiments on six small sample size datasets and one large-scale dataset show that our algorithm has comparable or better classification capability comparing with other eight state-of-the-art feature selection methods by the SVM classifier. We also show that our algorithm can obtain a low loss value, which means the solution of our algorithm can get very close to this NP-hard problem’s real solution. What is more, because we solve the original $\ell _{2,0}$ℓ2,0-norm constrained problem, we avoid the heavy work of tuning the regularization parameter because its meaning is explicit in our method, i.e., the number of selected features. At last, we evaluate the stability of our algorithm from two perspectives, i.e., the objective function values and the selected features, by experiments. From both perspectives, our algorithm shows satisfactory stability performance."
969,,Bayesian Sparse Topical Coding.,"Min Peng,Qianqian Xie,Hua Wang 0002,Yanchun Zhang,Gang Tian",https://doi.org/10.1109/TKDE.2018.2847707,TKDE,2019,"Bayes methods,Encoding,Data models,Facebook,Vocabulary,Probabilistic logic,Semantics","Sparse topic models (STMs) are widely used for learning a semantically rich latent sparse representation of short texts in large scale, mainly by imposing sparse priors or appropriate regularizers on topic models. However, it is difficult for these STMs to model the sparse structure and pattern of the corpora accurately, since their sparse priors always fail to achieve real sparseness, and their regularizers bypass the prior information of the relevance between sparse coefficients. In this paper, we propose a novel Bayesian hierarchical topic models called Bayesian Sparse Topical Coding with Poisson Distribution (BSTC-P) on the basis of Sparse Topical Coding with Sparse Groups (STCSG). Different from traditional STMs, it focuses on imposing hierarchical sparse prior to leverage the prior information of relevance between sparse coefficients. Furthermore, we propose a sparsity-enhanced BSTC, Bayesian Sparse Topical Coding with Normal Distribution (BSTC-N), via mathematic approximation. We adopt superior hierarchical sparse inducing prior, with the purpose of achieving the sparsest optimal solution. Experimental results on datasets of Newsgroups and Twitter show that both BSTC-P and BSTC-N have better performance on finding clear latent semantic representations. Therefore, they yield better performance than existing works on document classification tasks."
970,,Adaptive Distributed RDF Graph Fragmentation and Allocation based on Query Workload.,"Peng Peng 0001,Lei Zou 0001,Lei Chen 0002,Dongyan Zhao 0001",https://doi.org/10.1109/TKDE.2018.2841389,TKDE,2019,"Resource description framework,Resource management,Distributed databases,Query processing,Pattern matching,Graph theory,Semantic Web","As massive volumes of Resource Description Framework (RDF) data are growing, designing a distributed RDF database system to manage them is necessary. In designing this system, it is very common to partition the RDF data into some parts, called fragments, which are then distributed. Thus, the distribution design comprises two steps: fragmentation and allocation. In this study, we explore the workload for fragmentation and allocation, which aims to reduce the communication cost during SPARQL query processing. Specifically, we adaptively maintain some frequent access patterns (FAPs) to reflect the characteristics of the workload while ensuring the data integrity and approximation ratio. Based on these frequent access patterns, we propose three fragmentation strategies, namely vertical, horizontal, and mixed fragmentation, to divide RDF graphs while meeting different types of query processing objectives. After fragmentation, we discuss how to allocate these fragments to various sites while balancing the fragments. Finally, we discuss how to process queries based on the results of fragmentation and allocation. Experiments over large RDF datasets confirm the superior performance of our proposed solutions."
971,,Pair-Linking for Collective Entity Disambiguation - Two Could Be Better Than All.,"Minh C. Phan,Aixin Sun,Yi Tay,Jialong Han,Chenliang Li",https://doi.org/10.1109/TKDE.2018.2857493,TKDE,2019,"Coherence,Knowledge based systems,Joining processes,Optimization,Semantics,Information services,Sun","Collective entity disambiguation, or collective entity linking aims to jointly resolve multiple mentions by linking them to their associated entities in a knowledge base. Previous works are primarily based on the underlying assumption that entities within the same document are highly related. However, the extent to which these entities are actually connected in reality is rarely studied and therefore raises interesting research questions. For the first time, this paper shows that the semantic relationships between mentioned entities within a document are in fact less dense than expected. This could be attributed to several reasons such as noise, data sparsity, and knowledge base incompleteness. As a remedy, we introduce MINTREE, a new tree-based objective for the problem of entity disambiguation. The key intuition behind MINTREE is the concept of coherence relaxation which utilizes the weight of a minimum spanning tree to measure the coherence between entities. Based on this new objective, we design Pair-Linking, a novel iterative solution for the MINTREE optimization problem. The idea of Pair-Linking is simple: instead of considering all the given mentions, Pair-Linking iteratively selects a pair with the highest confidence at each step for decision making. Via extensive experiments on eight benchmark datasets, we show that our approach is not only more accurate but also surprisingly faster than many state-of-the-art collective linking algorithms."
972,,Efficient Structural Clustering on Probabilistic Graphs.,"Yu-Xuan Qiu,Rong-Hua Li,Jianxin Li 0001,Shaojie Qiao,Guoren Wang,Jeffrey Xu Yu,Rui Mao 0001",https://doi.org/10.1109/TKDE.2018.2872553,TKDE,2019,"Probability,Clustering algorithms,Pattern clustering,Graph theory,Data mining,Dynamic programming","Structural clustering is a fundamental graph mining operator which is not only able to find densely-connected clusters, but it can also identify hub vertices and outliers in the graph. Previous structural clustering algorithms are tailored to deterministic graphs. Many real-world graphs, however, are not deterministic, but are probabilistic in nature because the existence of the edge is often inferred using a variety of statistical approaches. In this paper, we formulate the problem of structural clustering on probabilistic graphs, with the aim of finding reliable clusters in a given probabilistic graph. Unlike the traditional structural clustering problem, our problem relies mainly on a novel concept called reliable structural similarity which measures the probability of the similarity between two vertices in the probabilistic graph. We develop a dynamic programming algorithm with several powerful pruning strategies to efficiently compute the reliable structural similarities. With the reliable structural similarities, we adapt an existing solution framework to calculate the structural clustering on probabilistic graphs. Comprehensive experiments on five real-life datasets demonstrate the effectiveness and efficiency of the proposed approaches."
973,,QANet - Tensor Decomposition Approach for Query-Based Anomaly Detection in Heterogeneous Information Networks.,"Vahid Ranjbar,Mostafa Salehi,Pegah Jandaghi,Mahdi Jalili",https://doi.org/10.1109/TKDE.2018.2873391,TKDE,2019,"Anomaly detection,Tensile stress,Image edge detection,Feature extraction,Biological system modeling,Computer science,Integrated circuit modeling","Complex networks have now become integral parts of modern information infrastructures. This paper proposes a user-centric method for detecting anomalies in heterogeneous information networks, in which nodes and/or edges might be from different types. In the proposed anomaly detection method, users interact directly with the system and anomalous entities can be detected through queries. Our approach is based on tensor decomposition and clustering methods. We also propose a network generation model to construct synthetic heterogeneous information network to test the performance of the proposed method. The proposed anomaly detection method is compared with state-of-the-art methods in both synthetic and real-world networks. Experimental results show that the proposed tensor-based method considerably outperforms the existing anomaly detection methods."
974,,A Rapid Hybrid Clustering Algorithm for Large Volumes of High Dimensional Data.,"Punit Rathore,Dheeraj Kumar,James C. Bezdek,Sutharshan Rajasegarar,Marimuthu Palaniswami",https://doi.org/10.1109/TKDE.2018.2842191,TKDE,2019,"Clustering algorithms,Sampling methods,Pattern clustering,Dimensionality reduction,Data analysis","Clustering large volumes of high-dimensional data is a challenging task. Many clustering algorithms have been developed to address either handling datasets with a very large sample size or with a very high number of dimensions, but they are often impractical when the data is large in both aspects. To simultaneously overcome both the `curse of dimensionality' problem due to high dimensions and scalability problems due to large sample size, we propose a new fast clustering algorithm called FensiVAT. FensiVAT is a hybrid, ensemble-based clustering algorithm which uses fast data-space reduction and an intelligent sampling strategy. In addition to clustering, FensiVAT also provides visual evidence that is used to estimate the number of clusters (cluster tendency assessment) in the data. In our experiments, we compare FensiVAT with nine state-of-the-art approaches which are popular for large sample size or high-dimensional data clustering. Experimental results suggest that FensiVAT, which can cluster large volumes of high-dimensional datasets in a few seconds, is the fastest and most accurate method of the ones tested."
975,,Top-k Durable Graph Pattern Queries on Temporal Graphs.,"Konstantinos Semertzidis,Evaggelia Pitoura",https://doi.org/10.1109/TKDE.2018.2823754,TKDE,2019,"Pattern matching,Indexes,IP networks,Proteins,Evolution (biology),Impedance matching","Graphs offer a natural model for the relationships and interactions among entities, such as those occurring among users in social and cooperation networks, and proteins in biological networks. Since most such networks are dynamic, to capture their evolution over time, we assume a sequence of graph snapshots where each graph snapshot represents the state of the network at a different time instance. Given this sequence, we seek to find the top-
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 
<i>most durable matches</i>
 of an input graph pattern query, that is, the matches that exist for the longest period of time. The straightforward way to address this problem is to apply a state-of-the-art graph pattern matching algorithm at each snapshot and then aggregate the results. However, for large networks and long sequences, this approach is computationally expensive, since all matches have to be generated at each snapshot, including those appearing only once. We propose a new approach that uses a compact representation of the sequence of graph snapshots, appropriate time indexes to prune the search space, and strategies to determine the duration of the seeking matches. Finally, we present experiments with real datasets that illustrate the efficiency and effectiveness of our approach."
976,,Viral Cascade Probability Estimation and Maximization in Diffusion Networks.,"Arman Sepehr,Hamid Beigy",https://doi.org/10.1109/TKDE.2018.2840998,TKDE,2019,"Estimation,Graphical models,Social network services,Monte Carlo methods,Toy manufacturing industry,Computational modeling,Data models","People use social networks to share millions of stories every day, but these stories rarely become viral. Can we estimate the probability that a story becomes a viral cascade? If so, can we find a set of users that are more likely to trigger viral cascades? These estimation and maximization problems are very challenging since both rare-event nature of viral cascades and efficiency requirement should be considered. Unfortunately, this problem still remains largely unexplored to date. In this paper, given temporal dynamics of a network, we first develop an efficient viral cascade probability estimation method, ViCE, that leverages an special importance sampling approximation to achieve high accuracy, even in the cases of very small probability of influence. We then show that the most influential nodes in this model is NP-hard, and develop an efficient viral cascade probability maximization method, ViCEM, that maximizes a surrogate submodular function using a greedy algorithm. Experiments on both synthetic and real-world data show that ViCE can accurately estimate viral cascade probabilities using fewer samples than naive sampling by at least two orders of magnitude, and also ViCEM finds a set of users with higher viral cascade probability than alternatives. Additionally, experiments show that these algorithms are robust across different network topologies."
977,,Parallel Trajectory-to-Location Join.,"Shuo Shang,Lisi Chen,Kai Zheng 0001,Christian S. Jensen,Zhewei Wei,Panos Kalnis",https://doi.org/10.1109/TKDE.2018.2854705,TKDE,2019,"Trajectory,Correlation,Spatiotemporal phenomena,Semantics,Collaboration,Merging,Parallel processing","The matching between trajectories and locations, called Trajectory-to-Location join (TL-Join), is fundamental functionality in spatiotemporal data management. Given a set of trajectories, a set of locations, and a threshold 8, the TL-Join finds all (trajectory, location) pairs from the two sets with spatiotemporal correlation above 8. This join targets diverse applications, including location recommendation, event tracking, and trajectory activity analyses. We address three challenges in relation to the TL-Join: how to define the spatiotemporal correlation between trajectories and locations, how to prune the search space effectively when computing the join, and how to perform the computation in parallel. Specifically, we define new metrics to measure the spatiotemporal correlation between trajectories and locations. We develop a novel parallel collaborative (PCol) search method based on a divide-and-conquer strategy. For each location o, we retrieve the trajectories with high spatiotemporal correlation to o, and then we merge the results. An upper bound on the spatiotemporal correlation and a heuristic scheduling strategy are developed to prune the search space. The trajectory searches from different locations are independent and are performed in parallel, and the result merging cost is independent of the degree of parallelism. Studies of the performance of the developed algorithms using large spatiotemporal data sets are reported."
978,1,PrivateGraph - Privacy-Preserving Spectral Analysis of Encrypted Graphs in the Cloud.,"Sagar Sharma,James Powers,Keke Chen",https://doi.org/10.1109/TKDE.2018.2847662,TKDE,2019,"Cloud computing,Encryption,Spectral analysis,Sparse matrices,Data privacy","Big graphs, such as user interactions in social networks and customer rating matrices in collaborative filters, possess great values for both businesses and research. They are not only big but often keep evolving, which requires a large amount of computing resources to maintain. With the wide deployment of public cloud resources, owners of big graphs may want to use cloud resources to obtain storage and computation scalability. However, privacy and ownership of the graphs in the cloud has become a major concern. In this paper, we study privacy-preserving algorithms for one of the important graph analysis techniques-graph spectral analysis for outsourced graph in the cloud. The core operation: eigendecomposition of large matrix is also important to many data mining algorithms. We consider a cloud-centric framework with three collaborative parties: data contributors, data owner, and cloud provider. Graphs are represented as matrices such as adjacency matrix and Laplacian matrix, the elements of which are encrypted and submitted by distributed contributors. The data owner then interacts with the cloud-side programs to conduct spectral analysis, while protecting data privacy from the honest-but-curious cloud provider. For a N × N graph matrix, we aim to design algorithms with the cloud handling expensive storage and computation in O(N
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) complexity, while data owner and data contributors' algorithms take only O(N). To achieve this goal, we develop the privacy-preserving versions of the two approximate eigendecomposition algorithms: the Lanczos algorithm and the Nyström algorithm, considering two encryption methods: additive homomorphic encryption (AHE) methods and somewhat homomorphic encryption (SHE) methods. Both dense and sparse matrices are studied, while sparse matrices also involve a differentially private data submission protocol to allow the trade-off between data sparsity and privacy. Experimental results show that the Nyströ algorithm with sparse encoding can dramatically reduce data owners' costs. SHE-based methods have lower computational time while AHE-based methods have lower communication/storage costs."
979,,Majority Voting and Pairing with Multiple Noisy Labeling.,"Victor S. Sheng,Jing Zhang 0015,Bin Gu,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2017.2659740,TKDE,2019,"Labeling,Noise measurement,Training,Crowdsourcing,Training data,Data models,Logistics","With the crowdsourcing of small tasks becoming easier, it is possible to obtain non-expert/imperfect labels at low cost. With low-cost imperfect labeling, it is straightforward to collect multiple labels for the same data items. This paper proposes strategies of utilizing these multiple labels for supervised learning, based on two basic ideas: majority voting and pairing. We show several interesting results based on our experiments. (i) The strategies based on the majority voting idea work well under the situation where the certainty level is high. (ii) On the contrary, the pairing strategies are more preferable under the situation where the certainty level is low. (iii) Among the majority voting strategies, soft majority voting can reduce the bias and roughness, and perform better than majority voting. (iv) Pairing can completely avoid the bias by having both sides (potentially correct and incorrect/noisy information) considered. Beta estimation is applied to reduce the impact of the noise in pairing. Our experimental results show that pairing with Beta estimation always performs well under different certainty levels. (v) All strategies investigated are labeling quality agnostic strategies for real-world applications, and some of them perform better than or at least very close to the gnostic strategies."
980,,Heterogeneous Information Network Embedding for Recommendation.,"Chuan Shi,Binbin Hu,Wayne Xin Zhao,Philip S. Yu",https://doi.org/10.1109/TKDE.2018.2833443,TKDE,2019,"Recommender systems,Heterogeneous networks,Semantics,Data models,Data mining,Task analysis,Predictive models","Due to the flexibility in modelling data heterogeneity, heterogeneous information network (HIN) has been adopted to characterize complex and heterogeneous auxiliary data in recommender systems, called HIN based recommendation. It is challenging to develop effective methods for HIN based recommendation in both extraction and exploitation of the information from HINs. Most of HIN based recommendation methods rely on path based similarity, which cannot fully mine latent structure features of users and items. In this paper, we propose a novel heterogeneous network embedding based approach for HIN based recommendation, called HERec. To embed HINs, we design a meta-path based random walk strategy to generate meaningful node sequences for network embedding. The learned node embeddings are first transformed by a set of fusion functions, and subsequently integrated into an extended matrix factorization (MF) model. The extended MF model together with fusion functions are jointly optimized for the rating prediction task. Extensive experiments on three real-world datasets demonstrate the effectiveness of the HERec model. Moreover, we show the capability of the HERec model for the cold-start problem, and reveal that the transformed embedding information from HINs can improve the recommendation performance."
981,,Reader Comment Digest through Latent Event Facets and News Specificity.,"Bei Shi,Wai Lam",https://doi.org/10.1109/TKDE.2018.2859977,TKDE,2019,"Diseases,Africa,Drugs,Graphical user interfaces,Visualization,Tag clouds","When a significant event occurs, many news articles from different newsagents often report it. Moreover, these newsagents also provide platforms for their readers to write comments expressing their views or understanding. Through digesting these reader comments, we can gain insights into the reactions, suggestions, personal experiences, or public opinions with respect to the emerging event. However, these reader comments from different sources are often rapidly accumulated resulting in an enormous volume. It becomes difficult to manually analyze these comments. In this paper, we propose a framework that can digest reader comments automatically through latent event facets and news specificity. An event facet refers to the aspect of the event concerned by many readers. Specifically, some of the reader comments, despite coming from different sources, discuss a certain facet of the event. Such facets provide an effective means for organizing news comments in a global manner. On the other hand, some comments discuss the specific topic of the corresponding news article. These specific topics demonstrate the specific focus of readers on the piece of news locally. Such reader comment digest in different granularities facilitates readers deeper understanding of these enormous comments. To achieve the above desirable goal of digesting reader comments, we propose an unsupervised model called EFNS which is capable of capturing the intricate fine-grained associations among events, news, and comments. We also develop a multiplicative-update method to infer the parameters and prove the convergence of our algorithm. Our framework can also visualize reader comments according to the relationship with latent event facets and the degree of news specificity. Experimental results show that our proposed EFNS model can provide an effective way to digest news reader comments and outperform the state-of-the-art method."
982,,DeepClue - Visual Interpretation of Text-Based Deep Stock Prediction.,"Lei Shi 0002,Zhiyang Teng,Le Wang,Yue Zhang 0004,Alexander Binder",https://doi.org/10.1109/TKDE.2018.2854193,TKDE,2019,"Predictive models,Machine learning,Neural networks,Stock markets,Prediction algorithms,Time series analysis,Data visualization","The recent advance of deep learning has enabled trading algorithms to predict stock price movements more accurately. Unfortunately, there is a significant gap in the real-world deployment of this breakthrough. For example, professional traders in their long-term careers have accumulated numerous trading rules, the myth of which they can understand quite well. On the other hand, deep learning models have been hardly interpretable. This paper presents DeepClue, a system built to bridge text-based deep learning models and end users through visually interpreting the key factors learned in the stock price prediction model. We make three contributions in DeepClue. First, by designing the deep neural network architecture for interpretation and applying an algorithm to extract relevant predictive factors, we provide a useful case on what can be interpreted out of the prediction model for end users. Second, by exploring hierarchies over the extracted factors and displaying these factors in an interactive, hierarchical visualization interface, we shed light on how to effectively communicate the interpreted model to end users. Specially, the interpretation separates the predictables from the unpredictables for stock prediction through the use of intercept model parameters and a risk visualization design. Third, we evaluate the integrated visualization system through two case studies in predicting the stock price with financial news and company-related tweets from social media. Quantitative experiments comparing the proposed neural network architecture with state-of-the-art models and the human baseline are conducted and reported. Feedbacks from an informal user study with domain experts are summarized and discussed in details. The study results demonstrate the effectiveness of DeepClue in helping to complete stock market investment and analysis tasks."
983,,Schema-Agnostic Progressive Entity Resolution.,"Giovanni Simonini,George Papadakis 0001,Themis Palpanas,Sonia Bergamaschi",https://doi.org/10.1109/TKDE.2018.2852763,TKDE,2019,"Erbium,Big Data,Task analysis,Resource description framework,Guidelines,Indexes,Reliability","Entity Resolution (ER) is the task of finding entity profiles that correspond to the same real-world entity. Progressive ER aims to efficiently resolve large datasets when limited time and/or computational resources are available. In practice, its goal is to provide the best possible partial solution by approximating the optimal comparison order of the entity profiles. So far, Progressive ER has only been examined in the context of structured (relational) data sources, as the existing methods rely on schema knowledge to save unnecessary comparisons: they restrict their search space to similar entities with the help of schema-based blocking keys (i.e., signatures that represent the entity profiles). As a result, these solutions are not applicable in Big Data integration applications, which involve large and heterogeneous datasets, such as relational and RDF databases, JSON files, Web corpus etc. To cover this gap, we propose a family of schema-agnostic Progressive ER methods, which do not require schema information, thus applying to heterogeneous data sources of any schema variety. First, we introduce two naïve schema-agnostic methods, showing that straightforward solutions exhibit a poor performance that does not scale well to large volumes of data. Then, we propose four different advanced methods. Through an extensive experimental evaluation over 7 real-world, established datasets, we show that all the advanced methods outperform to a significant extent both the naïve and the state-of-the-art schema-based ones. We also investigate the relative performance of the advanced methods, providing guidelines on the method selection."
984,,Passive and Partially Active Fault Tolerance for Massively Parallel Stream Processing Engines.,"Li Su 0005,Yongluan Zhou",https://doi.org/10.1109/TKDE.2017.2720602,TKDE,2019,"Topology,Fault tolerance,Fault tolerant systems,Engines,Data models,Storms,Semantics","Fault-tolerance techniques for stream processing engines can be categorized into passive and active approaches. However, both approaches have their own inadequacies in Massively Parallel Stream Processing Engines (MPSPE). The passive approach incurs a long recovery latency especially when a number of correlated nodes fail simultaneously, while the active approach requires extra replication resources. In this paper, we propose a new fault-tolerance framework, which is Passive and Partially Active (PPA). In a PPA scheme, the passive approach is applied to all tasks while only a selected set of tasks will be actively replicated. The number of actively replicated tasks depends on the available resources. If tasks without active replicas fail, tentative outputs will be generated before the completion of the recovery process. We also propose effective and efficient algorithms to optimize a partially active replication plan to maximize the quality of tentative outputs. We implemented PPA on top of Storm, an open-source MPSPE and conducted extensive experiments using both real and synthetic datasets to verify the effectiveness of our approach."
985,,Modeling and Predicting Community Structure Changes in Time-Evolving Social Networks.,"Etienne Gael Tajeuna,Mohamed Bouguessa,Shengrui Wang",https://doi.org/10.1109/TKDE.2018.2851586,TKDE,2019,"Social network services,Predictive models,Analytical models,Feature extraction,Data models,History,Computer science","As time evolves, communities in a social network may undergo various changes known as critical events. For instance, a community can either split into several other communities, expand into a larger community, shrink to a smaller community, remain stable or merge into another community. Prediction of critical events has attracted increasing attention in the recent literature. Learning the evolution of communities over time is a key step towards predicting the critical events the communities may undergo. This is an important and difficult issue in the study of social networks. In the work to date, there is a lack of formal approaches for modeling and predicting critical events over time. This motivates our effort to design a new statistical method for event prediction in order to make better use of histories of past changes. To this end, this paper proposes a sliding window analysis from which we develop a model that simultaneously exploits an autoregressive model and survival analysis techniques. The autoregressive model is employed here to simulate the evolution of the community structure, whereas the survival analysis techniques allow the prediction of future changes the community may undergo."
986,,Robust Image Hashing with Tensor Decomposition.,"Zhenjun Tang,Lv Chen,Xianquan Zhang,Shichao Zhang",https://doi.org/10.1109/TKDE.2018.2837745,TKDE,2019,"Tensile stress,Robustness,Classification algorithms,Transforms,Matrix decomposition,Image coding,Histograms","This paper presents a new image hashing that is designed with tensor decomposition (TD), referred to as TD hashing, where image hash generation is viewed as deriving a compact representation from a tensor. Specifically, a stable three-order tensor is first constructed from the normalized image, so as to enhance the robustness of our TD hashing. A popular TD algorithm, called Tucker decomposition, is then exploited to decompose the three-order tensor into a core tensor and three orthogonal factor matrices. As the factor matrices can reflect intrinsic structure of original tensor, hash construction with the factor matrices makes a desirable discrimination of the TD hashing. To examine these claims, there are 14,551 images selected for our experiments. A receiver operating characteristics (ROC) graph is used to conduct theoretical analysis and the ROC comparisons illustrate that the TD hashing outperforms some state-of-the-art algorithms in classification performance between the robustness and discrimination."
987,,Efficient Vertical Mining of High Average-Utility Itemsets Based on Novel Upper-Bounds.,"Tin C. Truong,Hai V. Duong,Bac Le,Philippe Fournier-Viger",https://doi.org/10.1109/TKDE.2018.2833478,TKDE,2019,"Itemsets,Data mining,Runtime,Urban areas,Tin,Length measurement","Mining High Average-Utility Itemsets (HAUIs) in a quantitative database is an extension of the traditional problem of frequent itemset mining, having several practical applications. Discovering HAUIs is more challenging than mining frequent itemsets using the traditional support model since the average-utilities of itemsets do not satisfy the downward-closure property. To design algorithms for mining HAUIs that reduce the search space of itemsets, prior studies have proposed various upper-bounds on the average-utilities of itemsets. However, these algorithms can generate a huge amount of unpromising HAUI candidates, which result in high memory consumption and long runtimes. To address this problem, this paper proposes four tight average-utility upper-bounds, based on a vertical database representation, and three efficient pruning strategies. Furthermore, a novel generic framework for comparing average-utility upper-bounds is presented. Based on these theoretical results, an efficient algorithm named dHAUIM is introduced for mining the complete set of HAUIs. dHAUIM represents the search space and quickly compute upper-bounds using a novel IDUL structure. Extensive experiments show that dHAUIM outperforms four state-of-the-art algorithms for mining HAUIs in terms of runtime on both real-life and synthetic databases. Moreover, results show that the proposed pruning strategies dramatically reduce the number of candidate HAUIs."
988,,A Unified Framework for Community Detection and Network Representation Learning.,"Cunchao Tu,Xiangkai Zeng,Hao Wang,Zhengyan Zhang,Zhiyuan Liu 0001,Maosong Sun,Bo Zhang 0056,Leyu Lin",https://doi.org/10.1109/TKDE.2018.2852958,TKDE,2019,"Complex networks,Network theory (graphs),Pattern clustering,Predictive models,Social network services,Social networking (online)","Network representation learning (NRL) aims to learn low-dimensional vectors for vertices in a network. Most existing NRL methods focus on learning representations from local context of vertices (such as their neighbors). Nevertheless, vertices in many complex networks also exhibit significant global patterns widely known as communities. It's intuitive that vertices in the same community tend to connect densely and share common attributes. These patterns are expected to improve NRL and benefit relevant evaluation tasks, such as link prediction and vertex classification. Inspired by the analogy between network representation learning and text modeling, we propose a unified NRL framework by introducing community information of vertices, named as Community-enhanced Network Representation Learning (CNRL). CNRL simultaneously detects community distribution of each vertex and learns embeddings of both vertices and communities. Moreover, the proposed community enhancement mechanism can be applied to various existing NRL models. In experiments, we evaluate our model on vertex classification, link prediction, and community detection using several real-world datasets. The results demonstrate that CNRL significantly and consistently outperforms other state-of-the-art methods while verifying our assumptions on the correlations between vertices and communities."
989,,Addressing Interpretability and Cold-Start in Matrix Factorization for Recommender Systems.,"Michail Vlachos,Celestine Dünner,Reinhard Heckel,Vassilios G. Vassiliadis,Thomas P. Parnell,Kubilay Atasu",https://doi.org/10.1109/TKDE.2018.2829521,TKDE,2019,"Recommender systems,Clustering algorithms,Collaboration,Cognition,Graphics processing units,Companies,Training","We consider the problem of generating interpretable recommendations by identifying overlapping co-clusters of clients and products, based only on positive or implicit feedback. Our approach is applicable on very large datasets because it exhibits almost linear complexity in the input examples and the number of co-clusters. We show, both on real industrial data and on publicly available datasets, that the recommendation accuracy of our algorithm is competitive to that of state-of-the-art matrix factorization techniques. In addition, our technique has the advantage of offering recommendations that are textually and visually interpretable. Our formulation can also address cold-start problems by gracefully meshing collaborative and content-based reasoning. Finally, we present efficient Graphical Processing Unit (GPU) implementations and demonstrate a speedup of more than 270 times over our baseline CPU implementation on a cluster of 16 GPUs."
990,,A New Method for Measuring Topological Structure Similarity between Complex Trajectories.,"Huimeng Wang,Yunyan Du,Jiawei Yi,Yong Sun,Fuyuan Liang",https://doi.org/10.1109/TKDE.2018.2872523,TKDE,2019,"Trajectory,Sea measurements,Oceans,Computational efficiency,Spatial databases,Data mining","We proposed a new framework to measure the similarity of topological structure between complex trajectories. There are three steps in the framework. A complex trajectory is first represented by a graph structure which consists of nodes and edges. Secondly, we developed a Comprehensive Structure Matching (CSM) algorithm to identify all common structures between the complex trajectories of interest. Thirdly, we used the Jaccard similarity coefficient to evaluate the similarity between two complex trajectories. We used synthetic graph data to evaluate the CSM method and examine its performance by comparing against that of the VF2 and the exact graph edit distance (EGED) algorithms. Results show that the CSM algorithm outperforms the EGED in terms of the computation efficiency. The CSM is more comprehensive than the VF2 algorithm as it further considers the partial isomorphism. We used the CSM algorithm to examine the 1993 to 2012 complex trajectories of anticyclonic eddies in the South China Sea (SCS). The CSM successfully found the complex trajectories that are similar to a thoroughly-studied ACE3 trajectory in the SCS. From the similar trajectories, we identified a dominant migrating path of the ocean eddies in the northern SCS. The CSM also successfully identified some new complex trajectories that propagated across the 18°N parallel in the SCS, which were not reported before. It also further identified multiple common structure models of the complex trajectories. These findings help us better understand the behaviors and the evolution of the mesoscale eddies in the SCS."
991,,Decoding Chinese User Generated Categories for Fine-Grained Knowledge Harvesting.,"Chengyu Wang 0001,Yan Fan,Xiaofeng He,Aoying Zhou",https://doi.org/10.1109/TKDE.2018.2865942,TKDE,2019,"Internet,Encyclopedias,Electronic publishing,Taxonomy,Knowledge based systems,Semantics","User Generated Categories (UGCs) are short but informative phrases that reflect how people describe and organize entities. UGCs express semantic relations among entities implicitly hence serve as a rich data source for knowledge harvesting. However, most UGC relation extraction methods focus on English and heavily rely on lexical and syntactic patterns. Applying them directly to Chinese UGCs poses significant challenges because Chinese is an analytic language with flexible language expressions. In this paper, we aim at harvesting fine-grained relations from Chinese UGCs automatically. Based on neural networks and negative sampling, we introduce two word embedding projection models to identify is-a relations. The accuracy of prediction results is improved via a collective refinement algorithm and a hypernym expansion method. We further propose a graph clique mining algorithm to harvest non-taxonomic relations from UGCs, together with their textual patterns. Two experiments are conducted to validate our approach based on Chinese Wikipedia. The first experiment verifies the is-a relation extraction approach achieves high accuracy, outperforming state-of-the-art methods. The second experiment shows that the proposed method can harvest non-taxonomic relations of large quantity and high accuracy, with minimal human intervention."
992,,Detecting a Variety of Long-Term Stealthy User Behaviors on High Speed Links.,"Pinghui Wang,Peng Jia 0004,Jing Tao,Xiaohong Guan",https://doi.org/10.1109/TKDE.2018.2873319,TKDE,2019,"Monitoring,IP networks,Information filters,Servers,Data mining","Monitoring user behaviors over high speed links is important for applications such as network anomaly detection. Previous work focuses on monitoring anomalies such as extremely frequent users occurring in a short timeslot such as 1 minute. Little attention has been paid to detect users with stealthy behaviors (e.g., persistent, co-occurrence, anti-co-occurrence, and periodic behaviors) over a long period of time at the timeslot granularity. Due to limited computation and storage resources on routers, it is prohibitive to collect massive network traffic in a long period of time. We develop an end-to-end method for solving challenges in both long-term online traffic collection and offline user behavior analysis. We conduct extensive experiments on a variety of real-world traffic to evaluate the performance of detecting persistent, co-occurrence, anti-co-occurrence, and periodic behaviors, and the results demonstrate that our method significantly outperforms state-of-the-art methods."
993,,Dual Hypergraph Regularized PCA for Biclustering of Tumor Gene Expression Data.,"Xuesong Wang 0001,Jian Liu,Yuhu Cheng,Aiping Liu,Enhong Chen",https://doi.org/10.1109/TKDE.2018.2874881,TKDE,2019,"Principal component analysis,Manifolds,Gene expression,Tumors,Laplace equations,Clustering methods,Clustering algorithms","Clustering is a powerful approach to analyze gene expression data which is crucial to the investigation of effective treatment of cancer. Many graph regularize-based clustering methods have been proposed and shown to be superior to the traditional clustering methods. However, they only focus on the inner structure in samples and fail to take the feature manifold into account. In gene expression data, it's practical to hypothesize that both the samples and the genes lie on nonlinear low dimensional manifolds, namely sample manifold and gene manifold, respectively. Therefore in this paper, incorporating the geometric structures in both samples and features, we propose a Dual Hypergraph Regularized PCA (DHPCA) method for biclustering of tumor data. First, for gene expression data, we construct two hypergraphs, i.e., sample hypergraph and gene hypergraph, to estimate the intrinsic geometric structures of samples and genes. Then, we introduce the hypergraph regularization on both gene side and sample side. Finally, our biclustering method is formulated as two hypergraph regularized PCA with closed-form solution. We experimentally validate our proposed DHPCA algorithm on real applications and the promising results indicate its potential in high dimension data analysis."
994,,Efficient Representative Subset Selection over Sliding Windows.,"Yanhao Wang 0001,Yuchen Li 0001,Kian-Lee Tan",https://doi.org/10.1109/TKDE.2018.2854182,TKDE,2019,"Approximation algorithms,Heuristic algorithms,Indexes,Microsoft Windows,Data mining,Kernel,Data models","Representative subset selection (RSS) is an important tool for users to draw insights from massive datasets. Existing literature models RSS as the submodular maximization problem to capture the “diminishing returns” property of the representativeness of selected subsets, but often only has a single constraint (e.g., cardinality), which limits its applications in many real-world problems. To capture the data recency issue and support different types of constraints, we formulate dynamic RSS in data streams as maximizing submodular functions subject to general d-knapsack constraints (SMDK) over sliding windows. We propose a KnapWindow framework (KW) for SMDK. KW utilizes the KnapStream algorithm (KS) for SMDK in append-only streams as a subroutine. It maintains a sequence of checkpoints and KS instances over the sliding window. Theoretically, KW is 1-ε/1+d-approximate for SMDK. Furthermore, we propose a KnapWindowPlus framework (KW) to improve upon KW. KW
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 builds an index SubKnapChk to manage the checkpoints and KS instances. SubKnapChk deletes a checkpoint whenever it can be approximated by its successors. By keeping much fewer checkpoints, KW
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 achieves higher efficiency than KW while still guaranteeing a 1-ε'/2+2d-approximate solution for SMDK. Finally, we evaluate the efficiency and solution quality of KW and KW
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 in real-world datasets. The experimental results demonstrate that KW achieves more than two orders of magnitude speedups over the batch baseline and preserves high-quality solutions for SMDK over sliding windows. KW
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 further runs 5-10 times faster than KW while providing solutions with equivalent or even better utilities."
995,,Inferring Higher-Order Structure Statistics of Large Networks from Sampled Edges.,"Pinghui Wang,Yiyan Qi,John C. S. Lui,Don Towsley,Junzhou Zhao,Jing Tao",https://doi.org/10.1109/TKDE.2017.2685584,TKDE,2019,"Data mining,Complex networks,Graph theory,Probability,Statistical analysis,Inference mechanisms","Recently exploring locally connected subgraphs (also known as motifs or graphlets) of complex networks attracts a lot of attention. Previous work made the strong assumption that the graph topology of interest is known in advance. In practice, sometimes researchers have to deal with the situation where the graph topology is unknown because it is expensive to collect and store all topological information. Hence, typically what is available to researchers is only a snapshot of the graph, i.e., a subgraph of the graph. Crawling methods such as breadth first sampling can be used to generate the snapshot. However, these methods fail to sample a streaming graph represented as a high speed stream of edges. Therefore, graph mining applications such as network traffic monitoring usually use random edge sampling (i.e., sample each edge with a fixed probability) to collect edges and generate a sampled graph, which we call a “ RESampled graph”. Clearly, a RESampled graph's motif statistics may be quite different from those of the original graph. To resolve this, we propose a framework Minfer, which takes the given RESampled graph and accurately infers the underlying graph's motif statistics. Experiments using large scale datasets show the accuracy and efficiency of our method."
996,,DeepDirect - Learning Directions of Social Ties with Edge-Based Network Embedding.,"Chaokun Wang,Changping Wang,Zheng Wang,Xiaojun Ye,Jeffrey Xu Yu,Bin Wang 0021",https://doi.org/10.1109/TKDE.2018.2877748,TKDE,2019,"Network topology,Task analysis,Modeling,Supervised learning,Facebook,Logistics","There is a lot of research work on social ties, few of which is about the directionality of social ties. However, the directionality is actually a basic but important attribute of social ties. In this paper, we present a supervised learning problem, the tie direction learning (TDL) problem, which aims to learn the directionality function of directed social networks. Two ways are introduced to solve the TDL problem: one is based on hand-crafted features and the other, named DeepDirect, learns the social tie representation through the topological information of the network. In DeepDirect, a novel network embedding approach, which directly maps the social ties to low-dimensional embedding vectors by deep learning techniques, is proposed. DeepDirect embeds the network considering three different aspects: preserving network topology, utilizing labeled data, and generating pseudo-labels based on observed directionality patterns. Two novel applications are proposed for the learned directionality function, i.e., direction discovery on undirected ties and direction quantification on bidirectional ties. Experiments are conducted on five different real-world data sets about these two tasks. The experimental results demonstrate our methods, especially DeepDirect, are effective and promising."
997,,Webpage Depth Viewability Prediction Using Deep Sequential Neural Networks.,"Chong Wang,Shuai Zhao,Achir Kalra,Cristian Borcea,Yi Chen 0001",https://doi.org/10.1109/TKDE.2018.2839599,TKDE,2019,"Predictive models,Advertising,Machine learning,Media,Data models,Pricing,Bidirectional control","Display advertising is the most important revenue source for publishers in the online publishing industry. The ad pricing standards are shifting to a new model in which ads are paid only if they are viewed. Consequently, an important problem for publishers is to predict the probability that an ad at a given page depth will be shown on a user's screen for a certain dwell time. This paper proposes deep learning models based on Long Short-Term Memory (LSTM) to predict the viewability of any page depth for any given dwell time. The main novelty of our best model consists in the combination of bi-directional LSTM networks, encoder-decoder structure, and residual connections. The experimental results over a dataset collected from a large online publisher demonstrate that the proposed LSTM-based sequential neural networks outperform the comparison methods in terms of prediction performance."
998,,Learning Customer Behaviors for Effective Load Forecasting.,"Xishun Wang,Minjie Zhang,Fenghui Ren",https://doi.org/10.1109/TKDE.2018.2850798,TKDE,2019,"Load forecasting,Smart grids,Load modeling,Energy consumption,Aggregates,Predictive models,Neural networks","Load forecasting has been deeply studied because of its critical role in Smart Grid. In current Smart Grid, there are various types of customers with different energy consumption patterns. Customer's energy consumption patterns are referred to as customer behaviors. It would significantly benefit load forecasting in a grid if customer behaviors could be taken into account. This paper proposes an innovative method that aggregates different types of customers by their identified behaviors, and then predicts the load of each customer cluster, so as to improve load forecasting accuracy of the whole grid. Sparse Continuous Conditional Random Fields (sCCRF) is proposed to effectively identify different customer behaviors through learning. A hierarchical clustering process is then introduced to aggregate customers according to the identified behaviors. Within each customer cluster, a representative sCCRF is fine-tuned to predict the load of its cluster. The final load of the whole grid is obtained by summing the loads of each cluster. The proposed method for load forecasting in Smart Grid has two major advantages. 1) Learning customer behaviors not only improves the prediction accuracy but also has a low computational cost. 2) sCCRF can effectively model the load forecasting problem of one customer, and simultaneously select key features to identify its energy consumption pattern. Experiments conducted from different perspectives demonstrate the advantages of the proposed load forecasting method. Further discussion is provided, indicating that the approach of learning customer behaviors can be extended as a general framework to facilitate decision making in other market domains."
999,,Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms.,"Qing Wang 0016,Chunqiu Zeng,Wubai Zhou,Tao Li 0001,S. S. Iyengar 0001,Larisa Shwartz,Genady Ya. Grabarnik",https://doi.org/10.1109/TKDE.2018.2866041,TKDE,2019,"Collaborative filtering,Recommender systems,Learning (artificial intelligence)","Online interactive recommender systems strive to promptly suggest users appropriate items (e.g., movies and news articles) according to the current context including both user and item content information. Such contextual information is often unavailable in practice, where only the users' interaction data on items can be utilized by recommender systems. The lack of interaction records, especially for new users and items, inflames the performance of recommendation further. To address these issues, both collaborative filtering, one of the most popular recommendation techniques relying on the interaction data only, and bandit mechanisms, capable of achieving the balance between exploitation and exploration, are adopted into an online interactive recommendation setting assuming independent items (i.e., arms). This assumption rarely holds in reality, since the real-world items tend to be correlated with each other. In this paper, we study online interactive collaborative filtering problems by considering the dependencies among items. We explicitly formulate item dependencies as the clusters of arms in the bandit setting, where the arms within a single cluster share the similar latent topics. In light of topic modeling techniques, we come up with a novel generative model to generate the items from their underlying topics. Furthermore, an efficient particle-learning based online algorithm is developed for inferring both latent parameters and states of our model by taking advantage of the fully adaptive inference strategy of particle learning techniques. Additionally, our inferred model can be naturally integrated with existing multi-armed selection strategies in an interactive collaborative filtering setting. Empirical studies on two real-world applications, online recommendations on movies and news, demonstrate both the effectiveness and efficiency of our proposed approach."
1000,,A General Domain Specific Feature Transfer Framework for Hybrid Domain Adaptation.,"Pengfei Wei,Yiping Ke,Chi Keong Goh",https://doi.org/10.1109/TKDE.2018.2864732,TKDE,2019,"Adaptation models,Training,Sentiment analysis,Data models,Kernel,Vocabulary,Noise measurement","Heterogeneous domain adaptation needs supplementary information to link up different domains. However, such supplementary information may not always be available in real cases. In this paper, a new problem setting called hybrid domain adaptation is investigated. It is a special case of heterogeneous domain adaptation, in which different domains share some common features, but also have their own domain specific features. We leverage upon common features instead of supplementary information to achieve effective adaptation. We propose a general domain specific feature transfer framework, which can link up different domains using common features and simultaneously reduce domain divergences. Specifically, we learn the translations between common features and domain specific features. Then, we cross-use the learned translations to transfer the domain specific features of one domain to another domain. Finally, we compose a homogeneous space in which the domain divergences are minimized. We instantiate the general framework to a linear case and a nonlinear case. Extensive experiments verify the effectiveness of the two cases."
1001,,I/O Efficient Core Graph Decomposition - Application to Degeneracy Ordering.,"Dong Wen,Lu Qin,Ying Zhang 0001,Xuemin Lin 0001,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2018.2833070,TKDE,2019,"Graph theory,Memory management,Computational modeling","Core decomposition is a fundamental graph problem with a large number of applications. Most existing approaches for core decomposition assume that the graph is kept in memory of a machine. Nevertheless, many real-world graphs are too big to reside in memory. In this paper, we study I/O efficient core decomposition following a semi-external model, which only allows node information to be loaded in memory. We propose a semi-external algorithm and an optimized algorithm for I/O efficient core decomposition. To handle dynamic graph updates, we firstly show that our algorithm can be naturally extended to handle edge deletion. Then, we propose an I/O efficient core maintenance algorithm to handle edge insertion, and an improved algorithm to further reduce I/O and CPU cost. In addition, based on our core decomposition algorithms, we further propose an I/O efficient semi-external algorithm for degeneracy ordering, which is an important graph problem that is highly related to core decomposition. We also consider how to maintain the degeneracy order. We conduct extensive experiments on 12 real large graphs. Our optimal core decomposition algorithm significantly outperforms the existing I/O efficient algorithm in terms of both processing time and memory consumption. They are very scalable to handle web-scale graphs. As an example, we are the first to handle a web graph with 978.5 million nodes and 42.6 billion edges using less than 4.2 GB memory. We also show that our proposed algorithms for degeneracy order computation and maintenance can handle big graphs efficiently with small memory overhead."
1002,,Efficient Multi-Class Probabilistic SVMs on GPUs.,"Zeyi Wen,Jiashuai Shi,Bingsheng He,Jian Chen 0011,Yawen Chen",https://doi.org/10.1109/TKDE.2018.2866097,TKDE,2019,"Training,Graphics processing units,Kernel,Support vector machines,Memory management,Acceleration,Probabilistic logic","Recently, many researchers have been working on improving other traditional machine learning algorithms (besides deep learning) using high-performance hardware such as Graphics Processing Units (GPUs). The recent success of machine learning is not only due to more effective algorithms, but also more efficient systems and implementations. In this paper, we propose a novel and efficient solution to multi-class SVMs with probabilistic output (MP-SVMs) accelerated by GPUs. MP-SVMs are an important technique for many pattern recognition applications. However, MP-SVMs are very time-consuming to use, because using an MP-SVM classifier requires training many binary SVMs and performing probability estimation by combining results of all the binary SVMs. GPUs have much higher computation capability than CPUs and are potentially excellent hardware to accelerate MP-SVMs. Still, two key challenges for efficient GPU accelerations for MP-SVM are: (i) many kernel values are repeatedly computed as a binary SVM classifier is trained iteratively, resulting in repeated accesses to the high latency GPU memory; (ii) performing training or estimating probability in a highly parallel way requires a much larger memory footprint than the GPU memory. To overcome the challenges, we propose a solution called GMP-SVM which exploits two-level (i.e., binary SVM level and MP-SVM level) optimization for training MP-SVMs and high parallelism for estimating probability. GMP-SVM reduces high latency memory accesses and memory consumption through batch processing, kernel value reusing and sharing, and support vector sharing. Experimental results show that GMP-SVM outperforms the GPU baseline by two to five times, and LibSVM with OpenMP by an order of magnitude. Also, GMP-SVM produces the same SVM classifier as LibSVM."
1003,,A Nonparametric Approach to Uncovering Connected Anomalies by Tree Shaped Priors.,"Nannan Wu,Feng Chen 0001,Jianxin Li 0002,Jinpeng Huai,Baojian Zhou,Bo Li 0005,Naren Ramakrishnan",https://doi.org/10.1109/TKDE.2018.2868097,TKDE,2019,"Approximation algorithms,Optimization,Steiner trees,Topology,Roads,Feature extraction,Computer science","The area of anomaly detection has recently been expanded in the graph-based data. Anomalous vertices are often exhibited as a connected subgraph. Few works, however, have focused on connected anomalous subgraph detection because of the challenge of optimizing graph functionals under connectivity constraints. We employ Non-Parametric Graph Scan (NPGS) statistics for detecting anomalies within graph-based data. Based on the NPGS statistics, we proposed an efficient approximate approach to the connected anomalous subgraph detection problem that provides provable guarantees on performance and quality. In particular, we first decompose the problem into a sequence of subproblems, each of which can be reduced to a Budget Price-Collecting Steiner Tree (BPCST) problem, and then develop efficient exact and approximate algorithms for a special category of graphs in which the anomalous subgraphs can be reformulated in a fixed tree topology. Our method has a wide variety of applications, such as disease outbreak detection, road traffic congestion detection, and event detection in social media, because the NPGS statistics is free of distribution assumptions and can be applied to heterogeneous graph data."
1004,,Improved Consistent Weighted Sampling Revisited.,"Wei Wu 0011,Bin Li 0015,Ling Chen 0006,Chengqi Zhang,Philip S. Yu",https://doi.org/10.1109/TKDE.2018.2876250,TKDE,2019,"Computational complexity,Indexes,Approximation algorithms,Big Data,Machine learning,Text mining","Min-Hash is a popular technique for efficiently estimating the Jaccard similarity of binary sets. Consistent Weighted Sampling (CWS) generalizes the Min-Hash scheme to sketch weighted sets and has drawn increasing interest from the community. Due to its constant-time complexity independent of the values of the weights, Improved CWS (ICWS) is considered as the state-of-the-art CWS algorithm. In this paper, we revisit ICWS and analyze its underlying mechanism to show that there actually exists dependence between the two components of the hash-code produced by ICWS, which violates the condition of independence. To remedy the problem, we propose an Improved ICWS (I
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
CWS) algorithm which not only shares the same theoretical computational complexity as ICWS but also abides by the required conditions of the CWS scheme. The experimental results on a number of synthetic data sets and real-world text data sets demonstrate that our I
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
CWS algorithm can estimate the Jaccard similarity more accurately, and also competes with or outperforms the compared methods, including ICWS, in classification and top-K retrieval, after relieving the underlying dependence."
1005,,Complete Random Forest Based Class Noise Filtering Learning for Improving the Generalizability of Classifiers.,"Shuyin Xia,Guoyin Wang 0001,Zizhong Chen,Yanlin Duan,Qun Liu",https://doi.org/10.1109/TKDE.2018.2873791,TKDE,2019,"Vegetation,Training,Noise measurement,Learning systems,Support vector machines,Decision trees","The existing noise detection methods required the classifiers or distance measurements or data overall distribution, and `curse of dimensionality' and other restrictions made them insufficiently effective in complex data, e.g., different attribute weights, high-dimensionality, containing feature noise, nonlinearity, etc. This is also the main reason that the existing noise filtering methods were not widely applied and formed an effective learning framework. To address this problem, we propose here a complete and efficient random forest method (CRF) specifically for the class noise detection by simulating the grid generation and expansion. The CRF is not based on distance measures or overall distribution or classifiers; besides, the voting mechanism makes it able to effectively process datasets containing feature noise. Furthermore, we introduce CRF based class noise filtering learning framework (CRF-NFL) and derive its mathematical model. The framework is then applied to many widely used classifiers including some state-of-the-art algorithms, e.g., k-means tree, GBDT, and XGBoost. Moreover, its parallelized is designed for large-scale data. The CRF-NFL show much better generalizability than the conventional classifiers and the relative density-based method, which is the most effective noise filtering method as far as we know. All research has formed an open source library, called CRF-NFL: http://www.cquptshuyinxia.com/CRF-NFL.html."
1006,,Towards Confidence Interval Estimation in Truth Discovery.,"Houping Xiao,Jing Gao 0004,Qi Li 0012,Fenglong Ma,Lu Su,Yunlong Feng,Aidong Zhang",https://doi.org/10.1109/TKDE.2018.2837026,TKDE,2019,"Estimation,Task analysis,Robustness,Data mining,Sociology,Statistics","The demand for automatic extraction of true information (i.e., truths) from conflicting multi-source data has soared recently. A variety of truth discovery methods have witnessed great successes via jointly estimating source reliability and truths. All existing truth discovery methods focus on providing a point estimator for each object's truth, but in many real-world applications, confidence interval estimation of truths is more desirable, since confidence interval contains richer information. To address this challenge, in this paper, we propose a novel truth discovery method (ETCIBoot) to construct confidence interval estimates as well as identify truths, where the bootstrapping techniques are nicely integrated into the truth discovery procedure. Due to the properties of bootstrapping, the estimators obtained by ETCIBoot are more accurate and robust compared with the state-of-the-art truth discovery approaches. The proposed framework is further adapted to deal with large-scale truth discovery task in distributed paradigm. Theoretically, we prove the asymptotical consistency of the confidence interval obtained by ETCIBoot. Experimentally, we demonstrate that ETCIBoot is not only effective in constructing confidence intervals but also able to obtain better truth estimates."
1007,,Dynamic Talent Flow Analysis with Deep Sequence Prediction Modeling.,"Huang Xu,Zhiwen Yu 0001,Jingyuan Yang,Hui Xiong 0001,Hengshu Zhu",https://doi.org/10.1109/TKDE.2018.2873341,TKDE,2019,"Predictive models,Companies,Data models,Analytical models,Forecasting,Recurrent neural networks","Talent flow analysis is a process for analyzing and modeling the flows of employees into and out of targeted organizations, regions, or industries. A clear understanding of talent flows is critical for many applications, such as human resource planning, brain drain monitoring, and future workforce forecasting. However, existing studies on talent flow analysis are either qualitative or limited by coarse level quantitative modeling. To this end, in this paper, we provide a fine-grained data-driven approach to model the dynamics and evolving nature of talent flows by leveraging the rich information available in job transition networks. Specifically, we first investigate how to enrich the sparse talent flow data by exploiting the correlations between the stock price movement and the talent flows of public companies. Then, we formalize the talent flow modeling problem as to predict the increments of the edge weights in the dynamic job transition network. In this way, the problem is transformed into a multi-step time series forecasting problem. A deep sequence prediction model is developed based on the recurrent neural network model, which consumes multiple input sources derived from dynamic job transition networks. Finally, experimental results on real-world data show that the proposed model outperforms other benchmark models in terms of prediction accuracy. The results also indicate that the proposed model can provide reasonable performance even if the historical talent flow data are not completely available."
1008,,Modeling the Parameter Interactions in Ranking SVM with Low-Rank Approximation.,"Jun Xu 0001,Wei Zeng 0008,Yanyan Lan,Jiafeng Guo,Xueqi Cheng",https://doi.org/10.1109/TKDE.2018.2851257,TKDE,2019,"Support vector machines,Training,Matrix decomposition,Analytical models,Linear programming,Matrix converters,Task analysis","Ranking SVM, which formalizes the problem of learning a ranking model as that of learning a binary SVM on preference pairs of documents, is a state-of-the-art ranking model in information retrieval. The dual form solution of a linear Ranking SVM model can be written as a linear combination of the preference pairs, i.e., w = Σ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">(i,j)</sub>
 α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
 x
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sub>
-x
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</sub>
), where α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
 denotes the Lagrange parameters associated with each preference pair (i,j). It is observed that there exist obvious interactions among the document pairs because two preference pairs could share a same document as their items, e.g., preference pairs (d
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
,d
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
) and (d
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
,d
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sub>
) share the document d
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
. Thus it is natural to ask if there also exist interactions over the model parameters α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
, which may be leveraged to construct better ranking models. This paper aims to answer the question. We empirically found that there exists a low-rank structure over the rearranged Ranking SVM model parameters α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
, which indicates that the interactions do exist. Based on the discovery, we made modifications on the original Ranking SVM model by explicitly applying low-rank constraints to the Lagrange parameters, achieving two novel algorithms called Factorized Ranking SVM and Regularized Ranking SVM, respectively. Specifically, in Factorized Ranking SVM each parameter α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
 is decomposed as a product of two low-dimensional vectors, i.e., α
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ij</sub>
=〈v
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sub>
,v
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</sub>
〉, where vectors v
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sub>
 and v
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</sub>
 correspond to document i and j, respectively; In Regularized Ranking SVM, a nuclear norm is applied to the rearranged parameters matrix for controlling its rank. Experimental results on three LETOR datasets show that both of the proposed methods can outperform state-of-the-art learning to rank models including the conventional Ranking SVM."
1009,,Exploiting the Dynamic Mutual Influence for Predicting Social Event Participation.,"Tong Xu 0001,Hengshu Zhu,Hao Zhong,Guannan Liu,Hui Xiong 0001,Enhong Chen",https://doi.org/10.1109/TKDE.2018.2851222,TKDE,2019,"Social network services,Decision making,Games,Social networking (online)","It is commonly seen that social events are organized through online social network services (SNSs), and thus there are vested interests in studying event-oriented social gathering through SNSs. The focus of existing studies has been put on the analysis of event profiles or individual participation records. While there is significant dynamic mutual influence among target users through their social connections, the impact of dynamic mutual influence on the people's social gathering remains unknown. To that end, in this paper, we develop a discriminant framework, which allows to integrate the dynamic mutual dependence of potential event participants into the discrimination process. Specifically, we formulate the group-oriented event participation problem as a two-stage variant discriminant framework to capture the users' profiles as well as their latent social connections. The validation on real-world data sets show that our method can effectively predict the event participation with a significant margin compared with several state-of-the-art baselines. This validates the hypothesis that dynamic mutual influence could play an important role in the decision-making process of social event participation. Moreover, we propose the network pruning method to further improve the efficiency of our technical framework. Finally, we provide a case study to illustrate the application of our framework for event plan design task."
1010,,Comments Mining With TF-IDF - The Inherent Bias and Its Removal.,"Inbal Yahav,Onn Shehory,David G. Schwartz",https://doi.org/10.1109/TKDE.2018.2840127,TKDE,2019,"Message systems,Sentiment analysis,Feature extraction,Business,Facebook,Data science,Text analysis","Text mining have gained great momentum in recent years, with user-generated content becoming widely available. One key use is comment mining, with much attention being given to sentiment analysis and opinion mining. An essential step in the process of comment mining is text pre-processing; a step in which each linguistic term is assigned with a weight that commonly increases with its appearance in the studied text, yet is offset by the frequency of the term in the domain of interest. A common practice is to use the well-known tf-idf formula to compute these weights. This paper reveals the bias introduced by between-participants' discourse to the study of comments in social media, and proposes an adjustment. We find that content extracted from discourse is often highly correlated, resulting in dependency structures between observations in the study, thus introducing a statistical bias. Ignoring this bias can manifest in a non-robust analysis at best and can lead to an entirely wrong conclusion at worst. We propose an adjustment to tf-idf that accounts for this bias. We illustrate the effects of both the bias and correction with with seven Facebook fan pages data, covering different domains, including news, finance, politics, sport, shopping, and entertainment."
1011,,Hierarchical Multi-Clue Modelling for POI Popularity Prediction with Heterogeneous Tourist Information.,"Yang Yang 0002,Yaqian Duan,Xinze Wang,Zi Huang,Ning Xie 0003,Heng Tao Shen",https://doi.org/10.1109/TKDE.2018.2842190,TKDE,2019,"Predictive models,Social network services,Visualization,Collaboration,Semantics,Biological system modeling,Data models","Predicting the popularity of Point of Interest (POI) has become increasingly crucial for location-based services, such as POI recommendation. Most of the existing methods can seldom achieve satisfactory performance due to the scarcity of POI's information, which tendentiously confines the recommendation to popular scene spots, and ignores the unpopular attractions with potentially precious values. In this paper, we propose a novel approach, termed Hierarchical Multi-Clue Fusion (HMCF), for predicting the popularity of POIs. Specifically, in order to cope with the problem of data sparsity, we propose to comprehensively describe POI using various types of user generated content (UGC) (e.g., text and image) from multiple sources. Then, we devise an effective POI modelling method in a hierarchical manner, which simultaneously injects semantic knowledge as well as multi-clue representative power into POIs. For evaluation, we construct a multi-source POI dataset by collecting all the textual and visual content of several specific provinces in China from four main-stream tourism platforms during 2006 to 2017. Extensive experimental results show that the proposed method can significantly improve the performance of predicting the attractions' popularity as compared to several baseline methods."
1012,,D22HistoSketch - Discriminative and Dynamic Similarity-Preserving Sketching of Streaming Histograms.,"Dingqi Yang,Bin Li 0015,Laura Rettig,Philippe Cudré-Mauroux",https://doi.org/10.1109/TKDE.2018.2867468,TKDE,2019,"Histograms,Streaming media,Task analysis,Maintenance engineering,Machine learning,Weight measurement,Memory management","Histogram-based similarity has been widely adopted in many machine learning tasks. However, measuring histogram similarity is a challenging task for streaming histograms, where the elements of a histogram are observed one after the other in an online manner. The ever-growing cardinality of histogram elements over the data streams makes any similarity computation inefficient in that case. To tackle this problem, we propose in this paper D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
HistoSketch, a similarity-preserving sketching method for streaming histograms to efficiently approximate their Discriminative and Dynamic similarity. D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
HistoSketch can fast and memory-efficiently maintain a set of compact and fixed-size sketches of streaming histograms to approximate the similarity between histograms. To provide high-quality similarity approximations, D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
HistoSketch considers both discriminative and gradual forgetting weights for similarity measurement, and seamlessly incorporates them in the sketches. Based on both synthetic and real-world datasets, our empirical evaluation shows that our method is able to efficiently and effectively approximate the similarity between streaming histograms while outperforming state-of-the-art sketching methods. Compared to full streaming histograms with both discriminative and gradual forgetting weights in particular, D
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
HistoSketch is able to dramatically reduce the classification time (with a 7500x speedup) at the expense of a small loss in accuracy only (about 3.25 percent)."
1013,1,Privacy-Preserving Social Media Data Publishing for Personalized Ranking-Based Recommendation.,"Dingqi Yang,Bingqing Qu,Philippe Cudré-Mauroux",https://doi.org/10.1109/TKDE.2018.2840974,TKDE,2019,"Data privacy,Social network services,Publishing,Distortion,Privacy,Engines,Loss measurement","Personalized recommendation is crucial to help users find pertinent information. It often relies on a large collection of user data, in particular users' online activity (e.g., tagging/rating/checking-in) on social media, to mine user preference. However, releasing such user activity data makes users vulnerable to inference attacks, as private data (e.g., gender) can often be inferred from the users' activity data. In this paper, we proposed PrivRank, a customizable and continuous privacy-preserving social media data publishing framework protecting users against inference attacks while enabling personalized ranking-based recommendations. Its key idea is to continuously obfuscate user activity data such that the privacy leakage of user-specified private data is minimized under a given data distortion budget, which bounds the ranking loss incurred from the data obfuscation process in order to preserve the utility of the data for enabling recommendations. An empirical evaluation on both synthetic and real-world datasets shows that our framework can efficiently provide effective and continuous protection of user-specified private data, while still preserving the utility of the obfuscated data for personalized ranking-based recommendation. Compared to state-of-the-art approaches, PrivRank achieves both a better privacy protection and a higher utility in all the ranking-based recommendation use cases we tested."
1014,,Accelerated and Inexact Soft-Impute for Large-Scale Matrix and Tensor Completion.,"Quanming Yao,James T. Kwok",https://doi.org/10.1109/TKDE.2018.2867533,TKDE,2019,"Tensile stress,Acceleration,Sparse matrices,Approximation algorithms,Convergence,Radio frequency,Prediction algorithms","Matrix and tensor completion aim to recover a low-rank matrix/ tensor from limited observations and have been commonly used in applications such as recommender systems and multi-relational data mining. A state-of-the-art matrix completion algorithm is Soft-Impute, which exploits the special “sparse plus low-rank” structure of the matrix iterates to allow efficient SVD in each iteration. Though Soft-Impute is a proximal algorithm, it is generally believed that acceleration destroys the special structure and is thus not useful. In this paper, we show that Soft-Impute can indeed be accelerated without comprising this structure. To further reduce the iteration time complexity, we propose an approximate singular value thresholding scheme based on the power method. Theoretical analysis shows that the proposed algorithm still enjoys the fast O(1=T
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) convergence rate of accelerated proximal algorithms. We also extend the proposed algorithm to tensor completion with the scaled latent nuclear norm regularizer. We show that a similar “sparse plus low-rank” structure also exists, leading to low iteration complexity and fast O(1=T
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) convergence rate. Besides, the proposed algorithm can be further extended to nonconvex low-rank regularizers, which have better empirical performance than the convex nuclear norm regularizer. Extensive experiments demonstrate that the proposed algorithm is much faster than Soft-Impute and other state-of-the-art matrix and tensor completion algorithms."
1015,,Applying Simulated Annealing and Parallel Computing to the Mobile Sequential Recommendation.,"Zeyang Ye,Keli Xiao,Yong Ge,Yuefan Deng",https://doi.org/10.1109/TKDE.2018.2827047,TKDE,2019,"Public transportation,Simulated annealing,Computational efficiency,Parallel processing,Search problems,Space exploration,Global Positioning System","We speed up the solution of the mobile sequential recommendation (MSR) problem that requires searching optimal routes for empty taxi cabs through mining massive taxi GPS data. We develop new methods that combine parallel computing and the simulated annealing with novel global and local searches. While existing approaches usually involve costly offline algorithms and methodical pruning of the search space, our new methods provide direct real-time search for the optimal route without the offline preprocessing. Our methods significantly reduce computational time for the high dimensional MSR problems from days to seconds based on the real-world data as well as the synthetic ones. We efficiently provide solutions to MSR problems with thousands of pick-up points without offline training, compared to the published record of 25 pick-up points."
1016,,Harnessing Multi-Source Data about Public Sentiments and Activities for Informed Design.,"Linlin You,Bige Tunçer,Hexu Xing",https://doi.org/10.1109/TKDE.2018.2828431,TKDE,2019,"Data visualization,Data mining,Training,Data models,Data integration,Data collection","The intelligence of Smart Cities (SC) is represented by its ability in collecting, managing, integrating, analyzing, and mining multi-source data for valuable insights. In order to harness multi-source data for an informed place design, this paper presents “Public Sentiments and Activities in Places” multi-source data analysis flow (PSAP) in an Informed Design Platform (IDP). In terms of key contributions, PSAP implements 1) an Interconnected Data Model (IDM) to manage multi-source data independently and integrally, 2) an efficient and effective data mining mechanism based on multi-dimension and multi-measure queries (MMQs), and 3) concurrent data processing cascades with Sentiments in Places Analysis Mechanism (SPAM) and Activities in Places Analysis Mechanism (APAM), to fuse social network data with other data on public sentiment and activity comprehensively. As proved by a holistic evaluation, both SPAM and APAM outperform compared methods. Specifically, SPAM improves its classification accuracy gradually and significantly from 72.37 to about 85 percent within nine crowd-calibration cycles, and APAM with an ensemble classifier achieves the highest precision of 92.13 percent, which is approximately 13 percent higher than the second best method. Finally, by applying MMQs on “Sentiment&Activity Linked Data”, various place design insights of our testbed are mined to improve its livability."
1017,,Graph Structure Fusion for Multiview Clustering.,"Kun Zhan,Chaoxi Niu,Changlu Chen,Feiping Nie 0001,Changqing Zhang,Yi Yang 0001",https://doi.org/10.1109/TKDE.2018.2872061,TKDE,2019,"Linear programming,Learning systems,Laplace equations,Clustering algorithms,Kernel,Clustering methods,Indexes","Most existing multiview clustering methods take graphs, which are usually predefined independently in each view, as input to uncover data distribution. These methods ignore the correlation of graph structure among multiple views and clustering results highly depend on the quality of predefined affinity graphs. We address the problem of multiview clustering by seamlessly integrating graph structures of different views to fully exploit the geometric property of underlying data structure. The proposed method is based on the assumption that the intrinsic underlying graph structure would assign corresponding connected component in each graph to the same cluster. Different graphs from multiple views are integrated by using the Hadamard product since different views usually together admit the same underlying structure across multiple views. Specifically, these graphs are integrated into a global one and the structure of the global graph is adaptively tuned by a well-designed objective function so that the number of components of the graph is exactly equal to the number of clusters. It is worth noting that we directly obtain cluster indicators from the graph itself without performing further graph-cut or k-means clustering algorithms. Experiments show the proposed method obtains better clustering performance than the state-of-the-art methods."
1018,,MUSE - Minimum Uncertainty and Sample Elimination Based Binary Feature Selection.,"Zisheng Zhang,Keshab K. Parhi",https://doi.org/10.1109/TKDE.2018.2865778,TKDE,2019,"Feature extraction,Uncertainty,Redundancy,Quantization (signal),Impurities,Mutual information,Computational modeling","This paper presents a novel incremental feature selection method based on minimum uncertainty and feature sample elimination (referred as MUSE). Feature selection is an important step in machine learning. In an incremental feature selection approach, past approaches have attempted to increase class relevance while simultaneously minimizing redundancy with previously selected features. One example of such an approach is the feature selection method of minimum Redundancy Maximum Relevance (mRMR). The proposed approach differs from prior mRMR approach in how the redundancy of the current feature with previously selected features is reduced. In the proposed approach, the feature samples are divided into a pre-specified number of bins; this step is referred to as feature quantization. A novel uncertainty score for each feature is computed by summing the conditional entropies of the bins, and the feature with the lowest uncertainty score is selected. For each bin, its impurity is computed by taking the minimum of the probability of Class 1 and of Class 2. The feature samples corresponding to the bins with impurities below a threshold are discarded and are not used for selection of the subsequent features. The significance of the MUSE feature selection method is demonstrated using the two datasets: arrhythmia and hand digit recognition (Gisette), and datasets for seizure prediction from five dogs and two humans. It is shown that the proposed method outperforms the prior mRMR feature selection method for most cases. For the arrhythmia dataset, the proposed method achieves 30 percent higher sensitivity at the expense of 7 percent loss of specificity. For the Gisette dataset, the proposed method achieves 15 percent higher accuracy for Class 2, at the expense of 3 percent lower accuracy for Class 1. With respect to seizure prediction among 5 dogs and 2 humans, the proposed method achieves higher area-under-curve (AUC) for all subjects."
1019,,HCBC - A Hierarchical Case-Based Classifier Integrated with Conceptual Clustering.,"Qi Zhang 0020,Chongyang Shi,Zhendong Niu,Longbing Cao",https://doi.org/10.1109/TKDE.2018.2824317,TKDE,2019,"Lattices,Weight measurement,Resource management,Cognition,Indexes,Semantics","The structured case representation improves case-based reasoning (CBR) by exploring structures in the case base and the relevance of case structures. Recent CBR classifiers have mostly been built upon the attribute-value case representation rather than structured case representation, in which the structural relations embodied in their representation structure are accordingly overlooked in improving the similarity measure. This results in retrieval inefficiency and limitations on the performance of CBR classifiers. This paper proposes a hierarchical case-based classifier, HCBC, which introduces a concept lattice to hierarchically organize cases. By exploiting structural case relations in the concept lattice, a novel dynamic weighting model is proposed to enhance the concept similarity measure. Based on this similarity measure, HCBC retrieves the top-K concepts that are most similar to a new case by using a bottom-up pruning-based recursive retrieval (PRR) algorithm. The concepts extracted in this way are applied to suggest a class label for the case by a weighted majority voting. Experimental results show that HCBC outperforms other classifiers in terms of classification performance and robustness on categorical data, and also works confidently well on numeric datasets. In addition, PRR effectively reduces the search space and greatly improves the retrieval efficiency of HCBC."
1020,,IAD - Interaction-Aware Diffusion Framework in Social Networks.,"Xi Zhang 0008,Yuan Su,Siyu Qu,Sihong Xie,Binxing Fang,Philip S. Yu",https://doi.org/10.1109/TKDE.2018.2857492,TKDE,2019,"Diffusion processes,Social network services,Predictive models,Sentiment analysis,Computer science,Data mining","In networks, multiple contagions, such as information and purchasing behaviors, may interact with each other as they spread simultaneously. However, most of the existing information diffusion models are built on the assumption that each individual contagion spreads independently, regardless of their interactions. Gaining insights into such interaction is crucial to understand the contagion adoption behaviors, and thus can make better predictions. In this paper, we study the contagion adoption behavior under a set of interactions, specifically, the interactions among users, contagions' contents, and sentiments, which are learned from social network structures and texts. We develop an effective and efficient interaction-aware diffusion (IAD) framework, incorporating these interactions into a unified model. We also present a generative process to distinguish user roles, a co-training method to determine contagions' categories and a new topic model to obtain topic-specific sentiments. Evaluation on the large-scale Weibo dataset demonstrates that our proposal can learn how different users, contagion categories, and sentiments interact with each other efficiently. With these interactions, we can make a more accurate prediction than the state-of-art baselines. Moreover, we can better understand how the interactions influence the propagation process and thus can suggest useful directions for information promotion or suppression in viral marketing."
1021,,Attributed Network Alignment - Problem Definitions and Fast Solutions.,"Si Zhang,Hanghang Tong",https://doi.org/10.1109/TKDE.2018.2866440,TKDE,2019,"Symmetric matrices,Topology,Approximation algorithms,Network topology,Optimization,Feature extraction,Modeling","Networks are prevalent and often collected from multiple sources in many high-impact domains, which facilitate many emerging applications that require the connections across multiple networks. Network alignment (i.e., to find the node correspondence between different networks) has become the very first step in many applications and thus has been studied in decades. Although some existing works can use the attribute information as part of the alignment process, they still have certain limitations. For example, some existing network alignment methods can use node attribute similarities as part of the prior alignment information, whereas most of them solely explore the topology consistency without the consistency among attributes of the underlying networks. On the other hand, traditional graph matching methods encode both the node and edge attributes (and possibly the topology) into an affinity matrix and formulate it as a constrained nonconvex quadratic maximization problem. However, these methods cannot scale well to the large-scale networks. In this paper, we propose a family of network alignment algorithms (FINAL) to efficiently align the attributed networks. The key idea is to leverage the node/edge attribute information to guide the (topology-based) alignment process. We formulate this problem as a convex quadratic optimization problem, and develop effective and efficient algorithms to solve it. Moreover, we derive FINAL On-Query, an online variant of FINAL that can find similar nodes for the query nodes across networks. We perform extensive evaluations on real networks to substantiate the superiority of our proposed approaches."
1022,,Ensemble Learning from Crowds.,"Jing Zhang 0015,Ming Wu,Victor S. Sheng",https://doi.org/10.1109/TKDE.2018.2860992,TKDE,2019,"Noise measurement,Inference algorithms,Crowdsourcing,Training,Labeling,Data models,Learning systems","Traditional learning from crowdsourced labeled data consists of two stages: inferring true labels for instances from their multiple noisy labels and building a learning model using these instances with the inferred labels. This straightforward two-stage learning scheme suffers from two weaknesses: (1) the accuracy of inference may be very low; (2) useful information may be lost during inference. In this paper, we proposed a novel ensemble method for learning from crowds. Our proposed method is a meta-learning scheme. It first uses a bootstrapping process to create M sub-datasets from an original crowdsourced labeled dataset. For each sub-dataset, each instance is duplicated with different weights according to the distribution and class memberships of its multiple noisy labels. A base classifier is then trained from this extended sub-dataset. Finally, unlabeled instances are predicted by aggregating the outputs of these M base classifiers. Because the proposed method gets rid of the inference procedure and uses the full dataset to train learning models, it preserves the useful information for learning as much as possible. Experimental results on nine simulated and two real-world crowdsourcing datasets consistently show that the proposed ensemble learning method significantly outperforms five state-of-the-art methods."
1023,,"Causal Modeling-Based Discrimination Discovery and Removal - Criteria, Bounds, and Algorithms.","Lu Zhang 0021,Yongkai Wu,Xintao Wu",https://doi.org/10.1109/TKDE.2018.2872988,TKDE,2019,"Mathematical model,Predictive models,Measurement,Law,Data models,Prediction algorithms,Task analysis","Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data are used for predictive analysis (e.g., building classifiers). The main drawback of existing methods is that they cannot distinguish the part of influence that is really caused by discrimination from all correlated influences. In our approach, we make use of the causal graph to capture the causal structure of the data. Then, we model direct and indirect discrimination as the path-specific effects, which accurately identify the two types of discrimination as the causal effects transmitted along different paths in the graph. For certain situations where indirect discrimination cannot be exactly measured due to the unidentifiability of some path-specific effects, we develop an upper bound and a lower bound to the effect of indirect discrimination. Based on the theoretical results, we propose effective algorithms for discovering direct and indirect discrimination, as well as algorithms for precisely removing both types of discrimination while retaining good data utility. Experiments using the real dataset show the effectiveness of our approaches."
1024,,Unsupervised Nonnegative Adaptive Feature Extraction for Data Representation.,"Yan Zhang 0053,Zhao Zhang 0001,Sheng Li 0001,Jie Qin,Guangcan Liu,Meng Wang 0001,Shuicheng Yan",https://doi.org/10.1109/TKDE.2018.2877746,TKDE,2019,"Feature extraction,Manifolds,Data mining,Iron,Adaptation models,Kernel,Linear programming","In this paper, we propose a novel unsupervised Nonnegative Adaptive Feature Extraction (NAFE) algorithm for data representation and classification. The formulation of NAFE integrates the sparsity constrained nonnegative matrix factorization (NMF), representation learning, and adaptive reconstruction weight learning into a unified model. Specifically, NAFE performs feature and weight learning over the new robust representations of NMF for more accurate measure and representation. For nonnegative adaptive feature extraction, our NAFE first utilizes the sparsity constrained NMF to obtain the new and robust representations of the original data. To preserve the manifold structures of the learnt new representations, we also incorporate a neighborhood reconstruction error over the weight matrix for joint minimization. Note that to further improve the representation power, the weights are jointly shared in the new low-dimensional nonnegative representation space, low-dimensional nonlinear manifold space, and low-dimensional projective subspace, i.e., local neighborhood information is clearly preserved in different feature spaces so that informative representations and features can be jointly obtained. To enable NAFE to extract features from new data, we also include a feature approximation error by a linear projection so that the learnt extractor can obtain features from new data efficiently. Extensive simulations show that our formulation can deliver state-of-the-art results on several public databases for feature extraction and classification, compared with several related methods."
1025,,Fusion OLAP - Fusing the Pros of MOLAP and ROLAP Together for In-Memory OLAP.,"Yansong Zhang,Yu Zhang,Shan Wang 0001,Jiaheng Lu",https://doi.org/10.1109/TKDE.2018.2867522,TKDE,2019,"Indexes,Computational modeling,Data models,Adaptation models,Coprocessors,Real-time systems","OLAP models can be categorized with two types: MOLAP (multidimensional OLAP) and ROLAP (relational OLAP). In particular, MOLAP is efficient in multidimensional computing at the cost of cube maintenance, while ROLAP reduces the data storage size at the cost of expensive multidimensional join operations. In this paper, we propose a novel Fusion OLAP model to fuse the multidimensional computing model and relational storage model together to make the best aspects of both MOLAP and ROLAP worlds. This is achieved by mapping the relation tables into virtual multidimensional model and binding the multidimensional operations into a set of vector indexes to enable multidimensional computing on relation tables. The Fusion OLAP model can be integrated into the state-of-the-art in-memory databases with additional surrogate key indexes and vector indexes. We compared the Fusion OLAP implementations with three leading analytical in-memory databases. Our comprehensive experimental results show that Fusion OLAP implementation can achieve up to 35, 365, and 169 percent performance improvements based on the Hyper, Vectorwise, and MonetDB databases, respectively, for the Star Schema Benchmark (SSB) with scale factor 100."
1026,,A New Formulation of Linear Discriminant Analysis for Robust Dimensionality Reduction.,"Haifeng Zhao 0001,Zheng Wang 0037,Feiping Nie 0001",https://doi.org/10.1109/TKDE.2018.2842023,TKDE,2019,"Robustness,Optimization,Dimensionality reduction,Linear programming,Linear discriminant analysis,Principal component analysis,Matrix decomposition","Dimensionality reduction is a critical technology in the domain of pattern recognition, and linear discriminant analysis (LDA) is one of the most popular supervised dimensionality reduction methods. However, whenever its distance criterion of objective function uses 
<inline-formula><tex-math notation=""LaTeX"">$L_2$</tex-math></inline-formula>
-norm, it is sensitive to outliers. In this paper, we propose a new formulation of linear discriminant analysis via joint 
<inline-formula><tex-math notation=""LaTeX"">$L_{2,1}$</tex-math></inline-formula>
-norm minimization on objective function to induce robustness, so as to efficiently alleviate the influence of outliers and improve the robustness of proposed method. An efficient iterative algorithm is proposed to solve the optimization problem and proved to be convergent. Extensive experiments are performed on an artificial data set, on UCI data sets, and on four face data sets, which sufficiently demonstrates the efficiency of comparing to other methods and robustness to outliers of our approach."
1027,,Triangle Lasso for Simultaneous Clustering and Optimization in Graph Datasets.,"Yawei Zhao,Kai Xu 0004,En Zhu,Xinwang Liu,Xinzhong Zhu,Jianping Yin",https://doi.org/10.1109/TKDE.2018.2865342,TKDE,2019,"Task analysis,Robustness,Optimization,Noise measurement,Data analysis,Network topology,Convex functions","Recently, network lasso has dawn much attention due to its remarkable performance on simultaneous clustering and optimization. However, it usually suffers from the imperfect data (noise, missing values, etc.), and yields sub-optimal solutions. The reason is that it finds the similar instances according to their features directly, which is usually impacted by the imperfect data, and thus returns sub-optimal results. In this paper, we propose triangle lasso to avoid its disadvantage for graph datasets. In a graph dataset, each instance is represented by a vertex. If two instances have many common adjacent vertices, they tend to become similar. Although some instances are profiled by the imperfect data, it is still able to find the similar counterparts. Furthermore, we develop an efficient algorithm based on Alternating Direction Method of Multipliers (ADMM) to obtain a moderately accurate solution. In addition, we present a dual method to obtain the accurate solution with the low additional time consumption. We demonstrate through extensive numerical experiments that triangle lasso is robust to the imperfect data. It usually yields a better performance than the state-of-the-art method when performing data analysis tasks in practical scenarios."
1028,,Adaptive Cost-Sensitive Online Classification.,"Peilin Zhao,Yifan Zhang 0004,Min Wu 0008,Steven C. H. Hoi,Mingkui Tan,Junzhou Huang",https://doi.org/10.1109/TKDE.2018.2826011,TKDE,2019,"Predictive models,Task analysis,Prediction algorithms,Optimization,Sensitivity,Indexes","Cost-Sensitive Online Classification has drawn extensive attention in recent years, where the main approach is to directly online optimize two well-known cost-sensitive metrics: (i) weighted sum of sensitivity and specificity and (ii) weighted misclassification cost. However, previous existing methods only considered first-order information of data stream. It is insufficient in practice, since many recent studies have proved that incorporating second-order information enhances the prediction performance of classification models. Thus, we propose a family of cost-sensitive online classification algorithms with adaptive regularization in this paper. We theoretically analyze the proposed algorithms and empirically validate their effectiveness and properties in extensive experiments. Then, for better trade off between the performance and efficiency, we further introduce the sketching technique into our algorithms, which significantly accelerates the computational speed with quite slight performance loss. Finally, we apply our algorithms to tackle several online anomaly detection tasks from real world. Promising results prove that the proposed algorithms are effective and efficient in solving cost-sensitive online classification problems in various real-world domains."
1029,,DLTA - A Framework for Dynamic Crowdsourcing Classification Tasks.,"Libin Zheng,Lei Chen 0002",https://doi.org/10.1109/TKDE.2018.2849385,TKDE,2019,"Task analysis,Crowdsourcing,Resource management,Reliability,Quality control,Labeling,Adaptation models","The increasing popularity of crowdsourcing markets enables the application of crowdsourcing classification tasks. How to conduct quality control in such an application to achieve accurate classification results from noisy workers is an important and challenging task, and has drawn broad research interests. However, most existing works do not exploit the label acquisition phase, which results in their disability of making a proper budget allocation. Moreover, some works impractically make the assumption of managing workers, which is not supported by common crowdsourcing platforms such as AMT or CrowdFlower. To overcome these drawbacks, in this paper, we devise a Dynamic Label Acquisition and Answer Aggregation (DLTA) framework for crowdsourcing classification tasks. The framework proceeds in a sequence of rounds, adaptively conducting label inference and label acquisition. In each round, it analyzes the collected answers of previous rounds to perform proper budget allocation, and then issues the resultant query to the crowd. To support DLTA, we propose a generative model for the collection of labels, and correspondingly strategies for label inference and budget allocation. Experimental results show that compared with existing methods, DLTA obtains competitive accuracy in the binary case. Besides, its extended version, which plugs in the state-of-the-art inference technique, achieves the highest accuracy."
1030,,Harmonic Mean Linear Discriminant Analysis.,"Shuai Zheng 0002,Chris Ding,Feiping Nie 0001,Heng Huang",https://doi.org/10.1109/TKDE.2018.2861858,TKDE,2019,"Harmonic analysis,Linear discriminant analysis,Dimensionality reduction,Principal component analysis,Null space,Machine learning algorithms,Two dimensional displays","In machine learning and data mining, dimensionality reduction is one of the main tasks. Linear Discriminant Analysis (LDA) is a widely used supervised dimensionality reduction algorithm and it has attracted a lot of research interests. Classical Linear Discriminant Analysis finds a subspace to minimize within-class distance and maximize between-class distance, where between-class distance is computed using arithmetic mean of all between-class distances. However, arithmetic mean between-class distance has some limitations. First, arithmetic mean gives equal weight to all between-class distances, and large between-class distance could dominate the result. Second, it does not consider pairwise between-class distance and thus some classes may overlap with each other in the subspace. In this paper, we propose two formulations of harmonic mean based Linear Discriminant Analysis: HLDA and HLDAp, to demonstrate the benefit of harmonic mean between-class distance and overcome the limitations of classical LDA. We compare our algorithm with 11 existing single-label algorithms on seven datasets and five existing multi-label algorithms on two datasets. On some single-label experiment data, the classification accuracy absolute percentage increase can reach 39 percent compared to state-of-art existing algorithms; on multi-label data, significant improvement on five evaluation metric has been achieved compared to existing algorithms."
1031,,Modeling Large-Scale Dynamic Social Networks via Node Embeddings.,"Aakas Zhiyuli,Xun Liang 0001,YanFang Chen,Xiaoyong Du 0001",https://doi.org/10.1109/TKDE.2018.2872602,TKDE,2019,"Social network services,Heuristic algorithms,Task analysis,Training,Computational modeling,Network topology,Topology","Given the edge list of a social network, the node embedding method learns the structural features for every node and embeds the features into a vector space. The current related work on node embedding exploits only a portion of existing networks, e.g., static networks. However, social networks are inherently hierarchical and dynamic systems in which the topology changes constantly and the strength of influence of information among neighbors varies with different numbers of hops. We propose a highly efficient node embedding method, DNPS, that is faster and more accurate than state-of-the-art methods and that can further boost the training progress, especially under dynamic conditions. In this paper, we attempt to model the hierarchical and dynamic features of social networks by designing a damping-based sampling algorithm corresponding to a local search-based incremental learning algorithm, which can easily be extended to large-scale scenarios. We conduct extensive experiments on six real-world social networks with three challenging tasks, including missing link prediction, dynamic link prediction, and multi-label classification. The results of the experiments on these tasks demonstrate that the proposed method significantly outperforms the existing methods with different settings."
1032,,Finding Optimal Skyline Product Combinations under Price Promotion.,"Xu Zhou,Kenli Li 0001,ZhiBang Yang,Keqin Li 0001",https://doi.org/10.1109/TKDE.2018.2823707,TKDE,2019,"Approximation algorithms,Aggregates,Tools,Indexes,TV,Portable computers,Information science","Nowadays, with the development of e-commerce, a growing number of customers choose to go shopping online. To find attractive products from online shopping marketplaces, the skyline query is a useful tool which offers more interesting and preferable choices for customers. The skyline query and its variants have been extensively investigated. However, to the best of our knowledge, they have not taken into account the requirements of customers in certain practical application scenarios. Recently, online shopping marketplaces usually hold some price promotion campaigns to attract customers and increase their purchase intention. Considering the requirements of customers in this practical application scenario, we are concerned about product selection under price promotion. We formulate a constrained optimal product combination (COPC) problem. It aims to find out the skyline product combinations which both meet a customer's willingness to pay and bring the maximum discount rate. The COPC problem is significant to offer powerful decision support for customers under price promotion, which is certified by a customer study. To process the COPC problem effectively, we first propose a two list exact (TLE) algorithm. The COPC problem is proven to be NP-hard, and the TLE algorithm is not scalable because it needs to process an exponential number of product combinations. Additionally, we design a lower bound approximate (LBA) algorithm that has a guarantee about the accuracy of the results and an incremental greedy (IG) algorithm that has good performance. The experiment results demonstrate the efficiency and effectiveness of our proposed algorithms."
1033,,Progressive Approaches for Pareto Optimal Groups Computation.,"Xu Zhou,Kenli Li 0001,ZhiBang Yang,Guoqing Xiao 0001,Keqin Li 0001",https://doi.org/10.1109/TKDE.2018.2837117,TKDE,2019,"Heuristic algorithms,Aggregates,Probabilistic logic,Tools,Indexes,Computer science","Group skyline query is a powerful tool for optimal group analysis. Most of the existing group skyline queries select optimal groups by comparing the dominance relationship between aggregate-based points; such feature creates difficulties for users to specify an appropriate aggregate function. Besides, many significant groups that have great attractions to users in practice may be overlooked. To address these issues, the group skyline (GSky) query is formulated on the basis of a general definition of group dominance operator. While the existing GSky query algorithms are effective, there is still room for improvement in terms of progressiveness and efficiency. In this paper, we propose some new lemmas which facilitate direct generation of the GSky query results. Consecutively, we design a layered unit-based (LU) algorithm that applies a layered optimum strategy. Additionally, for the GSky query over the data that are dynamically produced and cannot be indexed, we propose a novel index-independent algorithm, called sorted-based progressive (SP) algorithm. The experimental results demonstrate the effectiveness, efficiency, and progressiveness of the proposed algorithms. By comparing with the state-of-the-art algorithm for the GSky query, our LU algorithm is more scalable and two orders of magnitude faster."
1034,,A Hardware-Accelerated Solution for Hierarchical Index-Based Merge-Join.,"Zimeng Zhou,Chenyun Yu,Sarana Nutanong,Yufei Cui,Chenchen Fu,Chun Jason Xue",https://doi.org/10.1109/TKDE.2018.2822707,TKDE,2019,"Indexes,Field programmable gate arrays,Query processing,Acceleration,Sorting","Hardware acceleration through field programmable gate arrays (FPGAs) has recently become a technique of growing interest for many data-intensive applications. Join query is one of the most fundamental database query types useful in relational database management systems. However, the available solutions so far have been beset by higher costs in comparison to other query types. In this paper, we develop a novel solution to accelerate the processing of sort-merge join queries with low match rates. Specifically, our solution makes use of hierarchical indexes to identify result-yielding regions in the solution space in order to take advantage of result sparseness. Further, in addition to one-dimensional equi-join query processing, our solution supports processing of multidimensional similarity join queries. Experimental results show that our solution is superior to the best existing method in a low match rate setting; the method achieves a speedup factor of 4.8 for join queries with a match rate of 5 percent."
1035,,One-Step Multi-View Spectral Clustering.,"Xiaofeng Zhu 0001,Shichao Zhang,Wei He 0017,Rongyao Hu,Cong Lei,Pengfei Zhu",https://doi.org/10.1109/TKDE.2018.2873378,TKDE,2019,"Clustering methods,Redundancy,Data mining,Noise measurement,Iterative methods,Correlation,Optimization methods","Previous multi-view spectral clustering methods are a two-step strategy, which first learns a fixed common representation (or common affinity matrix) of all the views from original data and then conducts k-means clustering on the resulting common affinity matrix. The two-step strategy is not able to output reasonable clustering performance since the goal of the first step (i.e., the common affinity matrix learning) is not designed for achieving the optimal clustering result. Moreover, the two-step strategy learns the common affinity matrix from original data, which often contain noise and redundancy to influence the quality of the common affinity matrix. To address these issues, in this paper, we design a novel One-step Multi-view Spectral Clustering (OMSC) method to output the common affinity matrix as the final clustering result. In the proposed method, the goal of the common affinity matrix learning is designed to achieving optimal clustering result and the common affinity matrix is learned from low-dimensional data where the noise and redundancy of original high-dimensional data have been removed. We further propose an iterative optimization method to fast solve the proposed objective function. Experimental results on both synthetic datasets and public datasets validated the effectiveness of our proposed method, comparing to the state-of-the-art methods for multi-view clustering."
1036,,Low-Rank Sparse Subspace for Spectral Clustering.,"Xiaofeng Zhu 0001,Shichao Zhang,Yonggang Li,Jilian Zhang,Lifeng Yang,Yue Fang",https://doi.org/10.1109/TKDE.2018.2858782,TKDE,2019,"Clustering methods,Sparse matrices,Redundancy,Correlation,Laplace equations,Feature extraction,Technological innovation","Traditional graph clustering methods consist of two sequential steps, i.e., constructing an affinity matrix from the original data and then performing spectral clustering on the resulting affinity matrix. This two-step strategy achieves optimal solution for each step separately, but cannot guarantee that it will obtain the globally optimal clustering results. Moreover, the affinity matrix directly learned from the original data will seriously affect the clustering performance, since high-dimensional data are usually noisy and may contain redundancy. To address the above issues, this paper proposes a Low-rank Sparse Subspace (LSS) clustering method via dynamically learning the affinity matrix from low-dimensional space of the original data. Specifically, we learn a transformation matrix to project the original data to their low-dimensional space, by conducting feature selection and subspace learning in the sample self-representation framework. Then, we utilize the rank constraint and the affinity matrix directly obtained from the original data to construct a dynamic and intrinsic affinity matrix. Moreover, each of these three matrices is updated iteratively while fixing the other two. In this way, the affinity matrix learned from the low-dimensional space is the final clustering results. Extensive experiments are conducted on both synthetic and real datasets to show that our proposed LSS method outperforms the state-of-the-art clustering methods."
66,,Server Location Verification (SLV) and Server Location Pinning - Augmenting TLS Authentication.,"AbdelRahman Abdou,Paul C. van Oorschot",https://doi.org/10.1145/3139294,TOPS,2018,[],
67,,FOSSIL - A Resilient and Efficient System for Identifying FOSS Functions in Malware Binaries.,"Saed Alrabaee,Paria Shirani,Lingyu Wang 0001,Mourad Debbabi",https://doi.org/10.1145/3175492,TOPS,2018,[],
68,,Utilizing Performance Counters for Compromising Public Key Ciphers.,"Sarani Bhattacharya,Debdeep Mukhopadhyay",https://doi.org/10.1145/3156015,TOPS,2018,[],
69,,Enhancing Branch Monitoring for Security Purposes - From Control Flow Integrity to Malware Analysis and Debugging.,"Marcus Botacin,Paulo Lício de Geus,André Grégio",https://doi.org/10.1145/3152162,TOPS,2018,[],
70,,Security Evaluation of a Banking Fraud Analysis System.,"Michele Carminati,Mario Polino,Andrea Continella,Andrea Lanzi,Federico Maggi,Stefano Zanero",https://doi.org/10.1145/3178370,TOPS,2018,[],
71,,"VULCON - A System for Vulnerability Prioritization, Mitigation, and Management.","Katheryn A. Farris,Ankit Shah 0002,George Cybenko,Rajesh Ganesan,Sushil Jajodia",https://doi.org/10.1145/3196884,TOPS,2018,[],
72,,Abstract Non-Interference - A Unifying Framework for Weakening Information-flow.,"Roberto Giacobazzi,Isabella Mastroeni",https://doi.org/10.1145/3175660,TOPS,2018,[],
73,,Attribute Inference Attacks in Online Social Networks.,"Neil Zhenqiang Gong,Bin Liu 0020",https://doi.org/10.1145/3154793,TOPS,2018,[],
74,,Data Usage Control for Distributed Systems.,"Florian Kelbert,Alexander Pretschner",https://doi.org/10.1145/3183342,TOPS,2018,[],
75,,Efficient Privacy-Preserving Matrix Factorization for Recommendation via Fully Homomorphic Encryption.,"Jinsu Kim,Dongyoung Koo,Yuna Kim,Hyunsoo Yoon,Junbum Shin,Sungwook Kim",https://doi.org/10.1145/3212509,TOPS,2018,[],
76,,Technological and Human Factors of Malware Attacks - A Computer Security Clinical Trial Approach.,"Fanny Lalonde Lévesque,Sonia Chiasson,Anil Somayaji,José M. Fernandez 0001",https://doi.org/10.1145/3210311,TOPS,2018,[],
77,,Scalable Private Set Intersection Based on OT Extension.,"Benny Pinkas,Thomas Schneider 0003,Michael Zohner",https://doi.org/10.1145/3154794,TOPS,2018,[],
78,,Handling Anti-Virtual Machine Techniques in Malicious Software.,"Hao Shi,Jelena Mirkovic,Abdulla Alwabel",https://doi.org/10.1145/3139292,TOPS,2018,[],
79,,GyrosFinger - Fingerprinting Drones for Location Tracking Based on the Outputs of MEMS Gyroscopes.,"Yunmok Son,Juhwan Noh,Jaeyeong Choi,Yongdae Kim",https://doi.org/10.1145/3177751,TOPS,2018,[],
80,,The Password Life Cycle.,"Elizabeth Stobert,Robert Biddle",https://doi.org/10.1145/3183341,TOPS,2018,[],
81,,Amandroid - A Precise and General Inter-component Data Flow Analysis Framework for Security Vetting of Android Apps.,"Fengguo Wei,Sankardas Roy,Xinming Ou,Robby",https://doi.org/10.1145/3183575,TOPS,2018,[],
82,,A Video-based Attack for Android Pattern Lock.,"Guixin Ye,Zhanyong Tang,Dingyi Fang,Xiaojiang Chen,Willy Wolff,Adam J. Aviv,Zheng Wang 0001",https://doi.org/10.1145/3230740,TOPS,2018,[],
83,,Implementing Support for Pointers to Private Data in a General-Purpose Secure Multi-Party Compiler.,"Yihua Zhang,Marina Blanton,Ghada Almashaqbeh",https://doi.org/10.1145/3154600,TOPS,2018,[],
84,,Verifiable Graph Processing.,"Yupeng Zhang 0001,Charalampos Papamanthou,Jonathan Katz",https://doi.org/10.1145/3233181,TOPS,2018,[],
85,,FIMCE - A Fully Isolated Micro-Computing Environment for Multicore Systems.,"Siqi Zhao,Xuhua Ding",https://doi.org/10.1145/3195181,TOPS,2018,[],
1037,,Game-Theoretic Cross Social Media Analytic - How Yelp Ratings Affect Deal Selection on Groupon?,"Chih-Yu Wang 0001,Yan Chen 0007,K. J. Ray Liu",https://doi.org/10.1109/TKDE.2017.2779494,TKDE,2018,"Social network services,Games,Stochastic processes,Knowledge engineering,Nash equilibrium,Advertising","Deal selection on Groupon is a typical social learning and decision making process, where the quality of a deal is usually unknown to the customers. The customers must acquire this knowledge through social learning from other social medias such as reviews on Yelp. Additionally, the quality of a deal depends on both the state of the vendor and decisions of other customers on Groupon. How social learning and network externality affect the decisions of customers in deal selection on Groupon is our main interest. We develop a data-driven game-theoretic framework to understand the rational deal selection behaviors cross social medias. The sufficient condition of the Nash equilibrium is identified. A value-iteration algorithm is proposed to find the optimal deal selection strategy. We conduct a year-long experiment to trace the competitions among deals on Groupon and the corresponding Yelp ratings. We utilize the dataset to analyze the deal selection game with realistic settings. Finally, the performance of the proposed social learning framework is evaluated with real data. The results suggest that customers do make decisions in a rational way instead of following naive strategies, and there is still room to improve their decisions with assistance from the proposed framework."
1038,,Density-Based Place Clustering Using Geo-Social Network Data.,"Dingming Wu 0001,Jieming Shi,Nikos Mamoulis",https://doi.org/10.1109/TKDE.2017.2782256,TKDE,2018,"Social network services,Cleaning,Urban planning,Data models,Entropy,Clustering algorithms,Spatial databases","Spatial clustering deals with the unsupervised grouping of places into clusters and finds important applications in urban planning and marketing. Current spatial clustering models disregard information about the people and the time who and when are related to the clustered places. In this paper, we show how the density-based clustering paradigm can be extended to apply on places which are visited by users of a geo-social network. Our model considers spatio-temporal information and the social relationships between users who visit the clustered places. After formally defining the model and the distance measure it relies on, we provide alternatives to our model and the distance measure. We evaluate the effectiveness of our model via a case study on real data; in addition, we design two quantitative measures, called social entropy and community score, to evaluate the quality of the discovered clusters. The results show that temporal-geo-social clusters have special properties and cannot be found by applying simple spatial clustering approaches and other alternatives."
1039,,Propagation-Based Temporal Network Summarization.,"Bijaya Adhikari,Yao Zhang 0003,Sorour E. Amiri,Aditya Bharadwaj,B. Aditya Prakash",https://doi.org/10.1109/TKDE.2017.2776282,TKDE,2018,"Silicon,Integrated circuit modeling,Heuristic algorithms,Eigenvalues and eigenfunctions,Event detection,Algorithm design and analysis","Modern networks are very large in size and also evolve with time. As their sizes grow, the complexity of performing network analysis grows as well. Getting a smaller representation of a temporal network with similar properties will help in various data mining tasks. In this paper, we study the novel problem of getting a smaller diffusion-equivalent representation of a set of time-evolving networks. We first formulate a well-founded and general temporal-network condensation problem based on the so-called systemmatrix of the network. We then propose NETCONDENSE, a scalable and effective algorithm which solves this problem using careful transformations in sub-quadratic running time, and linear space complexities. Our extensive experiments show that we can reduce the size of large real temporal networks (from multiple domains such as social, co-authorship, and email) significantly without much loss of information. We also show the wide-applicability of NETCONDENSE by leveraging it for several tasks: for example, we use it to understand, explore, and visualize the original datasets and to also speed-up algorithms for the influence-maximization and event detection problems on temporal networks."
1040,1,Privacy Characterization and Quantification in Data Publishing.,"M. H. Afifi,Kai Zhou,Jian Ren 0001",https://doi.org/10.1109/TKDE.2018.2797092,TKDE,2018,"Data privacy,Publishing,Privacy,Measurement,Data models,Couplings,Entropy","The increasing interest in collecting and publishing large amounts of individuals' data as public for purposes such as medical research, market analysis, and economical measures has created major privacy concerns about individual's sensitive information. To deal with these concerns, many Privacy-Preserving Data Publishing (PPDP) techniques have been proposed in literature. However, they lack a proper privacy characterization and measurement. In this paper, we first present a novel multi-variable privacy characterization and quantification model. Based on this model, we are able to analyze the prior and posterior adversarial belief about attribute values of individuals. We can also analyze the sensitivity of any identifier in privacy characterization. Then, we show that privacy should not be measured based on one metric. We demonstrate how this could result in privacy misjudgment. We propose two different metrics for quantification of privacy leakage, distribution leakage, and entropy leakage. Using these metrics, we analyzed some of the most well-known PPDP techniques such as k-anonymity, l-diversity, and t-closeness. Based on our framework and the proposed metrics, we can determine that all the existing PPDP schemes have limitations in privacy characterization. Our proposed privacy characterization and measurement framework contributes to better understanding and evaluation of these techniques. Thus, this paper provides a foundation for design and analysis of PPDP schemes."
1041,,Personalized and Diverse Task Composition in Crowdsourcing.,"Maha Alsayasneh,Sihem Amer-Yahia,Éric Gaussier,Vincent Leroy 0001,Julien Pilourdault,Ria Mae Borromeo,Motomichi Toyama,Jean-Michel Renders",https://doi.org/10.1109/TKDE.2017.2755660,TKDE,2018,"Throughput,Crowdsourcing,Optimization,Urban areas,Clustering algorithms,Buildings","We study task composition in crowdsourcing and the effect of personalization and diversity on performance. A central process in crowdsourcing is task assignment, the mechanism through which workers find tasks. On popular platforms such as Amazon Mechanical Turk, task assignment is facilitated by the ability to sort tasks by dimensions such as creation date or reward amount. Task composition improves task assignment by producing for each worker, a personalized summary of tasks, referred to as a Composite Task (CT). We propose different ways of producing CTs and formulate an optimization problem that finds for a worker, the most relevant and diverse CTs. We show empirically that workers' experience is greatly improved due to personalization that enforces an adequation of CTs with workers' skills and preferences. We also study and formalize various ways of diversifying tasks in each CT. Task diversity is grounded in organization studies that have shown its impact on worker motivation [33]. Our experiments show that diverse CTs contribute to improving outcome quality. More specifically, we show that while task throughput and worker retention are best with ranked lists, crowdwork quality reaches its best with CTs diversified by requesters, thereby confirming that workers look to expose their “good” work to many requesters."
1042,,Space Filling Approach for Distributed Processing of Top-k Dominating Queries.,"Daichi Amagata,Takahiro Hara,Makoto Onizuka",https://doi.org/10.1109/TKDE.2018.2790387,TKDE,2018,"Distributed databases,Query processing,Approximation algorithms,Silicon,Distributed processing,Filling","A top-k dominating query returns k data objects that dominate the highest number of data objects in a given dataset. This query provides us with a set of intuitively preferred data, thus can support a wide variety of multi-criteria decision-making applications, e.g., e-commerce and web search. Due to the growth of data centers and cloud computing infrastructures, the above applications are increasingly being operated in distributed environments. These motivate us to address the problem of distributed top-k dominating query processing. We propose an efficient decentralized algorithm that exploits virtual points and returns the exact answer. The virtual points are utilized to focus on the data space to be preferentially searched and also to limit the search space to prune unnecessary computation and data forwarding. We also propose two other algorithms, which return an approximate answer set while further reducing query processing time. Extensive experiments on both real and synthetic data demonstrate the efficiency and scalability of our algorithms."
1043,,Automatic Segmentation of Dynamic Network Sequences with Node Labels.,"Sorour E. Amiri,Liangzhe Chen,B. Aditya Prakash",https://doi.org/10.1109/TKDE.2017.2771776,TKDE,2018,"Integrated circuit modeling,Social network services,Time series analysis,Eigenvalues and eigenfunctions,Silicon,Feature extraction,Diseases","Given a sequence of snapshots of flu propagating over a population network, can we find a segmentation when the patterns of the disease spread change, possibly due to interventions? In this paper, we study the problem of segmenting graph sequences with labeled nodes. Memes on the Twitter network, diseases over a contact network, movie-cascades over a social network, etc. are all graph sequences with labeled nodes. Most related work on this subject is on plain graphs and hence ignores the label dynamics. Others require fix parameters or feature engineering. We propose SNAPNETS, to automatically find segmentations of such graph sequences, with different characteristics of nodes of each label in adjacent segments. It satisfies all the desired properties (being parameter free, comprehensive and scalable) by leveraging a principled, multi-level, flexible framework which maps the problem to a path optimization problem over a weighted DAG. Also, we develop the parallel framework of SNAPNETS which speeds up its running time. Finally, we propose an extension of SNAPNETS to handle the dynamic graph structures and use it to detect anomalies (and events) in network sequences. Extensive experiments on several diverse real datasets show that it finds cut points matching ground-truth or meaningful external signals and detects anomalies outperforming non-trivial baselines. We also show that the segmentations are easily interpretable, and that SNAPNETS scales near-linearly with the size of the input. Finally, we show how to use SNAPNETS to detect anomaly in a sequence of dynamic networks."
1044,,Capturing the Spatiotemporal Evolution in Road Traffic Networks.,"Tarique Anwar,Chengfei Liu,Hai Le Vu,Md. Saiful Islam 0003,Timos Sellis",https://doi.org/10.1109/TKDE.2018.2795001,TKDE,2018,"Roads,Urban areas,Spatiotemporal phenomena,Physical layer,Indexes,Real-time systems,Australia","The urban road networks undergo frequent traffic congestions during the peak hours and around the city center. Capturing the spatiotemporal evolution of the congestion scenario in real-time in an urban-scale can aid in developing smart traffic management systems, and guiding commuters in making informed decision about route choice. The congestion scenario is often represented by a set of distinguishable network partitions that have a homogeneous level of congestion inside them but are heterogeneous to others. Due to the dynamic nature of traffic, these partitions evolve with time in terms of their structure and location. In this paper, we propose a comprehensive framework to capture the evolution by incrementally updating the partitions in an efficient manner using a two-layer approach. The physical layer maintains a set of small-sized road network building blocks in a fine granularity, and performs low-level computations to incrementally update them, whereas the logical layer performs high-level computations in order to serve as an interface to query the physical layer about the congested partitions in a coarse granularity. We also propose an in-memory index called Bin that compactly stores the historical sets of building blocks in the main memory with no information loss, and facilitates their efficient retrieval. Our experimental results show that the proposed method is much efficient than the existing re-partitioning methods without significant sacrifice in accuracy. The proposed Bin consumes a minimum space with least redundancy at different time stamps."
1045,,Mining Precise-Positioning Episode Rules from Event Sequences.,"Xiang Ao,Ping Luo 0001,Jin Wang 0007,Fuzhen Zhuang,Qing He 0003",https://doi.org/10.1109/TKDE.2017.2773493,TKDE,2018,"Data mining,Finite element analysis,Time-frequency analysis,Security,Time factors,Data structures","Episode Rule Mining is a popular framework for discovering sequential rules from event sequential data. However, traditional episode rule mining methods only tell that the consequent event is likely to happen within a given time interval after the occurrence of the antecedent events. As a result, they cannot satisfy the requirement of many time sensitive applications, such as program security trading and intelligent transportation management due to the lack of fine-grained response time. In this study, we come up with the concept of fixed-gap episode to address this problem. A fixed-gap episode consists of an ordered set of events where the elapsed time between any two consecutive events is a constant. Based on this concept, we formulate the problem of mining precise-positioning episode rules in which the occurrence time of each event in the consequent is clearly specified. In addition, we develop a trie-based data structure to mine such precise-positioning episode rules with several pruning strategies incorporated for improving the performance as well as reducing memory consumption. Experimental results on real datasets show the superiority of our proposed algorithms."
1046,,Using Reenactment to Retroactively Capture Provenance for Transactions.,"Bahareh Sadat Arab,Dieter Gawlick,Vasudha Krishnaswamy,Venkatesh Radhakrishnan,Boris Glavic",https://doi.org/10.1109/TKDE.2017.2769056,TKDE,2018,"Databases,Silicon,History,Computational modeling,Debugging,Encoding,Concurrency control","Database provenance explains how results are derived by queries. However, many use cases such as auditing and debugging of transactions require understanding of how the current state of a database was derived by a transactional history. We present MV-semirings, a provenance model for queries and transactional histories that supports two common multi-version concurrency control protocols: snapshot isolation (SI) and read committed snapshot isolation (RC-SI). Furthermore, we introduce an approach for retroactively capturing such provenance using reenactment, a novel technique for replaying a transactional history with provenance capture. Reenactment exploits the time travel and audit logging capabilities of modern DBMS to replay parts of a transactional history using queries. Importantly, our technique requires no changes to the transactional workload or underlying DBMS and results in only moderate runtime overhead for transactions. We have implemented our approach on top of a commercial DBMS and our experiments confirm that by applying novel optimizations we can efficiently capture provenance for complex transactions over large data sets."
1047,,Janus - A Hybrid Scalable Multi-Representation Cloud Datastore.,"Vaibhav Arora,Faisal Nawab,Divyakant Agrawal,Amr El Abbadi",https://doi.org/10.1109/TKDE.2017.2773607,TKDE,2018,"Pipelines,Real-time systems,Engines,Cloud computing,Servers,Distributed databases,Throughput","Cloud-based data-intensive applications have to process high volumes of transactional and analytical requests on large-scale data. Businesses base their decisions on the results of analytical requests, creating a need for real-time analytical processing. We propose Janus, a hybrid scalable cloud datastore, which enables the efficient execution of diverse workloads by storing data in different representations. Janus manages big datasets in the context of datacenters, thus supporting scaling out by partitioning the data across multiple servers. This requires Janus to efficiently support distributed transactions. In order to support the different datacenter requirements, Janus also allows diverse partitioning strategies for the different representations. Janus proposes a novel data movement pipeline to continuously ensure up to date data between the different representations. Unlike existing multi-representation storage systems and Change Data Capture (CDC) pipelines, the data movement pipeline in Janus supports partitioning and handles both distributed transactions and diverse partitioning strategies. In this paper, we focus on supporting Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) workloads, and hence use row and column-oriented representations, which are the most efficient representations for these workloads. Our evaluations over Amazon AWS illustrate that Janus can provide real-time analytical results, in addition to processing high-throughput transactional workloads."
1048,,Efficient and Scalable Integrity Verification of Data and Query Results for Graph Databases.,"Muhammad Umer Arshad,Ashish Kundu,Elisa Bertino,Arif Ghafoor,Chinmay Kundu",https://doi.org/10.1109/TKDE.2017.2776221,TKDE,2018,"Databases,Data models,Publishing,Digital signatures,Social network services,Complexity theory","Graphs are used for representing and understanding objects and their relationships for numerous applications such as social networks, Semantic Webs, and biological networks. Integrity assurance of data and query results for graph databases is an essential security requirement. In this paper, we propose two efficient integrity verification schemes-HMACs for graphs (gHMAC) for two-party data sharing, and redactable HMACs for graphs (rgHMAC) for third-party data sharing, such as a cloud-based graph database service. We compute one HMAC value for both the schemes and two other verification objects for rgHMAC scheme that are shared with the verifier. We show that the proposed schemes are provably secure with respect to integrity attacks on the structure and/or content of graphs and query results. The proposed schemes have linear complexity in terms of the number of vertices and edges in the graphs, which is shown to be optimal. Our experimental results corroborate that the proposed HMAC-based schemes for graphs are highly efficient as compared to the digital signature-based schemes-computation of HMAC tags is about 10 times faster than the computation of digital signatures."
1049,,Cleaning Antipatterns in an SQL Query Log.,"Natalia Arzamasova,Martin Schäler,Klemens Böhm",https://doi.org/10.1109/TKDE.2017.2772252,TKDE,2018,"Cleaning,Data mining,Servers,Object recognition,Indexes,Data preprocessing","Today, many scientific data sets are open to the public. For their operators, it is important to know what the users are interested in. In this paper, we study the problem of extracting and analyzing patterns from the query log of a database. We focus on design errors (antipatterns), which typically lead to unnecessary SQL statements. Such antipatterns do not only have a negative effect on performance. They also introduce bias on any subsequent analysis of the SQL log. We propose a framework designed to discover patterns and antipatterns in arbitrary SQL query logs and to clean antipatterns. To study the usefulness of our approach and to reveal insights regarding the existence of antipatterns in real-world systems, we examine the SQL log of the SkyServer project, containing more than 40 million queries. Among the top 15 patterns, we have found six antipatterns. This result as well as other ones gives way to the conclusion that antipatterns might falsify refactoring and any other downstream analyses."
1050,,Characterizing and Predicting Early Reviewers for Effective Product Marketing on E-Commerce Websites.,"Ting Bai,Wayne Xin Zhao,Yulan He,Jian-Yun Nie,Ji-Rong Wen",https://doi.org/10.1109/TKDE.2018.2821671,TKDE,2018,"Task analysis,Predictive models,Communication channels,Social network services","Online reviews have become an important source of information for users before making an informed purchase decision. Early reviews of a product tend to have a high impact on the subsequent product sales. In this paper, we take the initiative to study the behavior characteristics of early reviewers through their posted reviews on two real-world large e-commerce platforms, i.e., Amazon and Yelp. In specific, we divide product lifetime into three consecutive stages, namely early, majority, and laggards. A user who has posted a review in the early stage is considered as an early reviewer. We quantitatively characterize early reviewers based on their rating behaviors, the helpfulness scores received from others and the correlation of their reviews with product popularity. We have found that (1) an early reviewer tends to assign a higher average rating score; and (2) an early reviewer tends to post more helpful reviews. Our analysis of product reviews also indicates that early reviewers' ratings and their received helpfulness scores are likely to influence product popularity. By viewing the review posting process as a multiplayer competition game, we propose a novel margin-based embedding model for early reviewer prediction. Extensive experiments on two different e-commerce datasets have shown that our proposed approach outperforms a number of competitive baselines."
1051,,Improved Lower Bounds for Graph Edit Distance.,"David B. Blumenthal,Johann Gamper",https://doi.org/10.1109/TKDE.2017.2772243,TKDE,2018,"Approximation algorithms,Measurement,Upper bound,Algorithm design and analysis,Pareto optimization,Databases","The problem of deriving lower and upper bounds for the edit distance between undirected, labeled graphs has recently received increasing attention. However, only one algorithm has been proposed that allegedly computes not only an upper but also a lower bound for non-uniform edit costs and incorporates information about both node and edge labels. In this paper, we demonstrate that this algorithm is incorrect. We present a corrected version BRANCH that runs in O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
Δ
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
 + n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
) time, where Δ is the maximum of the maximum degrees of input graphs G and H. We also develop a speed-up BRANCHFAST that runs in O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
Δ
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 + n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
) time and computes an only slightly less accurate lower bound. The lower bounds produced by BRANCH and BRANCHFAST are shown to be pseudo-metrics on a collection of graphs. Finally, we suggest an anytime algorithm BRANCHTIGHT that iteratively improves BRANCH's lower bound. BRANCHTIGHT runs in O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
Δ
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 + I(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
Δ
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
 + n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
)) time, where the number of iterations I is controlled by the user. A detailed experimental evaluation shows that all suggested algorithms are Pareto optimal, that they are very effective when used as filters for edit distance range queries, and that they perform excellently when used within classification frameworks."
1052,,Self-Tuned Descriptive Document Clustering Using a Predictive Network.,"Austin J. Brockmeier,Tingting Mu,Sophia Ananiadou,John Yannis Goulermas",https://doi.org/10.1109/TKDE.2017.2781721,TKDE,2018,"Predictive models,Logistics,Feature extraction,Motion pictures,Clustering algorithms,Prediction algorithms,Analytical models","Descriptive clustering consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster. The description should inform a user about the contents of each cluster without further examination of the specific instances, enabling a user to rapidly scan for relevant clusters. Selection of descriptions often relies on heuristic criteria. We model descriptive clustering as an auto-encoder network that predicts features from cluster assignments and predicts cluster assignments from a subset of features. The subset of features used for predicting a cluster serves as its description. For text documents, the occurrence or count of words, phrases, or other attributes provides a sparse feature representation with interpretable feature labels. In the proposed network, cluster predictions are made using logistic regression models, and feature predictions rely on logistic or multinomial regression models. Optimizing these models leads to a completely self-tuned descriptive clustering approach that automatically selects the number of clusters and the number of features for each cluster. We applied the methodology to a variety of short text documents and showed that the selected clustering, as evidenced by the selected feature subsets, are associated with a meaningful topical organization."
1053,,RNN-DBSCAN - A Density-Based Clustering Algorithm Using Reverse Nearest Neighbor Density Estimates.,"Avory Bryant,Krzysztof J. Cios",https://doi.org/10.1109/TKDE.2017.2787640,TKDE,2018,"Clustering algorithms,Complexity theory,Measurement,Approximation algorithms,Indexes,Algorithm design and analysis","A new density-based clustering algorithm, RNN-DBSCAN, is presented which uses reverse nearest neighbor counts as an estimate of observation density. Clustering is performed using a DBSCAN-like approach based on k nearest neighbor graph traversals through dense observations. RNN-DBSCAN is preferable to the popular density-based clustering algorithm DBSCAN in two aspects. First, problem complexity is reduced to the use of a single parameter (choice of k nearest neighbors), and second, an improved ability for handling large variations in cluster density (heterogeneous density). The superiority of RNN-DBSCAN is demonstrated on several artificial and real-world datasets with respect to prior work on reverse nearest neighbor based clustering approaches (RECORD, IS-DBSCAN, and ISB-DBSCAN) along with DBSCAN and OPTICS. Each of these clustering approaches is described by a common graph-based interpretation wherein clusters of dense observations are defined as connected components, along with a discussion on their computational complexity. Heuristics for RNN-DBSCAN parameter selection are presented, and the effects of k on RNN-DBSCAN clusterings discussed. Additionally, with respect to scalability, an approximate version of RNN-DBSCAN is presented leveraging an existing approximate k nearest neighbor technique."
1054,,"A Comprehensive Survey of Graph Embedding - Problems, Techniques, and Applications.","Hongyun Cai 0001,Vincent W. Zheng,Kevin Chen-Chuan Chang",https://doi.org/10.1109/TKDE.2018.2807452,TKDE,2018,"Taxonomy,Task analysis,Electronic mail,Social network services,Toy manufacturing industry,Two dimensional displays,Machine learning","Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios."
1055,,Topology-Driven Diversity for Targeted Influence Maximization with Application to User Engagement in Social Networks.,"Antonio Caliò,Roberto Interdonato,Chiara Pulice,Andrea Tagarelli",https://doi.org/10.1109/TKDE.2018.2820010,TKDE,2018,"Cultural differences,Social network services,Linear programming,Task analysis,Behavioral sciences","Research on influence maximization ofter has to cope with marketing needs relating to the propagation of information towards specific users. However, little attention has been paid to the fact that the success of an information diffusion campaign might depend not only on the number of the initial influencers to be detected but also on their 
<i>diversity</i>
 w.r.t. the target of the campaign. Our main hypothesis is that if we learn seeds that are not only capable of influencing but also are linked to more diverse (groups of) users, then the influence triggers will be diversified as well, and hence the target users will get higher chance of being engaged. Upon this intuition, we define a novel problem, named 
<i>Diversity-sensitive Targeted Influence Maximization (DTIM)</i>
, which assumes to model user diversity by exploiting only topological information within a social graph. To the best of our knowledge, we are the first to bring the concept of topology-driven diversity into targeted IM problems, for which we define two alternative definitions. Accordingly, we propose approximate solutions of DTIM, which detect a size- 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 set of users that maximizes the diversity-sensitive capital objective function, for a given selection of target users. We evaluate our DTIM methods on a special case of user engagement in online social networks, which concerns users who are not actively involved in the community life. Experimental evaluation on real networks has demonstrated the meaningfulness of our approach, also highlighting the opportunity of further development of solutions for DTIM applications."
1056,,A Thorough Evaluation of Distance-Based Meta-Features for Automated Text Classification.,"Sérgio D. Canuto,Daniel Xavier de Sousa,Marcos André Gonçalves,Thierson Couto Rosa",https://doi.org/10.1109/TKDE.2018.2820051,TKDE,2018,"Feature extraction,Data engineering,Computational efficiency,Genetic algorithms,Classification,Text classification","We address the problem of automatically learning to classify texts by exploiting information derived from meta-features, i.e., features derived from the original bag-of-words representation. Specifically, we provide an in-depth analysis on the recently proposed distance-based meta-features, a data engineering technique that relies on the distance between documents to transform the original feature space into a new one, potentially smaller and more informed. Despite its potential, the meta-feature space may be unnecessarily complex and highly dimensional, which increases the tendency of overfitting, limits the application of meta-features in different contexts, and increases computational costs. In this work, we propose the use of multi-objective strategies to reduce the number of meta-features while maximizing the classification effectiveness, when considering the adequacy of the selected meta-features to a particular dataset or classification method. We present effective and efficient proposals for meta-feature selection that can substantially reduce the number of meta-features by up to 89 percent while keeping or improving the classification effectiveness, something not possible with any of the evaluated baselines. We also use our selection strategies as evaluation tools to analyze different combinations of meta-features. We found very compact combinations of meta-features that can achieve high classification effectiveness in most datasets, despite their peculiarities."
1057,,$\sf {SIMkNN}$ - A Scalable Method for in-MemorykNN Search over Moving Objects in Road Networks.,"Bin Cao 0004,Chenyu Hou,Suifei Li,Jing Fan,Jianwei Yin,Baihua Zheng,Jie Bao",https://doi.org/10.1109/TKDE.2018.2808971,TKDE,2018,"Indexes,Roads,Search problems,Query processing,Artificial neural networks,Real-time systems,Time factors","Nowadays, many location-based applications require the ability of querying k-nearest neighbors over a very large scale of moving objects in road networks, e.g., taxi-calling and ride-sharing services. Traditional grid index with equal-sized cells can not adapt to the skewed distribution of moving objects in real scenarios. Thus, to obtain the fast querying response time, the grid needs to be split into more smaller cells which introduces the side-effect of higher memory cost, i.e., maintaining such a large volume of cells requires a much larger memory space at the server side. In this paper, we present SIMkNN, a scalable and in-memory kNN query processing technique. SIMkNN is dual-index driven, where we adopt a R-tree to store the topology of the road network and a hierarchical grid model to manage the moving objects in non-uniform distribution. To answer a kNN query in real time, SIMkNN adopts the strategy that incrementally enlarges the search area for network distance based nearest neighbor evaluation. It is far from trivial to perform the space expansion within the hierarchical grid index. For a given cell, we first define its neighbors in different directions, then propose a cell communication technique which allows each cell in the hierarchical grid index to be aware of its neighbors at anytime. Accordingly, an efficient space expansion algorithm to generate the estimation area is proposed. The experimental evaluation shows that SIMkNN outperforms the baseline algorithm in terms of time and memory efficiency."
1058,,On Generalizing Collective Spatial Keyword Queries.,"Harry Kai-Ho Chan,Cheng Long,Raymond Chi-Wing Wong",https://doi.org/10.1109/TKDE.2018.2800746,TKDE,2018,"Cost function,Approximation algorithms,Algorithm design and analysis,Heuristic algorithms,Indexes,Spatial databases,Search problems","With the proliferation of spatial-textual data such as location-based services and geo-tagged websites, spatial keyword queries are ubiquitous in real life. One example of spatial-keyword query is the so-called collective spatial keyword query (CoSKQ) which is to find for a given query consisting a query location and several query keywords a set of objects which covers the query keywords collectively and has the smallest costwrt the query location. In the literature, many different functions were proposed for defining the cost and correspondingly, many different approaches were developed for the CoSKQ problem. In this paper, we study the CoSKQ problem systematically by proposing a unified cost function and a unified approach for the CoSKQ problem (with the unified cost function). The unified cost function includes all existing cost functions as special cases and the unified approach solves the CoSKQ problem with the unified cost function in a unified way. Experiments were conducted on both real and synthetic datasets which verified our proposed approach."
1059,,PurTreeClust - A Clustering Algorithm for Customer Segmentation from Massive Customer Transaction Data.,"Xiaojun Chen 0006,Yixiang Fang,Min Yang 0007,Feiping Nie 0001,Zhou Zhao,Joshua Zhexue Huang",https://doi.org/10.1109/TKDE.2017.2763620,TKDE,2018,"Clustering algorithms,Clustering methods,Measurement,Computational complexity,Companies,Knowledge discovery,Data engineering","Clustering of customer transaction data is an important procedure to analyze customer behaviors in retail and e-commerce companies. Note that products from companies are often organized as a product tree, in which the leaf nodes are goods to sell, and the internal nodes (except root node) could be multiple product categories. Based on this tree, we propose the “personalized product tree”, named purchase tree, to represent a customer's transaction records. So the customers' transaction data set can be compressed into a set of purchase trees. We propose a partitional clustering algorithm, named PurTreeClust, for fast clustering of purchase trees. A new distance metric is proposed to effectively compute the distance between two purchase trees. To cluster the purchase tree data, we first rank the purchase trees as candidate representative trees with a novel separate density, and then select the top k customers as the representatives of k customer groups. Finally, the clustering results are obtained by assigning each customer to the nearest representative. We also propose a gap statistic based method to evaluate the number of clusters. A series of experiments were conducted on ten real-life transaction data sets, and experimental results show the superior performance of the proposed method."
1060,,TAKer - Fine-Grained Time-Aware Microblog Search with Kernel Density Estimation.,"Qin Chen,Qinmin Hu,Jimmy Xiangji Huang,Liang He 0001",https://doi.org/10.1109/TKDE.2018.2794538,TKDE,2018,"Predictive models,Kernel,Estimation,Meteorology,Information retrieval,Global warming,Market research","Temporal information has been widely used to promote the information retrieval (IR) performance, especially for microblog search which usually prefers the latest news and events. Previous studies mainly focused on incorporating the document-level temporal information into retrieval, while the temporal relevance of each query word was not well investigated. In this paper, we propose a word temporal predictor to characterize the word-level temporal relevance by fine-grained time-aware kernel density estimation over the feedback documents. In addition, we present a fine-grained time-aware framework to integrate the proposed word temporal predictor with the traditional document temporal predictor for retrieval. Finally, we incorporate the framework into two state-of-the-art retrieval models, namely language model (LM) and BM25. The experimental results on the TREC 2011-2014 Microblog collections, show that our proposed word temporal predictor is effective to boost the retrieval performance within both LM and BM25 frameworks. In particular, we achieve significant improvements over the strong baselines with optimized settings in most cases. Furthermore, our fine-grained time-aware models with word temporal predictor are comparable to if not better than the state-of-the-art temporal retrieval models."
1061,,Inferring Cognitive Wellness from Motor Patterns.,"Yiqiang Chen,Chunyu Hu,Bin Hu 0001,Lisha Hu,Han Yu 0001,Chunyan Miao",https://doi.org/10.1109/TKDE.2018.2820024,TKDE,2018,"Feature extraction,Classification,Data collection,Sensors,Magnetic resonance imaging,Support vector machines","Changes in the motor pattern have been shown to be useful advanced indicators of cognitive disorders, such as Parkinson's disease (PD) and cerebral small vessel disease (SVD). It would be highly advantageous to tap into data containing people's motor patterns from motion sensing devices to analyze subtle changes in cognitive abilities, thereby providing personalized interventions before the actual onset of such conditions. However, this goal is very challenging due to two main technical problems: 1) the size of data labeled by doctors is small, and 2) the available data tends to be highly imbalanced (the vast majority tend to be from normal subjects with only a small fraction from subjects with cognitive disorder). In order to effectively deal with these challenges to infer cognitive wellness from motor patterns with high accuracy, we propose the MOtor-Cognitive Analytics (MOCA) framework. The proposed MOCA first uses the random oversampling iterative random forest based feature selection method to reduce the feature space dimensionality and avoid overfitting, and then adds a bias in the optimization problem of weighted extreme learning machine to achieve good generalization ability in handling imbalanced small-sampling dataset. Experimental results on two real-world datasets including SVD and stroke patients show that MOCA can effectively reduce the rate of misdiagnosis and significantly outperform state-of-the-art methods in inferring people's cognitive capabilities. This work opens up opportunities for population-level pre-screening using motion sensing devices and can inform current discussions on reforming the health-care infrastructure."
1062,,"We Like, We Post - A Joint User-Post Approach for Facebook Post Stance Labeling.","Wei-Fan Chen,Lun-Wei Ku",https://doi.org/10.1109/TKDE.2018.2810875,TKDE,2018,"Labeling,Hidden Markov models,Data models,Predictive models,Neural networks,Sentiment analysis,Gold","Web post and user stance labeling is challenging not only because of the informality and variation in language on the Web but also because of the lack of labeled data on fast-emerging new topics-even the labeled data we do have are usually heavily skewed. In this paper, we propose a joint user-post approach for stance labeling to mitigate the latter two difficulties. In labeling post stance, the proposed approach considers post content as well as posting and liking behavior, which involves users. Sentiment analysis is applied to posts to acquire their initial stance, and then the post and user stance are updated iteratively with correlated posting-related actions. The whole process works with limited labeled data, which solves the first problem. We use real interaction between authors and readers for stance labeling. Experimental results show that the proposed approach not only substantially improves content-based post stance labeling, but also yields better performance for the minor stance class, which solves the second problem."
1063,,Towards Why-Not Spatial Keyword Top-k Queries - A Direction-Aware Approach.,"Lei Chen 0031,Yafei Li,Jianliang Xu,Christian S. Jensen",https://doi.org/10.1109/TKDE.2017.2778731,TKDE,2018,"Indexes,Search problems,Spatial databases,Legged locomotion,Query processing,Linear programming","With the continued proliferation of location-based services, a growing number of web-accessible data objects are geo-tagged and have text descriptions. An important query over such web objects is the direction-aware spatial keyword query that aims to retrieve the top-k objects that best match query parameters in terms of spatial distance and textual similarity in a given query direction. In some cases, it can be difficult for users to specify appropriate query parameters. After getting a query result, users may find some desired objects are unexpectedly missing and may therefore question the entire result. Enabling why-not questions in this setting may aid users to retrieve better results, thus improving the overall utility of the query functionality. This paper studies the directionaware why-not spatial keyword top-k query problem. We propose efficient query refinement techniques to revive missing objects by minimally modifying users' direction-aware queries. We prove that the best refined query directions lie in a finite solution space for a special case and reduce the search for the optimal refinement to a linear programming problem for the general case. Extensive experimental studies demonstrate that the proposed techniques outperform a baseline method by two orders of magnitude and are robust in a broad range of settings."
1064,1,A Two-Phase Algorithm for Differentially Private Frequent Subgraph Mining.,"Xiang Cheng 0003,Sen Su,Shengzhi Xu,Li Xiong 0001,Ke Xiao,Mingxing Zhao",https://doi.org/10.1109/TKDE.2018.2793862,TKDE,2018,"Privacy,Noise measurement,Algorithm design and analysis,Itemsets,Data privacy","Mining frequent subgraphs from a collection of input graphs is an important task for exploratory data analysis on graph data. However, if the input graphs contain sensitive information, releasing discovered frequent subgraphs may pose considerable threats to individual privacy. In this paper, we study the problem of frequent subgraph mining (FSM) under the rigorous differential privacy model. We present a two-phase differentially private FSM algorithm, which is referred to as DFG. In DFG, frequent subgraphs are privately identified in the first phase, and the noisy support of each identified frequent subgraph is calculated in the second phase. In particular, to privately identity frequent subgraphs, we propose a frequent subgraph identification approach, which can improve the accuracy of discovered frequent subgraphs through candidate pruning. Moreover, to compute the noisy support of each identified frequent subgraph, we devise a lattice-based noisy support computation approach, which leverages the inclusion relations between the discovered frequent subgraphs to improve the accuracy of the noisy supports. Through formal privacy analysis, we prove that DFG satisfies ε-differential privacy. Extensive experimental results on real datasets show that DFG can privately find frequent subgraphs while achieving high data utility."
1065,,Learning Multiple Factors-Aware Diffusion Models in Social Networks.,"Chung-Kuang Chou,Ming-Syan Chen",https://doi.org/10.1109/TKDE.2017.2786209,TKDE,2018,"Social network services,Integrated circuit modeling,Predictive models,Estimation,Uncertainty,Adaptation models","Information diffusion is a natural phenomenon occurring in social networks. The adoption behavior of a node toward an information piece in a social network can be affected by different factors, e.g., freshness and hotness. Previously, many diffusion models are proposed to consider one or several fixed factors. In fact, the factors affecting adoption decision of a node are different from one to another and may not be seen before. For a different scenario of diffusion with new factors, previous diffusion models may not model the diffusion well, or are not applicable at all. Moreover, uncertainty of information exposure intrinsically exists between two connected nodes, which causes modeling diffusion more challenge in social networks. In this work, our aim is to design a diffusion model in which factors considered are flexible to be extended and changed and the uncertainly of information exposure is explicitly tackled. Therefore, with different factors, our diffusion model can be adapted to more scenarios of diffusion without requiring the modification of the learning framework. We conduct comprehensive experiments to show that our diffusion model is effective on two important tasks of information diffusion, namely activation prediction and spread estimation."
1066,,Efficient Parameter Estimation for Information Retrieval Using Black-Box Optimization.,"Alberto Costa,Emanuele Di Buccio,Massimo Melucci,Giacomo Nannicini",https://doi.org/10.1109/TKDE.2017.2761749,TKDE,2018,"Optimization,Mathematical model,Search problems,Computational modeling,Numerical models,Smoothing methods,Linear programming","The retrieval function is one of the most important components of an Information Retrieval (IR) system, because it determines to what extent some information is relevant to a user query. Most retrieval functions have “free parameters” whose value must be set before retrieval, significantly affecting the effectiveness of an IR system. Choosing the optimum values for such parameters is therefore of paramount importance. However, the optimum can only be found after a computationally expensive process, especially when the generalization error is estimated via cross-validation. In this paper, we propose to determine free parameter values by solving an optimization problem aimed at maximizing a measure of retrieval effectiveness. We employ the black-box optimization paradigm, since the analytical expression of the measure of effectiveness with respect to the free parameters is unknown. We consider different methods for solving the black-box optimization problem: a simple grid-search over the whole domain, and more sophisticated techniques such as line search and surrogate model based algorithms. Experimental results on several test collections not only provide useful insight about effectiveness, but also about efficiency: they indicate that with appropriate optimization techniques, the computational cost of parameter optimization can be greatly reduced without compromising retrieval effectiveness, even when taking generalization into account."
1067,,"Duplicate Reduction in Graph Mining - Approaches, Analysis, and Evaluation.","Soumyava Das,Sharma Chakravarthy",https://doi.org/10.1109/TKDE.2018.2795003,TKDE,2018,"Databases,Data mining,Partitioning algorithms,Scalability,Optimization,Measurement,Sorting","At the core of graph mining lies independent expansion of substructures where a substructure (also referred to as a subgraph) independently grows into a number of larger substructures in each iteration. Such an independent expansion, invariably, leads to the generation of duplicates. In the presence of graph partitions, duplicates are generated both within and across partitions. Eliminating these duplicates (for correctness) not only incurs generation and storage cost but also additional computation for its elimination. Our primary aim is to design techniques to reduce generating duplicate substructures as we show that they cannot be eliminated. This paper introduces three constraint-based optimization techniques, each significantly improving the overall mining cost by reducing the number of duplicates generated. These alternatives provide flexibility to choose the right technique based on graph properties. We establish theoretical correctness of each technique as well as its analysis with respect to graph characteristics such as degree, number of unique labels, and label distribution. We also investigate the applicability of their combination for improvements in duplicate reduction. Finally, we discuss the effects of the constraints with respect to the partitioning schemes used in graph mining. Our experiments demonstrate significant benefits of these constraints in terms of storage, computation, and communication cost (specific to partitioned approaches) across graphs with varied characteristics."
1068,,Untangling Blockchain - A Data Processing View of Blockchain Systems.,"Tien Tuan Anh Dinh,Rui Liu 0002,Meihui Zhang,Gang Chen 0001,Beng Chin Ooi,Ji Wang",https://doi.org/10.1109/TKDE.2017.2781227,TKDE,2018,"Protocols,Peer-to-peer computing,Bitcoin,Data processing,Database systems,Data structures","Blockchain technologies are gaining massive momentum in the last few years. Blockchains are distributed ledgers that enable parties who do not fully trust each other to maintain a set of global states. The parties agree on the existence, values, and histories of the states. As the technology landscape is expanding rapidly, it is both important and challenging to have a firm grasp of what the core technologies have to offer, especially with respect to their data processing capabilities. In this paper, we first survey the state of the art, focusing on private blockchains (in which parties are authenticated). We analyze both in-production and research systems in four dimensions: distributed ledger, cryptography, consensus protocol, and smart contract. We then present BLOCKBENCH, a benchmarking framework for understanding performance of private blockchains against data processing workloads. We conduct a comprehensive evaluation of three major blockchain systems based on BLOCKBENCH, namely Ethereum, Parity, and Hyperledger Fabric. The results demonstrate several trade-offs in the design space, as well as big performance gaps between blockchain and database systems. Drawing from design principles of database systems, we discuss several research directions for bringing blockchain performance closer to the realm of databases."
1069,,Efficient Recommendation of Aggregate Data Visualizations.,"Humaira Ehsan,Mohamed A. Sharaf,Panos K. Chrysanthis",https://doi.org/10.1109/TKDE.2017.2765634,TKDE,2018,"Data visualization,Aggregates,Visualization,Measurement,Search problems,Visual databases,Data aggregation","Data visualization is a common and effective technique for data exploration. However, for complex data, it is infeasible for an analyst to manually generate and browse all possible visualizations for insights. This observation motivated the need for automated solutions that can effectively recommend such visualizations. The main idea underlying those solutions is to evaluate the utility of all possible visualizations and then recommend the top-k visualizations. This process incurs high data processing cost, that is further aggravated by the presence of numerical dimensional attributes. To address that challenge, we propose novel view recommendation schemes, which incorporate a hybrid multi-objective utility function that captures the impact of numerical dimension attributes. Our first scheme, Multi-Objective View Recommendation for Data Exploration (MuVE), adopts an incremental evaluation of our multi-objective utility function, which allows pruning of a large number of low-utility views and avoids unnecessary objective evaluations. Our second scheme, upper MuVE (uMuVE), further improves the pruning power by setting the upper bounds on the utility of views and allowing interleaved processing of views, at the expense of increased memory usage. Finally, our third scheme, Memory-aware uMuVE (MuMuVE), provides pruning power close to that of uMuVE, while keeping memory usage within a pre-specified limit."
1070,,Diverse Relevance Feedback for Time Series with Autoencoder Based Summarizations.,"Bahaeddin Eravci,Hakan Ferhatosmanoglu",https://doi.org/10.1109/TKDE.2018.2820119,TKDE,2018,"Time series analysis,Radio frequency,Diversity reception,Task analysis,Feature extraction,Time measurement,Neural networks","We present a relevance feedback based browsing methodology using different representations for time series data. The outperforming representation type, e.g., among dual-tree complex wavelet transformation, Fourier, symbolic aggregate approximation (SAX), is learned based on user annotations of the presented query results with representation feedback. We present the use of autoencoder type neural networks to summarize time series or its representations into sparse vectors, which serves as another representation learned from the data. Experiments on 85 real data sets confirm that diversity in the result set increases precision, representation feedback incorporates item diversity and helps to identify the appropriate representation. The results also illustrate that the autoencoders can enhance the base representations, and achieve comparably accurate results with reduced data sizes."
1071,,Community Deception or - How to Stop Fearing Community Detection Algorithms.,"Valeria Fionda,Giuseppe Pirrò",https://doi.org/10.1109/TKDE.2017.2776133,TKDE,2018,"Detection algorithms,Image edge detection,Algorithm design and analysis,Knowledge engineering,Robustness,Facebook","In this paper, we research the community deception problem. Tackling this problem consists in developing techniques to hide a target community (C) from community detection algorithms. This need emerges whenever a group (e.g., activists, police enforcements, or network participants in general) want to observe and cooperate in a social network while avoiding to be detected. We introduce and formalize the community deception problem and devise an efficient algorithm that allows to achieve deception by identifying a certain number (b) of C's members connections to be rewired. Deception can be practically achieved in social networks like Facebook by friending or unfriending network members as indicated by our algorithm. We compare our approach with another technique based on modularity. By considering a variety of (large) real networks, we provide a systematic evaluation of the robustness of community detection algorithms to deception techniques. Finally, we open some challenging research questions about the design of detection algorithms robust to deception techniques."
1072,,Efficient Information Flow Maximization in Probabilistic Graphs.,"Christian Frey,Andreas Züfle,Tobias Emrich,Matthias Renz",https://doi.org/10.1109/TKDE.2017.2780123,TKDE,2018,"Reliability,Probabilistic logic,Social network services,Communication networks,Monte Carlo methods,Wireless sensor networks,Uncertainty","Reliable propagation of information through large networks, e.g., communication networks, social networks, or sensor networks is very important in many applications concerning marketing, social networks, and wireless sensor networks. However, social ties of friendship may be obsolete, and communication links may fail, inducing the notion of uncertainty in such networks. In this paper, we address the problem of optimizing information propagation in uncertain networks given a constrained budget of edges. We show that this problem requires to solve two NP-hard subproblems: the computation of expected information flow, and the optimal choice of edges. To compute the expected information flow to a source vertex, we propose the F-tree as a specialized data structure, that identifies independent components of the graph for which the information flow can either be computed analytically and efficiently, or for which traditional Monte-Carlo sampling can be applied independently of the remaining network. For the problem of finding the optimal edges, we propose a series of heuristics that exploit properties of this data structure. Our evaluation shows that these heuristics lead to high quality solutions, thus yielding high information flow, while maintaining low running time."
1073,,Link Weight Prediction Using Supervised Learning Methods and Its Application to Yelp Layered Network.,"Chenbo Fu,Minghao Zhao,Lu Fan,Xinyi Chen,Jinyin Chen,Zhefu Wu,Yongxiang Xia,Qi Xuan",https://doi.org/10.1109/TKDE.2018.2801854,TKDE,2018,"Prediction algorithms,Feature extraction,Supervised learning,Social network services,Algorithm design and analysis,Measurement,Indexes","Real-world networks feature weights of interactions, where link weights often represent some physical attributes. In many situations, to recover the missing data or predict the network evolution, we need to predict link weights in a network. In this paper, we first proposed a series of new centrality indices for links in line graph. Then, utilizing these line graph indices, as well as a number of original graph indices, we designed three supervised learning methods to realize link weight prediction both in the networks of single layer and multiple layers, which perform much better than several recently proposed baseline methods. We found that the resource allocation index (RA) plays a more important role in the weight prediction than other topological properties, and the line graph indices are at least as important as the original graph indices in link weight prediction. In particular, the success application of our methods on Yelp layered network suggests that we can indeed predict the offline co-foraging behaviors of users just based on their online social interactions, which may open a new direction for link weight prediction algorithms, and meanwhile provide insights to design better restaurant recommendation systems."
1074,,Matching Heterogeneous Event Data.,"Yu Gao,Shaoxu Song,Xiaochen Zhu,Jianmin Wang 0001,Xiang Lian,Lei Zou 0001",https://doi.org/10.1109/TKDE.2018.2815695,TKDE,2018,"Production,Companies,Syntactics,Data integration,Software,Encoding","Identifying events from different sources is essential to various business process applications such as provenance querying or process mining. Distinct features of heterogeneous events, including opaque names and dislocated traces, prevent existing data integration techniques from performing well. To address these issues, in this paper, (1) we propose an event similarity function by iteratively evaluating similar neighbors. (2) In addition to event nodes, we further employ the similarity of edges (indicating relationships among events) in event matching. We prove NP-hardness of finding the optimal event matching w.r.t. node and edge similarities, and propose an efficient heuristic for event matching. Experiments demonstrate that the proposed event matching approach can achieve significantly higher accuracy than state-of-the-art matching methods. In particular, by considering the event edge similarity, our heuristic matching algorithm further improves the matching accuracy without introducing much overhead."
1075,,BEATS - Blocks of Eigenvalues Algorithm for Time Series Segmentation.,"Aurora González-Vidal,Payam M. Barnaghi,Antonio F. Skarmeta",https://doi.org/10.1109/TKDE.2018.2817229,TKDE,2018,"Time series analysis,Clustering algorithms,Approximation algorithms,Discrete cosine transforms,Signal processing algorithms,Machine learning algorithms,Standards","The massive collection of data via emerging technologies like the Internet of Things (IoT) requires finding optimal ways to reduce the observations in the time series analysis domain. The IoT time series require aggregation methods that can preserve and represent the key characteristics of the data. In this paper, we propose a segmentation algorithm that adapts to unannounced mutations of the data (i.e., data drifts). The algorithm splits the data streams into blocks and groups them in square matrices, computes the Discrete Cosine Transform (DCT), and quantizes them. The key information is contained in the upper-left part of the resulting matrix. We extract this sub-matrix, compute the modulus of its eigenvalues, and remove duplicates. The algorithm, called BEATS, is designed to tackle dynamic IoT streams, whose distribution changes over time. We implement experiments with six datasets combining real, synthetic, real-world data, and data with drifts. Compared to other segmentation methods like Symbolic Aggregate approXimation (SAX), BEATS shows significant improvements. Trying it with classification and clustering algorithms it provides efficient results. BEATS is an effective mechanism to work with dynamic and multi-variate data, making it suitable for IoT data sources. The datasets, code of the algorithm and the analysis results can be accessed publicly at: https://github.com/auroragonzalez/BEATS."
1076,,Efficient Maintenance of Shortest Distances in Dynamic Graphs.,"Sergio Greco,Cristian Molinaro,Chiara Pulice",https://doi.org/10.1109/TKDE.2017.2772233,TKDE,2018,"Heuristic algorithms,Algorithm design and analysis,Maintenance engineering,Indexes,Complexity theory,Social network services","Computing shortest distances is a central task in many domains. The growing number of applications dealing with dynamic graphs calls for incremental algorithms, as it is impractical to recompute shortest distances from scratch every time updates occur. In this paper, we address the problem of maintaining all-pairs shortest distances in dynamic graphs. We propose efficient incremental algorithms to process sequences of edge deletions/insertions/updates and vertex deletions/insertions. The proposed approach relies on some general operators that can be easily “instantiated” both in main memory and on top of different underlying DBMSs. We provide complexity analyses of the proposed algorithms. Experimental results on several real-world datasets show that current main-memory algorithms become soon impractical, disk-based ones are needed for larger graphs, and our approach significantly outperforms state-of-the-art algorithms."
1077,,Approximate Order-Sensitive k-NN Queries over Correlated High-Dimensional Data.,"Yu Gu 0002,Yandan Guo,Yang Song,Xiangmin Zhou,Ge Yu 0001",https://doi.org/10.1109/TKDE.2018.2812153,TKDE,2018,"Indexes,Principal component analysis,Correlation,Dimensionality reduction,Data mining,Gaussian distribution,Data preprocessing","The k Nearest Neighbor (k-NN) query has been gaining more importance in extensive applications involving information retrieval, data mining, and databases. Specifically, in order to trade off accuracy for efficiency, approximate solutions for the k-NN query are extensively explored. However, the precision is usually order-insensitive, which is defined on the result set instead of the result sequence. In many situations, it cannot reasonably reflect the query result quality. In this paper, we focus on the approximate k-NN query problem with the order-sensitive precision requirement and propose a novel scheme based on the projection-filter-refinement framework. Basically, we adopt PCA to project the high-dimensional data objects into the low-dimensional space. Then, a filter condition is inferred to execute efficient pruning over the projected data. In addition, an index strategy named OR-tree is proposed to reduce the I/O cost. The extensive experiments based on several real-world data sets and a synthetic data set are conducted to verify the effectiveness and efficiency of the proposed solution. Compared to the state-of-the-art methods, our method can support order-sensitive k -NN queries with higher result precision while retaining satisfactory CPU and I/O efficiency."
1078,,Sparse-TDA - Sparse Realization of Topological Data Analysis for Multi-Way Classification.,"Wei Guo 0008,Krithika Manohar,Steven L. Brunton,Ashis Gopal Banerjee",https://doi.org/10.1109/TKDE.2018.2790386,TKDE,2018,"Training,Kernel,Feature extraction,Sparse matrices,Data analysis,Shape,Benchmark testing","Topological data analysis (TDA) has emerged as one of the most promising techniques to reconstruct the unknown shapes of high-dimensional spaces from observed data samples. TDA, thus, yields key shape descriptors in the form of persistent topological features that can be used for any supervised or unsupervised learning task, including multi-way classification. Sparse sampling, on the other hand, provides a highly efficient technique to reconstruct signals in the spatial-temporal domain from just a few carefully-chosen samples. Here, we present a new method, referred to as the Sparse-TDA algorithm, that combines favorable aspects of the two techniques. This combination is realized by selecting an optimal set of sparse pixel samples from the persistent features generated by a vector-based TDA algorithm. These sparse samples are selected from a low-rank matrix representation of persistent features using QR pivoting. We show that the Sparse-TDA method demonstrates promising performance on three benchmark problems related to human posture recognition and image texture classification."
1079,,Linking Fine-Grained Locations in User Comments.,"Jialong Han,Aixin Sun,Gao Cong,Wayne Xin Zhao,Zongcheng Ji,Minh C. Phan",https://doi.org/10.1109/TKDE.2017.2758780,TKDE,2018,"Social networking (online),Web pages,Knowledge based systems,Internet,Graph theory,Location awareness,Probability","Many domain-specific websites host a profile page for each entity (e.g., locations on Foursquare, movies on IMDb, and products on Amazon) for users to post comments on. When commenting on an entity, users often mention other entities for reference or comparison. Compared with web pages and tweets, the problem of disambiguating the mentioned entities in user comments has not received much attention. This paper investigates linking fine-grained locations in Foursquare comments. We demonstrate that the focal location, i.e., the location that a comment is posted on, provides rich contexts for the linking task. To exploit such information, we represent the Foursquare data in a graph, which includes locations, comments, and their relations. A probabilistic model named FocalLink is proposed to estimate the probability that a user mentions a location when commenting on a focal location, by following different kinds of relations. Experimental results show that FocalLink is consistently superior under different collective linking settings."
1080,,Second-Order Online Active Learning and Its Applications.,"Shuji Hao,Jing Lu,Peilin Zhao,Chi Zhang 0007,Steven C. H. Hoi,Chunyan Miao",https://doi.org/10.1109/TKDE.2017.2778097,TKDE,2018,"Algorithm design and analysis,Prediction algorithms,Predictive models,Labeling,Machine learning algorithms,Training","The goal of online active learning is to learn predictive models from a sequence of unlabeled data given limited label query budget. Unlike conventional online learning tasks, online active learning is considerably more challenging because of two reasons. First, it is difficult to design an effective query strategy to decide when is appropriate to query the label of an incoming instance given limited query budget. Second, it is also challenging to decide how to update the predictive models effectively whenever the true label of an instance is queried. Most existing approaches for online active learning are often based on a family of first-order online learning algorithms, which are simple and efficient but fall short in the slow convergence and sub-optimal solution in exploiting the labeled training data. To solve these issues, this paper presents a novel framework of Second-order Online Active Learning (SOAL) by fully exploiting both the first-order and second-order information. The proposed algorithms are able to achieve effective online learning efficacy, maximize the predictive accuracy, and minimize the labeling cost. To make SOAL more practical for real-world applications, especially for class-imbalanced online classification tasks (e.g., malicious web detection), we extend the SOAL framework by proposing the Costsensitive Second-order Online Active Learning algorithm named “SOALCS”, which is devised by maximizing the sum of weighted sensitivity and specificity or minimizing the cost of weighted mistakes of different classes. We conducted both theoretical analysis and empirical studies, including an extensive set of experiments on a variety of large-scale real-world datasets, in which the promising empirical results validate the efficacy and scalability of the proposed algorithms towards large-scale online learning tasks."
1081,,NAIS - Neural Attentive Item Similarity Model for Recommendation.,"Xiangnan He 0001,Zhankui He,Jingkuan Song,Zhenguang Liu,Yu-Gang Jiang,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2018.2831682,TKDE,2018,"Predictive models,Data models,Mathematical model,Real-time systems,Recommender systems,Optimization,Linear programming","Item-to-item collaborative filtering (aka.item-based CF) has been long used for building recommender systems in industrial settings, owing to its interpretability and efficiency in real-time personalization. It builds a user's profile as her historically interacted items, recommending new items that are similar to the user's profile. As such, the key to an item-based CF method is in the estimation of item similarities. Early approaches use statistical measures such as cosine similarity and Pearson coefficient to estimate item similarities, which are less accurate since they lack tailored optimization for the recommendation task. In recent years, several works attempt to learn item similarities from data, by expressing the similarity as an underlying model and estimating model parameters by optimizing a recommendation-aware objective function. While extensive efforts have been made to use shallow linear models for learning item similarities, there has been relatively less work exploring nonlinear neural network models for item-based CF. In this work, we propose a neural network model named Neural Attentive Item Similarity model (NAIS) for item-based CF. The key to our design of NAIS is an attention network, which is capable of distinguishing which historical items in a user profile are more important for a prediction. Compared to the state-of-the-art item-based CF method Factored Item Similarity Model (FISM) [1] , our NAIS has stronger representation power with only a few additional parameters brought by the attention network. Extensive experiments on two public benchmarks demonstrate the effectiveness of NAIS. This work is the first attempt that designs neural network models for item-based CF, opening up new research possibilities for future developments of neural recommender systems."
1082,,Discovering Canonical Correlations between Topical and Topological Information in Document Networks.,"Yuan He 0006,Cheng Wang 0001,Changjun Jiang",https://doi.org/10.1109/TKDE.2017.2767599,TKDE,2018,"Correlation,Analytical models,Social network services,Semantics,Gaussian distribution,Web pages,Probabilistic logic","Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (relational link) information. A key point in modeling such datasets is to discover proper denominators beneath the text and link. Most previous work introduces the assumption that documents closely linked with each other share common latent topics. However, the heterophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous factors, community and topic. Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. We also impose auxiliary word embeddings to improve the quality of topics. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are hyperlinked networks of web pages, social networks of friends, and coauthor networks of publications. Experimental results show that our approach achieves significant improvements compared with the current state of the art."
1083,,Reverse Approximate Nearest Neighbor Queries.,"Arif Hidayat,Shiyu Yang,Muhammad Aamir Cheema,David Taniar",https://doi.org/10.1109/TKDE.2017.2766065,TKDE,2018,"Monitoring,Fuels,Indexes,Australia,Information technology,Nearest neighbor searches,Query processing","Given a set of facilities and a set of users, a reverse nearest neighbors (RNN) query retrieves every user 
<inline-formula><tex-math notation=""LaTeX"">$u$</tex-math></inline-formula>
 for which the query facility 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
 is its closest facility. Since 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
 is the closest facility to 
<inline-formula><tex-math notation=""LaTeX"">$u$</tex-math></inline-formula>
, the user 
<inline-formula> <tex-math notation=""LaTeX"">$u$</tex-math></inline-formula>
 is said to be influenced by 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math> </inline-formula>
. In this paper, we propose a 
<i>relaxed</i>
 definition of influence where a user 
<inline-formula><tex-math notation=""LaTeX"">$u$ </tex-math></inline-formula>
 is said to be influenced by not only its closest facility but also every other facility that is 
<i>almost</i>
 as close to 
<inline-formula><tex-math notation=""LaTeX"">$u$</tex-math></inline-formula>
 as its closest facility is. Based on this definition of influence, we propose reverse approximate nearest neighbors (RANN) queries. Formally, given a value 
<inline-formula><tex-math notation=""LaTeX"">$x&gt;1$</tex-math></inline-formula>
, an RANN query 
<inline-formula> <tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
 returns every user 
<inline-formula><tex-math notation=""LaTeX"">$u$</tex-math> </inline-formula>
 for which 
<inline-formula><tex-math notation=""LaTeX"">$dist(u,q) \leq x\times NNDist(u)$</tex-math></inline-formula>
 where 
<inline-formula> <tex-math notation=""LaTeX"">$NNDist(u)$</tex-math></inline-formula>
 denotes the distance between a user 
<inline-formula><tex-math notation=""LaTeX"">$u$ </tex-math></inline-formula>
 and its nearest facility, i.e., 
<inline-formula><tex-math notation=""LaTeX"">$q$</tex-math></inline-formula>
 is an approximate nearest neighbor of 
<inline-formula><tex-math notation=""LaTeX"">$u$</tex-math></inline-formula>
. In this paper, we study both 
<i>snapshot</i>
 and 
<i>continuous</i>
 versions of RANN queries. In a snapshot RANN query, the underlying data sets do not change and the results of a query are to be computed only once. In the continuous version, the users continuously change their locations and the results of RANN queries are to be continuously monitored. Based on effective pruning techniques and several non-trivial observations, we propose efficient RANN query processing algorithms for both the snapshot and continuous RANN queries. We conduct extensive experiments on both real and synthetic data sets and demonstrate that our algorithm for both snapshot and continuous queries are significantly better than the competitors."
1084,,A General Framework for Implicit and Explicit Social Recommendation.,"Chin-Chi Hsu,Mi-Yen Yeh,Shou-De Lin",https://doi.org/10.1109/TKDE.2018.2821174,TKDE,2018,"Social network services,Recommender systems,Collaboration,Motion pictures,Mathematical model,Complexity theory","Research of social recommendation aims at exploiting social information to improve the quality of a recommender system. It can be further divided into two classes. Explicit social recommendation assumes the existence of not only the users' ratings on items, but also the explicit social connections between users. Implicit social recommendation assumes the availability of only the ratings but not the social connections between users, and attempts to infer implicit social connections between users with the goal to boost recommendation accuracy. This paper proposes a unified framework that is applicable to both explicit and implicit social recommendation. We propose an optimization framework to learn the degree of social correlation and rating prediction jointly, so these two tasks can mutually boost the performance of each other. Furthermore, a well-known challenge for implicit social recommendation is that it takes quadratic time to learn the strength of pairwise connections. This paper further proposes several practical tricks to reduce the complexity of our model to be linear to the observed ratings. The experiments show that the proposed model, with only two parameters, can significantly outperform the state-of-the-art solutions for both explicit and implicit social recommender systems."
1085,,Answering Natural Language Questions by Subgraph Matching over Knowledge Graphs.,"Sen Hu,Lei Zou 0001,Jeffrey Xu Yu,Haixun Wang,Dongyan Zhao 0001",https://doi.org/10.1109/TKDE.2017.2766634,TKDE,2018,"Resource description framework,Natural languages,Semantics,Query processing,Benchmark testing,Motion pictures,Standards","RDF question/answering (Q/A) allows users to ask questions in natural languages over a knowledge base represented by RDF. To answer a natural language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over RDF repository (RDF Q/A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, RDF Q/A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. More specifically, we propose two different frameworks to build the semantic query graph, one is relation (edge)-first and the other one is node-first. We compare our method with some state-of-the-art RDF Q/A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly."
1086,,Learning Dynamic Conditional Gaussian Graphical Models.,"Feihu Huang,Songcan Chen",https://doi.org/10.1109/TKDE.2017.2777462,TKDE,2018,"Graphical models,Sparse matrices,Kernel,Smoothing methods,Data models,Biological system modeling,Indexes","In the paper, we propose a class of dynamic conditional Gaussian graphical models (DCGGMs) based on a set of nonidentical distribution observations, which changes smoothly with time or condition. Specifically, the DCGGMs model the dynamic output network influenced by conditioning input variables, which are encoded by a set of varying parameters. Moreover, we propose a joint smooth graphical Lasso to estimate the DCGGMs, which combines kernel smoother with sparse group Lasso penalty. At the same time, we design an efficient accelerated proximal gradient algorithm to solve this estimator. Theoretically, we establish the asymptotic properties of our model on consistency and sparsistency under the high-dimensional settings. In particular, we highlight a class of consistency theory for dynamic graphical models, in which the sample size can be seen as n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4/5</sup>
 for estimating a local graphical model when the bandwidth parameter h of kernel smoother is chosen as h = n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-1/5</sup>
 for describing the dynamic. Finally, the extensive numerical experiments on both synthetic and real datasets are provided to support the effectiveness of the proposed method."
1087,,Leveraging Conceptualization for Short-Text Embedding.,"Heyan Huang,Yashen Wang,Chong Feng,Zhirun Liu,Qiang Zhou",https://doi.org/10.1109/TKDE.2017.2787709,TKDE,2018,"Semantics,Context modeling,Predictive models,Solids,Neural networks,Probabilistic logic","Most short-text embedding models typically represent each short-text only using the literal meanings of the words, which makes these models indiscriminative for the ubiquitous polysemy. In order to enhance the semantic representation capability of the short-texts, we (i) propose a novel short-text conceptualization algorithm to assign the associated concepts for each short-text, and then (ii) introduce the conceptualization results into learning the conceptual short-text embeddings. Hence, this semantic representation is more expressive than some widely-used text representation models such as the latent topic model. Wherein, the short-text conceptualization algorithm used here is based on a novel co-ranking framework, enabling the signals (i.e., the words and the concepts) to fully interplay to derive the solid conceptualization for the short-texts. Afterwards, we further extend the conceptual short-text embedding models by utilizing an attention-based model that selects the relevant words within the context to make more efficient prediction. The experiments on the real-world datasets demonstrate that the proposed conceptual short-text embedding model and short-text conceptualization algorithm are more effective than the state-of-the-art methods."
1088,,To Meet or Not to Meet - Finding the Shortest Paths in Road Networks.,"Weihuang Huang,Yikai Zhang 0001,Zechao Shang,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2017.2777851,TKDE,2018,"Approximation algorithms,Roads,Bridges,Uncertainty,Shortest path problem,Companies,Steiner trees","Finding the shortest path in road networks becomes one of important issues in location based services (LBS). The problem of finding the optimal meeting point for a group of users has also been well studied in existing works. In this paper, we investigate a new problem for two users. Each user has his/her own source and destination. However, whether to meet before going to their destinations is with some uncertainty. We model it as minimum path pair (MPP) query, which consists of two pairs of source and destination and a user-specified weight α to balance the two different needs. The result is a pair of paths connecting the two sources and destinations respectively, with minimal overall cost of the two paths and the shortest route between them. To solve MPP queries, we devise algorithms by enumerating node pairs. We adopt a location-based pruning strategy to reduce the number of node pairs for enumeration. An efficient algorithm based on point-to-point shortest path calculation is proposed to further improve query efficiency. We also give two fast approximate algorithms with approximation bounds. Extensive experiments are conducted to show the effectiveness and efficiency of our methods."
1089,,Computing Crowd Consensus with Partial Agreement.,"Nguyen Quoc Viet Hung,Huynh Huu Viet,Thanh Tam Nguyen,Matthias Weidlich,Hongzhi Yin,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2017.2750683,TKDE,2018,"Computational modeling,Crowdsourcing,Predictive models,Bayes methods,Data models,Robustness","Crowdsourcing has been widely established as a means to enable human computation at large-scale, in particular for tasks that require manual labelling of large sets of data items. Answers obtained from heterogeneous crowd workers are aggregated to obtain a robust result. However, existing methods for answer aggregation are designed for discrete tasks, where answers are given as a single label per item. In this paper, we consider-partial-agreement-tasks that are common in many applications such as image tagging and document annotation, where items are assigned sets of labels. Common approaches for the aggregation of partial-agreement answers either (i) reduce the problem to several instances of an aggregation problem for discrete tasks or (ii) consider each label independently. Going beyond the state-of-the-art, we propose a novel Bayesian nonparametric model to aggregate the partial-agreement answers in a generic way. This model enables us to compute the consensus of partially-sound and partially-complete worker answers, while taking into account mutual relationships in labels and different answer sets. We also show how this model is instantiated for incremental learning, incorporating new answers from crowd workers as they arrive. An evaluation of our method using real-world datasets reveals that it consistently outperforms the state-of-the-art in terms of precision, recall, and robustness against faulty workers and data sparsity."
1090,,Topic Models for Unsupervised Cluster Matching.,"Tomoharu Iwata,Tsutomu Hirao,Naonori Ueda",https://doi.org/10.1109/TKDE.2017.2778720,TKDE,2018,"Data models,Vocabulary,Dictionaries,Analytical models,Resource management,Sorting","We propose topic models for unsupervised cluster matching, which is the task of finding matching between clusters in different domains without correspondence information. For example, the proposed model finds correspondence between document clusters in English and German without alignment information, such as dictionaries and parallel sentences/documents. The proposed model assumes that documents in all languages have a common latent topic structure, and there are potentially infinite number of topic proportion vectors in a latent topic space that is shared by all languages. Each document is generated using one of the topic proportion vectors and language-specific word distributions. By inferring a topic proportion vector used for each document, we can allocate documents in different languages into common clusters, where each cluster is associated with a topic proportion vector. Documents assigned into the same cluster are considered to be matched. We develop an efficient inference procedure for the proposed model based on collapsed Gibbs sampling. The effectiveness of the proposed model is demonstrated with real data sets including multilingual corpora of Wikipedia and product reviews."
1091,,Unsupervised Coupled Metric Similarity for Non-IID Categorical Data.,"Songlei Jian,Longbing Cao,Kai Lu,Hang Gao",https://doi.org/10.1109/TKDE.2018.2808532,TKDE,2018,"Couplings,Frequency measurement,Medical services,Unsupervised learning,Task analysis,Data analysis","Appropriate similarity measures always play a critical role in data analytics, learning, and processing. Measuring the intrinsic similarity of categorical data for unsupervised learning has not been substantially addressed, and even less effort has been made for the similarity analysis of categorical data that is not independent and identically distributed (non-IID). In this work, a Coupled Metric Similarity (CMS) is defined for unsupervised learning which flexibly captures the value-to-attribute-to-object heterogeneous coupling relationships. CMS learns the similarities in terms of intrinsic heterogeneous intra- and inter-attribute couplings and attribute-to-object couplings in categorical data. The CMS validity is guaranteed by satisfying metric properties and conditions, and CMS can flexibly adapt to IID to non-IID data. CMS is incorporated into spectral clustering and k-modes clustering and compared with relevant state-of-the-art similarity measures that are not necessarily metrics. The experimental results and theoretical analysis show the CMS effectiveness of capturing independent and coupled data characteristics, which significantly outperforms other similarity measures on most datasets."
1092,,Supervised Search Result Diversification via Subtopic Attention.,"Zhengbao Jiang,Zhicheng Dou,Wayne Xin Zhao,Jian-Yun Nie,Ming Yue,Ji-Rong Wen",https://doi.org/10.1109/TKDE.2018.2810873,TKDE,2018,"Task analysis,Search problems,Supervised learning,Recurrent neural networks,Measurement","Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised methods have been proposed recently to learn ranking functions and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches based on the principle of Maximal Marginal Relevance (MMR). In this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the information contained in the sequence of selected documents, we use the attention mechanism to capture the subtopics to be focused on while selecting the next document, which naturally fits our task of document selection for diversification. As a preliminary attempt, we employ recurrent neural networks and max pooling to instantiate the framework. We use both distributed representations and traditional relevance features to model documents in the implementation. The framework is flexible to model query intent in either a flat list or a hierarchy. Experimental results show that the proposed method significantly outperforms all the existing search result diversification approaches."
1093,,Low-Rank Multi-View Embedding Learning for Micro-Video Popularity Prediction.,"Peiguang Jing,Yuting Su 0001,Liqiang Nie,Xu Bai,Jing Liu 0002,Meng Wang 0001",https://doi.org/10.1109/TKDE.2017.2785784,TKDE,2018,"Videos,Feature extraction,Sparse matrices,Matrix decomposition,Noise measurement,Social network services","Recently, a prevailing trend of user generated content (UGC) on social media sites is the emerging micro-videos. Microvideos afford many potential opportunities ranging from network content caching to online advertising, yet there are still little efforts dedicated to research on micro-video understanding. In this paper, we focus on popularity prediction of micro-videos by presenting a novel low-rank multi-view embedding learning framework. We name it as transductive low-rank multi-view regression (TLRMVR), and it is capable of boosting the performance of micro-video popularity prediction by jointly considering the intrinsic representations of the source and target samples. In particular, TLRMVR integrates low-rank multi-view embedding and regression analysis into a unified framework such that the lowest-rank representation shared by all views not only captures the global structure of all views, but also indicates the regression requirements. The framework is formulated as a regression model and it seeks a set of view-specific projection matrices with low-rank constraints to map multi-view features into a common subspace. In addition, a multi-graph regularization term is constructed to improve the generalization capability and further prevents the overfitting problem. Extensive experiments conducted on a publicly available dataset demonstrate that our proposed method achieve promising results as compared with state-of-the-art baselines."
1094,,High-Level Programming Abstractions for Distributed Graph Processing.,"Vasiliki Kalavri,Vladimir Vlassov,Seif Haridi",https://doi.org/10.1109/TKDE.2017.2762294,TKDE,2018,"Programming,Computational modeling,Semantics,Distributed databases,Data models,Analytical models,Algorithm design and analysis","Efficient processing of large-scale graphs in distributed environments has been an increasingly popular topic of research in recent years. Inter-connected data that can be modeled as graphs appear in application domains such as machine learning, recommendation, web search, and social network analysis. Writing distributed graph applications is inherently hard and requires programming models that can cover a diverse set of problems, including iterative refinement algorithms, graph transformations, graph aggregations, pattern matching, ego-network analysis, and graph traversals. Several high-level programming abstractions have been proposed and adopted by distributed graph processing systems and big data platforms. Even though significant work has been done to experimentally compare distributed graph processing frameworks, no qualitative study and comparison of graph programming abstractions has been conducted yet. In this survey, we review and analyze the most prevalent high-level programming models for distributed graph processing, in terms of their semantics and applicability. We review 34 distributed graph processing systems with respect to the graph processing models they implement and we survey applications that appear in recent distributed graph systems papers. Finally, we discuss trends and open research questions in the area of distributed graph processing."
1095,,THINKER - Entity Linking System for Turkish Language.,"Murat Kalender,Emin Erkan Korkmaz",https://doi.org/10.1109/TKDE.2017.2761743,TKDE,2018,"Knowledge based systems,Encyclopedias,Electronic publishing,Internet,Neural networks","Entity linking is one of the problems to be handled in order to process natural language and to enrich the existing unstructured text with metadata. The generation of assignments between knowledge base entities and lexical units is called entity linking. Although a number of systems have been proposed for linking entity mentions in various languages, there is currently no publicly available entity linking system specific to the Turkish language. This paper presents a novel entity linking system-THINKER - for linking Turkish content with entities defined in the Turkish dictionary (tdk.gov.tr) or Turkish Wikipedia (tr.wikipedia.org). Specifically, we first propose a novel machine learning based entity detection algorithm for the Turkish language. Then, we propose a collective disambiguation algorithm which utilizes a set of metrics for the linking task and, which is optimized using a genetic algorithm. The effectiveness of THINKER is validated empirically over generated data sets. The experimental results show that THINKER outperformed the state-of-the-art cross-lingual and multilingual entity linking systems in the literature. High entity linking performance (74.81 percent F1 score) is achieved by extending previous methods with some features specific to Turkish language and by developing a novel method that can learn better representations of entity embeddings."
1096,,MPI-FAUN - An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization.,"Ramakrishnan Kannan,Grey Ballard,Haesun Park",https://doi.org/10.1109/TKDE.2017.2767592,TKDE,2018,"Program processors,Computational modeling,Algorithm design and analysis,Sparse matrices,Approximation algorithms,Collaboration,Analytical models","Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors Wand H, for the given input matrix A, such that A WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans from few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online."
1097,1,FEDERAL - A Framework for Distance-Aware Privacy-Preserving Record Linkage.,"Dimitrios Karapiperis,Aris Gkoulalas-Divanis,Vassilios S. Verykios",https://doi.org/10.1109/TKDE.2017.2761759,TKDE,2018,"Couplings,Encoding,Privacy,Government,Electronic mail,Data structures,Measurement","In privacy-preserving record linkage, a number of data custodians encode their records and submit them to a trusted third-party who is responsible for identifying those records that refer to the same real-world entity. In this paper, we propose FEDERAL, a novel record linkage framework that implements methods for anonymizing both string and numerical data values, which are typically present in data records. These methods rely on a strong theoretical foundation for rigorously specifying the dimensionality of the anonymization space, into which the original values are embedded, to provide accuracy and privacy guarantees under various models of privacy attacks. A key component of the applied embedding process is the threshold that is required by the distance computations, which we prove can be formally specified to guarantee accurate results. We evaluate our framework using three real-world data sets with varying characteristics. Our experimental findings show that FEDERAL offers a complete and effective solution for accurately identifying matching anonymized record pairs (with recall rates constantly above 93 percent) in large-scale privacy-preserving record linkage tasks."
1098,,Tensor-Based Big Data Management Scheme for Dimensionality Reduction Problem in Smart Grid Systems - SDN Perspective.,"Devinder Kaur,Gagangeet Singh Aujla,Neeraj Kumar 0001,Albert Y. Zomaya,Charith Perera,Rajiv Ranjan",https://doi.org/10.1109/TKDE.2018.2809747,TKDE,2018,"Big Data,Smart devices,Tensile stress,Data models,Throughput,Electronic mail,Proposals","Smart grid (SG) is an integration of traditional power grid with advanced information and communication infrastructure for bidirectional energy flow between grid and end users. A huge amount of data is being generated by various smart devices deployed in SG systems. Such a massive data generation from various smart devices in SG systems may lead to various challenges for the networking infrastructure deployed between users and the grid. Hence, an efficient data transmission technique is required for providing desired QoS to the end users in this environment. Generally, the data generated by smart devices in SG has high dimensions in the form of multiple heterogeneous attributes, values of which are changed with time. The high dimensions of data may affect the performance of most of the designed solutions in this environment. Most of the existing schemes reported in the literature have complex operations for the data dimensionality reduction problem which may deteriorate the performance of any implemented solution for this problem. To address these challenges, in this paper, a tensor-based big data management scheme is proposed for dimensionality reduction problem of big data generated from various smart devices. In the proposed scheme, first the Frobenius norm is applied on high-order-tensors (used for data representation) to minimize the reconstruction error of the reduced tensors. Then, an empirical probability-based control algorithm is designed to estimate an optimal path to forward the reduced data using software-defined networks for minimization of the network load and effective bandwidth utilization. The proposed scheme minimizes the transmission delay incurred during the movement of the dimensionally reduced data between different nodes. The efficacy of the proposed scheme has been evaluated using extensive simulations carried out on the data traces using `R' programming and Matlab. The big data traces considered for evaluation consist of more than two million entries (2,075,259) collected at one minute sampling rate having hetrogenous features such as-voltage, energy, frequency, electric signals, etc. Moreover, a comparative study for different data traces and a real SG testbed is also presented to prove the efficacy of the proposed scheme. The results obtained depict the effectiveness of the proposed scheme with respect to the parameters such asnetwork delay, accuracy, and throughput."
1099,,Relationship between Variants of One-Class Nearest Neighbors and Creating Their Accurate Ensembles.,"Shehroz S. Khan,Amir Ahmad",https://doi.org/10.1109/TKDE.2018.2806975,TKDE,2018,"Training,Prototypes,Bagging,Training data,Finance,Benchmark testing,Fault diagnosis","In one-class classification problems, only the data for the target class is available, whereas the data for the non-target class may be completely absent. In this paper, we study one-class nearest neighbor (OCNN) classifiers and their different variants. We present a theoretical analysis to show the relationships among different variants of OCNN that may use different neighbors or thresholds to identify unseen examples of the non-target class. We also present a method based on inter-quartile range for optimizing parameters used in OCNN in the absence of non-target data during training. Then, we propose two ensemble approaches based on random subspace and random projection methods to create accurate OCNN ensembles. We tested the proposed methods on 15 benchmark and real world domain-specific datasets and show that random-projection ensembles of OCNN perform best."
1100,,Conditional Reliability in Uncertain Graphs.,"Arijit Khan 0001,Francesco Bonchi,Francesco Gullo,Andreas Nufer",https://doi.org/10.1109/TKDE.2018.2816653,TKDE,2018,"Biochemistry,Compounds,Reliability theory,Tagging,Twitter","Network reliability is a well-studied problem that requires to measure the probability that a target node is reachable from a source node in a probabilistic (or uncertain) graph, i.e., a graph where every edge is assigned a probability of existence. Many approaches and problem variants have been considered in the literature, with the majority of them assuming that edge-existence probabilities are fixed. Nevertheless, in real-world graphs, edge probabilities typically depend on external conditions. In metabolic networks, a protein can be converted into another protein with some probability depending on the presence of certain enzymes. In social influence networks, the probability that a tweet of some user will be re-tweeted by her followers depends on whether the tweet contains specific hashtags. In transportation networks, the probability that a network segment will work properly or not, might depend on external conditions such as weather or time of the day. In this paper, we overcome this limitation and focus on 
<i>conditional reliability</i>
, that is, assessing reliability when edge-existence probabilities depend on a set of conditions. In particular, we study the problem of determining the top-
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 conditions that maximize the reliability between two nodes. We deeply characterize our problem and show that, even employing polynomial-time reliability-estimation methods, it is 
<inline-formula> <tex-math notation=""LaTeX"">$\mathbf {NP}$</tex-math></inline-formula>
-hard, does not admit any 
<inline-formula><tex-math notation=""LaTeX"">$\mathbf {PTAS}$ </tex-math></inline-formula>
, and the underlying objective function is non-submodular. We then devise a practical method that targets both accuracy and efficiency. We also study natural generalizations of the problem with multiple source and target nodes. An extensive empirical evaluation on several large, real-life graphs demonstrates effectiveness and scalability of our methods."
1101,,Similarity Metrics for SQL Query Clustering.,"Gökhan Kul,Duc Thanh Anh Luong,Ting Xie,Varun Chandola,Oliver Kennedy,Shambhu J. Upadhyaya",https://doi.org/10.1109/TKDE.2018.2831214,TKDE,2018,"Measurement,Query processing,Task analysis,Benchmark testing,Clustering methods","Database access logs are the starting point for many forms of database administration, from database performance tuning, to security auditing, to benchmark design, and many more. Unfortunately, query logs are also large and unwieldy, and it can be difficult for an analyst to extract broad patterns from the set of queries found therein. Clustering is a natural first step towards understanding the massive query logs. However, many clustering methods rely on the notion of pairwise similarity, which is challenging to compute for SQL queries, especially when the underlying data and database schema is unavailable. We investigate the problem of computing similarity between queries, relying only on the query structure. We conduct a rigorous evaluation of three query similarity heuristics proposed in the literature applied to query clustering on multiple query log datasets, representing different types of query workloads. To improve the accuracy of the three heuristics, we propose a generic feature engineering strategy, using classical query rewrites to standardize query structure. The proposed strategy results in a significant improvement in the performance of all three similarity heuristics."
1102,,MV-FTL - An FTL That Provides Page-Level Multi-Version Management.,"Doogie Lee,Mincheol Shin,Won Gi Choi,Hongchan Roh,Sanghyun Park",https://doi.org/10.1109/TKDE.2017.2757016,TKDE,2018,"Concurrent computing,Benchmark testing,Compaction,Atomic layer deposition,Facebook,Concurrency control,Media","In this paper, we propose MV-FTL, a multi-version flash transition layer (FTL) that provides page-level multi-version management. By extending a unique characteristic of solid-state drives (SSDs), the out-of-place (OoP) update to multi-version management, MV-FTL can both guarantee atomic page updates from each transaction and provide concurrency without requiring redundant log data writes as well. For evaluation, we first modified SQLite, a lightweight database management system (DBMS), to cooperate with MV-FTL. Owing to the architectural simplicity of SQLite, we clearly show that MV-FTL improves both the performance and the concurrency aspects of the system. In addition, to prove the effectiveness in a full-fledged enterprise-level DBMS, we modified MyRocks, a MySQL variant by Facebook, to use our new Patch Compaction algorithm, which deeply relies on MV-FTL. The TPC-C and LinkBench benchmark tests demonstrated that MV-FTL reduces the overall amount of writes, implying that MV-FTL can be effective in such DBMSs."
1103,,Longest Increasing Subsequence Computation over Streaming Sequences.,"Youhuan Li,Lei Zou 0001,Huaming Zhang,Dongyan Zhao 0001",https://doi.org/10.1109/TKDE.2017.2761345,TKDE,2018,"Data structures,Computational modeling,Data models,Genomics,Bioinformatics,Real-time systems","In this paper, we propose a data structure, a quadruple neighbor list (QN-list, for short), to support real time queries of all longest increasing subsequence (LIS) and LIS with constraints over sequential data streams. The QN-List built by our algorithm requires O(w) space, where w is the time window size. The running time for building the initial QN-List takes O(w logw) time. Applying the QN-List, insertion of the new item takes O(logw) time and deletion of the first item takes O(w) time. To the best of our knowledge, this is the first work to support both LIS enumeration and LIS with constraints computation by using a single uniform data structure for real time sequential data streams. Our method outperforms the state-of-the-art methods in both time and space cost, not only theoretically, but also empirically."
1104,,Web Media and Stock Markets - A Survey and Future Directions from a Big Data Perspective.,"Qing Li 0005,Yan Chen,Jun Wang,Yuanzhu Chen,Hsinchun Chen",https://doi.org/10.1109/TKDE.2017.2763144,TKDE,2018,"Finance,Stock markets,Social network services,Media,Text mining,Big Data","Stock market volatility is influenced by information release, dissemination, and public acceptance. With the increasing volume and speed of social media, the effects of Web information on stock markets are becoming increasingly salient. However, studies of the effects of Web media on stock markets lack both depth and breadth due to the challenges in automatically acquiring and analyzing massive amounts of relevant information. In this study, we systematically reviewed 229 research articles on quantifying the interplay between Web media and stock markets from the fields of Finance, Management Information Systems, and Computer Science. In particular, we first categorized the representative works in terms of media type and then summarized the core techniques for converting textual information into machine-friendly forms. Finally, we compared the analysis models used to capture the hidden relationships between Web media and stock movements. Our goal is to clarify current cutting-edge research and its possible future directions to fully understand the mechanisms of Web information percolation and its impact on stock markets from the perspectives of investors cognitive behaviors, corporate governance, and stock market regulation."
1105,,Influence Maximization on Social Graphs - A Survey.,"Yuchen Li 0001,Ju Fan,Yanhao Wang 0001,Kian-Lee Tan",https://doi.org/10.1109/TKDE.2018.2807843,TKDE,2018,"Social network services,Classification algorithms,Integrated circuit modeling,Diffusion processes,Computational modeling,Stochastic processes","Influence Maximization (IM), which selects a set of k users (called seed set) from a social network to maximize the expected number of influenced users (called influence spread), is a key algorithmic problem in social influence analysis. Due to its immense application potential and enormous technical challenges, IM has been extensively studied in the past decade. In this paper, we survey and synthesize a wide spectrum of existing studies on IM from an algorithmic perspective, with a special focus on the following key aspects: (1) a review of well-accepted diffusion models that capture the information diffusion process and build the foundation of the IM problem, (2) a fine-grained taxonomy to classify existing IM algorithms based on their design objectives, (3) a rigorous theoretical comparison of existing IM algorithms, and (4) a comprehensive study on the applications of IM techniques in combining with novel context features of social networks such as topic, location, and time. Based on this analysis, we then outline the key challenges and research directions to expand the boundary of IM research."
1106,,Diagnosing and Minimizing Semantic Drift in Iterative Bootstrapping Extraction.,"Zhixu Li,Ying He 0010,Binbin Gu,An Liu 0002,Hongsong Li,Haixun Wang,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2017.2782697,TKDE,2018,"Semantics,Dogs,Data mining,Syntactics,Feature extraction,Cats","Semantic drift is a common problem in iterative information extraction. Previous approaches for minimizing semantic drift may incur substantial loss in recall. We observe that most semantic drifts are introduced by a small number of questionable extractions in the earlier rounds of iterations. These extractions subsequently introduce a large number of questionable results, which lead to the semantic drift phenomenon. We call these questionable extractions Drifting Points (DPs). If erroneous extractions are the “symptoms” of semantic drift, then DPs are the “causes” of semantic drift. In this paper, we propose a method to minimize semantic drift by identifying the DPs and removing the effect introduced by the DPs. We use isA (concept-instance) extraction as an example to describe our approach in cleaning information extraction errors caused by semantic drift, but we perform experiments on different relation extraction processes on three large real data extraction collections. The experimental results show that our DP cleaning method enables us to clean around 90 percent incorrect instances or patterns with about 90 percent precision, which outperforms the previous approaches we compare with."
1107,1,Cross-Bucket Generalization for Information and Privacy Preservation.,"Boyu Li 0003,Yanheng Liu,Xu Han 0005,Jindong Zhang",https://doi.org/10.1109/TKDE.2017.2773069,TKDE,2018,"Privacy,Hospitals,Diseases,Data privacy,Security,Indexes,Sociology","Generalization is an effective technique for protecting confidential information of individuals, and has been studied by proposing numerous algorithms. However, the previous works do not separate the protection against identity disclosure and sensitive disclosure. Thus, when the requirement of attribute protection is higher than that of identity protection, generalization for l-diversity causes overprotection for identity and large mounts of information utility loss. This paper presents a novel approach, called cross-bucket generalization, as a solution to meet the problem. The rationale is to divide microdata into equivalence groups and buckets. First, it provides separate protection for identity and sensitive values, and the level of protection can be flexibly adjusted based on actual demands. Second, the sizes of equivalence groups and buckets are minimized as far as possible by only satisfying the protection requirements, which avoid the overprotection for identity and reduce information loss. The experiments we conducted illustrate the effectiveness of our solution."
1108,,In Search of Indoor Dense Regions - An Approach Using Indoor Positioning Data.,"Huan Li 0003,Hua Lu 0001,Lidan Shou,Gang Chen 0001,Ke Chen 0005",https://doi.org/10.1109/TKDE.2018.2799215,TKDE,2018,"Uncertainty,Topology,Search problems,Algorithm design and analysis,Security,Airports,Sensors","As people spend significant parts of daily lives indoors, it is useful and important to measure indoor densities and find the dense regions in many indoor scenarios like space management and security control. In this paper, we propose a data-driven approach that finds top-k indoor dense regions by using indoor positioning data. Such data is obtained by indoor positioning systems working at a relatively low frequency, and the reported locations in the data are discrete, from a preselected location set that does not continuously cover the entire indoor space. When a search is triggered, the object positioning information is already out-of-date and thus object locations are uncertain. To this end, we first integrate object location uncertainty into the definitions for counting objects in an indoor region and computing its density. Subsequently, we conduct a thorough analysis of the location uncertainty in the context of complex indoor topology, deriving upper and lower bounds of indoor region densities and introducing distance decaying effect into computing concrete indoor densities. Enabled by the uncertainty analysis outcomes, we design efficient search algorithms for solving the problem. Finally, we conduct extensive experimental studies on our proposals using synthetic and real data. The experimental results verify that the proposed search approach is efficient, scalable, and effective. The top-k indoor dense regions returned by our search are considerably consistent with ground truth, despite that the search uses neither historical data nor extra knowledge about objects."
1109,,SDE - A Novel Clustering Framework Based on Sparsity-Density Entropy.,"Sheng Li 0011,Lusi Li,Jun Yan 0007,Haibo He",https://doi.org/10.1109/TKDE.2018.2792021,TKDE,2018,"Entropy,Clustering algorithms,Clustering methods,Shape,Complexity theory,Data models,Self-organizing feature maps","Clustering of data with high dimension and variable densities poses a remarkable challenge to the traditional density-based clustering methods. Recently, entropy, a numerical measure of the uncertainty of information, can be used to measure the border degree of samples in data space and also select significant features in feature set. It was used in our new framework based on the sparsity-density entropy (SDE) to cluster the data with high dimension and variable densities. First, SDE conducts high-quality sampling for multidimensional data and selects the representative features using sparsity score entropy (SSE). Second, the clustering results and noises are obtained adopting a new density-variable clustering method called density entropy (DE). DE automatically determines the border set based on the global minimum of border degrees and then adaptively performs cluster analysis for each local cluster based on the local minimum of border degrees. The effectiveness and efficiency of the proposed SDE framework are validated on synthetic and real data sets in comparison with several clustering algorithms. The results showed that the proposed SDE framework concurrently detected the noises and processed the data with high dimension and various densities."
1110,,A Comment on &quot;Cross-Platform Identification of Anonymous Identical Users in Multiple Social Media Networks&quot;.,"Yongjun Li 0006,Zhaoting Su",https://doi.org/10.1109/TKDE.2018.2828812,TKDE,2018,"Social netowrk services,Identification,Algorithm design and analysis","The Friend Relationship-Based User Identification (FRUI) algorithm is considered to be the ideal method. However, if the seed User Matched Pairs (UMPs) were not suitable, FRUI would stop early due to Controversial UMPs. We highlight this gap and propose a minor change to make FRUI a more general identification algorithm."
1111,,Supervised Topic Modeling Using Hierarchical Dirichlet Process-Based Inverse Regression - Experiments on E-Commerce Applications.,"Weifeng Li 0002,Junming Yin,Hsinchun Chen",https://doi.org/10.1109/TKDE.2017.2786727,TKDE,2018,"Predictive models,Inference algorithms,Prediction algorithms,Approximation algorithms,Semantics,Measurement","The proliferation of e-commerce calls for mining consumer preferences and opinions from user-generated text. To this end, topic models have been widely adopted to discover the underlying semantic themes (i.e., topics). Supervised topic models have emerged to leverage discovered topics for predicting the response of interest (e.g., product quality and sales). However, supervised topic modeling remains a challenging problem because of the need to prespecify the number of topics, the lack of predictive information in topics, and limited scalability. In this paper, we propose a novel supervised topic model, Hierarchical Dirichlet Process-based Inverse Regression (HDP-IR). HDP-IR characterizes the corpus with a flexible number of topics, which prove to retain as much predictive information as the original corpus. Moreover, we develop an efficient inference algorithm capable of examining large-scale corpora (millions of documents or more). Three experiments were conducted to evaluate the predictive performance over major e-commerce benchmark testbeds of online reviews. Overall, HDP-IR outperformed existing state-of-the-art supervised topic models. Particularly, retaining sufficient predictive information improved predictive R-squared by over 17.6 percent; having topic structure flexibility contributed to predictive R-squared by at least 4.1 percent. HDP-IR provides an important step for future study on user-generated texts from a topic perspective."
1112,1,Differentially Private Distributed Online Learning.,"Chencheng Li,Pan Zhou,Li Xiong 0001,Qian Wang 0002,Ting Wang 0006",https://doi.org/10.1109/TKDE.2018.2794384,TKDE,2018,"Data privacy,Distributed databases,Privacy,Optimization,Big Data,Computational modeling,Perturbation methods","In the big data era, the generation of data presents some new characteristics, including wide distribution, high velocity, high dimensionality, and privacy concern. To address these challenges for big data analytics, we develop a privacy-preserving distributed online learning framework on the data collected from distributed data sources. Specifically, each node (i.e., data source) has the capacity of learning a model from its local dataset, and exchanges intermediate parameters with a random part of their own neighboring (logically connected) nodes. Hence, the topology of the communications in our distributed computing framework is unfixed in practice. As online learning always performs on the sensitive data, we introduce the notion of differential privacy (DP) into our distributed online learning algorithm (DOLA) to protect the data privacy during the learning, which prevents an adversary from inferring any significant sensitive information. Our model is of general value for big data analytics in the distributed setting, because it can provide rigorous and scalable privacy proof and have much less computational complexity when compared to classic schemes, e.g., secure multiparty computation (SMC). To tackle high-dimensional incoming data entries, we study a sparse version of the DOLA with novel DP techniques to save the computing resources and improve the utility. Furthermore, we present two modified private DOLAs to meet the need of practical applications. One is to convert the DOLA to distributed stochastic optimization in an offline setting, the other is to use the mini-batches approach to reduce the amount of the perturbation noise and improve the utility. We conduct experiments on real datasets in a configured distributed platform. Numerical experiment results validate the feasibility of our private DOLAs."
1113,,Realizing Memory-Optimized Distributed Graph Processing.,"Panagiotis Liakos,Katia Papakonstantinopoulou,Alex Delis",https://doi.org/10.1109/TKDE.2017.2779797,TKDE,2018,"Optimization,Memory management,Facebook,Global Positioning System,Scalability,Computational modeling","A multitude of contemporary applications heavily involve graph data whose size appears to be ever-increasing. This trend shows no signs of subsiding and has caused the emergence of a number of distributed graph processing systems including Pregel, Apache Giraph, and GraphX. However, the unprecedented scale now reached by real-world graphs hardens the task of graph processing due to excessive memory demands even for distributed environments. By and large, such contemporary graph processing systems employ ineffective in-memory representations of adjacency lists. Therefore, memory usage patterns emerge as a primary concern in distributed graph processing. We seek to address this challenge by exploiting empirically-observed properties demonstrated by graphs generated by human activity. In this paper, we propose 1) three compressed adjacency list representations that can be applied to any distributed graph processing system, 2) a variable-byte encoded representation of out-edge weights for space-efficient support of weighted graphs, and 3) a tree-based compact out-edge representation that allows for efficient mutations on the graph elements. We experiment with publicly-available graphs whose size reaches two-billion edges and report our findings in terms of both space-efficiency and execution time. Our suggested compact representations do reduce respective memory requirements for accommodating the graph elements up-to 5 times if compared with state-of-the-art methods. At the same time, our memory-optimized methods retain the efficiency of uncompressed structures and enable the execution of algorithms for large scale graphs in settings where contemporary alternative structures fail due to memory errors."
1114,,Scalable Content-Aware Collaborative Filtering for Location Recommendation.,"Defu Lian,Yong Ge,Fuzheng Zhang,Nicholas Jing Yuan,Xing Xie 0001,Tao Zhou 0001,Yong Rui",https://doi.org/10.1109/TKDE.2018.2789445,TKDE,2018,"Collaboration,Semantics,Social network services,Sparse matrices,Optimization,Electronic mail,Filtering","Location recommendation plays an essential role in helping people find attractive places. Though recent research has studied how to recommend locations with social and geographical information, few of them addressed the cold-start problem of new users. Because mobility records are often shared on social networks, semantic information can be leveraged to tackle this challenge. A typical method is to feed them into explicit-feedback-based content-aware collaborative filtering, but they require drawing negative samples for better learning performance, as users' negative preference is not observable in human mobility. However, prior studies have empirically shown sampling-based methods do not perform well. To this end, we propose a scalable Implicit-feedback-based Content-aware Collaborative Filtering (ICCF) framework to incorporate semantic content and to steer clear of negative sampling. We then develop an efficient optimization algorithm, scaling linearly with data size and feature size, and quadratically with the dimension of latent space. We further establish its relationship with graph Laplacian regularized matrix factorization. Finally, we evaluate ICCF with a large-scale LBSN dataset in which users have profiles and textual content. The results show that ICCF outperforms several competing baselines, and that user information is not only effective for improving recommendations but also coping with cold-start scenarios."
1115,,Attributed Social Network Embedding.,"Lizi Liao,Xiangnan He 0001,Hanwang Zhang,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2018.2819980,TKDE,2018,"Task analysis,Distance measurement,Neural networks,Computational modeling,Deep learning","Embedding network data into a low-dimensional vector space has shown promising performance for many real-world applications, such as node classification and entity retrieval. However, most existing methods focused only on leveraging network structure. For social networks, besides the network structure, there also exists rich information about social actors, such as user profiles of friendship networks and textual content of citation networks. These rich attribute information of social actors reveal the homophily effect, exerting huge impacts on the formation of social networks. In this paper, we explore the rich evidence source of attributes in social networks to improve network embedding. We propose a generic Attributed Social Network Embedding framework (ASNE), which learns representations for social actors (i.e., nodes) by preserving both the structural proximity and attribute proximity. While the structural proximity captures the global network structure, the attribute proximity accounts for the homophily effect. To justify our proposal, we conduct extensive experiments on four real-world social networks. Compared to the state-of-the-art network embedding approaches, ASNE can learn more informative representations, achieving substantial gains on the tasks of link prediction and node classification. Specifically, ASNE significantly outperforms node2vec with an 8.2 percent relative improvement on the link prediction task, and a 12.7 percent gain on the node classification task."
1116,,CRAFTER - A Tree-Ensemble Clustering Algorithm for Static Datasets with Mixed Attributes and High Dimensionality.,"Sangdi Lin,Bahareh Azarnoush,George C. Runger",https://doi.org/10.1109/TKDE.2018.2807444,TKDE,2018,"Clustering algorithms,Radio frequency,Training,Partitioning algorithms,Complexity theory,Computational efficiency,Data mining","Clustering is an important aspect of data mining, while clustering high-dimensional mixed-attribute data in a scalable fashion still remains a challenging problem. In this paper, we propose a tree-ensemble clustering algorithm for static datasets, CRAFTER, to tackle this problem. CRAFTER is able to handle categorical and numeric attributes simultaneously, and scales well with the dimensionality and the size of datasets. CRAFTER leverages the advantages of a tree-ensemble to handle mixed attributes and high dimensionality. The concept of the class probability estimates is utilized to identify the representative data points for clustering. Through a series of experiments on both synthetic and real datasets, we have demonstrated that CRAFTER is superior than Random Forest Clustering (RFC), an existing tree-based clustering method, in terms of both the clustering quality and the computational cost."
1117,,Clarifying Trust in Social Internet of Things.,"Zhiting Lin,Liang Dong 0001",https://doi.org/10.1109/TKDE.2017.2762678,TKDE,2018,"Social network services,Peer-to-peer computing,Computational modeling,Internet of Things,Adaptation models,Data models","A social approach can be exploited for the Internet of Things (IoT) to manage a large number of connected objects. These objects operate as autonomous agents to request and provide information and services to users. Establishing trustworthy relationships among the objects greatly improves the effectiveness of node interaction in the social IoT and helps nodes overcome perceptions of uncertainty and risk. However, there are limitations in the existing trust models. In this paper, a comprehensive model of trust is proposed that is tailored to the social IoT. The model includes ingredients such as trustor, trustee, goal, trustworthiness evaluation, decision, action, result, and context. Building on this trust model, we clarify the concepts of trust in the social IoT in five aspects such as: 1) mutuality of trustor and trustee; 2) inferential transfer of trust; 3) transitivity of trust; 4) trustworthiness update; and 5) trustworthiness affected by dynamic environment. With network connectivities that are from real-world social networks, a series of simulations are conducted to evaluate the performance of the social IoT operated with the proposed trust model. An experimental IoT network is used to further validate the proposed trust model."
1118,,Minority Oversampling in Kernel Adaptive Subspaces for Class Imbalanced Datasets.,"Chin-Teng Lin,Tsung-Yu Hsieh,Yu-Ting Liu,Yang-Yin Lin,Chieh-Ning Fang,Yu-Kai Wang,Gary G. Yen,Nikhil R. Pal,Chun-Hsiang Chuang",https://doi.org/10.1109/TKDE.2017.2779849,TKDE,2018,"Neurons,Kernel,Self-organizing feature maps,Data models,Machine learning algorithms,Algorithm design and analysis,Feature extraction","The class imbalance problem in machine learning occurs when certain classes are underrepresented relative to the others, leading to a learning bias toward the majority classes. To cope with the skewed class distribution, many learning methods featuring minority oversampling have been proposed, which are proved to be effective. To reduce information loss during feature space projection, this study proposes a novel oversampling algorithm, named minority oversampling in kernel adaptive subspaces (MOKAS), which exploits the invariant feature extraction capability of a kernel version of the adaptive subspace self-organizing maps. The synthetic instances are generated from well-trained subspaces and then their pre-images are reconstructed in the input space. Additionally, these instances characterize nonlinear structures present in the minority class data distribution and help the learning algorithms to counterbalance the skewed class distribution in a desirable manner. Experimental results on both real and synthetic data show that the proposed MOKAS is capable of modeling complex data distribution and outperforms a set of state-of-the-art oversampling algorithms."
1119,,Road Traffic Speed Prediction - A Probabilistic Model Fusing Multi-Source Data.,"Lu Lin,Jianxin Li 0002,Feng Chen 0001,Jieping Ye,Jinpeng Huai",https://doi.org/10.1109/TKDE.2017.2718525,TKDE,2018,"Sensors,Roads,Data models,Trajectory,Predictive models,Social network services,Autoregressive processes","Road traffic speed prediction is a challenging problem in intelligent transportation system (ITS) and has gained increasing attentions. Existing works are mainly based on raw speed sensing data obtained from infrastructure sensors or probe vehicles, which, however, are limited by expensive cost of sensor deployment and maintenance. With sparse speed observations, traditional methods based only on speed sensing data are insufficient, especially when emergencies like traffic accidents occur. To address the issue, this paper aims to improve the road traffic speed prediction by fusing traditional speed sensing data with new-type “sensing” data from cross domain sources, such as tweet sensors from social media and trajectory sensors from map and traffic service platforms. Jointly modeling information from different datasets brings many challenges, including location uncertainty of low-resolution data, language ambiguity of traffic description in texts, and heterogeneity of cross-domain data. In response to these challenges, we present a unified probabilistic framework, called Topic-Enhanced Gaussian Process Aggregation Model (TEGPAM), consisting of three components, i.e., location disaggregation model, traffic topic model, and traffic speed Gaussian Process model, which integrate new-type data with traditional data. Experiments on real world data from two large cities validate the effectiveness and efficiency of our model."
1120,,Non-Overlapping Subsequence Matching of Stream Synopses.,"Su-Chen Lin,Mi-Yen Yeh,Ming-Syan Chen",https://doi.org/10.1109/TKDE.2017.2725833,TKDE,2018,"Algorithm design and analysis,Heuristic algorithms,Delays,Time series analysis,Pattern matching,Monitoring,Electronic mail","In this paper, we propose SUbsequence Matching framework with cell MERgence (SUMMER) for online subsequence matching between histogram-based stream synopsis structures under the dynamic time warping distance. Given a query synopsis pattern, SUMMER continuously identifies all the matching subsequences for a stream as the bins are generated. To effectively reduce the computation time, we design a Weighted Dynamic Time Warping (WDTW) algorithm, which computes the warping distance directly between two histogram-based synopses. Furthermore, a Stack-based Overlapping Filter Algorithm (SOFA) is provided to remove the overlapping subsequences to avoid the redundant information. Finally, we design an optional refinement module to relax the subsequence range limit and improve the matching accuracy. Our experiments on real datasets show that the proposed method significantly speeds up the pattern matching without compromising the accuracy required when compared with other approaches."
1121,,Finding Top-k Shortest Paths with Diversity.,"Huiping Liu,Cheqing Jin,Bin Yang 0002,Aoying Zhou",https://doi.org/10.1109/TKDE.2017.2773492,TKDE,2018,"Measurement,Robot motion,Planning,Wireless sensor networks,Computer science,Complexity theory,Vehicle routing","The classical K Shortest Paths (KSP) problem, which identifies the k shortest paths in a directed graph, plays an important role in many application domains, such as providing alternative paths for vehicle routing services. However, the returned k shortest paths may be highly similar, i.e., sharing significant amounts of edges, thus adversely affecting service qualities. In this paper, we formalize the K Shortest Paths with Diversity (KSPD) problem that identifies top-k shortest paths such that the paths are dissimilarwith each other and the total length of the paths is minimized. We first prove that the KSPD problem is NP-hard and then propose a generic greedy framework to solve the KSPD problem in the sense that (1) it supports a wide variety of path similarity metrics which are widely adopted in the literature and (2) it is also able to efficiently solve the traditional KSP problem if no path similarity metric is specified. The core of the framework includes the use of two judiciously designed lower bounds, where one is dependent on and the other one is independent on the chosen path similarity metric, which effectively reduces the search space and significantly improves efficiency. Empirical studies on five real-world and synthetic graphs and five different path similarity metrics offer insight into the design properties of the proposed general framework and offer evidence that the proposed lower bounds are effective."
1122,,MCS-GPM - Multi-Constrained Simulation Based Graph Pattern Matching in Contextual Social Graphs.,"Guanfeng Liu 0001,Yi Liu,Kai Zheng 0001,An Liu 0002,Zhixu Li,Yan Wang 0002,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2017.2785824,TKDE,2018,"Pattern matching,Indexes,Data models,Social network services,Context modeling,Time complexity,Computational modeling","Graph Pattern Matching (GPM) has been used in lots of areas, like biology, medical science, and physics. With the advent of Online Social Networks (OSNs), recently, GPM has been playing a significant role in social network analysis, which has been widely used in, for example, finding experts, social community mining, and social position detection. Given a query which contains a pattern graph G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Q</sub>
 and a data graph G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">D</sub>
, a GPM algorithm finds those subgraphs, G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</sub>
, that match G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Q</sub>
 in G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">D</sub>
. However, the existing GPM methods do not consider the multiple end-to-end constraints of the social contexts, like social relationships, social trust, and social positions on edges in G
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Q</sub>
, which are commonly found in various applications, such as crowdsourcing travel, social network based ecommerce, and study group selection, etc. In this paper, we first conceptually extend Bounded Simulation to Multi-Constrained Simulation (MCS), and propose a novel NP-Complete Multi-Constrained Graph Pattern Matching (MC-GPM) problem. Then, to address the efficiency issue in large-scale MC-GPM, we propose a new concept called Strong Social Component (SSC), consisting of participants with strong social connections. We also propose an approach to identifying SSCs, and propose a novel index method and a graph compression method for SSC. Moreover, we devise a multithreading heuristic algorithm, called M-HAMC, to bidirectionally search the MC-GPM results in parallel without decompressing graphs. An extensive empirical study over five real-world large-scale social graphs has demonstrated the effectiveness and efficiency of our approach."
1123,,Linguistic Petri Nets Based on Cloud Model Theory for Knowledge Representation and Reasoning.,"Hu-Chen Liu,Xue Luan,ZhiWu Li,Jianing Wu",https://doi.org/10.1109/TKDE.2017.2778256,TKDE,2018,"Cognition,Petri nets,Pragmatics,Knowledge representation,Knowledge based systems,Adaptation models,Modeling","Fuzzy Petri nets (FPNs) are a vital modeling technique for the construction of knowledge-based systems, which have been commonly used in many fields, such as fault diagnosis, risk assessment, workflow management, and disassembly process planning. However, the conventional FPNs have been blamed for the following reasons: 1) the representation parameters in FPNs cannot precisely model experts' experience since it is difficult to manage the fuzziness and randomness of knowledge assessments simultaneously, and 2) the weight coefficients in the existing approximate reasoning algorithms are hardly enough to reflect the associated weights of reordered places. In response, we propose a new type of FPNs, called cloud reasoning Petri nets (CRPNs) based on the concept of interval clouds and the hybrid averaging operator. The cloud production rules in a knowledge-based system are modeled by CRPNs, where the truth degrees of places, the certainty factors of rules, and the thresholds of transitions are represented by interval clouds. Moreover, a matrix operation-based reasoning algorithm is proposed to improve the efficiency of calculating final truth degrees, in which both local and ordered weight coefficients are taken into consideration. Finally, a practical example concerning a power system is provided to demonstrate the usefulness and advantages of the proposed CRPN model."
1124,,Efficient Detection of Soft Concatenation Mapping.,"Hao Liu 0026,Jiang Xiao,Haoyu Tan,Qiong Luo 0001,Jintao Zhao,Lionel M. Ni",https://doi.org/10.1109/TKDE.2018.2812822,TKDE,2018,"Data warehouses,Redundancy,Data integration,Data compression,Approximation algorithms,Electronic mail,Task analysis","In modern big data warehouse systems, we observe a common phenomenon that a column of data values can be derived from one or several other columns by transforming and concatenating these columns. We call this relationship between columns a Soft Concatenation Mapping (SCM). SCMs imply significant redundancy in the schema or data, and therefore can be exploited for data integration or data compression. In this paper, we formalize the problem of SCM detection and prove it is NP-hard. We then propose efficient approximate algorithms to detect all SCMs or an optimal set of SCMs in a table. Our experiments on both real-world and synthetic datasets show promising results."
1125,,Learning Multi-Instance Deep Ranking and Regression Network for Visual House Appraisal.,"Xiaobai Liu,Qian Xu,Jingjie Yang,Jacob Thalman,Shuicheng Yan,Jiebo Luo",https://doi.org/10.1109/TKDE.2018.2791611,TKDE,2018,"Appraisal,Visualization,Neural networks,Convolution,Boosting,Image retrieval","This paper presents a weakly supervised regression model for the visual house appraisal problem, which aims to predict the value of a house from its photos and textual descriptions (e.g., number of bedrooms). The key idea of our approach is a multi-layer neural network, called multi-instance Deep Ranking and Regression (MiDRR) net, which jointly solves two coupled tasks: ranking and regression, in the multiple instance setting. The network is trained using weakly supervised data, which do not require intensive human annotations. We also design a set of human heuristics to promote deep features through imposing constraints over the solution space, e.g., a house with three bedrooms often has a higher value than that with only two bedrooms. While these constraints are specific to the studied problem, the developed formula can be easily generalized to the other regression applications. For test and evaluation purposes, we collect a comprehensive house image benchmark that includes 900,000 photos from 30,000 houses recently traded in the USA, and apply the proposed MiDRR net to predict house values. Extensive evaluations with comparisons demonstrate that additional usage of imagery data as well as human heuristics can significantly boost system performance and that the proposed MiDRR net clearly outperforms the alternative methods."
1126,,Planning with Spatio-Temporal Search Control Knowledge.,"Xu Lu,Cong Tian,Zhenhua Duan,Hongwei Du 0001",https://doi.org/10.1109/TKDE.2018.2810144,TKDE,2018,"Planning,Roads,Automobiles,Junctions,Search problems,Benchmark testing,Tools","Knowledge based approaches developed for AI planning can convert an intractable planning problem to a tractable one. Current techniques often use temporal logics to express Search Control Knowledge (SCK) in logic based planning. However, traditional temporal logics are limited in expressiveness since they are unable to express spatial constraints which are as important as temporal ones in many planning domains. To this end, we propose a two-dimensional (spatial and temporal) logic namely PPTLSL by temporalizing separation logic with PPTL (Propositional Projection Temporal Logic) which is well-suited to specify SCK involving both spatial and temporal constraints in planning. We prove that PPTLSL is decidable essentially via an equisatisfiable translation from PPTLSL to its restricted form. Moreover, we implement a tool, S-TSolver, which effectively computes plans under the guidance of the spatio-temporal SCK expressed by PPTLSL formulas. The effectiveness of the tool is evaluated on selected benchmark domains from the International Planning Competition."
1127,,Exploring Triangle-Free Dense Structures.,"Can Lu,Jeffrey Xu Yu,Hao Wei",https://doi.org/10.1109/TKDE.2017.2764468,TKDE,2018,"Social network services,Collaboration,Distance measurement,Image edge detection,Analytical models,Indexes","Triadic closure is ubiquitous in social networks, which refers to the property among three individuals, A, B, and C, such that if there exist strong ties between A-B and A-C, then there must be a strong or weak tie between B-C. Related to triadic closure, the number of triangles has been extensively studied since it can be effectively used as a metric to analyze the structure and function of a network. In this paper, from a different viewpoint, we study triangle-free dense structures which have received little attention. We focus on 
<inline-formula><tex-math notation=""LaTeX""> $K_{3,3}$</tex-math></inline-formula>
 where there are two subsets of three vertices, a vertex in a subset has an edge connected to every vertex in another subset while it does not have an edge to any other vertex in the same subset. Such 
<inline-formula> <tex-math notation=""LaTeX"">$K_{n,n}$</tex-math></inline-formula>
 in general implies a philosophy contradiction: (a) Any two individuals are friends if they have no common friends, and (b) Any two individuals are not friends if they have common friends. However, we find such induced 
<inline-formula><tex-math notation=""LaTeX"">$K_{3,3}$</tex-math></inline-formula>
 does exist frequently, and they do not disappear over time over a real academic collaboration network. In addition, in the real datasets tested, nearly all edges appearing in 
<inline-formula><tex-math notation=""LaTeX"">$K_{3,3}$</tex-math></inline-formula>
 appear in some triangles. We analyze the expected numbers of induced 
<inline-formula><tex-math notation=""LaTeX"">$K_{3,3}$</tex-math></inline-formula>
 and triangles (
<inline-formula> <tex-math notation=""LaTeX"">$\Delta$</tex-math></inline-formula>
) in four representative random graph models, namely, Erdős-Rényi random graph model, Watts-Strogatz small-world model, Barabási-Albert preferential attachment model, and configuration model, and give an algorithm to enumerate all distinct 
<inline-formula><tex-math notation=""LaTeX"">$K_{3,3}$</tex-math> </inline-formula>
 in an undirected social network. We conduct extensive experiments on both real and synthetic datasets to confirm our findings. As an application, such 
<inline-formula><tex-math notation=""LaTeX"">$K_{3,3}$</tex-math></inline-formula>
 found helps to find new stars collaborated by well-known figures who themselves do not collaborate."
1128,,Semi-Supervised Feature Selection via Insensitive Sparse Regression with Application to Video Semantic Recognition.,"Tingjin Luo,Chenping Hou,Feiping Nie 0001,Hong Tao,Dongyun Yi",https://doi.org/10.1109/TKDE.2018.2810286,TKDE,2018,"Feature extraction,Semantics,Robustness,Training data,Laplace equations,Multimedia communication,Streaming media","Feature selection plays a significant role in dealing with high-dimensional data to avoid the curse of dimensionality. In many real applications, like video semantic recognition, handling few labeled and large unlabeled data samples from the same population is a recently addressed challenge in feature selection. To solve this problem, we propose a novel semi-supervised feature selection method via insensitive sparse regression (ISR). Specifically, we compute the soft label matrix by the special label propagation, which can predict the labels of the unlabeled data. To guarantee the robustness of ISR to the false labeled instances or outliers, we propose Insensitive Regression Model (IRM) by capped l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
-l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">p</sub>
-norm loss. The soft label is imposed as the weights of IRM to fully utilize the label information. Meanwhile, to perform feature selection, we incorporate l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2;q</sub>
-norm regularizer with IRM as the structural sparsity constraint when 0 <; q ≤ 1. Moreover, we put forward an effective approach for solving the formulated non-convex optimization problem. We analyze the performance of convergence rigorously and discuss the parameter determination problem. Extensive experimental results on several public data sets verify the effectiveness of our proposed algorithm in comparison with the state-of-art feature selection methods. Finally, we apply our method to video semantic recognition successfully."
1129,,Top-kCritical Vertices Query on Shortest Path.,"Jing Ma 0002,Bin Yao 0002,Xiaofeng Gao,Yanyan Shen,Minyi Guo",https://doi.org/10.1109/TKDE.2018.2808495,TKDE,2018,"Labeling,Acceleration,Bidirectional control,Optimization,Roads,Complexity theory,Indexes","Shortest path query is one of the most fundamental and classic problems in graph analytics, which returns the complete shortest path between any two vertices. However, in many real-life scenarios, only critical vertices on the shortest path are desirable and it is unnecessary to search for the complete path. This paper investigates the shortest path sketch by defining a top-k critical vertices (kCV) query on the shortest path. Given a source vertex s and target vertex t in a graph, kCV query can return the top-k significant vertices on the shortest path SP(s; t). The significance of the vertices can be predefined. The key strategy for seeking the sketch is to apply off-line preprocessed distance oracle to accelerate on-line real-time queries. This allows us to omit unnecessary vertices and obtain the most representative sketch of the shortest path directly. We further explore a series of methods and optimizations to answer kCV query on both centralized and distributed platforms, using exact and approximate approaches, respectively. We evaluate our methods in terms of time, space complexity and approximation quality. Experiments on large-scale real-world networks validate that our algorithms are of high efficiency and accuracy."
1130,,A New Query Recommendation Method Supporting Exploratory Search Based on Search Goal Shift Graphs.,"Chao Ma 0009,Bin Zhang 0001",https://doi.org/10.1109/TKDE.2018.2815544,TKDE,2018,"Task analysis,Search problems,Art,Information security,Web search,Search engines","Exploratory search is an increasingly important activity for Web searchers. However, the current search system can not provide sufficient support for exploratory search. Therefore, we made in-depth analysis for exploratory search processes, and found that there are a lot of search goal shift phenomena in exploratory search. Based on this fact, we have designed a new query recommendation method to support exploratory search. Firstly, according to the behavioral characteristics of searchers in the search goal shift processes, all the queries submitted in the search goal shift processes are extracted from search engine logs using machine learning. And then, we have used the queries to build a search goal shift graph; finally, the random walk algorithm is used to obtain the query recommendations in the search goal shift graph. In addition, we demonstrated the effectiveness of the method for exploratory search by comparing experiments with the other methods."
1131,,Making a Small World Smaller - Path Optimization in Networks.,"Sourav Medya,Petko Bogdanov,Ambuj K. Singh",https://doi.org/10.1109/TKDE.2018.2792470,TKDE,2018,"Delays,Airports,Minimization,Social network services,Algorithm design and analysis,Knowledge engineering,Data engineering","Reduction of end-to-end network delay is an optimization task with applications in multiple domains. Low delays enable improved information flow in social networks, quick spread of ideas in collaboration networks, low travel times for vehicles on road networks, and increased rate of packets in the case of communication networks. Delay reduction can be achieved by both improving the propagation capabilities of individual nodes and adding additional edges in the network. One of the main challenges in such network design problems is that the effects of local changes are not independent, and as a consequence, there is a combinatorial search-space of possible improvements. Thus, minimizing the cumulative propagation delay requires novel scalable and data-driven approaches. We consider the problem of network delay minimization via node upgrades. We show that the problem is NP-hard and prove strong inapproximability results about it (i.e., APX-hard) even for equal vertex delays. On the positive side, probabilistic approximations for a restricted version of the problem can be obtained. We propose a greedy heuristic to solve the general problem setting which has good quality in practice, but does not scale to very large instances. To enable scalability to real-world networks, we develop approximations for Greedy with probabilistic guarantees for every iteration, tailored to different models of delay distribution and network structures. Our methods scale almost linearly with the graph size and consistently outperform competitors in quality. We evaluate our approaches on several real-world graphs from different genres. We achieve up to two orders of magnitude speed-up compared to alternatives from the literature on moderate size networks, and obtain high-quality results in minutes on large datasets while competitors from the literature require more than four hours."
1132,,On the Complexity of Bounded View Propagation for Conjunctive Queries.,"Dongjing Miao,Zhipeng Cai 0001,Jianzhong Li 0001",https://doi.org/10.1109/TKDE.2017.2758361,TKDE,2018,"Computational complexity,Knowledge engineering,Data engineering,Indexes,Robustness","The view propagation problem is a class of view update problem in relational databases [7], involving deletion and insertion propagations. Given source database D, conjunctive query Q, view V generated by query Q(D) and a deletion (insertion) on view ΔV, deletion (insertion) propagation is to find a side effect free update ΔD on D such that the deletion (insertion) of ΔD from (into) D will delete (insert) the intentional ones ΔV without resulting in the deletion (insertion) of additional tuples from (into) the view. Generally, such a deletion (insertion) is side effect free. The related data management applications include query result explanation, data debugging, and anonymizing datasets, which rely on understanding how interventions in a database affect the output of a query. View propagation is a natural and typical way to define such interventions, which seems to be well-studied. However, in general, the candidate update on a source database is picked up aimlessly in advance, making the updated database to be very distant from the original one no matter whether it is the maximum one. In this paper, we formally define the bounded view propagation problem, where candidate update ΔD is bounded as a subset of potential C which is a fixed small tuple set of D. We study the complexity of this problem for conjunctive queries, and make contributions to the previous results of the problems of side-effect free deletion propagation. Specifically, our bounded view propagation problem decreases computational complexity regardless of conjunctive query structure. We show the fixed potential is actually a dichotomy for both deletion and insertion propagations, and figure out the results on combined complexity which is neglected previously. Based on our results, for view propagation, we map out a complete picture of the computational complexity hierarchy for conjunctive queries on both data and combined complexities. Moreover, this bounded version is an update forbidden case of view propagation, and our results can be applied to it."
1133,,On Efficiently Answering Why-Not Range-Based Skyline Queries in Road Networks.,"Xiaoye Miao,Yunjun Gao,Su Guo,Gang Chen 0001",https://doi.org/10.1109/TKDE.2018.2803821,TKDE,2018,"Roads,Indexes,Cognition,Heuristic algorithms,Probabilistic logic,Privacy","The range-based skyline (r-skyline) query on road networks retrieves the skyline objects for each of the query points that are within a road region, considering the objects' spatial and non-spatial attributes. However, reasoning about missing query results, specified by why-not questions, has not till recently received the attention it is worth of. In this paper, we systematically carry out the study of why-not questions on the r-skyline query in the road network environment (abbrev. as the why-not RSQ problem). We present three modification strategies, including modifying the query range, modifying the why-not point, and modifying both of them, for supporting the why-not RSQ problem. We also propose three efficient algorithms to tackle the why-not RSQ problem, where several newly presented effective concepts/techniques are leveraged, such as the concepts of skyline scope and skyline dominance region, non-spatial attribute modification pruning, and G-tree index. Extensive experimental evaluation using both real and synthetic data sets demonstrates the performance of our proposed algorithms."
1134,,Optimizing Quality for Probabilistic Skyline Computation and Probabilistic Similarity Search.,"Xiaoye Miao,Yunjun Gao,Linlin Zhou,Wei Wang,Qing Li 0001",https://doi.org/10.1109/TKDE.2018.2805824,TKDE,2018,"Probabilistic logic,Automobiles,Cleaning,Optimization,Uncertainty,Indexes,Probability density function","Probabilistic queries have been extensively explored to provide answers with confidence, in order to support the real-life applications struggling with uncertain data, such as sensor networks and data integration. However, the uncertainty of data may propagate, and thus, the results returned by probabilistic queries contain much noise, which degrades query quality significantly. In this paper, we propose an efficient optimization framework, termed as QueryClean, for both probabilistic skyline computation and probabilistic similarity search. The goal of QueryClean is to optimize query quality via selecting a group of uncertain objects to clean under limited resource available, where a joint-entropy based quality function is leveraged. We develop an efficient structure called ASI to index the possible result sets of probabilistic queries, which helps to avoid many types of probabilistic query evaluations over a large number of the possible worlds for quality computation. Moreover, we present exact and approximate algorithms for the optimization problem, using two newly presented heuristics. Considerable experimental results on both real and synthetic data sets demonstrate the efficiency and scalability of our proposed framework QueryClean."
1135,,Computing Maximized Effectiveness Distance for Recall-Based Metrics.,Alistair Moffat,https://doi.org/10.1109/TKDE.2017.2754371,TKDE,2018,"Measurement,Information retrieval,Mathematical model,Indexes,Tools,Australia","Given an effectiveness metric M(·), two ordered document rankings X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
 and X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
 generated by a score-based information retrieval activity, and relevance labels in regard to some subset (possibly empty) of the documents appearing in the two rankings, Tan and Clarke's Maximized Effectiveness Distance (MED) computes the greatest difference in metric score that can be achieved that is consistent with all provided information, crystallized via a set of relevance assignments to the unlabeled documents such that |M(X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
) - M(X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
)| is maximized. The closer the maximized effectiveness distance is to zero, the more similar X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
 and X
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
 can be considered to be from the point of view of the metric M(·). Here, we consider issues that arise when Tan and Clarke's definitions are applied to recall-based metrics, notably normalized discounted cumulative gain (NDCG), and average precision (AP). In particular, we show that MED can be applied to NDCG without requiring an a priori assumption in regard to the total number of relevant documents; we also show that making such an assumption leads to different outcomes for both NDCG and average precision (AP) compared to when no such assumption is made."
1136,,ComClus - A Self-Grouping Framework for Multi-Network Clustering.,"Jingchao Ni,Wei Cheng 0002,Wei Fan,Xiang Zhang 0001",https://doi.org/10.1109/TKDE.2017.2771762,TKDE,2018,"Knowledge engineering,Clustering methods,Clustering algorithms,Electronic mail,Tensile stress,Algorithm design and analysis,Nickel","Joint clustering of multiple networks has been shown to be more accurate than performing clustering on individual networks separately. This is because multi-network clustering algorithms typically assume there is a common clustering structure shared by all networks, and different networks can provide compatible and complementary information for uncovering this underlying clustering structure. However, this assumption is too strict to hold in many emerging applications, where multiple networks usually have diverse data distributions. More popularly, the networks in consideration belong to different underlying groups. Only networks in the same underlying group share similar clustering structures. Better clustering performance can be achieved by considering such groups differently. As a result, an ideal method should be able to automatically detect network groups so that networks in the same group share a common clustering structure. To address this problem, we propose a new method, COMCLUS, to simultaneously group and cluster multiple networks. COMCLUS is novel in combining the clustering approach of non-negative matrix factorization (NMF) and the feature subspace learning approach of metric learning. Specifically, it treats node clusters as features of networks and learns proper subspaces from such features to differentiate different network groups. During the learning process, the two procedures of network grouping and clustering are coupled and mutually enhanced. Moreover, COMCLUS can effectively leverage prior knowledge on how to group networks such that network grouping can be conducted in a semi-supervised manner. This will enable users to guide the grouping process using domain knowledge so that network clustering accuracy can be further boosted. Extensive experimental evaluations on a variety of synthetic and real datasets demonstrate the effectiveness and scalability of the proposed method."
1137,,Uncertain Graph Sparsification.,"Panos Parchas,Nikolaos Papailiou,Dimitris Papadias,Francesco Bonchi",https://doi.org/10.1109/TKDE.2018.2819651,TKDE,2018,"Probabilistic computing,Task analysis,Social network services,Query processing,Uncertainty,Mathematics","Uncertain graphs are prevalent in several applications including communications systems, biological databases, and social networks. The ever increasing size of the underlying data renders both graph storage and query processing extremely expensive. Sparsification has often been used to reduce the size of deterministic graphs by maintaining only the important edges. However, adaptation of deterministic sparsification methods fails in the uncertain setting. To overcome this problem, we introduce the first sparsification techniques aimed explicitly at uncertain graphs. The proposed methods reduce the number of edges and redistribute their probabilities in order to decrease the graph size, while preserving its underlying structure. The resulting graph can be used to efficiently and accurately approximate any query and mining tasks on the original graph. An extensive experimental evaluation with real and synthetic datasets illustrates the effectiveness of our techniques on several common graph tasks, including clustering coefficient, page rank, reliability, and shortest path distance."
1138,,EMOMA - Exact Match in One Memory Access.,"Salvatore Pontarelli,Pedro Reviriego,Michael Mitzenmacher",https://doi.org/10.1109/TKDE.2018.2818716,TKDE,2018,"Memory management,Random access memory,Hardware,Data structures,System-on-chip,Field programmable gate arrays,Performance evaluation","An important function in modern routers and switches is to perform a lookup for a key. Hash-based methods, and in particular cuckoo hash tables, are popular for such lookup operations, but for large structures stored in off-chip memory, such methods have the downside that they may require more than one off-chip memory access to perform the key lookup. Although the number of off-chip memory accesses can be reduced using on-chip approximate membership structures such as Bloom filters, some lookups may still require more than one off-chip memory access. This can be problematic for some hardware implementations, as having only a single off-chip memory access enables a predictable processing of lookups and avoids the need to queue pending requests. We provide a data structure for hash-based lookups based on cuckoo hashing that uses only one off-chip memory access per lookup, by utilizing an on-chip pre-filter to determine which of multiple locations holds a key. We make particular use of the flexibility to move elements within a cuckoo hash table to ensure the pre-filter always gives the correct response. While this requires a slightly more complex insertion procedure and some additional memory accesses during insertions, it is suitable for most packet processing applications where key lookups are much more frequent than insertions. An important feature of our approach is its simplicity. Our approach is based on simple logic that can be easily implemented in hardware, and hardware implementations would benefit most from the single off-chip memory access per lookup."
1139,,Dynamic Data Exchange in Distributed RDF Stores.,"Anthony Potter,Boris Motik,Yavor Nenov,Ian Horrocks",https://doi.org/10.1109/TKDE.2018.2818696,TKDE,2018,"Servers,Resource description framework,Query processing,Heuristic algorithms,Partitioning algorithms,Query processing","When RDF datasets become too large to be managed by centralised systems, they are often distributed in a cluster of shared-nothing servers, and queries are answered using a distributed join algorithm. Although such solutions have been extensively studied in relational and RDF databases, we argue that existing approaches exhibit two drawbacks. First, they usually decide statically(i.e., at query compile time) how to shuffle the data, which can lead to missed opportunities for local computation. Second, they often materialise large intermediate relations whose size is determined by the entire dataset (and not the data stored in each server), so these relations can easily exceed the memory of individual servers. As a possible remedy, we present a novel distributed join algorithm for RDF. Our approach decides when to shuffle data dynamically, which ensures that query answers that can be wholly produced within a server involve only local computation. It also uses a novel flow control mechanism to ensure that every query can be answered even if each server has a bounded amount of memory that is much smaller than the intermediate relations. We complement our algorithm with a new query planning approach that balances the cost of communication against the cost of local processing at each server. Moreover, as in several existing approaches, we distribute RDF data using graph partitioning so as to maximise local computation, but we refine the partitioning algorithm to produce more balanced partitions. We show empirically that our techniques can outperform the state of the art by orders of magnitude in terms of query evaluation times, network communication, and memory use. In particular, bounding the memory use in individual servers can mean the difference between success and failure for answering queries with large answer sets."
1140,,"Deep Air Learning - Interpolation, Prediction, and Feature Analysis of Fine-Grained Air Quality.","Zhongang Qi,Tianchun Wang,Guojie Song,Weisong Hu,Xi Li 0001,Zhongfei Zhang",https://doi.org/10.1109/TKDE.2018.2823740,TKDE,2018,"Atmospheric modeling,Feature extraction,Semisupervised learning,Predictive models,Deep learning,Analytical models","The interpolation, prediction, and feature analysis of fine-gained air quality are three important topics in the area of urban air computing. The solutions to these topics can provide extremely useful information to support air pollution control, and consequently generate great societal and technical impacts. Most of the existing work solves the three problems separately by different models. In this paper, we propose a general and effective approach to solve the three problems in one model called the Deep Air Learning (DAL). The main idea of DAL lies in embedding feature selection and semi-supervised learning in different layers of the deep learning network. The proposed approach utilizes the information pertaining to the unlabeled spatio-temporal data to improve the performance of the interpolation and the prediction, and performs feature selection and association analysis to reveal the main relevant features to the variation of the air quality. We evaluate our approach with extensive experiments based on real data sources obtained in Beijing, China. Experiments show that DAL is superior to the peer models from the recent literature when solving the topics of interpolation, prediction, and feature analysis of fine-gained air quality."
1141,,A Fast Parallel Community Discovery Model on Complex Networks Through Approximate Optimization.,"Shaojie Qiao,Nan Han,Yunjun Gao,Rong-Hua Li,Jianbin Huang,Jun Guo,Louis Alberto Gutierrez,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2018.2803818,TKDE,2018,"Complex networks,Optimization,Approximation algorithms,Clustering algorithms,Computational modeling,Terrain factors,Detection algorithms","Community discovery plays an essential role in the analysis of the structural features of complex networks. Since online networks grow increasingly large and complex over time, the methods traditionally used for community discovery cannot efficiently handle large-scale network data. This introduces the important problem of how to effectively and efficiently discover large communities from complex networks. In this study, we propose a fast parallel community discovery model called picaso (a parallel community discovery algorithm based on approximate optimization), which integrates two new techniques: (1) Mountain model, which works by utilizing graph theory to approximate the selection of nodes needed for merging, and (2) Landslide algorithm, which is used to update the modularity increment based on the approximated optimization. In addition, the GraphX distribution computing framework is employed in order to achieve parallel community detection over complex networks. In the proposed model, clustering on modularity is used to initialize the Mountain model as well as to compute the weight of each edge in the networks. The relationships among the communities are then simplified by applying the Landslide algorithm, which allows us to obtain the community structures of the complex networks. Extensive experiments were conducted on real and synthetic complex network datasets, and the results demonstrate that the proposed algorithm can outperform the state of the art methods, in effectiveness and efficiency, when working to solve the problem of community detection. Moreover, we demonstratively prove that overall time performance approximates to four times faster than similar approaches. Effectively our results suggest a new paradigm for large-scale community discovery of complex networks."
1142,,A Location-Query-Browse Graph for Contextual Recommendation.,"Yongli Ren,Martin Tomko,Flora Dilys Salim,Jeffrey Chan,Charles L. A. Clarke,Mark Sanderson",https://doi.org/10.1109/TKDE.2017.2766059,TKDE,2018,"Mobile communication,Search engines,Web search,Context modeling,Mobile handsets,Bipartite graph,Electronic mail","Traditionally, recommender systems modelled the physical and cyber contextual influence on people's moving, querying, and browsing behaviors in isolation. Yet, searching, querying, and moving behaviors are intricately linked, especially indoors. Here, we introduce a tripartite location-query-browse graph (LQB) for nuanced contextual recommendations. The LQB graph consists of three kinds of nodes: locations, queries, and Web domains. Directed connections only between heterogeneous nodes represent the contextual influences, while connections of homogeneous nodes are inferred from the contextual influences of the other nodes. This tripartite LQB graph is more reliable than any monopartite or bipartite graph in contextual location, query, and Web content recommendations. We validate this LQB graph in an indoor retail scenario with extensive dataset of three logs collected from over 120,000 anonymized, opt-in users over a 1-year period in a large inner-city mall in Sydney, Australia. We characterize the contextual influences that correspond to the arcs in the LQB graph, and evaluate the usefulness of the LQB graph for location, query, and Web content recommendations. The experimental results show that the LQB graph successfully captures the contextual influence and significantly outperforms the state of the art in these applications."
1143,,Bidding Machine - Learning to Bid for Directly Optimizing Profits in Display Advertising.,"Kan Ren,Weinan Zhang 0001,Ke Chang,Yifei Rong,Yong Yu 0001,Jun Wang 0012",https://doi.org/10.1109/TKDE.2017.2775228,TKDE,2018,"Estimation,Optimization,Forecasting,Advertising,Predictive models,Real-time systems,Regression tree analysis","Real-time bidding (RTB) based display advertising has become one of the key technological advances in computational advertising. RTB enables advertisers to buy individual ad impressions via an auction in real-time and facilitates the evaluation and the bidding of individual impressions across multiple advertisers. In RTB, the advertisers face three main challenges when optimizing their bidding strategies, namely (i) estimating the utility (e.g., conversions, clicks) of the ad impression, (ii) forecasting the market value (thus the cost) of the given ad impression, and (iii) deciding the optimal bid for the given auction based on the first two. Previous solutions assume the first two are solved before addressing the bid optimization problem. However, these challenges are strongly correlated and dealing with any individual problem independently may not be globally optimal. In this paper, we propose Bidding Machine, a comprehensive learning to bid framework, which consists of three optimizers dealing with each challenge above, and as a whole, jointly optimizes these three parts. We show that such a joint optimization would largely increase the campaign effectiveness and the profit. From the learning perspective, we show that the bidding machine can be updated smoothly with both offline periodical batch or online sequential training schemes. Our extensive offline empirical study and online A/B testing verify the high effectiveness of the proposed bidding machine."
1144,,Efficient Detection of Overlapping Communities Using Asymmetric Triangle Cuts.,"Mojtaba Rezvani,Weifa Liang,Chengfei Liu,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2018.2815554,TKDE,2018,"Measurement,Social network services,Image edge detection,Big Data,Clustering algorithms,Computer science,Australia","Real social networks contain many communities, where members within each community are densely connected with each other, while they are sparsely connected with the members outside of the community. Since each member can join multiple communities simultaneously, communities in social networks are usually overlapping with each other. How to efficiently and effectively identify overlapping communities in a large social network becomes a fundamental problem in the big data era. Most existing studies on community finding focused on non-overlapping communities based on several well-known community fitness metrics. However, recent investigations have shown that these fitness metrics may suffer free rider and separation effects where the overlapping region of two communities always belongs to the denser one, rather to both of them. In this paper, we study the overlapping community detection problem in social networks that not only takes the quality of the found overlapping communities but also incorporate both free rider and separation effects on the found communities into consideration. Specifically, in this paper, we first propose a novel community fitness metric - triangle based fitness metric, for overlapping community detection that can minimize the free rider and separation effects on found overlapping communities, and show that the problem is NP-hard. We then propose an efficient yet scalable algorithm for the problem that can deliver a feasible solution. We finally validate the effectiveness of the proposed fitness metric and evaluate the performance of the proposed algorithm, through conducting extensive experiments on real-world datasets with over 100 million vertices and edges. Experimental results demonstrate that the proposed algorithm is very promising."
1145,,Sampling and Reconstruction Using Bloom Filters.,"Neha Sengupta,Amitabha Bagchi,Srikanta Bedathur,Maya Ramanath",https://doi.org/10.1109/TKDE.2017.2785803,TKDE,2018,"Arrays,Indexes,Twitter,Electronic mail,Dictionaries","In this paper, we address the problem of sampling from a set and reconstructing a set stored as a Bloom filter. To the best of our knowledge our work is the first to address this question. We introduce a novel hierarchical data structure called BloomSampleTree that helps us design efficient algorithms to extract an almost uniform sample from the set stored in a Bloom filter and also allows us to reconstruct the set efficiently. In the case where the hash functions used in the Bloom filter implementation are partially invertible, in the sense that it is easy to calculate the set of elements that map to a particular hash value, we propose a second, more space-efficient method called HashInvert for the reconstruction. We study the properties of these two methods both analytically as well as experimentally. We provide bounds on run times for both methods and sample quality for the BloomSampleTree based algorithm, and show through an extensive experimental evaluation that our methods are efficient and effective."
1146,,Ensemble Learning for Multi-Type Classification in Heterogeneous Networks.,"Francesco Serafino 0002,Gianvito Pio,Michelangelo Ceci",https://doi.org/10.1109/TKDE.2018.2822307,TKDE,2018,"Task analysis,Heterogeneous networks,Databases,Learning systems","Heterogeneous networks are networks consisting of different types of objects and links. They can be found in several fields, ranging from the Internet to social sciences, biology, epidemiology, geography, finance, and many others. In the literature, several methods have been proposed for the analysis of network data, but they usually focus on homogeneous networks, where all the objects are of the same type, and links among them describe a single type of relationship. More recently, the complexity of real scenarios has impelled researchers to design methods for the analysis of heterogeneous networks, especially focused on classification and clustering tasks. However, they often make assumptions on the structure of the network that are too restrictive or do not fully exploit different forms of network correlation and autocorrelation. Moreover, when nodes which are the main subject of the classification task are linked to several nodes of the network having missing values, standard methods can lead to either building incomplete classification models or to discarding possibly relevant dependencies (correlation or autocorrelation). In this paper, we propose an ensemble learning approach for multi-type classification. We adopt the system Mr-SBC, which is originally able to analyze heterogeneous networks of arbitrary structure, within an ensemble learning approach. The ensemble allows us to improve the classification accuracy of Mr-SBC by exploiting i) the possible presence of correlation and autocorrelation phenomena and ii) the classification of instances (which contain missing values) of other node types in the network. As a beneficial side effect, we have also that the models are more stable in terms of standard deviation of the accuracy, over different samples used for training. Experiments performed on real-world datasets show that the proposed method is able to significantly outperform the standard implementation of Mr-SBC. Moreover, it gives Mr-SBC the advantage of outperforming four other well-known algorithms for the classification of data organized in a network."
1147,,DPPred - An Effective Prediction Framework with Concise Discriminative Patterns.,"Jingbo Shang,Meng Jiang 0001,Wenzhu Tong,Jinfeng Xiao,Jian Peng 0001,Jiawei Han 0001",https://doi.org/10.1109/TKDE.2017.2757476,TKDE,2018,"Data models,Predictive models,Vegetation,Numerical models,Computational modeling,Robustness","In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical, and high dimensional features into a comprehensive structure with rich interpretable information in the data. In this paper, we propose a novel Discriminative Pattern-based Prediction framework (DPPred) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability. Specifically, DPPred adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models. DPPred selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models. Extensive experiments show that in many scenarios, DPPred provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts. In particular, taking a clinical application dataset as a case study, our DPPred outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns."
1148,,Automated Phrase Mining from Massive Text Corpora.,"Jingbo Shang,Jialu Liu,Meng Jiang 0001,Xiang Ren 0001,Clare R. Voss,Jiawei Han 0001",https://doi.org/10.1109/TKDE.2018.2812203,TKDE,2018,"Knowledge based systems,Pragmatics,Labeling,Encyclopedias,Electronic publishing,Internet","As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus and has various downstream applications including information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. None of the state-of-the-art models, even data-driven models, is fully automated because they require human experts for designing rules or labeling phrases. In this paper, we propose a novel framework for automated phrase mining, AutoPhrase, which supports any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, AutoPhrase has shown significant improvements in both effectiveness and efficiency on five real-world datasets across different domains and languages. Besides, AutoPhrase can be extended to model single-word quality phrases."
1149,,Robust Prototype-Based Learning on Data Streams.,"Junming Shao,Feng Huang,Qinli Yang,Guangchun Luo",https://doi.org/10.1109/TKDE.2017.2772239,TKDE,2018,"Prototypes,Data models,Adaptation models,Distributed databases,Heuristic algorithms,Maintenance engineering,Data mining","In this paper, we propose a prototype-based classification model for evolving data streams, called SyncStream, which allows dynamically modeling time-changing concepts, making predictions in a local fashion. Instead of learning a single model on a fixed or adaptive sliding window of historical data or ensemble learning a set of weighted base classifiers, SyncStream captures evolving concepts by dynamically maintaining a set of prototypes in a proposed P-Tree, which are obtained based on the error-driven representativeness learning and synchronization-inspired constrained clustering. To identify abrupt concept drifts in data streams, PCA and statistical analysis based heuristic approaches have been introduced. To further learn the associations among distributed data streams, the extended P-Tree structure and KNN-style strategy are introduced. We demonstrate that our new data stream classification approach has several attractive benefits: (a) SyncStream is capable of dynamically modeling the evolving concepts from even a small set of prototypes. (b) Owing to synchronization-based constrained clustering and P-Tree, SyncStream supports efficient and effective data representation and maintenance. (c) SyncStream is also tolerant of inappropriate or noisy examples via error-driven representativeness learning. (d) SyncStream allows learning relationship among distributed data streams at the instance level. The experimental results indicate its efficiency and effectiveness."
1150,,NHAD - Neuro-Fuzzy Based Horizontal Anomaly Detection in Online Social Networks.,"Vishal Sharma 0001,Ravinder Kumar 0002,Wen-Huang Cheng,Mohammed Atiquzzaman,Kathiravan Srinivasan,Albert Y. Zomaya",https://doi.org/10.1109/TKDE.2018.2818163,TKDE,2018,"Social network services,Anomaly detection,Real-time systems,Electronic mail,Support vector machines,Benchmark testing,Computer science","Use of social network is the basic functionality of today's life. With the advent of more and more online social media, the information available and its utilization have come under the threat of several anomalies. Anomalies are the major cause of online frauds which allow information access by unauthorized users as well as information forging. One of the anomalies that act as a silent attacker is the horizontal anomaly. These are the anomalies caused by a user because of his/her variable behavior towards different sources. Horizontal anomalies are difficult to detect and hazardous for any network. In this paper, a self-healing neuro-fuzzy approach (NHAD) is used for the detection, recovery, and removal of horizontal anomalies efficiently and accurately. The proposed approach operates over the five paradigms, namely, missing links, reputation gain, significant difference, trust properties, and trust score. The proposed approach is evaluated with three datasets: DARPA'98 benchmark dataset, synthetic dataset, and real-time traffic. Results show that the accuracy of the proposed NHAD model for 10 to 30 percent anomalies in synthetic dataset ranges between 98.08 and 99.88 percent. The evaluation over DARPA'98 dataset demonstrates that the proposed approach is better than the existing solutions as it provides 99.97 percent detection rate for anomalous class. For real-time traffic, the proposed NHAD model operates with an average accuracy of 99.42 at 99.90 percent detection rate."
1151,,SHINE+ - A General Framework for Domain-Specific Entity Linking with Heterogeneous Information Networks.,"Wei Shen 0004,Jiawei Han 0001,Jianyong Wang 0001,Xiaojie Yuan,Zhenglu Yang",https://doi.org/10.1109/TKDE.2017.2730862,TKDE,2018,"Joining processes,Encyclopedias,Electronic publishing,Internet,Knowledge engineering,Sociology","Heterogeneous information networks that consist of multi-type, interconnected objects are becoming increasingly popular, such as social media networks and bibliographic networks. The task of linking named entity mentions detected from unstructured Web text with their corresponding entities in a heterogeneous information network is of practical importance for the problem of information network population. This task is challenging due to name ambiguity and limited knowledge existing in the network. Most existing entity linking methods focus on linking entities with Wikipedia and cannot be applied to our task. In this paper, we present SHINE+, a general framework for linking named entitieS in Web free text with a Heterogeneous I nformation NEtwork. We propose a probabilistic linking model, which unifies an entity popularity model with an entity object model. As the entity knowledge contained in the information network is insufficient, we propose a knowledge population algorithm to iteratively enrich the network entity knowledge by leveraging the context information of mentions mapped by the linking model with high confidence, which subsequently boosts the linking performance. Experimental results over two real heterogeneous information networks (i.e., DBLP and IMDb) demonstrate the effectiveness and efficiency of our proposed framework in comparison with the baselines."
1152,,Frog - Asynchronous Graph Processing on GPU with Hybrid Coloring Model.,"Xuanhua Shi,Xuan Luo,Junling Liang,Peng Zhao,Sheng Di,Bingsheng He,Hai Jin 0001",https://doi.org/10.1109/TKDE.2017.2745562,TKDE,2018,"Graphics processing units,Image color analysis,Computational modeling,Parallel processing,Kernel,Data transfer,Acceleration","GPUs have been increasingly used to accelerate graph processing for complicated computational problems regarding graph theory. Many parallel graph algorithms adopt the asynchronous computing model to accelerate the iterative convergence. Unfortunately, the consistent asynchronous computing requires locking or atomic operations, leading to significant penalties/overheads when implemented on GPUs. As such, the coloring algorithm is adopted to separate the vertices with potential updating conflicts, guaranteeing the consistency/correctness of the parallel processing. Common coloring algorithms, however, may suffer from low parallelism because of a large number of colors generally required for processing a large-scale graph with billions of vertices. We propose a light-weight asynchronous processing framework called Frog with a preprocessing/hybrid coloring model. The fundamental idea is based on the Pareto principle (or 80-20 rule) about coloring algorithms as we observed through masses of real-world graph coloring cases. We find that a majority of vertices (about 80 percent) are colored with only a few colors, such that they can be read and updated in a very high degree of parallelism without violating the sequential consistency. Accordingly, our solution separates the processing of the vertices based on the distribution of colors. In this work, we mainly answer three questions: (1) how to partition the vertices in a sparse graph with maximized parallelism, (2) how to process large-scale graphs that cannot fit into GPU memory, and (3) how to reduce the overhead of data transfers on PCIe while processing each partition. We conduct experiments on real-world data (Amazon, DBLP, YouTube, RoadNet-CA, WikiTalk, and Twitter) to evaluate our approach and make comparisons with well-known non-preprocessed (such as Totem, Medusa, MapGraph, and Gunrock) and preprocessed (Cusha) approaches, by testing four classical algorithms (BFS, PageRank, SSSP, and CC). On all the tested applications and datasets, Frog is able to significantly outperform existing GPU-based graph processing systems except Gunrock and MapGraph. MapGraph gets better performance than Frog when running BFS on RoadNet-CA. The comparison between Gunrock and Frog is inconclusive. Frog can outperform Gunrock more than 1.04X when running PageRank and SSSP, while the advantage of Frog is not obvious when running BFS and CC on some datasets especially for RoadNet-CA."
1153,,Hashtagger+ - Efficient High-Coverage Social Tagging of Streaming News.,"Bichen Shi,Gevorg Poghosyan,Georgiana Ifrim,Neil Hurley",https://doi.org/10.1109/TKDE.2017.2754253,TKDE,2018,"Twitter,Tagging,Data models,Real-time systems,Adaptation models,Computational modeling,Algorithm design and analysis","News and social media now play a synergistic role and neither domain can be grasped in isolation. On one hand, platforms such as Twitter have taken a central role in the dissemination and consumption of news. On the other hand, news editors rely on social media for following their audience's attention and for crowd-sourcing news stories. Twitter hashtags function as a key connection between Twitter crowds and the news media, by naturally naming and contextualizing stories, grouping the discussion of news and marking topic trends. In this work, we propose Hashtagger+, an efficient learning-to-rank framework for merging news and social streams in real-time, by recommending Twitter hashtags to news articles. We provide an extensive study of different approaches for streaming hashtag recommendation, and show that pointwise learning-to-rank is more effective than multi-class classification as well as more complex learning-to-rank approaches. We improve the efficiency and coverage of a state-of-the-art hashtag recommendation model by proposing new techniques for data collection and feature computation. In our comprehensive evaluation on real-data, we show that we drastically outperform the accuracy and efficiency of prior methods. Our prototype system delivers recommendations in under 1 minute, with a Precision@1 of 94 percent and article coverage of 80 percent. This is an order of magnitude faster than prior approaches, and brings improvements of 5 percent in precision and 20 percent in coverage. By effectively linking the news stream to the social stream via the recommended hashtags, we open the door to solving many challenging problems related to story detection and tracking. To showcase this potential, we present an application of our recommendations to automated news story tracking via social tags. Our recommendation framework is implemented in a real-time Web system available from insight4news.ucd.ie."
1154,1,Privacy Enhanced Matrix Factorization for Recommendation with Local Differential Privacy.,"Hyejin Shin,Sungwook Kim,Junbum Shin,Xiaokui Xiao",https://doi.org/10.1109/TKDE.2018.2805356,TKDE,2018,"Privacy,Recommender systems,Data privacy,Perturbation methods,Dimensionality reduction,Motion pictures,Servers","Recommender systems are collecting and analyzing user data to provide better user experience. However, several privacy concerns have been raised when a recommender knows user's set of items or their ratings. A number of solutions have been suggested to improve privacy of legacy recommender systems, but the existing solutions in the literature can protect either items or ratings only. In this paper, we propose a recommender system that protects both user's items and ratings. For this, we develop novel matrix factorization algorithms under local differential privacy (LDP). In a recommender system with LDP, individual users randomize their data themselves to satisfy differential privacy and send the perturbed data to the recommender. Then, the recommender computes aggregates of the perturbed data. This framework ensures that both user's items and ratings remain private from the recommender. However, applying LDP to matrix factorization typically raises utility issues with i) high dimensionality due to a large number of items and ii) iterative estimation algorithms. To tackle these technical challenges, we adopt dimensionality reduction technique and a novel binary mechanism based on sampling. We additionally introduce a factor that stabilizes the perturbed gradients. With MovieLens and LibimSeTi datasets, we evaluate recommendation accuracy of our recommender system and demonstrate that our algorithm performs better than the existing differentially private gradient descent algorithm for matrix factorization under stronger privacy requirements."
1155,,A Comprehensive Study on Social Network Mental Disorders Detection via Online Social Media Mining.,"Hong-Han Shuai,Chih-Ya Shen,De-Nian Yang,Yi-Feng Lan,Wang-Chien Lee,Philip S. Yu,Ming-Syan Chen",https://doi.org/10.1109/TKDE.2017.2786695,TKDE,2018,"Social network services,Feature extraction,Mental disorders,Psychology,Data mining,Tensile stress,Internet","The explosive growth in popularity of social networking leads to the problematic usage. An increasing number of social network mental disorders (SNMDs), such as Cyber-Relationship Addiction, Information Overload, and Net Compulsion, have been recently noted. Symptoms of these mental disorders are usually observed passively today, resulting in delayed clinical intervention. In this paper, we argue that mining online social behavior provides an opportunity to actively identify SNMDs at an early stage. It is challenging to detect SNMDs because the mental status cannot be directly observed from online social activity logs. Our approach, new and innovative to the practice of SNMD detection, does not rely on self-revealing of those mental factors via questionnaires in Psychology. Instead, we propose a machine learning framework, namely, Social Network Mental Disorder Detection (SNMDD), that exploits features extracted from social network data to accurately identify potential cases of SNMDs. We also exploit multi-source learning in SNMDD and propose a new SNMD-based Tensor Model (STM) to improve the accuracy. To increase the scalability of STM, we further improve the efficiency with performance guarantee. Our framework is evaluated via a user study with 3,126 online social network users. We conduct a feature analysis, and also apply SNMDD on large-scale datasets and analyze the characteristics of the three SNMD types. The results manifest that SNMDD is promising for identifying online social network users with potential SNMDs."
1156,,Health Monitoring on Social Media over Time.,"Sumit Sidana,Sihem Amer-Yahia,Marianne Clausel,Majdeddine Rebai,Son T. Mai,Massih-Reza Amini",https://doi.org/10.1109/TKDE.2018.2795606,TKDE,2018,"Monitoring,Twitter,Data models,Random variables,Sociology,Statistics","Social media has become a major source for analyzing all aspects of daily life. Thanks to dedicated latent topic analysis methods such as the Ailment Topic Aspect Model (ATAM), public health can now be observed on Twitter. In this work, we are interested in using social media to monitor people's health overtime. The use of tweets has several benefits including instantaneous data availability at virtually no cost. Early monitoring of health data is complementary to post-factum studies and enables a range of applications such as measuring behavioral risk factors and triggering health campaigns. We formulate two problems: health transition detection and health transition prediction. We first propose the Temporal Ailment Topic Aspect Model (TM-ATAM), a new latent model dedicated to solving the first problem by capturing transitions that involve health-related topics. TM-ATAM is a non-obvious extension to ATAM that was designed to extract health-related topics. It learns health-related topic transitions by minimizing the prediction error on topic distributions between consecutive posts at different time and geographic granularities. To solve the second problem, we develop T-ATAM, a Temporal Ailment Topic Aspect Model where time is treated as a random variable natively inside ATAM. Our experiments on an 8-month corpus of tweets show that TM-ATAM outperforms TM-LDA in estimating health-related transitions from tweets for different geographic populations. We examine the ability of TM-ATAM to detect transitions due to climate conditions in different geographic regions. We then show how T-ATAM can be used to predict the most important transition and additionally compare T-ATAM with CDC (Center for Disease Control) data and Google Flu Trends."
1157,,UniWalk - Unidirectional Random Walk Based Scalable SimRank Computation over Large Graph.,"Junshuai Song,Xiongcai Luo,Jun Gao 0003,Chang Zhou,Hu Wei,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2017.2779126,TKDE,2018,"Monte Carlo methods,Scalability,Optimization,Indexes,Face,Maintenance engineering,Electronic mail","SimRank is an important measure of vertex-pair similarity according to the structure of graphs. Although progress has been achieved, existing methods still face challenges to handle large graphs. Besides huge index construction and maintenance cost, existing methods may require considerable search space and time overheads in the online SimRank query. In this paper, we design a Monte Carlo based method, UniWalk, to enable the fast top-k SimRank computation over large undirected graphs. UniWalk directly locates the top-k similar vertices for any single source vertex u via R sampling paths originating from u, which avoids selecting candidate vertex set C and the following O(1C1R) bidirectional sampling paths. We also devise a path enumeration strategy to improve the SimRank precision by using path probabilities instead of path frequencies when sampling, a space-efficient method to reduce intermediate results, and a path-sharing strategy to lower the redundant path sampling cost for multiple source vertices. Furthermore, we extend UniWalk to existing distributed graph processing frameworks to improve its scalability. We conduct extensive experiments to illustrate that UniWalk has high scalability, and outperforms the state-of-the-art methods by orders of magnitude."
1158,,Correction to &quot;K Nearest Neighbour Joins for Big Data on MapReduce - A Theoretical and Experimental Analysis&quot;.,"Ge Song 0001,Justine Rochas,Lea El Beze,Fabrice Huet,Frédéric Magoulès",https://doi.org/10.1109/TKDE.2017.2748438,TKDE,2018,"Big Data,Nearest  neighbor methods","Presents corrections to the paper, “K nearest neighbour joins for big data on MapReduce: A theoretical and experimental analysis,” (Song, G., et al), IEEE Trans. Knowl. Data Eng., vol. 28, no. 9, pp. 2376–2392, Sep. 2016. "
1159,,Mining Summaries for Knowledge Graph Search.,"Qi Song,Yinghui Wu,Peng Lin,Xin Dong 0001,Hui Sun",https://doi.org/10.1109/TKDE.2018.2807442,TKDE,2018,"Parallel algorithms,Motion pictures,Scalability,Data mining,Query processing,Knowledge based systems,Time factors","Querying heterogeneous and large-scale knowledge graphs is expensive. This paper studies a graph summarization framework to facilitate knowledge graph search. (1) We introduce a class of reduced summaries. Characterized by approximate graph pattern matching, these summaries are capable of summarizing entities in terms of their neighborhood similarity up to a certain hop, using small and informative graph patterns. (2) We study a diversified graph summarization problem. Given a knowledge graph, it is to discover top-k summaries that maximize a bi-criteria function, characterized by both informativeness and diversity. We show that diversified summarization is feasible for large graphs, by developing both sequential and parallel summarization algorithms. (a) We show that there exists a 2-approximation algorithm to discover diversified summaries. We further develop an anytime sequential algorithm which discovers summaries under resource constraints. (b) We present a new parallel algorithm with quality guarantees. The algorithm is parallel scalable, which ensures its feasibility in distributed graphs. (3) We also develop a summary-based query evaluation scheme, which only refers to a small number of summaries. Using real-world knowledge graphs, we experimentally verify the effectiveness and efficiency of our summarization algorithms, and query processing using summaries."
1160,,An Efficient Ride-Sharing Framework for Maximizing Shared Route.,"Na Ta 0001,Guoliang Li 0001,Tianyu Zhao,Jianhua Feng,Hanchao Ma,Zhiguo Gong",https://doi.org/10.1109/TKDE.2017.2760880,TKDE,2018,"Vehicles,Roads,Upper bound,Approximation algorithms,Position measurement,Computational modeling,Resource management","Ride-sharing (RS) has great values in saving energy and alleviating traffic pressure. Existing studies can be improved for better efficiency. Therefore, we propose a new ride-sharing model, where each driver has a requirement that if the driver shares a ride with a rider, the shared route percentage (i.e., the ratio of the shared route's distance to the driver's total travel distance) exceeds an expectation rate of the driver, e.g., 0.8. We consider two variants of this problem. The first considers multiple drivers and multiple riders and aims to compute driver-rider pairs to maximize the overall shared route percentage (SRP). We model this problem as the maximum weighted bigraph matching problem, where the vertices are drivers and riders, edges are driver-rider pairs, and edge weights are driver-rider's SRP. However, it is rather expensive to compute the SRP values for large numbers of driver-rider pairs on road networks. To address this problem, we propose an efficient method to prune many unnecessary driver-rider pairs and avoid computing the SRP values for every pair. To improve the efficiency, we propose an approximate method with error bound guarantee. The basic idea is that we compute an upper bound and a lower bound for each driver-rider pair in constant time. Then, we estimate an upper bound and a lower bound of the graph matching. Next, we select some driver-rider pairs, compute their real shortest-route distance, and update the lower and upper bounds of the maximum graph matching. We repeat above steps until the ratio of the upper bound to the lower bound is not larger than a given approximate rate. The second considers multiple drivers and a single rider and aims to find the top-
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 drivers for the rider with the largest SRP. We first prune a large number of drivers that cannot meet the SRP requirements. Then, we propose a best-first algorithm that progressively selects the drivers with high probability to be in the top-
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula>
 results and prunes the drivers that cannot be in the top-
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math></inline-formula>
 results. Extensive experiments on real-world datasets demonstrate the superiority of our method."
1161,,Profit Maximization for Viral Marketing in Online Social Networks - Algorithms and Analysis.,"Jing Tang 0004,Xueyan Tang,Junsong Yuan",https://doi.org/10.1109/TKDE.2017.2787757,TKDE,2018,"Approximation algorithms,Measurement,Greedy algorithms,Social network services,Integrated circuit modeling,Upper bound","Information can be disseminated widely and rapidly through Online Social Networks (OSNs) with “word-of-mouth” effects. Viral marketing is such a typical application in which new products or commercial activities are advertised by some seed users in OSNs to other users in a cascading manner. The selection of initial seed users yields a tradeoff between the expense and reward of viral marketing. In this paper, we define a general profit metric that naturally combines the benefit of influence spread with the cost of seed selection in viral marketing. We carry out a comprehensive study on finding a set of seed nodes to maximize the profit of viral marketing. We show that the profit metric is significantly different from the influence metric in that it is no longer monotone. This characteristic differentiates the profit maximization problem from the traditional influence maximization problem. We develop new seed selection algorithms for profit maximization with strong approximation guarantees. We also derive several upper bounds to benchmark the practical performance of an algorithm on any specific problem instance. Experimental evaluations with real OSN datasets demonstrate the effectiveness of our algorithms and techniques."
1162,,Efficient Parallel Skyline Query Processing for High-Dimensional Data.,"MingJie Tang,Yongyang Yu,Walid G. Aref,Qutaibah M. Malluhi,Mourad Ouzzani",https://doi.org/10.1109/TKDE.2018.2809598,TKDE,2018,"Query processing,Distributed databases,Partitioning algorithms,Merging,Indexes,Computer science,Task analysis","Given a set of multidimensional data points, skyline queries retrieve those points that are not dominated by any other points in the set. Due to the ubiquitous se of skyline queries, such as in preference-based query answering and decision making, and the large amount of data that these queries have to deal with, enabling their scalable processing is of critical importance. However, there are several outstanding challenges that have not been well addressed. More specifically, in this paper, we are tackling the data straggler and data skew challenges introduced by distributed skyline query processing, as well as the ensuing high computation cost of merging skyline candidates. We thus introduce a new efficient three-phase approach for large scale processing of skyline queries. In the first preprocessing phase, the data is partitioned along the Z-order curve. We utilize a novel data partitioning approach that formulates data partitioning as an optimization problem to minimize the size of intermediate data. In the second phase, each compute node partitions the input data points into disjoint subsets, and then performs the skyline computation on each subset to produce skyline candidates in parallel. In the final phase, we build an index and employ an efficient algorithm to merge the generated skyline candidates. Extensive experiments demonstrate that the proposed skyline algorithm achieves more than one order of magnitude enhancement in performance compared to existing state-of-the-art approaches."
1163,,SLADE - A Smart Large-Scale Task Decomposer in Crowdsourcing.,"Yongxin Tong,Lei Chen 0002,Zimu Zhou,H. V. Jagadish,Lidan Shou,Weifeng Lv",https://doi.org/10.1109/TKDE.2018.2797962,TKDE,2018,"Task analysis,Crowdsourcing,Approximation algorithms,Electronic mail,Computer science,Reliability engineering","Crowdsourcing has been shown to be effective in a wide range of applications, and is seeing increasing use. A large-scale crowdsourcing task often consists of thousands or millions of atomic tasks, each of which is usually a simple task such as binary choice or simple voting. To distribute a large-scale crowdsourcing task to limited crowd workers, a common practice is to pack a set of atomic tasks into a task bin and send to a crowd worker in a batch. It is challenging to decompose a large-scale crowdsourcing task and execute batches of atomic tasks, which ensures reliable answers at a minimal total cost. Large batches lead to unreliable answers of atomic tasks, while small batches incur unnecessary cost. In this paper, we investigate a general crowdsourcing task decomposition problem, called the Smart Large-scAle task DEcomposer (SLADE) problem, which aims to decompose a large-scale crowdsourcing task to achieve the desired reliability at a minimal cost. We prove the NP-hardness of the SLADE problem and propose solutions in both homogeneous and heterogeneous scenarios. For the homogeneous SLADE problem, where all the atomic tasks share the same reliability requirement, we propose a greedy heuristic algorithm and an efficient and effective approximation framework using an optimal priority queue (OPQ) structure with provable approximation ratio. For the heterogeneous SLADE problem, where the atomic tasks can have different reliability requirements, we extend the OPQ-based framework leveraging a partition strategy, and also prove its approximation guarantee. Finally, we verify the effectiveness and efficiency of the proposed solutions through extensive experiments on representative crowdsourcing platforms."
1164,,Reverse k Nearest Neighbor Search over Trajectories.,"Sheng Wang 0007,Zhifeng Bao,J. Shane Culpepper,Timos Sellis,Gao Cong",https://doi.org/10.1109/TKDE.2017.2776268,TKDE,2018,"Trajectory,Planning,Public transportation,Indexes,Search problems,Vehicles","GPS enables mobile devices to continuously provide new opportunities to improve our daily lives. For example, the data collected in applications created by Uber or Public Transport Authorities can be used to plan transportation routes, estimate capacities, and proactively identify low coverage areas. In this paper, we study a new kind of query-Reverse k Nearest Neighbor Search over Trajectories (RkNNT), which can be used for route planning and capacity estimation. Given a set of existing routes D
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">R</sub>
, a set of passenger transitions D
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">T</sub>
, and a query route Q, an RkNNT query returns all transitions that take Q as one of its k nearest travel routes. To solve the problem, we first develop an index to handle dynamic trajectory updates, so that the most up-to-date transition data are available for answering an RkNNT query. Then we introduce a filter refinement framework for processing RkNNT queries using the proposed indexes. Next, we show how to use RkNNT to solve the optimal route planning problem MaxRkNNT (MinRkNNT), which is to search for the optimal route from a start location to an end location that could attract the maximum (or minimum) number of passengers based on a predefined travel distance threshold. Experiments on real datasets demonstrate the efficiency and scalability of our approaches. To the best of our knowledge, this is the first work to study the RkNNT problem for route planning."
1165,,Redundancy Reduction for Prevalent Co-Location Patterns.,"Lizhen Wang,Xuguang Bao,Lihua Zhou",https://doi.org/10.1109/TKDE.2017.2759110,TKDE,2018,"Redundancy,Data mining,Semantics,Measurement,Spatial databases,Indexes","Spatial co-location pattern mining is an interesting and important task in spatial data mining which discovers the subsets of spatial features frequently observed together in nearby geographic space. However, the traditional framework of mining prevalent colocation patterns produces numerous redundant co-location patterns, which makes it hard for users to understand or apply. To address this issue, in this paper, we study the problem of reducing redundancy in a collection of prevalent co-location patterns by utilizing the spatial distribution information of co-location instances. We first introduce the concept of semantic distance between a co-location pattern and its super-patterns, and then define redundant co-locations by introducing the concept of d-covered, where δ (0 ≤ δ ≤ 1) is a coverage measure. We develop two algorithms RRclosed and RRnull to perform the redundancy reduction for prevalent co-location patterns. The former adopts the post-mining framework that is commonly used by existing redundancy reduction techniques, while the latter employs the mine-and-reduce framework that pushes redundancy reduction into the co-location mining process. Our performance studies on the synthetic and real-world data sets demonstrate that our method effectively reduces the size of the original collection of closed co-location patterns by about 50 percent. Furthermore, the RRnull method runs much faster than the related closed co-location pattern mining algorithm."
1166,,TaxiRec - Recommending Road Clusters to Taxi Drivers Using Ranking-Based Extreme Learning Machines.,"Ran Wang 0001,Chi-Yin Chow,Yan Lyu,Victor C. S. Lee,Sam Kwong,Yanhua Li,Jia Zeng",https://doi.org/10.1109/TKDE.2017.2772907,TKDE,2018,"Public transportation,Roads,Vehicles,Recommender systems,Trajectory,Data models,Feature extraction","Utilizing large-scale GPS data to improve taxi services has become a popular research problem in the areas of data mining, intelligent transportation, geographical information systems, and the Internet of Things. In this paper, we utilize a large-scale GPS data set generated by over 7,000 taxis in a period of one month in Nanjing, China, and propose TaxiRec: a framework for evaluating and discovering the passenger-finding potentials of road clusters, which is incorporated into a recommender system for taxi drivers to seek passengers. In TaxiRec, the underlying road network is first segmented into a number of road clusters, a set of features for each road cluster is extracted from real-life data sets, and then a ranking-based extreme learning machine (ELM) model is proposed to evaluate the passenger-finding potential of each road cluster. In addition, TaxiRec can use this model with a training cluster selection algorithm to provide road cluster recommendations when taxi trajectory data is incomplete or unavailable. Experimental results demonstrate the feasibility and effectiveness of TaxiRec."
1167,,Privacy-Preserving Collaborative Model Learning - The Case of Word Vector Training.,"Qian Wang 0002,Minxin Du,Xiuying Chen,Yanjiao Chen,Pan Zhou,Xiaofeng Chen 0001,Xinyi Huang",https://doi.org/10.1109/TKDE.2018.2819673,TKDE,2018,"Encryption,Neural networks,Servers,Training,Privacy,Collaboration","Nowadays, machine learning is becoming a new paradigm for mining hidden knowledge in big data. The collection and manipulation of big data not only create considerable values, but also raise serious privacy concerns. To protect the huge amount of potentially sensitive data, a straightforward approach is to encrypt data with specialized cryptographic tools. However, it is challenging to utilize or operate on encrypted data, especially to perform machine learning algorithms. In this paper, we investigate the problem of training high quality word vectors over large-scale encrypted data (from distributed data owners) with the privacy-preserving collaborative neural network learning algorithms. We leverage and also design a suite of arithmetic primitives (e.g., multiplication, fixed-point representation, sigmoid function computation, etc.) on encrypted data, served as components of our construction. We theoretically analyze the security and efficiency of our proposed construction, and conduct extensive experiments on representative real-world datasets to verify its practicality and effectiveness."
1168,,Rule-Based Entity Resolution on Database with Hidden Temporal Information.,"Hongzhi Wang 0001,Xiaoou Ding,Jianzhong Li 0001,Hong Gao 0001",https://doi.org/10.1109/TKDE.2018.2816018,TKDE,2018,"Currencies,Erbium,Clustering algorithms,Heuristic algorithms,Data integrity,Market research,Task analysis","In this paper, we deal with the problem of rule-based entity resolution on imprecise temporal data. Entity resolution (ER) is widely explored in research community, but the problem on temporal data, especially without available timestamps, has not been studied well yet. Because of the elapsing of time, records referring to the same entity observed in different time periods may be different. Besides traditional similarity-based ER approaches, by carefully exploring several data quality rules, e.g., matching dependency and data currency, much information can be obtained to facilitate to cope with this problem. In this paper, we use such rules to derive temporal records' information of time order and trend of their attributes' evolvement with elapsing of time. Specifically, we first block records into smaller blocks, and then by exploring data currency constraints, we propose a temporal clustering approach with two steps, i.e., the skeleton clustering and the banding clustering. Experimental results on both real and synthetic data show that our entity resolution method can achieve both high accuracy and efficiency on datasets with hidden temporal information."
1169,1,Selecting Optimal Subset to Release Under Differentially Private M-Estimators from Hybrid Datasets.,"Meng Wang 0007,Zhanglong Ji,Hyeon-Eui Kim,Shuang Wang 0002,Li Xiong 0001,Xiaoqian Jiang",https://doi.org/10.1109/TKDE.2017.2773545,TKDE,2018,"Data privacy,Data models,Privacy,Sociology,Statistics,Logistics,Sensitivity","Privacy concern in data sharing especially for health data gains particularly increasing attention nowadays. Now, some patients agree to open their information for research use, which gives rise to a new question of how to effectively use the public information to better understand the private dataset without breaching privacy. In this paper, we specialize this question as selecting an optimal subset of the public dataset for M-estimators in the framework of differential privacy (DP) in [1]. From a perspective of non-interactive learning, we first construct the weighted private density estimation from the hybrid datasets under DP. Along the same line as [2], we analyze the accuracy of the DP M-estimators based on the hybrid datasets. Our main contributions are (i) we find that the bias-variance tradeoff in the performance of our M-estimators can be characterized in the sample size of the released dataset; (ii) based on this finding, we develop an algorithm to select the optimal subset of the public dataset to release under DP. Our simulation studies and application to the real datasets confirm our findings and set a guideline in the real application."
1170,,Exploring Hierarchical Structures for Recommender Systems.,"Suhang Wang,Jiliang Tang,Yilin Wang,Huan Liu 0001",https://doi.org/10.1109/TKDE.2018.2789443,TKDE,2018,"Music,Recommender systems,Art,Films,Motion pictures,Clocks,Ice","Items in real-world recommender systems exhibit certain hierarchical structures. Similarly, user preferences also present hierarchical structures. Recent studies show that incorporating the hierarchy of items or user preferences can improve the performance of recommender systems. However, hierarchical structures are often not explicitly available, especially those of user preferences. Thus, there's a gap between the importance of hierarchies and their availability. In this paper, we investigate the problem of exploring the implicit hierarchical structures for recommender systems when they are not explicitly available. We propose a novel recommendation framework to bridge the gap, which enables us to explore the implicit hierarchies of users and items simultaneously. We then extend the framework to integrate explicit hierarchies when they are available, which gives a unified framework for both explicit and implicit hierarchical structures. Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework by incorporating implicit and explicit structures."
1171,,Search Result Diversity Evaluation Based on Intent Hierarchies.,"Xiaojie Wang 0003,Ji-Rong Wen,Zhicheng Dou,Tetsuya Sakai,Rui Zhang 0003",https://doi.org/10.1109/TKDE.2017.2729559,TKDE,2018,"Power measurement,Weight measurement,Companies,Gain measurement,Q measurement,Gold,Standards","Search result diversification aims at returning diversified document lists to cover different user intents of a query. Existing diversity measures assume that the intents of a query are disjoint, and do not consider their relationships. In this paper, we introduce intent hierarchies to model the relationships between intents, and present four weighing schemes. Based on intent hierarchies, we propose several hierarchical measures that take into account the relationships between intents. We demonstrate the feasibility of hierarchical measures by using a new test collection based on TREC Web Track 2009-2013 diversity test collections and by using NTCIR-11 IMine test collection. Our main experimental findings are: (1) Hierarchical measures are more discriminative and intuitive than existing measures. In terms of intuitiveness, it is preferable for hierarchical measures to use the whole intent hierarchies than to use only the leaf nodes. (2) The types of intent hierarchies used affect the discriminative power and intuitiveness of hierarchical measures. We suggest the best type of intent hierarchies to be used according to whether the nonuniform weights are available. (3) To measure the benefits of the diversification algorithms which use automatically mined hierarchical intents, it is important to use hierarchical measures instead of existing measures."
1172,,Efficient Computation of G-Skyline Groups.,"Changping Wang,Chaokun Wang,Gaoyang Guo,Xiaojun Ye,Philip S. Yu",https://doi.org/10.1109/TKDE.2017.2777994,TKDE,2018,"Algorithm design and analysis,Optimization,Decision making,Search problems,Aggregates,Advertising,Companies","The skyline of a data point set is made up of the best points in the set, and is very important for multi-criteria decision making. In these years, the skyline problem attracts more and more attention, and many variants of the traditional skyline emerge in the database field. One recent and important variant is group-based skyline, which aims to find the best groups of points in a given set. In this paper, we bring forward an efficient approach, called minimum dominance search (MDS), to solve the g-skyline problem, a latest group-based skyline problem. MDS consists of two steps: In the first step, we construct a novel g-skyline support structure, i.e., minimum dominance graph (MDG), which proves to be a minimum g-skyline support structure. In the second step, we search for g-skyline groups based on the MDG through two searching algorithms, and a skyline-combination based optimization strategy is employed to improve these two algorithms. We conduct comprehensive experiments on both synthetic and real-world data sets, and show that our algorithms are orders of magnitude faster than the state-of-the-art in most cases."
1173,,MOSS-5 - A Fast Method of Approximating Counts of 5-Node Graphlets in Large Graphs.,"Pinghui Wang,Junzhou Zhao,Xiangliang Zhang 0001,Zhenguo Li,Jiefeng Cheng,John C. S. Lui,Don Towsley,Jing Tao,Xiaohong Guan",https://doi.org/10.1109/TKDE.2017.2756836,TKDE,2018,"Proteins,Malware,Sampling methods,Electronic mail,Kernel","Counting 3-, 4-, and 5-node graphlets in graphs is important for graph mining applications such as discovering abnormal/ evolution patterns in social and biology networks. In addition, it is recently widely used for computing similarities between graphs and graph classification applications such as protein function prediction and malware detection. However, it is challenging to compute these graphlet counts for a large graph or a large set of graphs due to the combinatorial nature of the problem. Despite recent efforts in counting 3-node and 4-node graphlets, little attention has been paid to characterizing 5-node graphlets. In this paper, we develop a computationally efficient sampling method to estimate 5-node graphlet counts. We not only provide a fast sampling method and unbiased estimators of graphlet counts, but also derive simple yet exact formulas for the variances of the estimators which are of great value in practice-the variances can be used to bound the estimates' errors and determine the smallest necessary sampling budget for a desired accuracy. We conduct experiments on a variety of real-world datasets, and the results show that our method is several orders of magnitude faster than the state-of-the-art methods with the same accuracy."
1174,,String Similarity Search - A Hash-Based Approach.,"Hao Wei,Jeffrey Xu Yu,Can Lu",https://doi.org/10.1109/TKDE.2017.2756932,TKDE,2018,"Indexes,DNA,Search problems,Sequential analysis,Cleaning,Data mining","String similarity search is a fundamental query that has been widely used for DNA sequencing, error-tolerant query autocompletion, and data cleaning needed in database, data warehouse, and data mining. In this paper, we study string similarity search based on edit distance that is supported by many database management systems such as Oracle and PostgreSQL. Given the edit distance, ed(s, t), between two strings, s and t, the string similarity search is to find every string t in a string database D which is similar to a query string s such that ed(s, t) ≤ τ for a given threshold τ. In the literature, most existing work takes a filter-and-verify approach, where the filter step is introduced to reduce the high verification cost of two strings by utilizing an index built offline for D. The two up-to-date approaches are prefix filtering and local filtering. In this paper, we study string similarity search where strings can be either short or long. Our approach can support long strings, which are not well supported by the existing approaches due to the size of the index built and the time to build such index. We propose two new hash-based labeling techniques, named OX label and XX label, for string similarity search. We assign a hash-label, H
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">s</sub>
, to a string s, and prune the dissimilar strings by comparing two hash-labels, H
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">s</sub>
 and H
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">t</sub>
, for two strings s and t in the filter step. The key idea is to take the dissimilar bit-patterns between two hash-labels. We discuss our hash-based approaches, address their pruning power, and give the algorithms. Our hash-based approaches achieve high efficiency, and keep its index size and index construction time one order of magnitude smaller than the existing approaches in our experiment at the same time."
1175,,Classifier Ensemble by Exploring Supplementary Ordering Information.,Ou Wu,https://doi.org/10.1109/TKDE.2018.2818138,TKDE,2018,"Task analysis,Measurement,Stacking,Reliability,Support vector machines,Optimization,Training","Supplementary information has been proven to be particularly useful in many machine learning tasks. In ensemble learning for a set of trained base classifiers, there also exists abundant implicit supplementary information about the performance orderings for the trained base classifiers in previous literature. However, few classifier ensemble studies consider exploring and utilizing supplementary information. The current study proposes a new learning method for stack classifier ensembles by considering the implicit supplementary ordering information regarding a set of trained classifiers. First, a new metric learning algorithm for measuring the similarities between two arbitrary learning tasks is introduced. Second, supplementary ordering information for the trained classifiers of a given learning task is inferred based on the learned similarities and related performance results reported in the previous literature. Third, a set of ordered soft constraints is generated based on the supplementary ordering information, and achieving the optimal combination weights of the trained classifiers is formalized into a goal programming problem. The optimal combination weights are then obtained. Finally, the experimental results verify the effectiveness of the proposed new classifier ensemble method."
1176,,K-Ary Tree Hashing for Fast Graph Classification.,"Wei Wu 0011,Bin Li 0015,Ling Chen 0006,Xingquan Zhu 0001,Chengqi Zhang",https://doi.org/10.1109/TKDE.2017.2782278,TKDE,2018,"Kernel,Feature extraction,Indexing,Computer science,Chemical compounds,Data mining","Existing graph classification usually relies on an exhaustive enumeration of substructure patterns, where the number of substructures expands exponentially w.r.t. with the size of the graph set. Recently, the Weisfeiler-Lehman (WL) graph kernel has achieved the best performance in terms of both accuracy and efficiency among state-of-the-art methods. However, it is still time-consuming, especially for large-scale graph classification tasks. In this paper, we present a K-Ary Tree based Hashing (KATH) algorithm, which is able to obtain competitive accuracy with a very fast runtime. The main idea of KATH is to construct a traversal table to quickly approximate the subtree patterns in WL using K-ary trees. Based on the traversal table, KATH employs a recursive indexing process that performs only r times of matrix indexing to generate all (r - 1)-depth K-ary trees, where the leaf node labels of a tree can uniquely specify the pattern. After that, the MinHash scheme is used to fingerprint the acquired subtree patterns for a graph. Our experimental results on both real world and synthetic data sets show that KATH runs significantly faster than state-of-the-art methods while achieving competitive or better accuracy."
1177,,Product Adoption Rate Prediction in a Competitive Market.,"Le Wu,Qi Liu 0003,Richang Hong,Enhong Chen,Yong Ge,Xing Xie 0001,Meng Wang 0001",https://doi.org/10.1109/TKDE.2017.2763944,TKDE,2018,"Smart devices,Social network services,Recommender systems,Predictive models,Electronic mail,Time-frequency analysis","As the worlds of commerce and the Internet technology become more inextricably linked, a large number of user consumption series become available for online market intelligence analysis. A critical demand along this line is to predict the future product adoption state of each user, which enables a wide range of applications such as targeted marketing. Nevertheless, previous works only aimed at predicting if a user would adopt a particular product or not with a binary buy-or-not representation. The problem of tracking and predicting users' adoption rates, i.e., the frequency and regularity of using each product over time, is still under-explored. To this end, we present a comprehensive study of product adoption rate prediction in a competitive market. This task is nontrivial as there are three major challenges in modeling users' complex adoption states: the heterogeneous data sources around users, the unique user preference and the competitive product selection. To deal with these challenges, we first introduce a flexible factor-based decision function to capture the change of users' product adoption rate over time, where various factors that may influence users' decisions from heterogeneous data sources can be leveraged. Using this factor-based decision function, we then provide two corresponding models to learn the parameters of the decision function with both generalized and personalized assumptions of users' preferences. We further study how to leverage the competition among different products and simultaneously learn product competition and users' preferences with both generalized and personalized assumptions. Finally, extensive experiments on two real-world datasets show the superiority of our proposed models."
1178,,Multi-Instance Learning with Discriminative Bag Mapping.,"Jia Wu 0001,Shirui Pan,Xingquan Zhu 0001,Chengqi Zhang,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2017.2788430,TKDE,2018,"Algorithm design and analysis,Supervised learning,Training,Electronic mail,Vocabulary,Labeling","Multi-instance learning (MIL) is a useful tool for tackling labeling ambiguity in learning because it allows a bag of instances to share one label. Bag mapping transforms a bag into a single instance in a new space via instance selection and has drawn significant attention recently. To date, most existing work is based on the original space, using all instances inside each bag for bag mapping, and the selected instances are not directly tied to an MIL objective. As a result, it is difficult to guarantee the distinguishing capacity of the selected instances in the new bag mapping space. In this paper, we propose a discriminative mapping approach for multi-instance learning (MILDM) that aims to identify the best instances to directly distinguish bags in the new mapping space. Accordingly, each instance bag can be mapped using the selected instances to a new feature space, and hence any generic learning algorithm, such as an instance-based learning algorithm, can be used to derive learning models for multi-instance classification. Experiments and comparisons on eight different types of real-world learning tasks (including 14 data sets) demonstrate that MILDM outperforms the state-of-the-art bag mapping multi-instance learning approaches. Results also confirm that MILDM achieves balanced performance between runtime efficiency and classification effectiveness."
1179,,NetCycle+ - A Framework for Collective Evolution Inference in Dynamic Heterogeneous Networks.,"Yun Xiong,Yizhou Zhang,Xiangnan Kong,Yangyong Zhu",https://doi.org/10.1109/TKDE.2018.2792020,TKDE,2018,"Correlation,Time series analysis,Convolution,Inference algorithms,Algorithm design and analysis,Heuristic algorithms,Heterogeneous networks","Collective inference has attracted considerable attention in the last decade, where the response variables within a group of instances are correlated and should be inferred collectively, instead of independently. Previous works on collective inference mainly focus on exploiting the autocorrelation among instances in a static network during the inference process. There are also approaches on time series prediction, which mainly exploit the autocorrelation within an instance at different time points during the inference process. However, in many real-world applications, the response variables of related instances can co-evolve over time and their evolutions are not following a static correlation across time, but are following an internal life cycle. In this paper, we study the problem of collective evolution inference, where the goal is to predict the values of the response variables for a group of related instances at the end of their life cycles. This problem is extremely important for various applications, e.g., predicting fund-raising results in crowd-funding and predicting gene-expression levels in bioinformatics. This problem is also highly challenging because different instances in the network can co-evolve over time and they can be at different stages of their life cycles and thus have different evolving patterns. Moreover, the instances in collective evolution inference problems are usually connected through heterogeneous information networks (HINs for short), which involve complex relationships among the instances interconnected by multiple types of links. We propose an approach, called NetCycle+, by incorporating information from both the correlation among related instances and their life cycles. Furthermore, in order to study the deep dependencies between nodes in the network, we extend the graph convolution model into our algorithm. We compared our approach with existing methods of collective inference and time series analysis on two real-world networks. The results demonstrate that our proposed approach can improve the inference performance by considering the autocorrelation through networks and the life cycles of the instances."
1180,,Range Queries on Multi-Attribute Trajectories.,"Jianqiu Xu,Hua Lu 0001,Ralf Hartmut Güting",https://doi.org/10.1109/TKDE.2017.2787711,TKDE,2018,"Trajectory,Indexes,Standards,Semantics,Silver,Image color analysis","Motivated by the trend of providing comprehensive knowledge about trajectory data, we study multi-attribute trajectories each of which contains a sequence of time-stamped locations and a set of characteristic attributes. This enriches the data representation by providing a comprehensive description of moving objects and thus enables new types of queries on moving object trajectories. In this paper, we consider answering range queries that return trajectories (i) containing particular attribute values and (ii) passing a certain area during the query time. We integrate standard trajectories and attributes into one unified framework and propose an index structure as well as the query algorithm. The structure is general and flexible in terms of handling both multi-attribute trajectories and standard trajectories, answering a range of queries and supporting update-intensive applications. The evaluation is conducted in a prototype database system and experimental results demonstrate that our method outperforms alternative methods by a factor of 3-10 on a data set of one million real trajectories and synthetic attribute values."
1181,,Authenticating Aggregate Queries over Set-Valued Data with Confidentiality.,"Cheng Xu 0004,Qian Chen 0020,Haibo Hu 0001,Jianliang Xu,Xiaojun Hei",https://doi.org/10.1109/TKDE.2017.2773541,TKDE,2018,"Aggregates,Authentication,Genomics,Bioinformatics,Protocols,Indexes,Query processing","With recent advances in data-as-a-service (DaaS) and cloud computing, aggregate query services over set-valued data are becoming widely available for business intelligence that drives decision making. However, as the service provider is often a third-party delegate of the data owner, the integrity of the query results cannot be guaranteed and is thus imperative to be authenticated. Unfortunately, existing query authentication techniques either do not work for set-valued data or they lack data confidentiality. In this paper, we propose authenticated aggregate queries over set-valued data that not only ensure the integrity of query results but also preserve the confidentiality of source data. As many aggregate queries are composed of multiset operations such as set union and subset, we first develop a family of privacy-preserving authentication protocols for primitive multiset operations. Using these protocols as building blocks, we present a privacy-preserving authentication framework for various aggregate queries and further optimize their authentication performance. Security analysis and empirical evaluation show that our proposed privacy-preserving authentication techniques are feasible and robust under a wide range of system workloads."
1182,,Online Product Quantization.,"Donna Xu,Ivor W. Tsang,Ying Zhang 0001",https://doi.org/10.1109/TKDE.2018.2817526,TKDE,2018,"Quantization (signal),Computational modeling,Data models,Maintenance engineering,Computational efficiency,Indexes","Approximate nearest neighbor (ANN) search has achieved great success in many tasks. However, existing popular methods for ANN search, such as hashing and quantization methods, are designed for static databases only. They cannot handle well the database with data distribution evolving dynamically, due to the high computational effort for retraining the model based on the new database. In this paper, we address the problem by developing an online product quantization (online PQ) model and incrementally updating the quantization codebook that accommodates to the incoming streaming data. Moreover, to further alleviate the issue of large scale computation for the online PQ update, we design two budget constraints for the model to update partial PQ codebook instead of all. We derive a loss bound which guarantees the performance of our online PQ model. Furthermore, we develop an online PQ model over a sliding window with both data insertion and deletion supported, to reflect the real-time behavior of the data. The experiments demonstrate that our online PQ model is both time-efficient and effective for ANN search in dynamic large scale databases compared with baseline methods and the idea of partial PQ codebook update further reduces the update cost."
1183,,Ultra High-Dimensional Nonlinear Feature Selection for Big Biological Data.,"Makoto Yamada,Jiliang Tang,Jose Lugo-Martinez,Ermin Hodzic,Raunak Shrestha,Avishek Saha,Hua Ouyang,Dawei Yin,Hiroshi Mamitsuka,Süleyman Cenk Sahinalp,Predrag Radivojac,Filippo Menczer,Yi Chang 0001",https://doi.org/10.1109/TKDE.2018.2789451,TKDE,2018,"Feature extraction,Kernel,Computational modeling,Biological system modeling,Data models,Redundancy","Machine learning methods are used to discover complex nonlinear relationships in biological and medical data. However, sophisticated learning models are computationally unfeasible for data with millions of features. Here, we introduce the first feature selection method for nonlinear learning problems that can scale up to large, ultra-high dimensional biological data. More specifically, we scale up the novel Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) to handle millions of features with tens of thousand samples. The proposed method is guaranteed to find an optimal subset of maximally predictive features with minimal redundancy, yielding higher predictive power and improved interpretability. Its effectiveness is demonstrated through applications to classify phenotypes based on module expression in human prostate cancer patients and to detect enzymes among protein structures. We achieve high accuracy with as few as 20 out of one million features-a dimensionality reduction of 99.998 percent. Our algorithm can be implemented on commodity cloud computing platforms. The dramatic reduction of features may lead to the ubiquitous deployment of sophisticated prediction models in mobile health care applications."
1184,,Querying a Collection of Continuous Functions.,"Guolei Yang,Ying Cai",https://doi.org/10.1109/TKDE.2018.2802936,TKDE,2018,"Silicon,Data structures,Query processing,Temperature sensors,Indexing","We introduce a new query primitive called Function Query (FQ). An FQ operates on a set of math functions and retrieves the functions whose output with a given input satisfies a query condition (e.g., being among top k, within a given range). While FQ finds its natural uses in querying a database of math functions, it can also be applied on a database of discrete values. We show that by interpreting the database as a set of user-defined functions, FQ can achieve the same functionality as existing analytic queries such as top-k query and scalar product query. We address the challenge of efficient execution of FQ. The core of our solution is a novel data structure called Intersection-tree. Our research takes advantage of the fact that 1) the intersections of a set of continuous functions partition their domain into a number of subdomains, and 2) in each of these subdomains, the functions can be sorted based on their output. We evaluate the performance of the proposed techniques through analysis, prototyping, and experiments using both synthetic and real-world data. When querying a database of functions, our techniques scale well. When applied on a database of discrete values, our techniques are more versatile and outperform existing techniques in terms of various performance metrics."
1185,,AMDO - An Over-Sampling Technique for Multi-Class Imbalanced Problems.,"Xuebing Yang,Qiuming Kuang,Wensheng Zhang 0002,Guoping Zhang",https://doi.org/10.1109/TKDE.2017.2761347,TKDE,2018,"Algorithm design and analysis,Heuristic algorithms,Data mining,Electronic mail,Computational complexity,Training","Multi-class imbalanced problems have attracted growing attention from the real-world classification tasks in engineering. The underlying skewed distribution of multiple classes poses difficulties for learning algorithms, which becomes more challenging when considering overlapping between classes, lack of representative data, and mixed-type data. In this work, we address this problem in a data-oriented way. Motivated by a recently proposed over-sampling technique designed for numeric data sets, Mahalanobis Distance-based Over-sampling (MDO), we use this technique to capture the covariance structure of the minority class and to generate synthetic samples along the probability contours for learning algorithms. Based on MDO, we further improve the over-sampling strategy and generalize it for mixed-type data sets. The established technique, Adaptive Mahalanobis Distance-based Over-sampling (AMDO), introduces GSVD (Generalized Singular Value Decomposition) for mixed-type data, develops a partially balanced resampling scheme and optimizes the sample synthesis. Theoretical analysis is conducted to demonstrate the reasonability of AMDO. Extensive experimental testing is performed on 15 multi-class imbalanced benchmarks and two data sets for precipitation phase recognition in comparison with several state-of-the-art multi-class imbalanced learning methods. The results validate the effectiveness and robustness of our proposal."
1186,,A Unified View of Social and Temporal Modeling for B2B Marketing Campaign Recommendation.,"Jingyuan Yang,Chuanren Liu,Mingfei Teng,Ji Chen,Hui Xiong 0001",https://doi.org/10.1109/TKDE.2017.2783926,TKDE,2018,"Recommender systems,Companies,Graphical models,Probabilistic logic,Knowledge representation","Business to Business (B2B) marketing aims at meeting the needs of other businesses instead of individual consumers, and thus entails management of more complex business needs than consumer marketing. The buying processes of the business customers involve series of different marketing campaigns providing multifaceted information about the products or services. While most existing studies focus on individual consumers, little has been done to guide business customers due to the dynamic and complex nature of these business buying processes. To this end, in this paper, we focus on providing a unified view of social and temporal modeling for B2B marketing campaign recommendation. Along this line, we first exploit the temporal behavior patterns in the B2B buying processes and develop a marketing campaign recommender system. Specifically, we start with constructing a temporal graph as the knowledge representation of the buying process of each business customer. Temporal graph can effectively extract and integrate the campaign order preferences of individual business customers. It is also worth noting that our system is backward compatible since the participating frequency used in conventional static recommender systems is naturally embedded in our temporal graph. The campaign recommender is then built in a low-rank graph reconstruction framework based on probabilistic graphical models. Our framework can identify the common graph patterns and predict missing edges in the temporal graphs. In addition, since business customers very often have different decision makers from the same company, we also incorporate social factors, such as community relationships of the business customers, for further improving overall performances of the missing edge prediction and recommendation. Finally, we have performed extensive empirical studies on real-world B2B marketing data sets and the results show that the proposed method can effectively improve the quality of the campaign recommendations for challenging B2B marketing tasks."
1187,,A Novel Representation and Compression for Queries on Trajectories in Road Networks.,"Xiaochun Yang 0001,Bin Wang 0015,Kai Yang,Chengfei Liu,Baihua Zheng",https://doi.org/10.1109/TKDE.2017.2776927,TKDE,2018,"Trajectory,Roads,Encoding,Presses,Query processing,Entropy,Memory","Recording and querying time-stamped trajectories incurs high cost of data storage and computing. In this paper, we explore several characteristics of the trajectories in road networks, which have motivated the idea of coding trajectories by associating timestamps with relative spatial path and locations. Such a representation contains a large number of duplicate information to achieve a lower entropy compared with the existing representations, thereby drastically cutting the storage cost. We propose several techniques to compress spatial path and locations separately, which can support fast positioning and achieve better compression ratio. For locations, we propose two novel encoding schemes such that the binary code can preserve distance information, which is very helpful for LBS applications. In addition, an unresolved question in this area is whether it is possible to perform a search directly on the compressed trajectories, and if the answer is yes, then how. Here, we show that directly querying compressed trajectories based on our encoding scheme is possible and can be done efficiently. We design a set of primitive operations for this purpose, and propose index structures to reduce query response time. We demonstrate the advantage of our method and compare it against existing ones through a thorough experimental study on real trajectories in road network."
1188,,A Topic Modeling Approach for Traditional Chinese Medicine Prescriptions.,"Liang Yao,Yin Zhang 0006,Baogang Wei,Wenjin Zhang,Zhe Jin",https://doi.org/10.1109/TKDE.2017.2787158,TKDE,2018,"Data models,Diseases,Adaptation models,Data mining,Analytical models,Probabilistic logic","In traditional Chinese medicine (TCM), prescriptions are the daughters of doctors' clinical experiences, which have been the main way to cure diseases in China for several thousand years. In the long Chinese history, a large number of prescriptions have been invented based on TCM theories. Regularities in the prescriptions are important for both clinical practice and novel prescription development. Previous works used many methods to discover regularities in prescriptions, but rarely described how a prescription is generated using TCM theories. In this work, we propose a topic model which characterizes the generative process of prescriptions in TCM theories and further incorporate domain knowledge into the topic model. Using 33,765 prescriptions in TCM prescription books, the model can reflect the prescribing patterns in TCM. Our method can outperform several previous topic models and group recommendation methods on generalization performance, herbs recommendation, symptoms suggestion, and prescribing patterns discovery."
1189,,Scalable Distributed Nonnegative Matrix Factorization with Block-Wise Updates.,"Jiangtao Yin,Lixin Gao 0001,Zhongfei Zhang",https://doi.org/10.1109/TKDE.2017.2785326,TKDE,2018,"Euclidean distance,Loss measurement,Parallel processing,Matrix decomposition,Partitioning algorithms,Convergence","Nonnegative Matrix Factorization (NMF) has been applied with great success on a wide range of applications. As NMF is increasingly applied to massive datasets such as web-scale dyadic data, it is desirable to leverage a cluster of machines to store those datasets and to speed up the factorization process. However, it is challenging to efficiently implement NMF in a distributed environment. In this paper, we show that by leveraging a new form of update functions, we can perform local aggregation and fully explore parallelism. Therefore, the new form is much more efficient than the traditional form in distributed implementations. Moreover, under the new form of update functions, we can perform frequent updates and lazy updates, which aim to use the most recently updated data whenever possible and avoid unnecessary computations. As a result, frequent updates and lazy updates are more efficient than their traditional concurrent counterparts. Through a series of experiments on a local cluster as well as the Amazon EC2 cloud, we demonstrate that our implementations with frequent updates or lazy updates are up to two orders of magnitude faster than the existing implementation with the traditional form of update functions."
1190,,SLA Definition for Multi-Tenant DBMS and its Impact on Query Optimization.,"Shaoyi Yin,Abdelkader Hameurlain,Franck Morvan",https://doi.org/10.1109/TKDE.2018.2817235,TKDE,2018,"Query processing,Optimization,Measurement,Economics,Hardware,Search problems","In the cloud context, users are often called tenants. A cloud DBMS shared by many tenants is called a multi-tenant DBMS. The resource consolidation in such a DBMS allows the tenants to only pay for the resources that they consume, while providing the opportunity for the provider to increase its economic gain. For this, a Service Level Agreement (SLA) is usually established between the provider and a tenant. However, in the current systems, the SLA is often defined by the provider, while the tenant should agree with it before using the service. In addition, only the availability objective is described in the SLA, but not the performance objective. In this paper, an SLA negotiation framework is proposed, in which the provider and the tenant define the performance objective together in a fair way. To demonstrate the feasibility and the advantage of this framework, we evaluate its impact on query optimization. We formally define the problem by including the cost-efficiency aspect, we design a cost model and study the plan search space for this problem, we revise two search methods to adapt to the new context, and we propose a heuristic to solve the resource contention problem caused by concurrent queries of multiple tenants. We also conduct a performance evaluation to show that, our optimization approach (i.e., driven by the SLA) can be much more cost-effective than the traditional approach which always minimizes the query completion time."
1191,,Sparse Feature Attacks in Adversarial Learning.,"Zhizhou Yin,Fei Wang,Wei Liu 0007,Sanjay Chawla",https://doi.org/10.1109/TKDE.2018.2790928,TKDE,2018,"Games,Robustness,Data models,Electronic mail,Game theory,Gallium nitride,Transforms","Adversarial learning is the study of machine learning techniques deployed in non-benign environments. Example applications include classification for detecting spam, network intrusion detection, and credit card scoring. In fact, as the use of machine learning grows in diverse application domains, the possibility for adversarial behavior is likely to increase. When adversarial learning is modelled in a game-theoretic setup, the standard assumption about the adversary (player) behavior is the ability to change all features of the classifiers (the opponent player) at will. The adversary pays a cost proportional to the size of the “attack”. We refer to this form of adversarial behavior as a dense feature attack. However, the aim of an adversary is not just to subvert a classifier but carry out data transformation in a way such that spam continues to remain effective. We demonstrate that an adversary could potentially achieve this objective by carrying out a sparse feature attack. We design an algorithm to show how a classifier should be designed to be robust against sparse adversarial attacks. Our main insight is that sparse feature attacks are best defended by designing classifiers which use ℓ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
 regularizers."
1192,,Semi-Supervised Ensemble Clustering Based on Selected Constraint Projection.,"Zhiwen Yu 0002,Peinan Luo,Jiming Liu 0001,Hau-San Wong,Jane You,Guoqiang Han,Jun Zhang 0003",https://doi.org/10.1109/TKDE.2018.2818729,TKDE,2018,"Clustering algorithms,Data mining,Computer science,Gene expression","Traditional cluster ensemble approaches have several limitations. (1) Few make use of prior knowledge provided by experts. (2) It is difficult to achieve good performance in high-dimensional datasets. (3) All of the weight values of the ensemble members are equal, which ignores different contributions from different ensemble members. (4) Not all pairwise constraints contribute to the final result. In the face of this situation, we propose double weighting semi-supervised ensemble clustering based on selected constraint projection(DCECP) which applies constraint weighting and ensemble member weighting to address these limitations. Specifically, DCECP first adopts the random subspace technique in combination with the constraint projection procedure to handle high-dimensional datasets. Second, it treats prior knowledge of experts as pairwise constraints, and assigns different subsets of pairwise constraints to different ensemble members. An adaptive ensemble member weighting process is designed to associate different weight values with different ensemble members. Third, the weighted normalized cut algorithm is adopted to summarize clustering solutions and generate the final result. Finally, nonparametric statistical tests are used to compare multiple algorithms on real-world datasets. Our experiments on 15 high-dimensional datasets show that DCECP performs better than most clustering algorithms."
1193,,Index-Based Densest Clique Percolation Community Search in Networks.,"Long Yuan,Lu Qin,Wenjie Zhang 0001,Lijun Chang,Jianye Yang",https://doi.org/10.1109/TKDE.2017.2783933,TKDE,2018,"Search problems,Indexes,Proteins,Social network services,Memory management,Semantics,Query processing","Community search is important in graph analysis and can be used in many real applications. In the literature, various community models have been proposed. However, most of them cannot well identify the overlaps between communities which is an essential feature of real graphs. To address this issue, the k-clique percolation community model was proposed and has been proven effective in many applications. Motivated by this, in this paper, we adopt the k-clique percolation community model and study the densest clique percolation community search problem which aims to find the k-clique percolation community with the maximum k value that contains a given set of query nodes. We adopt an index-based approach to solve this problem. Based on the observation that a k-clique percolation community is a union of maximal cliques, we devise a novel compact index, DCPC-Index, to preserve the maximal cliques and their connectivity information of the input graph. With DCPC-Index, we can answerthe densest clique percolation community query efficiently. Besides, we also propose an index construction algorithm based on the definition of DCPC-Index and further improve the algorithm in terms of efficiency and memory consumption. We conduct extensive performance studies on real graphs and the experimental results demonstrate the efficiency of our index-based query processing algorithm and index construction algorithm."
1194,,Document Summarization for Answering Non-Factoid Queries.,"Evi Yulianti,Ruey-Cheng Chen,Falk Scholer,W. Bruce Croft,Mark Sanderson",https://doi.org/10.1109/TKDE.2017.2754373,TKDE,2018,"Knowledge discovery,Feature extraction,Data mining,Search engines,Optimization,Google,Web search","We formulate a document summarization method to extract passage-level answers for non-factoid queries, referred to as answer-biased summaries. We propose to use external information from related Community Question Answering (CQA) content to better identify answer bearing sentences. Three optimization-based methods are proposed: (i) query-biased, (ii) CQA-answer-biased, and (iii) expanded-query-biased, where expansion terms were derived from related CQA content. A learning-to-rank-based method is also proposed that incorporates a feature extracted from related CQA content. Our results show that even if a CQA answer does not contain a perfect answer to a query, their content can be exploited to improve the extraction of answer-biased summaries from other corpora. The quality of CQA content is found to impact on the accuracy of optimization-based summaries, though medium quality answers enable the system to achieve a comparable (and in some cases superior) accuracy to state-of-the-art techniques. The learning-to-rank-based summaries, on the other hand, are not significantly influenced by CQA quality. We provide a recommendation of the best use of our proposed approaches in regard to the availability of different quality levels of related CQA content. As a further investigation, the reliability of our approaches was tested on another publicly available dataset."
1195,1,On Power Law Growth of Social Networks.,"Chengxi Zang,Peng Cui 0001,Christos Faloutsos,Wenwu Zhu 0001",https://doi.org/10.1109/TKDE.2018.2801844,TKDE,2018,"Social network services,Silicon,Mathematical model,Generators,Stochastic processes,Microscopy,Differential equations","What is the growth dynamics of social networks, like Facebook or WeChat? Does it truly exhibit exponential early-growth, as predicted by the celebrated models, like the Bass model? How about the dynamics of links, for which there are few published models? For the first time, we examine the growth of WeChat which is the largest online social network in China, together with several other real social networks. We observe Power-Law growth dynamics for both nodes and links, a fact that breaks the textbook models featuring Sigmoid curves. We propose NETTIDE, along with differential equations for the growth of nodes and links. Our model fits the growth dynamics of real social networks well; it encompasses many traditional growth dynamics as special cases, while remaining parsimonious in parameters. The NETTIDE for link growth is the first one of its kind, accurately fitting real data, and capturing densification phenomenon. We further formulate two stochastic generators, which interpret the growth of nodes and links through survival analysis and micro-level interactions within a social network, respectively. The proposed generators reproduce realistic growth dynamics of social networks. When applied on the WeChat data, our NETTIDE forecasted > 730 days ahead with 3 percent error."
1196,,FBSGraph - Accelerating Asynchronous Graph Processing via Forward and Backward Sweeping.,"Yu Zhang 0027,Xiaofei Liao,Hai Jin 0001,Lin Gu 0002,Bing Bing Zhou",https://doi.org/10.1109/TKDE.2017.2781241,TKDE,2018,"Convergence,Synchronization,Acceleration,Clustering algorithms,Data models,Computational modeling,Distributed computing","Graph algorithm is pervasive in many applications ranging from targeted advertising to natural language processing. Recently, Asynchronous Graph Processing (AGP) is becoming a promising model to support graph algorithm on large-scale distributed computing platforms because it enables faster convergence speed and lower synchronization cost than the synchronous model for no barrier between iterations. However, existing AGP methods still suffer from poor performance for inefficient vertex state propagation. In this paper, we propose an effective and low-cost forward and backward sweeping execution method to accelerate state propagation for AGP, based on a key observation that states in AGP can be propagated between vertices much faster when the vertices are processed sequentially along the graph path within each round. Through dividing graph into paths and asynchronously processing vertices on each path in an alternative forward and backward way according to their order on this path, vertex states in our approach can be quickly propagated to other vertices and converge in a faster way with only little additional overhead. In order to efficiently support it over distributed platforms, we also propose a scheme to reduce the communication overhead along with a static priority ordering scheme to further improve the convergence speed. Experimental results on a cluster with 1,024 cores show that our approach achieves excellent scalability for large-scale graph algorithms and the overall execution time is reduced by at least 39.8 percent, in comparison with the most cutting-edge methods."
1197,,Identifying Genetic Risk Factors for Alzheimer&apos;s Disease via Shared Tree-Guided Feature Learning Across Multiple Tasks.,"Weizhong Zhang,Tingjin Luo,Shuang Qiu,Jieping Ye,Deng Cai 0001,Xiaofei He 0001,Jie Wang 0005",https://doi.org/10.1109/TKDE.2018.2816029,TKDE,2018,"Task analysis,Genetics,Indexes,Diseases,Training,Hafnium,Bioinformatics","The genome-wide association study (GWAS) is a popular approach to identify disease-associated genetic factors for Alzhemer's Disease (AD). However, it remains challenging because of the small number of samples, very high feature dimensionality and complex structures. To accurately identify genetic risk factors for AD, we propose a novel method based on an in-depth exploration of the hierarchical structure among the features and the commonality across related tasks. Specifically, we first extract and encode the tree hierarchy among features; then, we integrate the tree structures with multi-task feature learning (MTFL) to learn the shared features-that are predictive of AD-among related tasks simultaneously. Thus, we can unify the strength of both the prior structure information and MTFL to boost the prediction performance. However, due to the highly complex regularizer that encodes the tree structure and the extremely high feature dimensionality, the learning process can be computationally prohibitive. To address this, we further develop a novel safe screening rule to quickly identify and remove the irrelevant features before training. Experiment results demonstrate that the proposed approach significantly outperforms the state-of-the-art in detecting genetic risk factors of AD and the speedup gained by the proposed screening can be several orders of magnitude."
1198,,Workload Management in Database Management Systems - A Taxonomy.,"Mingyi Zhang 0001,Patrick Martin 0001,Wendy Powley,Jianjun Chen 0001",https://doi.org/10.1109/TKDE.2017.2767044,TKDE,2018,"Business,Servers,Database systems,Taxonomy,Process control,Monitoring","Workload management is the discipline of effectively monitoring, managing and controlling work flow across computing systems. In particular, workload management in database management systems (DBMSs) is the process or act of monitoring and controlling work (i.e., requests) executing on a database system in order to make efficient use of system resources in addition to achieving any performance objectives assigned to that work. In the past decade, workload management studies and practice have made considerable progress in both academia and industry. New techniques have been proposed by researchers, and new features of workload management facilities have been implemented in most commercial database products. In this paper, we provide a systematic study of workload management in today's DBMSs by developing a taxonomy of workload management techniques. We apply the taxonomy to evaluate and classify existing workload management techniques implemented in the commercial databases and available in the recent research literature. We also introduce the underlying principles of today's workload management technology for DBMSs, discuss open problems, and outline some research opportunities in this research area."
1199,,Locality Reconstruction Models for Book Representation.,"Haijun Zhang 0002,Shuang Wang,Mingbo Zhao,Xiaofei Xu,Yunming Ye",https://doi.org/10.1109/TKDE.2018.2808953,TKDE,2018,"Graphical models,Distribution functions,Vocabulary,Electronic publishing,Feature extraction,Semantics,Databases","Books, as a representative of lengthy documents, convey rich semantics. Traditional document modeling methods, such as bag-of-words models, have difficulty capturing such rich semantics when only considering term-frequency features. In order to explore term spatial distributions over a book, a tree-structured book representation is investigated in this paper. Moreover, an efficient learning framework, Tree2Vector, is introduced for mapping tree-structured book data into vectorial space. In particular, we present two types of locality reconstruction (LR) models: Euclidean-type and cosine-type, during the transformation process of tree structures into vectorial representations. The LR is used for modeling the reconstruction process, in which each parent node in a tree is supposed to be reconstructed by its child nodes. The prominent advantage of this Tree2Vector framework is that it solely utilizes the local information within a single book tree. In addition, extensive experimental results demonstrate that Tree2Vector is able to deliver comparable or better performance in comparison to methods that consider the information of all trees in a database globally. Experimental results also suggest that cosine-type LR consistently performs better than Euclidean-type LR in applications of book and author recommendations."
1200,,Partially Related Multi-Task Clustering.,"Xiaotong Zhang 0003,Xianchao Zhang,Han Liu 0008,Xinyue Liu",https://doi.org/10.1109/TKDE.2018.2818705,TKDE,2018,"Task analysis,Clustering methods,Clustering algorithms,Manifolds,Encoding,Knowledge engineering","Multi-task clustering improves the clustering performance of each task by transferring knowledge across related tasks. Most existing multi-task clustering methods are based on the ideal assumption that the tasks are completely related. However, in real applications, the tasks are usually partially related. In these cases, brute-force transfer may cause negative effect which degrades the clustering performance. In this paper, we propose two multi-task clustering methods for partially related tasks: the self-adapted multi-task clustering (SAMTC) method and the manifold regularized coding multi-task clustering (MRCMTC) method, which can automatically identify and transfer related instances among the tasks, thus avoiding negative transfer. Both SAMTC and MRCMTC construct the similarity matrix for each target task by exploiting useful information from the source tasks through related instances transfer, and adopt spectral clustering to get the final clustering results. But, they learn the related instances from the source tasks in different ways. Experimental results on real data sets show the superiorities of the proposed algorithms over traditional single-task clustering methods and existing multi-task clustering methods on both completely and partially related tasks."
1201,,Multi-View Missing Data Completion.,"Lei Zhang 0116,Yao Zhao 0001,Zhenfeng Zhu,Dinggang Shen,Shuiwang Ji",https://doi.org/10.1109/TKDE.2018.2791607,TKDE,2018,"Semantics,Correlation,Sparse matrices,Information science,Electronic mail,Bridges","A growing number of multi-view data arises naturally in many scenarios, including medical diagnosis, webpage classification, and multimedia analysis. A challenge in learning from multi-view data is that not all instances are fully represented in all views, resulting in missing view data. In this paper, we focus on feature-level completion for missing view of multi-view data. Aiming at capturing both semantic complementarity and identical distribution among different views, an Isomorphic Linear Correlation Analysis (ILCA) method is proposed to linearly map multi-view data to a feature-isomorphic subspace through learning a set of excellent isomorphic features, thereby unfolding the shared information from different views. Meanwhile, we assume that missing view obeys normal distribution. Then, the missing view data matrix can be modeled as a low-rank component plus a sparse contribution. Thus, to accomplish missing view completion, an Identical Distribution Pursuit Completion (IDPC) model based on the learned features is proposed, in which the identical distribution constraint of missing view to the other available one in the feature-isomorphic subspace is fully exploited. Comprehensive experiments on several multi-view datasets demonstrate that our proposed framework yields promising results."
1202,,Weakly-Supervised Deep Embedding for Product Review Sentiment Analysis.,"Wei Zhao 0019,Ziyu Guan,Long Chen 0007,Xiaofei He 0001,Deng Cai 0001,Beidou Wang,Quan Wang 0006",https://doi.org/10.1109/TKDE.2017.2756658,TKDE,2018,"Machine learning,Neural networks,Feature extraction,Sentiment analysis,Syntactics,Training","Product reviews are valuable for upcoming buyers in helping them make decisions. To this end, different opinion mining techniques have been proposed, where judging a review sentence's orientation (e.g., positive or negative) is one of their key challenges. Recently, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. We propose a novel deep learning framework for product review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learning a high level representation (an embedding space) which captures the general sentiment distribution of sentences through rating information; and (2) adding a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. We explore two kinds of low level network structure for modeling review sentences, namely, convolutional feature extractors and long short-term memory. To evaluate the proposed framework, we construct a dataset containing 1.1M weakly labeled review sentences and 11,754 labeled review sentences from Amazon. Experimental results show the efficacy of the proposed framework and its superiority over baselines."
1203,,A Survey of Location Prediction on Twitter.,"Xin Zheng,Jialong Han,Aixin Sun",https://doi.org/10.1109/TKDE.2018.2807840,TKDE,2018,"Twitter,Urban areas,Noise measurement,Geology,Task analysis","Locations, e.g., countries, states, cities, and point-of-interests, are central to news, emergency events, and people's daily lives. Automatic identification of locations associated with or mentioned in documents has been explored for decades. As one of the most popular online social network platforms, Twitter has attracted a large number of users who send millions of tweets on daily basis. Due to the world-wide coverage of its users and real-time freshness of tweets, location prediction on Twitter has gained significant attention in recent years. Research efforts are spent on dealing with new challenges and opportunities brought by the noisy, short, and context-rich nature of tweets. In this survey, we aim at offering an overall picture of location prediction on Twitter. Specifically, we concentrate on the prediction of user home locations, tweet locations, and mentioned locations. We first define the three tasks and review the evaluation metrics. By summarizing Twitter network, tweet content, and tweet context as potential inputs, we then structurally highlight how the problems depend on these inputs. Each dependency is illustrated by a comprehensive review of the corresponding strategies adopted in state-of-the-art approaches. In addition, we also briefly review two related problems, i.e., semantic location prediction and point-of-interest recommendation. Finally, we make a conclusion of the survey and list future research directions."
1204,,Correction to A Survey of Location Prediction on Twitter.,"Xin Zheng,Jialong Han,Aixin Sun",https://doi.org/10.1109/TKDE.2018.2867987,TKDE,2018,"Twitter,Urban areas,Noise measurement,Task analysis","The authors of ""A Survey of Location Prediction on Twitter"" which appeared in the September 2018 issue of this journal would like to point out a typo that occurred in the first footnote on page 1. Xin Zheng was mistakenly listed as the corresponding author. The correct corresponding author is Jialong Han. Please address any correspondence about this paper to him."
1205,,Structure Based User Identification across Social Networks.,"Xiaoping Zhou,Xun Liang 0001,Xiaoyong Du 0001,Jichao Zhao",https://doi.org/10.1109/TKDE.2017.2784430,TKDE,2018,"Social network services,Knowledge engineering,Reliability,Feature extraction,Electronic mail,Social computing","Identification of anonymous identical users of cross-platforms refers to the recognition of the accounts belonging to the same individual among multiple Social Network (SN) platforms. Evidently, cross-platform exploration may help solve many problems in social computing, in both theory and practice. However, it is still an intractable problem due to the fragmentation, inconsistency, and disruption of the accessible information among SNs. Different from the efforts implemented on user profiles and users' content, many studies have noticed the accessibility and reliability of network structure in most of the SNs for addressing this issue. Although substantial achievements have been made, most of the current network structure-based solutions, requiring prior knowledge of some given identified users, are supervised or semi-supervised. It is laborious to label the prior knowledge manually in some scenarios where prior knowledge is hard to obtain. Noticing that friend relationships are reliable and consistent in different SNs, we proposed an unsupervised scheme, termed Friend Relationship-based User Identification algorithm without Prior knowledge (FRUI-P). The FRUI-P first extracts the friend feature of each user in an SN into friend feature vector, and then calculates the similarities of all the candidate identical users between two SNs. Finally, a one-to-one map scheme is developed to identify the users based on the similarities. Moreover, FRUI-P is proved to be efficient theoretically. Results of extensive experiments demonstrated that FRUI-P performs much better than current state-of-art network structure-based algorithm without prior knowledge. Due to its high precision, FRUI-P can additionally be utilized to generate prior knowledge for supervised and semi-supervised schemes. In applications, the unsupervised anonymous identical user identification method accommodates more scenarios where the seed users are unobtainable."
1206,,Paradoxical Correlation Pattern Mining.,"Wenjun Zhou 0001,Hui Xiong 0001,Lian Duan,Keli Xiao,Robert Mee",https://doi.org/10.1109/TKDE.2018.2791602,TKDE,2018,"Correlation,Digital audio players,Niobium,Data mining,Databases,Business,Benchmark testing","Given a large transactional database, correlation computing/association analysis aims at efficiently finding strongly correlated items. For traditional association analysis, relationships among variables are usually measured at a global level. In this study, we investigate confounding factors that can help to capture abnormal correlation behaviors at a local level. Indeed, many real-world phenomena are localized to specific markets or subpopulations. Such local relationships may not be visible or may be miscalculated when collectively analyzing the entire data. In particular, confounding effects that change the direction of correlation are a most severe problem because the global correlations alone leads to errant conclusions. To this end, we propose CONFOUND, an efficient algorithm to identify paradoxical correlation patterns (i.e., where controlling for a third item changes the direction of association for strongly correlated pairs) using effective pruning strategies. Moreover, we also provide an enhanced version of this algorithm, called CONFOUND±, which substantially speeds up the confounder search step. Finally, experimental results showed that our proposed CONFOUND and CONFOUND± algorithms can effectively identify confounders and the computational performance is orders of magnitude faster than benchmark methods."
1207,,Range-Based Nearest Neighbor Queries with Complex-Shaped Obstacles.,"Huaijie Zhu,Xiaochun Yang 0001,Bin Wang 0015,Wang-Chien Lee",https://doi.org/10.1109/TKDE.2017.2779487,TKDE,2018,"Indexes,Spatial databases,Query processing,Acceleration,Mobile communication,Algorithm design and analysis,Artificial neural networks","In this paper, we study a novel variant of obstructed nearest neighbor queries, namely, range-based obstructed nearest neighbor(RONN) search. As a natural generalization of continuous obstructednearest-neighbor(CONN), an RONN query retrieves a set of obstructed nearest neighbors corresponding to every point in a specified range. We propose a new index, namely binary obstructed tree (called OB-tree), for indexing complex objects in the obstructed space. The novelty of OB-tree lies in the idea of dividing the obstructed space into non-obstructedsubspaces, aiming to efficiently retrieve highly qualified candidates for RONN processing. We develop an algorithm for construction of the OB-tree and propose a space division scheme, called optimal obstacle balance (OOB2) scheme, to address the tree balance problem. Accordingly, we propose an efficient algorithm, called RONN by OB-tree Acceleration (RONN-OBA), which exploits the OB-tree and a binary traversal order of data objects to accelerate query processing of RONN. In addition, we extend our work in several aspects regarding the shape of obstacles, and range-based k NN queries in obstructed space. At last, we conduct a comprehensive performance evaluation using both real and synthetic datasets to validate our ideas and the proposed algorithms. The experimental result shows that the RONN-OBA algorithm outperforms the two R-tree based algorithms and RONN-OA significantly."
1208,,Heterogeneous Metric Learning of Categorical Data with Hierarchical Couplings.,"Chengzhang Zhu,Longbing Cao,Qiang Liu 0004,Jianping Yin,Vipin Kumar",https://doi.org/10.1109/TKDE.2018.2791525,TKDE,2018,"Couplings,Frequency measurement,Kernel,Complexity theory,Indexes","Learning appropriate metric is critical for effectively capturing complex data characteristics. The metric learning of categorical data with hierarchical coupling relationships and local heterogeneous distributions is very challenging yet rarely explored. This paper proposes a Heterogeneous mEtric Learning with hIerarchical Couplings (HELIC for short) for this type of categorical data. HELIC captures both low-level value-to-attribute and high-level attribute-to-class hierarchical couplings, and reveals the intrinsic heterogeneities embedded in each level of couplings. Theoretical analyses of the effectiveness and generalization error bound verify that HELIC effectively represents the above complexities. Extensive experiments on 30 data sets with diverse characteristics demonstrate that HELIC-enabled classification significantly enhances the accuracy (up to 40.93 percent), compared with five state-of-the-art baselines."
1209,,High-Order Proximity Preserved Embedding for Dynamic Networks.,"Dingyuan Zhu,Peng Cui 0001,Ziwei Zhang,Jian Pei,Wenwu Zhu 0001",https://doi.org/10.1109/TKDE.2018.2822283,TKDE,2018,"Mathematical model,Acceleration,Perturbation methods,Complexity theory,Heuristic algorithms,Matrix decomposition,Linear programming","Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attention. However, most existing embedding methods focus on the static network while neglecting the evolving characteristic of real-world networks. Meanwhile, most of previous methods cannot well preserve the high-order proximity, which is a critical structural property of networks. These problems motivate us to seek an effective and efficient way to preserve the high-order proximity in embedding vectors when the networks evolve over time. In this paper, we propose a novel method of Dynamic High-order Proximity preserved Embedding (DHPE). Specifically, we adopt the generalized SVD (GSVD) to preserve the high-order proximity. Then, by transforming the GSVD problem to a generalized eigenvalue problem, we propose a generalized eigen perturbation to incrementally update the results of GSVD to incorporate the changes of dynamic networks. Further, we propose an accelerated solution to the DHPE model so that it achieves a linear time complexity with respect to the number of nodes and number of changed edges in the network. Our empirical experiments on one synthetic network and several real-world networks demonstrate the effectiveness and efficiency of the proposed method."
1210,,Multi-Label Learning with Global and Local Label Correlation.,"Yue Zhu,James T. Kwok,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2017.2785795,TKDE,2018,"Correlation,Manifolds,Matrix decomposition,Electronic mail,Training data,Estimation,Optimization","It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data."
1211,,Multi-Label Learning with Emerging New Labels.,"Yue Zhu,Kai Ming Ting,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2018.2810872,TKDE,2018,"Training,Task analysis,Robustness,Detectors,Learning systems,Adaptation models,Anomaly detection","In a multi-label learning task, an object possesses multiple concepts where each concept is represented by a class label. Previous studies on multi-label learning have focused on a fixed set of class labels, i.e., the class label set of test data is the same as that in the training set. In many applications, however, the environment is dynamic and new concepts may emerge in a data stream. In order to maintain a good predictive performance in this environment, a multi-label learning method must have the ability to detect and classify instances with emerging new labels. To this end, we propose a new approach called Multi-label learning with Emerging New Labels (MuENL). It has three functions: classify instances on currently known labels, detect the emergence of a new label, and construct a new classifier for each new label that works collaboratively with the classifier for known labels. In addition, we show that MuENL can be easily extended to handle sparse high dimensional data streams by simply reducing the original dimensionality, and then applying MuENL on the reduced dimensional space. Our empirical evaluation shows the effectiveness of MuENL on several benchmark datasets and MuENLHD on the sparse high dimensional Weibo dataset."
1212,,Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection.,"Xiaofeng Zhu 0001,Shichao Zhang,Rongyao Hu,Yonghua Zhu,Jingkuan Song",https://doi.org/10.1109/TKDE.2017.2763618,TKDE,2018,"Feature extraction,Correlation,Robustness,Training,Redundancy,Kernel","This paper proposes a new unsupervised spectral feature selection method to preserve both the local and global structure of the features as well as the samples. Specifically, our method uses the self-expressiveness of the features to represent each feature by other features for preserving the local structure of features, and a low-rank constraint on the weight matrix to preserve the global structure among samples as well as features. Our method also proposes to learn the graph matrix measuring the similarity of samples for preserving the local structure among samples. Furthermore, we propose a new optimization algorithm to the resulting objective function, which iteratively updates the graph matrix and the intrinsic space so that collaboratively improving each of them. Experimental analysis on 12 benchmark datasets showed that the proposed method outperformed the state-of-the-art feature selection methods in terms of classification performance."
1213,,Complementary Aspect-Based Opinion Mining.,"Yuan Zuo,Junjie Wu 0002,Hui Zhang 0028,Deqing Wang,Ke Xu 0001",https://doi.org/10.1109/TKDE.2017.2764084,TKDE,2018,"Media,Entropy,Analytical models,Data mining,Labeling,Social network services","Aspect-based opinion mining is finding elaborate opinions towards a subject such as a product or an event. With explosive growth of opinionated texts on the Web, mining aspect-level opinions has become a promising means for online public opinion analysis. In particular, the boom of various types of online media provides diverse yet complementary information, bringing unprecedented opportunities for cross media aspect-opinion mining. Along this line, we propose CAMEL, a novel topic model for complementary aspect-based opinion mining across asymmetric collections. CAMEL gains information complementarity by modeling both common and specific aspects across collections, while keeping all the corresponding opinions for contrastive study. An auto-labeling scheme called AME is also proposed to help discriminate between aspect and opinion words without elaborative human labeling, which is further enhanced by adding word embedding-based similarity as a new feature. Moreover, CAMEL-DP, a nonparametric alternative to CAMEL is also proposed based on coupled Dirichlet Processes. Extensive experiments on real-world multi-collection reviews data demonstrate the superiority of our methods to competitive baselines. This is particularly true when the information shared by different collections becomes seriously fragmented. Finally, a case study on the public event “2014 Shanghai Stampede” demonstrates the practical value of CAMEL for real-world applications."
86,,Evaluating the Privacy Guarantees of Location Proximity Services.,"George Argyros,Theofilos Petsios,Suphannee Sivakorn,Angelos D. Keromytis,Iasonas Polakis",https://doi.org/10.1145/3007209,TOPS,2017,[],
87,,"Don&apos;t Trust the Cloud, Verify - Integrity and Consistency for Cloud Object Stores.","Marcus Brandenburger,Christian Cachin,Nikola Knezevic",https://doi.org/10.1145/3079762,TOPS,2017,[],
88,,BLC - Private Matrix Factorization Recommenders via Automatic Group Learning.,"Alessandro Checco,Giuseppe Bianchi 0001,Douglas J. Leith",https://doi.org/10.1145/3041760,TOPS,2017,[],
89,,Quantifying Interdependent Risks in Genomic Privacy.,"Mathias Humbert,Erman Ayday,Jean-Pierre Hubaux,Amalio Telenti",https://doi.org/10.1145/3035538,TOPS,2017,[],
90,,"Measuring, Characterizing, and Detecting Facebook Like Farms.","Muhammad Ikram,Lucky Onwuzurike,Shehroze Farooqi,Emiliano De Cristofaro,Arik Friedman,Guillaume Jourjon,Mohamed Ali Kâafar,M. Zubair Shafiq",https://doi.org/10.1145/3121134,TOPS,2017,[],
91,,Pulse-Response - Exploring Human Body Impedance for Biometric Recognition.,"Ivan Martinovic,Kasper Bonne Rasmussen,Marc Roeschlin,Gene Tsudik",https://doi.org/10.1145/3064645,TOPS,2017,[],
92,,Authentication Challenges in a Global Environment.,"Stephanos Matsumoto,Raphael M. Reischuk,Pawel Szalachowski,Tiffany Hyun-Jin Kim,Adrian Perrig",https://doi.org/10.1145/3007208,TOPS,2017,[],
93,,Toward Improved Audio CAPTCHAs Based on Auditory Perception and Language Understanding.,"Hendrik Meutzner,Santosh Gupta,Viet-Hung Nguyen,Thorsten Holz,Dorothea Kolossa",https://doi.org/10.1145/2856820,TOPS,2017,[],
94,,Efficient Attack Graph Analysis through Approximate Inference.,"Luis Muñoz-González,Daniele Sgandurra,Andrea Paudice,Emil C. Lupu",https://doi.org/10.1145/3105760,TOPS,2017,[],
95,,Sancus 2.0 - A Low-Cost Security Architecture for IoT Devices.,"Job Noorman,Jo Van Bulck,Jan Tobias Mühlberg,Frank Piessens,Pieter Maene,Bart Preneel,Ingrid Verbauwhede,Johannes Götzfried,Tilo Müller,Felix C. Freiling",https://doi.org/10.1145/3079763,TOPS,2017,[],
96,,Iterative Analysis to Improve Key Properties of Critical Human-Intensive Processes - An Election Security Example.,"Leon J. Osterweil,Matt Bishop,Heather M. Conboy,Huong Phan,Borislava I. Simidchieva,George S. Avrunin,Lori A. Clarke,Sean Peisert",https://doi.org/10.1145/3041041,TOPS,2017,[],
97,,Fast Proxy Re-Encryption for Publish/Subscribe Systems.,"Yuriy Polyakov,Kurt Rohloff,Gyana Sahu,Vinod Vaikuntanathan",https://doi.org/10.1145/3128607,TOPS,2017,[],
98,,"Mo(bile) Money, Mo(bile) Problems - Analysis of Branchless Banking Applications.","Bradley Reaves,Jasmine Bowers,Nolen Scaife,Adam Bates 0001,Arnav Bhartiya,Patrick Traynor,Kevin R. B. Butler",https://doi.org/10.1145/3092368,TOPS,2017,[],
99,,Pareto Optimal Security Resource Allocation for Internet of Things.,"Antonino Rullo,Daniele Midi,Edoardo Serra,Elisa Bertino",https://doi.org/10.1145/3139293,TOPS,2017,[],
100,,Privacy Games Along Location Traces - A Game-Theoretic Framework for Optimizing Location Privacy.,"Reza Shokri,George Theodorakopoulos 0001,Carmela Troncoso",https://doi.org/10.1145/3009908,TOPS,2017,[],
101,,Long-Span Program Behavior Modeling and Attack Detection.,"Xiaokui Shu,Danfeng (Daphne) Yao,Naren Ramakrishnan,Trent Jaeger",https://doi.org/10.1145/3105761,TOPS,2017,[],
102,1,Differentially Private K-Means Clustering and a Hybrid Approach to Private Optimization.,"Dong Su,Jianneng Cao,Ninghui Li,Elisa Bertino,Min Lyu,Hongxia Jin",https://doi.org/10.1145/3133201,TOPS,2017,[],
103,,Texture to the Rescue - Practical Paper Fingerprinting Based on Texture Patterns.,"Ehsan Toreini,Siamak F. Shahandashti,Feng Hao 0001",https://doi.org/10.1145/3092816,TOPS,2017,[],
104,,Evaluating the Strength of Genomic Privacy Metrics.,Isabel Wagner,https://doi.org/10.1145/3020003,TOPS,2017,[],
1214,,Incremental Frequent Subgraph Mining on Large Evolving Graphs.,"Ehab Abdelhamid,Mustafa Canim,Mohammad Sadoghi,Bishwaranjan Bhattacharjee,Yuan-Chi Chang,Panos Kalnis",https://doi.org/10.1109/TKDE.2017.2743075,TKDE,2017,"Indexes,Data mining,Graphical models,Memory management,Itemsets,Computer security","Frequent subgraph mining is a core graph operation used in many domains, such as graph data management and knowledge exploration, bioinformatics, and security. Most existing techniques target static graphs. However, modern applications, such as social networks, utilize large evolving graphs. Mining these graphs using existing techniques is infeasible, due to the high computational cost. In this paper, we propose IncGM+, a fast incremental approach for continuous frequent subgraph mining on a single large evolving graph. We adapt the notion of “fringe” to the graph context, that is the set of subgraphs on the border between frequent and infrequent subgraphs. IncGM+ maintains fringe subgraphs and exploits them to prune the search space. To boost the efficiency, we propose an efficient index structure to maintain selected embeddings with minimal memory overhead. These embeddings are utilized to avoid redundant expensive subgraph isomorphism operations. Moreover, the proposed system supports batch updates. Using large real-world graphs, we experimentally verify that IncGM+ outperforms existing methods by up to three orders of magnitude, scales to much larger graphs and consumes less memory."
1215,,Beautiful and Damned. Combined Effect of Content Quality and Social Ties on User Engagement.,"Luca Maria Aiello,Rossano Schifanella,Miriam Redi,Stacey Svetlichnaya,Frank Liu,Simon Osindero",https://doi.org/10.1109/TKDE.2017.2747552,TKDE,2017,"Flickr,Production,Visualization,Content-based retrieval,Computer vision,Social network services","User participation in online communities is driven by the intertwinement of the social network structure with the crowd-generated content that flows along its links. These aspects are rarely explored jointly and at scale. By looking at how users generate and access pictures of varying beauty on Flickr, we investigate how the production of quality impacts the dynamics of online social systems. We develop a deep learning computer vision model to score images according to their aesthetic value and we validate its output through crowdsourcing. By applying it to over 15 B Flickr photos, we study for the first time how image beauty is distributed over a large-scale social system. Beautiful images are evenly distributed in the network, although only a small core of people get social recognition for them. To study the impact of exposure to quality on user engagement, we set up matching experiments aimed at detecting causality from observational data. Exposure to beauty is double-edged: following people who produce high-quality content increases one's probability of uploading better photos; however, an excessive imbalance between the quality generated by a user and the user's neighbors leads to a decline in engagement. Our analysis has practical implications for improving link recommender systems."
1216,,Sparse Poisson Latent Block Model for Document Clustering.,"Melissa Ailem,François Role,Mohamed Nadif",https://doi.org/10.1109/TKDE.2017.2681669,TKDE,2017,"Clustering algorithms,Data models,Sparse matrices,Partitioning algorithms,Mixture models,Algorithm design and analysis,Text mining","Over the last decades, several studies have demonstrated the importance of co-clustering to simultaneously produce groups of objects and features. Even to obtain object clusters only, using co-clustering is often more effective than one-way clustering, especially when considering sparse high dimensional data. In this paper, we present a novel generative mixture model for co-clustering such data. This model, the Sparse Poisson Latent Block Model (SPLBM), is based on the Poisson distribution, which arises naturally for contingency tables, such as document-term matrices. The advantages of SPLBM are two-fold. First, it is a rigorous statistical model which is also very parsimonious. Second, it has been designed from the ground up to deal with data sparsity problems. As a consequence, in addition to seeking homogeneous blocks, as other available algorithms, it also filters out homogeneous but noisy ones due to the sparsity of the data. Experiments on various datasets of different size and structure show that an algorithm based on SPLBM clearly outperforms state-of-the-art algorithms. Most notably, the SPLBM-based algorithm presented here succeeds in retrieving the natural cluster structure of difficult, unbalanced datasets which other known algorithms are unable to handle effectively."
1217,,Wellness Representation of Users in Social Media - Towards Joint Modelling of Heterogeneity and Temporality.,"Mohammad Akbari 0001,Xia Hu,Fei Wang,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2017.2722411,TKDE,2017,"Social network services,Sociology,Statistics,Matrix decomposition,Diabetes,Data models","The increasing popularity of social media has encouraged health consumers to share, explore, and validate health and wellness information on social networks, which provide a rich repository of Patient Generated Wellness Data (PGWD). While data-driven healthcare has attracted a lot of attention from academia and industry for improving care delivery through personalized healthcare, limited research has been done on harvesting and utilizing PGWD available on social networks. Recently, representation learning has been widely used in many applications to learn low-dimensional embedding of users. However, existing approaches for representation learning are not directly applicable to PGWD due to its domain nature as characterized by longitudinality, incompleteness, and sparsity of observed data as well as heterogeneity of the patient population. To tackle these problems, we propose an approach which directly learns the embedding from longitudinal data of users, instead of vector-based representation. In particular, we simultaneously learn a low-dimensional latent space as well as the temporal evolution of users in the wellness space. The proposed method takes into account two types of wellness prior knowledge: (1) temporal progression of wellness attributes; and (2) heterogeneity of wellness attributes in the patient population. Our approach scales well to large datasets using parallel stochastic gradient descent. We conduct extensive experiments to evaluate our framework at tackling three major tasks in wellness domain: attribute prediction, success prediction, and community detection. Experimental results on two real-world datasets demonstrate the ability of our approach in learning effective user representations."
1218,,A Semi-NMF-PCA Unified Framework for Data Clustering.,"Kais Allab,Lazhar Labiod,Mohamed Nadif",https://doi.org/10.1109/TKDE.2016.2606098,TKDE,2017,"Principal component analysis,Clustering algorithms,Linear programming,Partitioning algorithms,Optimization,Matrix decomposition,Clustering methods","In this work, we propose a novel way to consider the clustering and the reduction of the dimension simultaneously. Indeed, our approach takes advantage of the mutual reinforcement between data reduction and clustering tasks. The use of a low-dimensional representation can be of help in providing simpler and more interpretable solutions. We show that by doing so, our model is able to better approximate the relaxed continuous dimension reduction solution by the true discrete clustering solution. Experiment results show that our method gives better results in terms of clustering than the state-of-the-art algorithms devoted to similar tasks for data sets with different proprieties."
1219,,QDA - A Query-Driven Approach to Entity Resolution.,"Hotham Altwaijry,Dmitri V. Kalashnikov,Sharad Mehrotra",https://doi.org/10.1109/TKDE.2016.2623607,TKDE,2017,"Cleaning,Erbium,Merging,Semantics,Real-time systems,Standards,Clustering algorithms","This paper addresses the problem of query-aware data cleaning in the context of a user query. In particular, we develop a novel Query-Driven Approach (QDA) that systematically exploits the semantics of the predicates in SQL-like selection queries to reduce the data cleaning overhead. The objective of QDA is to issue the minimum number of cleaning steps that are necessary to answer a given SQL-like selection correctly. The comprehensive empirical evaluation of QDA demonstrates outstanding results - that is QDA is significantly better compared to traditional ER techniques, especially when the query is very selective."
1220,,Mining Top-k Co-Occurrence Patterns across Multiple Streams.,"Daichi Amagata,Takahiro Hara",https://doi.org/10.1109/TKDE.2017.2728537,TKDE,2017,"Data mining,Real-time systems,Monitoring,Heuristic algorithms,Data structures,Silicon,Social network services","The recent Bigdata and IoTera has presented a number of applications that generate objects in a streaming fashion. It is well-known that real-time mining of important patterns from data streams support many domains. In retail markets and social network services, for example, such patterns are itemsets and words that frequently appear in many user-accounts, i.e., co-occurrence patterns. To efficiently monitor co-occurrence patterns, we address the novel problem of mining top-k closed co-occurrence patterns across multiple streams. We employ sliding window setting in this problem, and each pattern is ranked based on count, which is the number of streams that have generated the pattern. Since objects are consecutively generated and deleted, the count of a given pattern is dynamic, which may change the rank of the pattern. This renders a challenge to monitoring the top-k answer in real-time. We propose an index-based algorithm that addresses the challenge and provides the exact answer. Specifically, we propose the CP-Graph, a hybrid index of graph and inverted file structures. The CP-Graph can efficiently compute the count of a given pattern and update the answer while pruning unnecessary patterns. Our experimental study on real datasets demonstrates the efficiency and scalability of our solution."
1221,,Supervised Taxonomies - Algorithms and Applications.,"Paul K. Amalaman,Christoph F. Eick,Chong Wang",https://doi.org/10.1109/TKDE.2017.2698451,TKDE,2017,"Taxonomy,Clustering algorithms,Complexity theory,Classification algorithms,Tools,Phylogeny,Organisms","This paper focuses on a new type of taxonomy called supervised taxonomy (ST). Supervised taxonomies are generated considering background information concerning class labels in addition to distance metrics, and are capable of capturing class-uniform regions in a dataset. A hierarchical, agglomerative clustering algorithm, called STAXAC that generates STs is proposed and its properties are analyzed. Experimental results are presented that show that STAXAC produces purer taxonomies than the neighbor-joining (NJ) algorithm - a very popular taxonomy generation algorithm. We introduced novel measures and algorithms that assess classification complexity, class modality, and show that STs can be used as the main input of an effective data-editing tool to enhance the accuracy of k-nearest neighbor classifiers. We demonstrated in our experimental evaluation that assessing the classification complexity of a ST provides us with a good estimate of the difficulty of the classification problem at hand. Moreover, a class modality discovery tool (CMD) has been provided that - based on a domain expert's notion of what constitutes a “note-worthy” subclass-determines if specific classes in the dataset are zero-modal, unimodal, and multi-modal."
1222,,gMark - Schema-Driven Generation of Graphs and Queries.,"Guillaume Bagan,Angela Bonifati,Radu Ciucanu,George H. L. Fletcher,Aurélien Lemay,Nicky Advokaat",https://doi.org/10.1109/TKDE.2016.2633993,TKDE,2017,"Benchmark testing,Query processing,Database languages,Estimation,Generators","Massive graph data sets are pervasive in contemporary application domains. Hence, graph database systems are becoming increasingly important. In the experimental study of these systems, it is vital that the research community has shared solutions for the generation of database instances and query workloads having predictable and controllable properties. In this paper, we present the design and engineering principles of gMark, a domainand query language-independent graph instance and query workload generator. A core contribution of gMark is its ability to target and control the diversity of properties of both the generated instances and the generated workloads coupled to these instances. Further novelties include support for regular path queries, a fundamental graph query paradigm, and schema-driven selectivity estimation of queries, a key feature in controlling workload chokepoints. We illustrate the flexibility and practical usability of gMark by showcasing the framework's capabilities in generating high quality graphs and workloads, and its ability to encode user-defined schemas across a variety of application domains."
1223,,Associated Activation-Driven Enrichment - Understanding Implicit Information from a Cognitive Perspective.,"Jie Bai,Linjing Li,Daniel Zeng,Qiudan Li",https://doi.org/10.1109/TKDE.2017.2745565,TKDE,2017,"Text mining,Semantics,Psychology,Text analysis,Knowledge based systems,Cognition,Data models","In this paper, we propose a novel text representation paradigm and a set of follow-up text representation models based on cognitive psychology theories. The intuition of our study is that the knowledge implied in a large collection of documents may improve the understanding of single documents. Based on cognitive psychology theories, we propose a general text enrichment framework, study the key factors to enable activation of implicit information, and develop new text representation methods to enrich text with the implicit information. Our study aims to mimic some aspects of human cognitive procedure in which given stimulant words serve to activate understanding implicit concepts. By incorporating human cognition into text representation, the proposed models advance existing studies by mining implicit information from given text and coordinating with most existing text representation approaches at the same time, which essentially bridges the gap between explicit and implicit information. Experiments on multiple tasks show that the implicit information activated by our proposed models matches human intuition and significantly improves the performance of the text mining tasks as well."
1224,,A Non-Parametric Algorithm for Discovering Triggering Patterns of Spatio-Temporal Event Types.,"Berna Bakir Batu,Tugba Taskaya-Temizel,H. Sebnem Düzgün",https://doi.org/10.1109/TKDE.2017.2754252,TKDE,2017,"Stochastic processes,Adaptation models,Data mining,Clustering methods,Space-time codes,Computational modeling","Temporal or spatio-temporal sequential pattern discovery is a well-recognized important problem in many domains like seismology, criminology, and finance. The majority of the current approaches are based on candidate generation which necessitates parameter tuning, namely, definition of a neighborhood, an interest measure, and a threshold value to evaluate candidates. However, their performance is limited as the success of these methods relies heavily on parameter settings. In this paper, we propose an algorithm which uses a nonparametric stochastic de-clustering procedure and a multivariate Hawkes model to define triggering relations within and among the event types and employs the estimated model to extract significant triggering patterns of event types. We tested the proposed method with real and synthetic data sets exhibiting different characteristics. The method gives good results that are comparable with the methods based on candidate generation in the literature."
1225,,VISUAL - Simulation of Visual Subgraph Query Formulation to Enable Automated Performance Benchmarking.,"Sourav S. Bhowmick,Huey-Eng Chua,Byron Choi,Curtis E. Dyreson",https://doi.org/10.1109/TKDE.2017.2690392,TKDE,2017,"Visualization,Graphical user interfaces,Usability,Human computer interaction,Benchmark testing,Visual databases","Visual graph interfaces improve the usability of graph databases by making it easier for users to formulate queries. Recently, a variety of interactive query formulation-based techniques (e.g., blending of visual query construction and processing, visual query suggestions) have been proposed to enhance query performance and usability. Comprehensive user studies are needed to exhaustively and systematically evaluate performance of the proposed techniques, but, unfortunately, user studies are expensive and time consuming. To reduce the cost and time needed, we present a novel synthetic visual subgraph query simulator called VISUAL. VISUAL realistically simulates subgraph query construction without requiring human users. It can automatically generate test subgraph queries having different user-specified characteristics by utilizing the underlying indexes and simulate their formulation based on different query formulation sequences. A key feature of this simulator is that it is built on top of an HCI-inspired, extensible quantitative model which enables us to model the visual query formulation process quantitatively. Our experimental study demonstrates the effectiveness of VISUAL in accurately simulating visual subgraph queries."
1226,,Probabilistic Keys.,"Pieta Brown,Sebastian Link",https://doi.org/10.1109/TKDE.2016.2633342,TKDE,2017,"Probabilistic logic,Query processing,Semantics,Indexes,Data models","Probabilistic databases address well the requirements of an increasing number of modern applications that produce large volumes of uncertain data from a variety of sources. Probabilistic keys enforce the integrity of entities in order to facilitate data processing in probabilistic database systems. For this purpose, we establish algorithms for an agile schema-and data-driven elicitation of the marginal probability by which keys should hold in a given application domain, and for reasoning about these keys. The efficiency of our elicitation framework is demonstrated theoretically and experimentally."
1227,,Synopsis - A Distributed Sketch over Voluminous Spatiotemporal Observational Streams.,"Thilina Buddhika,Matthew Malensek,Sangmi Lee Pallickara,Shrideep Pallickara",https://doi.org/10.1109/TKDE.2017.2734661,TKDE,2017,"Spatiotemporal phenomena,Query processing,Geospatial analysis,Feature extraction,Distributed databases,Vegetation,Real-time systems","Networked observational devices have proliferated in recent years, contributing to voluminous data streams from a variety of sources and problem domains. These streams often have a spatiotemporal component and include multidimensional features of interest. Processing such data in an offline fashion using batch systems or data warehouses is costly from both a storage and computational standpoint, and in many situations the insights derived from the data streams are useful only if they are timely. In this study, we propose SYNOPSIS, an online, distributed sketch that is constructed from voluminous spatiotemporal data streams. The sketch summarizes feature values and inter-feature relationships in memory to facilitate real-time query evaluations and to serve as input to computations expressed using analytical engines. As the data streams evolve, SYNOPSIS performs targeted dynamic scaling to ensure high accuracy and effective resource utilization. We evaluate our system in the context of two real-world spatiotemporal datasets and demonstrate its efficacy in both scalability and query evaluations."
1228,,Energy-Efficient Query Processing in Web Search Engines.,"Matteo Catena,Nicola Tonellotto",https://doi.org/10.1109/TKDE.2017.2681279,TKDE,2017,"Query processing,Energy consumption,Web search,Engines,Servers,Time factors,Indexes","Web search engines are composed by thousands of query processing nodes, i.e., servers dedicated to process user queries. Such many servers consume a significant amount of energy, mostly accountable to their CPUs, but they are necessary to ensure low latencies, since users expect sub-second response times (e.g., 500 ms). However, users can hardly notice response times that are faster than their expectations. Hence, we propose the Predictive Energy Saving Online Scheduling Algorithm (
<inline-formula><tex-math notation=""LaTeX"">$\sf{PESOS}$</tex-math> </inline-formula>
) to select the most appropriate CPU frequency to process a query on a per-core basis. 
<inline-formula><tex-math notation=""LaTeX""> $\sf{PESOS}$</tex-math> </inline-formula>
 aims at process queries by their deadlines, and leverage high-level scheduling information to reduce the CPU energy consumption of a query processing node. 
<inline-formula><tex-math notation=""LaTeX""> $\sf{PESOS}$</tex-math> </inline-formula>
 bases its decision on query efficiency predictors, estimating the processing volume and processing time of a query. We experimentally evaluate 
<inline-formula><tex-math notation=""LaTeX"">$\sf{PESOS}$ </tex-math></inline-formula>
 upon the TREC ClueWeb09B collection and the MSN2006 query log. Results show that 
<inline-formula><tex-math notation=""LaTeX""> $\sf{PESOS}$</tex-math> </inline-formula>
 can reduce the CPU energy consumption of a query processing node up to 
<inline-formula> <tex-math notation=""LaTeX"">${\sim}$</tex-math></inline-formula>
48 percent compared to a system running at maximum CPU core frequency. 
<inline-formula><tex-math notation=""LaTeX"">$\sf{PESOS}$</tex-math> </inline-formula>
 outperforms also the best state-of-the-art competitor with a 
<inline-formula><tex-math notation=""LaTeX"">${\sim}$</tex-math> </inline-formula>
20 percent energy saving, while the competitor requires a fine parameter tuning and it may incurs in uncontrollable latency violations."
1229,,Efficient Sub-Window Nearest Neighbor Search on Matrix.,"Tsz Nam Chan,Man Lung Yiu,Kien A. Hua",https://doi.org/10.1109/TKDE.2016.2633357,TKDE,2017,"Search problems,Upper bound,Nearest neighbor searches,Satellites,Clouds,Shape,Junctions","We study a nearest neighbor search problem on a matrix by its element values. Given a data matrix D and a query matrix q, the sub-window nearest neighbor search problem finds a sub-window of D that is the most similar to q. This problem has a wide range of applications, e.g., geospatial data integration, object detection, and motion estimation. In this paper, we propose an efficient progressive search solution that overcomes the drawbacks of existing solutions. First, we present a generic approach to build level based lower bound functions on top of basic lower bound functions. Second, we develop a novel lower bound function for a group of sub-windows, in order to boost the efficiency of our solution. Furthermore, we extend our solution to support irregular-shaped queries. Experimental results on real data demonstrate the efficiency of our proposed methods."
1230,,pSCAN - Fast and Exact Structural Graph Clustering.,"Lijun Chang,Wei Li 0109,Lu Qin,Wenjie Zhang 0001,Shiyu Yang",https://doi.org/10.1109/TKDE.2016.2618795,TKDE,2017,"Optimization,Maintenance engineering,Data models,Social network services,Collaboration,Biology,Computers","We study the problem of structural graph clustering, a fundamental problem in managing and analyzing graph data. Given an undirected unweighted graph, structural graph clustering is to assign vertices to clusters, and to identify the sets of hub vertices and outlier vertices as well, such that vertices in the same cluster are densely connected to each other while vertices in different clusters are loosely connected. In this paper, we develop a new two-step paradigm for scalable structural graph clustering based on our three observations. Then, we present a pSCAN approach, within the paradigm, aiming to reduce the number of structural similarity computations, and propose optimization techniques to speed up checking whether two vertices are structure-similar. pSCAN outputs exactly the same clusters as the existing approaches SCAN and SCAN++, and we prove that pSCAN is worst-case optimal. Moreover, we propose efficient techniques for updating the clusters when the input graph dynamically changes, and we also extend our techniques to other similarity measures, e.g., Jaccard similarity. Performance studies on large real and synthetic graphs demonstrate the efficiency of our new approach and our dynamic cluster maintenance techniques. Noticeably, for the twitter graph with 1 billion edges, our approach takes 25 minutes while the state-of-the-art approach cannot finish even after 24 hours."
1231,,Efficient Metric Indexing for Similarity Search and Similarity Joins.,"Lu Chen 0001,Yunjun Gao,Xinhan Li,Christian S. Jensen,Gang Chen 0001",https://doi.org/10.1109/TKDE.2015.2506556,TKDE,2017,"Indexes,Extraterrestrial measurements,Search problems,Query processing,Acceleration,Clustering algorithms","Spatial queries including similarity search and similarity joins are useful in many areas, such as multimedia retrieval, data integration, and so on. However, they are not supported well by commercial DBMSs. This may be due to the complex data types involved and the needs for flexible similarity criteria seen in real applications. In this paper, we propose a versatile and efficient disk-based index for metric data, the Space-fillingcurve and Pivot-based B
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
-tree (SPB-tree). This index leverages the B
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
-tree, and uses space-filling curve to cluster data into compact regions, thus achieving storage efficiency. It utilizes a small set of so-called pivots to reduce significantly the number of distance computations when using the index. Further, it makes use of a separate random access file to support abroad range of data. By design, it is easyto integrate the SPB-tree into an existing DBMS. We present efficient algorithms for processing similarity search and similarity joins, as well as corresponding cost models based on SPB-trees. Extensive experiments using both real and synthetic data show that, compared with state-of-the-art competitors, the SPB-tree has much lower construction cost, smallerstorage size, and supports more efficient similarity search and similarity joins with high accuracy cost models."
1232,,Towards Optimal Connectivity on Multi-Layered Networks.,"Chen Chen 0022,Jingrui He,Nadya Bliss,Hanghang Tong",https://doi.org/10.1109/TKDE.2017.2719026,TKDE,2017,"Optimization,Silicon,Power generation,Transportation,Weight measurement,Collaboration,Complexity theory","Networks are prevalent in many high impact domains. Moreover, cross-domain interactions are frequently observed in many applications, which naturally form the dependencies between different networks. Such kind of highly coupled network systems are referred to as multi-layered networks, and have been used to characterize various complex systems, including critical infrastructure networks, cyber-physical systems, collaboration platforms, biological systems, and many more. Different from single-layered networks where the functionality of their nodes is mainly affected by within-layer connections, multi-layered networks are more vulnerable to disturbance as the impact can be amplified through cross-layer dependencies, leading to the cascade failure to the entire system. To manipulate the connectivity in multi-layered networks, some recent methods have been proposed based on two-layered networks with specific types of connectivity measures. In this paper, we address the above challenges in multiple dimensions. First, we propose a family of connectivity measures (SUBLINE) that unifies a wide range of classic network connectivity measures. Third, we reveal that the connectivity measures in the SUBLINE family enjoy diminishing returns property, which guarantees a near-optimal solution with linear complexity for the connectivity optimization problem. Finally, we evaluate our proposed algorithm on real data sets to demonstrate its effectiveness and efficiency."
1233,,Scaling Up Markov Logic Probabilistic Inference for Social Graphs.,"Haiquan Chen,Wei-Shinn Ku,Haixun Wang,Liang Tang,Min-Te Sun",https://doi.org/10.1109/TKDE.2016.2625251,TKDE,2017,"Social network services,Markov processes,Probabilistic logic,Knowledge engineering,Graphical models,Cognition,Computer science","Link prediction is a fundamental problem in social network analysis. Although the link prediction problem is not new, the challenge of how to exploit various existing network information, such as network structure data and node attribute data, to enable AI-style knowledge inference for large social networks still remains unsolved. In this paper, we design and implement a scalable framework that treats link prediction as knowledge reasoning using Markov Logic Networks (MLNs). Differing from other probabilistic graphical models, MLNs allow undirected relationships with cycles and long-range (non-adjacent) dependency, which are essential and abound in social networks. In our framework, the prior knowledge is captured as the structure dependency (such as friendship) and the attribute dependency (such as social communities) in terms of inference rules, associated with uncertainty represented as probabilities. Next, we employ the random walk to discover the inference subgraph, on which probabilistic inference is performed, so that the required computation and storage cost can be significantly reduced without much sacrifice of the inference accuracy. Our extensive experiments with real-world datasets verify the superiority of our proposed approaches over two baseline methods and show that our approaches are able to provide a tunable tradeoff between inference accuracy and efficiency."
1234,,User Satisfaction Prediction with Mouse Movement Information in Heterogeneous Search Environment.,"Ye Chen,Yiqun Liu 0001,Min Zhang 0006,Shaoping Ma",https://doi.org/10.1109/TKDE.2017.2739151,TKDE,2017,"Mice,Search engines,Performance evaluation,Data mining,Feature extraction,Search problems,Metasearch","Satisfaction prediction is one of the prime concerns in search performance evaluation. It is a non-trivial task for three major reasons: (1) The definition of satisfaction is subjective and different users may have different opinions in the process of satisfaction judgment. (2) Most existing studies on satisfaction prediction mainly rely on users' click-through or query reformulation behaviors but there are many sessions without such interactions. (3) Most existing works primarily rely on the hypothesis that all results on search result pages (SERPs) are homogeneous, but a variety of heterogeneous search results have been aggregated into SERPs to improve the diversity and quality of search results recently. To shed light on these research questions, we construct an experimental search engine that could collect users' satisfaction feedback as well as mouse click-through/movement data. Inspired by recent studies in predicting search result relevance based on mouse movement patterns (namely, motifs), we propose to estimate search satisfaction with motifs extracted from mouse movement data on SERPs. Besides the existing frequency-based motif selection method, two novel selection strategies (distance-based and distribution-based) are also adopted to extract high-quality motifs for satisfaction prediction. Experimental results show that the proposed strategies outperform existing methods and have promising generalization capability for unseen users and queries in both a homogeneous and heterogeneous search environment."
1235,,Sample-Based Attribute Selective An DE for Large Data.,"Shenglei Chen,Ana M. Martínez,Geoffrey I. Webb,Limin Wang 0007",https://doi.org/10.1109/TKDE.2016.2608881,TKDE,2017,"Niobium,Bayes methods,Training,Training data,Information technology,Australia,Memory management","More and more applications have come with large data sets in the past decade. However, existing algorithms cannot guarantee to scale well on large data. Averaged n-Dependence Estimators (AnDE) allows for flexible learning from out-of-core data, by varying the value of n (number of super parents). Hence, AnDE is especially appropriate for large data learning. In this paper, we propose a sample-based attribute selection technique for AnDE. It needs one more pass through the training data, in which a multitude of approximate AnDE models are built and efficiently assessed by leave-one-out cross validation. The use of a sample reduces the training time. Experiments on 15 large data sets demonstrate that the proposed technique significantly reduces AnDE's error at the cost of a modest increase in training time. This efficient and scalable out-of-core approach delivers superior or comparable performance to typical in-core Bayesian network classifiers."
1236,,Metric Similarity Joins Using MapReduce.,"Gang Chen 0001,Keyu Yang,Lu Chen 0001,Yunjun Gao,Baihua Zheng,Chun Chen 0001",https://doi.org/10.1109/TKDE.2016.2631599,TKDE,2017,"Filtering,Extraterrestrial measurements,Load management,Algorithm design and analysis,Cleaning,Data mining","Given two object sets Q and O, a metric similarity join finds similar object pairs according to a certain criterion. This operation has a wide variety of applications in data cleaning and data mining, to name but a few. However, the rapidly growing volume of data nowadays challenges traditional metric similarity join methods, and thus, a distributed method is required. In this paper, we adopt a popular distributed framework, namely, MapReduce, to support scalable metric similarity joins. To ensure the load balancing, we present two sampling based partition methods. One utilizes the pivot and the space-filling curve mappings to cluster the data into one-dimensional space, and then selects high quality centroids to enable equal-sized partitions. The other uses the KD-tree partitioning technique to equally divide the data after the pivot mapping. To avoid unnecessary object pair evaluation, we propose a framework that maps the two involved object sets in order, where the range-object filtering, the double-pivot filtering, the pivot filtering, and the plane sweeping techniques are utilized for pruning. Extensive experiments with both real and synthetic data sets demonstrate that our solutions outperform significantly existing state-of-the-art competitors."
1237,,Time-Aware Boolean Spatial Keyword Queries.,"Gang Chen 0001,Jingwen Zhao,Yunjun Gao,Lei Chen 0002,Rui Chen",https://doi.org/10.1109/TKDE.2017.2742956,TKDE,2017,"Indexes,Query processing,Search problems,Computer science,Search engines","With advances in geo-positioning technologies and mobile internet, location-based services have attracted much attention, and spatial keyword queries are catching on fast. However, as far as we aware, no prior work considers the temporal information of geotagged objects. Temporal information is important in the spatial keyword query because many objects are not always valid. For example, visitors may plan their trips according to the opening time of attractions. In this paper, we identify and solve a novel problem, i.e., the time-aware Boolean spatial keyword query (TABSKQ), which returns the k objects that satisfy users' spatio-temporal description and textual constraint. We first present pruning strategies and algorithm based on the CIR
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
-tree (i.e., the CIR-tree with temporal information). Then, we propose an efficient index structure, called the TA-tree, and its corresponding algorithms, which can prune the search space using both spatio-temporal and textual information. Furthermore, we study an interesting TABSKQ variant, i.e., Joint TABSKQ (JTABSKQ), which aims to process a set of TABSKQs jointly, and extend our techniques to tackle it. Extensive experiments with real datasets offer insight into the performance of our proposed indices and algorithms."
1238,,Extracting Kernel Dataset from Big Sensory Data in Wireless Sensor Networks.,"Siyao Cheng,Zhipeng Cai 0001,Jianzhong Li 0001,Hong Gao 0001",https://doi.org/10.1109/TKDE.2016.2645212,TKDE,2017,"Wireless sensor networks,Correlation,Algorithm design and analysis,Distributed algorithms,Kernel,Redundancy,Complexity theory","The amount of sensory data manifests an explosive growth due to the increasing popularity of Wireless Sensor Networks (WSNs). The scale of sensory data in many applications has already exceeded several petabytes annually, which is beyond the computation and transmission capabilities of conventional WSNs. On the other hand, the information carried by big sensory data has high redundancy because of strong correlation among sensory data. In this paper, we introduce the novel concept of ϵ-Kernel Dataset, which is only a small data subset and can represent the vast information carried by big sensory data with the information loss rate being less than ϵ, where ϵ can be arbitrarily small. We prove that drawing the minimum ϵ-Kernel Dataset is polynomial time solvable and provide a centralized algorithm with O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
) time complexity. Furthermore, a distributed algorithm with constant complexity O(1) is designed. It is shown that the result returned by the distributed algorithm can satisfy the ϵ requirement with a near optimal size. Furthermore, two distributed algorithms of maintaining the correlation coefficients among sensor nodes are developed. Finally, the extensive real experiment results and simulation results are presented. The results indicate that all the proposed algorithms have high performance in terms of accuracy and energy efficiency."
1239,,An Empirical Evaluation of Techniques for Ranking Semantic Associations.,"Gong Cheng 0001,Fei Shao,Yuzhong Qu",https://doi.org/10.1109/TKDE.2017.2735970,TKDE,2017,"Semantics,Ontologies,Search engines,National security,Bioinformatics,Frequency measurement,Computers","Searching for associations between entities is needed in many domains like national security and bioinformatics. In recent years, it has been facilitated by the emergence of graph-structured semantic data on the Web, which offers structured semantic associations more explicit than those hiding in unstructured text for computers to discover. The increasing volume of semantic data often produces excessively many semantic associations, and requires ranking techniques to identify the more important ones for users. Despite the fruitful theoretical research on innovative ranking techniques, there is a lack of comprehensive empirical evaluation of these techniques. In this article, we carry out an extensive evaluation of eight techniques for ranking semantic associations, including two novel ones we propose. The practical effectiveness of these techniques is assessed based on 1,200 ground-truth rankings created by 30 human experts for real-life semantic associations and the explanations given by the experts. Our findings also suggest a number of directions in improving existing techniques and developing novel techniques for future work."
1240,,A Web Services Discovery Approach Based on Mining Underlying Interface Semantics.,"Bo Cheng 0001,Shuai Zhao 0001,Changbao Li,Junliang Chen",https://doi.org/10.1109/TKDE.2016.2645769,TKDE,2017,"Web services,Semantics,Ontologies,Metadata,Clustering algorithms,Vocabulary,Semantic Web","In recent years, Web service discovery has been a hot research topic. In this paper, we propose a novel Web services discovery approach, which can mine the underlying semantic structures of interaction interface parameters to help users find and employ Web services, and can match interfaces with high precision when the parameters of those interfaces contain meaningful synonyms, abbreviations, and combinations of disordered fragments. Our approach is based on mining the underlying semantics. First, we propose a conceptual Web services description model in which we include the type path for the interaction interface parameters in addition to the traditional text description. Then, based on this description model, we mine the underlying semantics of the interaction interface to create index libraries by clustering interaction interface names and fragments under the supervision of co-occurrence probability. This index library can help provide a high-efficiency interface that can match not only synonyms but also abbreviations and fragment combinations. Finally, we propose a Web service Operations Discovery algorithm (OpD). The OpD discovery results include two types of Web services: services with “Single” operations and services with “Composite” operations. The experimental evaluation shows that our approach performs better than other Web service discovery methods in terms of both discovery time and precision/ recall rate."
1241,,A Multi-Objective Optimization Approach for Question Routing in Community Question Answering Services.,"Xiang Cheng 0003,Shuguang Zhu,Sen Su,Gang Chen",https://doi.org/10.1109/TKDE.2017.2696008,TKDE,2017,"Routing,Training,Feature extraction,Knowledge discovery,Optimization,History,Training data","Community Question Answering (CQA) has increasingly become an important service for people asking questions and providing answers online, which enables people to help each other by sharing knowledge. Recently, with accumulation of users and contents, much concern has arisen over the efficiency and answer quality of CQA services. To address this problem, question routing has been proposed which aims at routing new questions to suitable answerers, who have both high possibility and high ability to answer the questions. In this paper, we formulate question routing as a multi-objective ranking problem, and present a multi-objective learning-to-rank approach for question routing (MLQR), which can simultaneously optimize the answering possibility and answer quality of routed users. In MLQR, realizing that questions are relatively short and usually attached with tags, we first propose a tagword topic model (TTM) to derive topical representations of questions. Based on TTM, we then develop features for each question-user pair, which are captured at both platform level and thread level. In particular, the platform-level features summarize the information of a user from his/her history posts in the CQA platform, while the thread-level features model the pairwise competitions of a user with others in his/her answered threads. Finally, we extend a state-of-the-art learning-to-rank algorithm for training a multi-objective ranking model. Extensive experimental results on real-world datasets show that our MLQR can outperform state-of-the-art methods in terms of both answering possibility and answer quality."
1242,,Efficient Implementation of Newton-Raphson Methods for Sequential Data Prediction.,"Burak C. Civek,Suleyman Serdar Kozat",https://doi.org/10.1109/TKDE.2017.2754380,TKDE,2017,"Prediction algorithms,Computational complexity,Newton method,Big Data applications,Algorithm design and analysis","We investigate the problem of sequential linear data prediction for real life big data applications. The second order algorithms, i.e., Newton-Raphson Methods, asymptotically achieve the performance of the “best” possible linear data predictor much faster compared to the first order algorithms, e.g., Online Gradient Descent. However, implementation of these second order methods results in a computational complexity in the order of 
<inline-formula><tex-math notation=""LaTeX"">$O(M^2)$</tex-math></inline-formula>
 for an 
<inline-formula> <tex-math notation=""LaTeX"">$M$</tex-math></inline-formula>
 dimensional feature vector, where the first order methods offer complexity in the order of 
<inline-formula><tex-math notation=""LaTeX"">$O(M)$</tex-math></inline-formula>
. Because of this extremely high computational need, their usage in real life big data applications is prohibited. To this end, in order to enjoy the outstanding performance of the second order methods, we introduce a highly efficient implementation where the computational complexity of these methods is reduced from 
<inline-formula><tex-math notation=""LaTeX"">$O(M^2)$ </tex-math></inline-formula>
 to 
<inline-formula><tex-math notation=""LaTeX"">$O(M)$</tex-math></inline-formula>
. The presented algorithm provides the well-known merits of the second order methods while offering a computational complexity similar to the first order methods. We do not rely on any statistical assumptions, hence, both regular and fast implementations achieve the same performance in terms of mean square error. We demonstrate the efficiency of our algorithm on several sequential big datasets. We also illustrate the numerical stability of the presented algorithm."
1243,,Filtering Out Infrequent Behavior from Business Process Event Logs.,"Raffaele Conforti,Marcello La Rosa,Arthur H. M. ter Hofstede",https://doi.org/10.1109/TKDE.2016.2614680,TKDE,2017,"Big data,Data mining,Data models,Business,Information analysis,Behavioral science,Filtering,Data analysis","In the era of “big data”, one of the key challenges is to analyze large amounts of data collected in meaningful and scalable ways. The field of process mining is concerned with the analysis of data that is of a particular nature, namely data that results from the execution of business processes. The analysis of such data can be negatively influenced by the presence of outliers, which reflect infrequent behavior or “noise”. In process discovery, where the objective is to automatically extract a process model from the data, this may result in rarely travelled pathways that clutter the process model. This paper presents an automated technique to the removal of infrequent behavior from event logs. The proposed technique is evaluated in detail and it is shown that its application in conjunction with certain existing process discovery algorithms significantly improves the quality of the discovered process models and that it scales well to large datasets."
1244,,FORWARD - A Model for FOrecasting Reservoir WAteR Dynamics Using Spatial Bayesian Network (SpaBN).,"Monidipa Das,Soumya K. Ghosh 0001,Pramesh Gupta,V. M. Chowdary,Ravoori Nagaraja,Vinay K. Dadhwal",https://doi.org/10.1109/TKDE.2016.2647240,TKDE,2017,"Reservoirs,Bayes methods,Predictive models,Forecasting,Soil,Meteorological factors","Natural systems, like the hydrological, climatological, atmospheric, or any other environmental processes, are extremely complex as well as dynamic in nature. It is therefore difficult to forecast, analyze, and quantify these processes by using simple empirical equations. Modeling and forecasting of reservoir water dynamics are not exceptions in this respect, as these involve various challenges due to the effect of meteorological factors, natural processes of stream flow, climatic change, and so on. The intent of our present work is to propose a novel forecasting model, FORWARD, that handles some of these issues in complex reservoir dynamics. FORWARD is based on a variant of spatial Bayesian network (SpaBN), having inherent capability of modeling impact of spatial variability of meteorological factors over the river catchment. The forecasting efficiency of FORWARD has been compared with four other linear and non-linear techniques based on six different statistical performance measures. The experimental results show the superiority of FORWARD over the other techniques. Though FORWARD has been demonstrated with respect to a case study on forecasting reservoir live capacity, the model possesses a generic structure that can also be applied in other domains by introducing minimal augmentation."
1245,,An Ensemble Approach to Link Prediction.,"Liang Duan,Shuai Ma 0001,Charu C. Aggarwal,Tiejun Ma,Jinpeng Huai",https://doi.org/10.1109/TKDE.2017.2730207,TKDE,2017,"Predictive models,Prediction algorithms,Collaboration,Algorithm design and analysis,Estimation,Social network services","A network with n nodes contains O(n
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
) possible links. Even for networks of modest size, it is often difficult to evaluate all pairwise possibilities for links in a meaningful way. Further, even though link prediction is closely related to missing value estimation problems, it is often difficult to use sophisticated models such as latent factor methods because of their computational complexity on large networks. Hence, most known link prediction methods are designed for evaluating the link propensity on a specified subset of links, rather than on the entire networks. In practice, however, it is essential to perform an exhaustive search over the entire networks. In this article, we propose an ensemble enabled approach to scaling up link prediction, by decomposing traditional link prediction problems into subproblems of smaller size. These subproblems are each solved with latent factor models, which can be effectively implemented on networks of modest size. By incorporating with the characteristics of link prediction, the ensemble approach further reduces the sizes of subproblems without sacrificing its prediction accuracy. The ensemble enabled approach has several advantages in terms of performance, and our experimental results demonstrate the effectiveness and scalability of our approach."
1246,,Discovering Newsworthy Themes from Sequenced Data - A Step Towards Computational Journalism.,"Qi Fan,Yuchen Li 0001,Dongxiang Zhang,Kian-Lee Tan",https://doi.org/10.1109/TKDE.2017.2685587,TKDE,2017,"History,Games,Approximation algorithms,Query processing,Electronic mail,Algorithm design and analysis,Engineering profession","Automatic discovery of newsworthy themes from sequenced data can relieve journalists from manually poring over a large amount of data in order to find interesting news. In this paper, we propose a novel 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-Sketch query that aims to find 
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math> </inline-formula>
 striking streaks to best summarize a subject. Our scoring function takes into account streak strikingness and streak coverage at the same time. We study the 
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math> </inline-formula>
-Sketch query processing in both offline and online scenarios, and propose various streak-level pruning techniques to find striking candidates. Among those candidates, we then develop approximate methods to discover the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 most representative streaks with theoretical bounds. We conduct experiments on four real datasets, and the results demonstrate the efficiency and effectiveness of our proposed algorithms: the running time achieves up to 500 times speedup and the quality of the generated summaries is endorsed by the anonymous users from Amazon Mechanical Turk."
1247,,Big Search in Cyberspace.,"Binxing Fang,Yan Jia 0001,Xiaoyong Li,Aiping Li,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2017.2699675,TKDE,2017,"Search engines,Cyberspace,Big Data,Sensors,Web 2.0,Social network services","With the rapid development of big data analytics, mobile computing, Internet of Things, cloud computing, and social networking, cyberspace has expanded to a cross-fused and ubiquitous space made up of human beings, things, and information. Internet applications have evolved from Web 1.0 to Web 2.0 and Web 3.0, and web information has seen an explosive growth, which is strongly promoting the advent of a global era of big data. In this ubiquitous cyberspace, traditional search engines can no longer fully satisfy the evolving needs of various types of users. Therefore, search engines must make completely innovative, revolutionary changes for the next generation of search, which is referred to as “big search”. This paper first studies the development needs of big search. Then, big search is defined, and the 5S properties (Sourcing, Sensing, Synthesizing, Solution, and Security) of big search, which are different from those of traditional search engines, are elaborated. Also, the paper provides a system architecture for big search, explores the key technologies that support the 5S properties, and describes potential application fields of big search technology. Finally, the research opportunities of big search are discussed."
1248,,User-Centric Similarity Search.,"Konstantinos Georgoulas,Akrivi Vlachou,Christos Doulkeridis,Yannis Kotidis",https://doi.org/10.1109/TKDE.2016.2602345,TKDE,2017,"Business,Indexes,Electronic mail,Extraterrestrial measurements,Nearest neighbor searches,Euclidean distance","User preferences play a significant role in market analysis. In the database literature, there has been extensive work on query primitives, such as the well known top-k query that can be used for the ranking of products based on the preferences customers have expressed. Still, the fundamental operation that evaluates the similarity between products is typically done ignoring these preferences. Instead products are depicted in a feature space based on their attributes and similarity is computed via traditional distance metrics on that space. In this work, we utilize the rankings of the products based on the opinions of their customers in order to map the products in a user-centric space where similarity calculations are performed. We identify important properties of this mapping that result in upper and lower similarity bounds, which in turn permit us to utilize conventional multidimensional indexes on the original product space in order to perform these user-centric similarity computations. We show how interesting similarity calculations that are motivated by the commonly used range and nearest neighbor queries can be performed efficiently, while pruning significant parts of the data set based on the bounds we derive on the user-centric similarity of products."
1249,,Bump Hunting in the Dark - Local Discrepancy Maximization on Graphs.,"Aristides Gionis,Michael Mathioudakis,Antti Ukkonen",https://doi.org/10.1109/TKDE.2016.2571693,TKDE,2017,"Twitter,Data mining,Image color analysis,Data models,Electronic mail,Indexes","We study the problem of discrepancy maximization on graphs: given a set of nodes Q of an underlying graph G, we aim to identify a connected subgraph of G that contains many more nodes from Q than other nodes. This variant of the discrepancy maximization problem extends the well-known notion of “bump hunting” in the Euclidean space [1]. We consider the problem under two access models. In the unrestricted-access model, the whole graph G is given as input, while in the local-access model we can only retrieve the neighbors of a given node in G using a possibly slow and costly interface. We prove that the basic problem of discrepancy maximization on graphs is NP-hard, and empirically evaluate the performance of four heuristics for solving it. For the local-access model, we consider three different algorithms that aim to recover a part of G large enough to contain an optimal solution, while using only a small number of calls to the neighbor-function interface. We perform a thorough experimental evaluation in order to understand the trade offs between the proposed methods and their dependencies on characteristics of the input graph."
1250,,The Interaction Between Schema Matching and Record Matching in Data Integration.,"Binbin Gu,Zhixu Li,Xiangliang Zhang 0001,An Liu 0002,Guanfeng Liu 0001,Kai Zheng 0001,Lei Zhao 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2016.2611577,TKDE,2017,"Indexes,Joining processes,Data integration,Semantics,Estimation,Computational modeling,Electronic mail","Schema Matching (SM) and Record Matching (RM) are two necessary steps in integrating multiple relational tables of different schemas, where SM unifies the schemas and RM detects records referring to the same real-world entity. The two processes have been thoroughly studied separately, but few attention has been paid to the interaction of SM and RM. In this work, we find that, even alternating them in a simple manner, SM and RM can benefit from each other to reach a better integration performance (i.e., in terms of precision and recall). Therefore, combining SM and RM is a promising solution for improving data integration. To this end, we define novel matching rules for SM and RM, respectively, that is, every SM decision is made based on intermediate RM results, and vice versa, such that SM and RM can be performed alternately. The quality of integration is guaranteed by a Matching Likelihood Estimation model and the control of semantic drift, which prevent the effect of mismatch magnification. To reduce the computational cost, we design an index structure based on q-grams and a greedy search algorithm that can reduce around 90 percent overhead of the interaction. Extensive experiments on three data collections show that the combination and interaction between SM and RM significantly outperforms previous works that conduct SM and RM separately."
1251,,Manifold Learning by Curved Cosine Mapping.,"Huamao Gu,Xun Wang,Xuewen Chen,Shaoping Deng,Jin-Qin Shi",https://doi.org/10.1109/TKDE.2017.2728790,TKDE,2017,"Manifolds,Bridges,Pattern recognition,Data models,Learning systems,Analytical models,Computational modeling","In the field of pattern recognition, data analysis, and machine learning, data points are usually modeled as high-dimensional vectors. Due to the curse-of-dimensionality, it is non-trivial to efficiently process the orginal data directly. Given the unique properties of nonlinear dimensionality reduction techniques, nonlinear learning methods are widely adopted to reduce the dimension of data. However, existing nonlinear learning methods fail in many real applications because of the too-strict requirements (for real data) or the difficulty in parameters tuning. Therefore, in this paper, we investigate the manifold learning methods which belong to the family of nonlinear dimensionality reduction methods. Specifically, we proposed a new manifold learning principle for dimensionality reduction named Curved Cosine Mapping (CCM). Based on the law of cosines in Euclidean space, CCM applies a brand new mapping pattern to manifold learning. In CCM, the nonlinear geometric relationships are obtained by utlizing the law of cosines, and then quantified as the dimensionality-reduced features. Compared with the existing approaches, the model has weaker theoretical assumptions over the input data. Moreover, to further reduce the computation cost, an optimized version of CCM is developed. Finally, we conduct extensive experiments over both artificial and real-world datasets to demonstrate the performance of proposed techniques."
1252,,Embedding Learning with Events in Heterogeneous Information Networks.,"Huan Gui,Jialu Liu,Fangbo Tao,Meng Jiang 0001,Brandon Norick,Lance M. Kaplan,Jiawei Han 0001",https://doi.org/10.1109/TKDE.2017.2733530,TKDE,2017,"Semantics,Business,Robustness,Predictive models,Optimization,Indexes,Supervised learning","In real-world applications, objects of multiple types are interconnected, forming Heterogeneous Information Networks. In such heterogeneous information networks, we make the key observation that many interactions happen due to some event and the objects in each event form a complete semantic unit. By taking advantage of such a property, we propose a generic framework called HyperEdge- BasedEmbedding (Hebe) to learn object embeddings with events in heterogeneous information networks, where a hyperedge encompasses the objects participating in one event. The Hebe framework models the proximity among objects in each event with two methods: (1) predicting a target object given other participating objects in the event, and (2) predicting if the event can be observed given all the participating objects. Since each hyperedge encapsulates more information of a given event, Hebe is robust to data sparseness and noise. In addition, Hebe is scalable when the data size spirals. Extensive experiments on large-scale real-world datasets show the efficacy and robustness of the proposed framework."
1253,,SSE - Semantically Smooth Embedding for Knowledge Graphs.,"Shu Guo,Quan Wang 0002,Bin Wang 0004,Lihong Wang,Li Guo 0001",https://doi.org/10.1109/TKDE.2016.2638425,TKDE,2017,"Semantics,Manifolds,Tensile stress,Laplace equations,Data models,Knowledge engineering,Benchmark testing","This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into low-dimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. Two lines of embedding strategies are tested, i.e., strategies based on latent distance models and strategies based on tensor factorization techniques. We empirically evaluate SSE on two benchmark tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. The results demonstrate the superiority and generality of SSE."
1254,,Influence Maximization in Trajectory Databases.,"Long Guo,Dongxiang Zhang,Gao Cong,Wei Wu 0020,Kian-Lee Tan",https://doi.org/10.1109/TKDE.2016.2621038,TKDE,2017,"Trajectory,Databases,Vehicles,Advertising,Upper bound,Q measurement,Estimation","In this paper, we study a novel problem of influence maximization in trajectory databases that is very useful in precise location-aware advertising. It finds k best trajectories to be attached with a given advertisement and maximizes the expected influence among a large group of audience. We show that the problem is NP-hard and propose both exact and approximate solutions to find the best set of trajectories. In the exact solution, we devise an expansion-based framework that enumerates trajectory combinations in a best-first manner and propose three types of upper bound estimation techniques to facilitate early termination. In addition, we propose a novel trajectory index to reduce the influence calculation cost. To support large k, we propose a greedy solution with an approximation ratio of (1 - 1/e), whose performance is further optimized by a new proposed cluster-based method. We also propose a threshold method that can support any approximation ratio ϵ ∈ (0, 1]. In addition, we extend our problem to support the scenario when there are a group of advertisements. In our experiments, we use real datasets to construct user profiles, motion patterns, and trajectory databases. The experimental results verified the efficiency of our proposed methods."
1255,,Malevolent Activity Detection with Hypergraph-Based Models.,"Antonella Guzzo,Andrea Pugliese 0001,Antonino Rullo,Domenico Saccà,Antonio Piccolo",https://doi.org/10.1109/TKDE.2017.2658621,TKDE,2017,"Security,Hidden Markov models,Analytical models,Servers,Data models,Indexes,Correlation","We propose a hypergraph-based framework for modeling and detecting malevolent activities. The proposed model supports the specification of order-independent sets of action symbols along with temporal and cardinality constraints on the execution of actions. We study and characterize the problems of consistency checking, equivalence, and minimality of hypergraph-based models. In addition, we define and characterize the general activity detection problem, that amounts to finding all subsequences that represent a malevolent activity in a sequence of logged actions. Since the problem is intractable, we also develop an index data structure that allows the security expert to efficiently extract occurrences of activities of interest."
1256,,Analyzing Sentiments in One Go - A Supervised Joint Topic Modeling Approach.,"Zhen Hai,Gao Cong,Kuiyu Chang,Peng Cheng,Chunyan Miao",https://doi.org/10.1109/TKDE.2017.2669027,TKDE,2017,"Sentiment analysis,Analytical models,Data models,Predictive models,Probabilistic logic,Semantics,Social network services","In this work, we focus on modeling user-generated review and overall rating pairs, and aim to identify semantic aspects and aspect-level sentiments from review data as well as to predict overall sentiments of reviews. We propose a novel probabilistic supervised joint aspect and sentiment model (SJASM) to deal with the problems in one go under a unified framework. SJASM represents each review document in the form of opinion pairs, and can simultaneously model aspect terms and corresponding opinion words of the review for hidden aspect and sentiment detection. It also leverages sentimental overall ratings, which often come with online reviews, as supervision data, and can infer the semantic aspects and aspect-level sentiments that are not only meaningful but also predictive of overall sentiments of reviews. Moreover, we also develop efficient inference method for parameter estimation of SJASM based on collapsed Gibbs sampling. We evaluate SJASM extensively on real-world review data, and experimental results demonstrate that the proposed model outperforms seven well-established baseline methods for sentiment analysis tasks."
1257,,Efficient Top-k Dominating Computation on Massive Data.,"Xixian Han,Jianzhong Li 0001,Hong Gao 0001",https://doi.org/10.1109/TKDE.2017.2665619,TKDE,2017,"Algorithm design and analysis,Approximation algorithms,Radiation detectors,Indexing,Computer science,Filtering algorithms","In many applications, top-k dominating query is an important operation to return k tuples with the highest domination scores in a potentially huge data space. It is analyzed that the existing algorithms have their performance problems when performed on massive data. This paper proposes a novel table-scan-based TDTS algorithm to efficiently compute top-k dominating results. TDTS first presorts the table for early termination. The early termination checking is proposed in this paper, along with the theoretical analysis of scan depth. The pruning operation for tuples is devised in this paper. The theoretical pruning effect shows that the number of tuples maintained in TDTS can be reduced substantially. The extensive experimental results, conducted on synthetic and real-life data sets, show that TDTS outperforms the existing algorithms significantly."
1258,,A Systematic Approach to Clustering Whole Trajectories of Mobile Objects in Road Networks.,"Binh Han,Ling Liu 0001,Edward Omiecinski",https://doi.org/10.1109/TKDE.2017.2652454,TKDE,2017,"Trajectory,Roads,Mobile communication,Mobile computing,Clustering algorithms,Algorithm design and analysis,Extraterrestrial measurements","Most of mobile object trajectory clustering analysis to date has been focused on clustering the location points or sub-trajectories extracted from trajectory data. This paper presents TraceMob, a systematic approach to clustering whole trajectories of mobile objects traveling in road networks. TraceMob as a whole trajectory clustering framework has three unique features. First, we design a quality measure for the distance between two whole trajectories. By quality, we mean that the distance measure can capture the complex characteristics of trajectories as a whole including their varying lengths and their constrained movement in the road network space. Second, we develop an algorithm that transforms whole trajectories in a road network space into multidimensional data points in a euclidean space while preserving their relative distances in the transformed metric space. This transformation enables us to effectively shift the clustering task for whole mobile object trajectories in the complex road network space to the traditional clustering task for multidimensional data in a euclidean space. Third, we develop a cluster validation method for evaluating the clustering quality in both the transformed metric space and the road network space. Extensive experimental evaluation with trajectories generated on real road network maps of different cities shows that TraceMob produces higher quality clustering results and outperforms existing approaches by an order of magnitude."
1259,,GALLOP - GlobAL Feature Fused LOcation Prediction for Different Check-in Scenarios.,"Yuxing Han,Junjie Yao,Xuemin Lin 0001,Liping Wang 0012",https://doi.org/10.1109/TKDE.2017.2705083,TKDE,2017,"Collaboration,Feature extraction,Trajectory,Prediction methods,Context,Business,Data processing","Location prediction is widely used to forecast users' next place to visit based on his/her mobility logs. It is an essential problem in location data processing, invaluable for surveillance, business, and personal applications. It is very challenging due to the sparsity issues of check-in data. An often ignored problem in recent studies is the variety across different check-in scenarios, which is becoming more urgent due to the increasing availability of more location check-in applications. In this paper, we propose a new feature fusion based prediction approach, GALLOP, i.e., GlobAL feature fused LOcation Prediction for different check-in scenarios. Based on the carefully designed feature extraction methods, we utilize a novel combined prediction framework. Specifically, we set out to utilize the density estimation model to profile geographical features, i.e., context information, the factorization method to extract collaborative information, and a graph structure to extract location transition patterns of users' temporal check-in sequence, i.e., content information. An empirical study on three different check-in datasets demonstrates impressive robustness and improvement of the proposed approach."
1260,,A Novel Cost-Based Model for Data Repairing.,"Shuang Hao,Nan Tang 0001,Guoliang Li 0001,Jian He,Na Ta 0001,Jianhua Feng",https://doi.org/10.1109/TKDE.2016.2637928,TKDE,2017,"Maintenance engineering,Urban areas,Semantics,Integrated circuits,Databases,Education,Fault tolerance","Integrity constraint based data repairing is an iterative process consisting of two parts: detect and group errors that violate given integrity constraints (ICs); and modify values inside each group such that the modified database satisfies those ICs. However, most existing automatic solutions treat the process of detecting and grouping errors straightforwardly (e.g., violations of functional dependencies using string equality), while putting more attention on heuristics of modifying values within each group. In this paper, we propose a revised semantics of violations and data consistency w.r.t. a set of ICs. The revised semantics relies on string similarities, in contrast to traditional methods that use syntactic error detection using string equality. Along with the revised semantics, we also propose a new cost model to quantify the cost of data repair by considering distances between strings. We show that the revised semantics provides a significant change for better detecting and grouping errors, which in turn improves both precision and recall of the following data repairing step. We prove that finding minimum-cost repairs in the new model is NP-hard, even for a single FD. We devise efficient algorithms to find approximate repairs. In addition, we develop indices and optimization techniques to improve the efficiency. Experiments show that our approach significantly outperforms existing automatic repair algorithms in both precision and recall."
1261,,BiRank - Towards Ranking on Bipartite Graphs.,"Xiangnan He 0001,Ming Gao 0001,Min-Yen Kan,Dingxian Wang",https://doi.org/10.1109/TKDE.2016.2611584,TKDE,2017,"Bipartite graph,Bayes methods,Electronic mail,Kernel,Data models,Bridges,Probabilistic logic","The bipartite graph is a ubiquitous data structure that can model the relationship between two entity types: for instance, users and items, queries and webpages. In this paper, we study the problem of ranking vertices of a bipartite graph, based on the graph's link structure as well as prior information about vertices (which we term a query vector). We present a new solution, BiRank, which iteratively assigns scores to vertices and finally converges to a unique stationary ranking. In contrast to the traditional random walk-based methods, BiRank iterates towards optimizing a regularization function, which smooths the graph under the guidance of the query vector. Importantly, we establish how BiRank relates to the Bayesian methodology, enabling the future extension in a probabilistic way. To show the rationale and extendability of the ranking methodology, we further extend it to rank for the more generic n-partite graphs. BiRank's generic modeling of both the graph structure and vertex features enables it to model various ranking hypotheses flexibly. To illustrate its functionality, we apply the BiRank and TriRank (ranking for tripartite graphs) algorithms to two real-world applications: a general ranking scenario that predicts the future popularity of items, and a personalized ranking scenario that recommends items of interest to users. Extensive experiments on both synthetic and real-world datasets demonstrate BiRank's soundness (fast convergence), efficiency (linear in the number of graph edges), and effectiveness (achieving state-of-the-art in the two real-world tasks)."
1262,,Efficient Pattern-Based Aggregation on Sequence Data.,"Zhian He,Petrie Wong,Ben Kao,Eric Lo 0001,Reynold Cheng,Ziqiang Feng",https://doi.org/10.1109/TKDE.2016.2618856,TKDE,2017,"Aggregates,Indexes,Algorithm design and analysis,Memory management,Systems architecture,Estimation","A Sequence OLAP(S-OLAP) system provides a platform on which pattern-based aggregate (PBA) queries on a sequence database are evaluated. In its simplest form, a PBA query consists of a pattern template T and an aggregate function F. A pattern template is a sequence of variables, each is defined over a domain. Each variable is instantiated with all possible values in its corresponding domain to derive all possible patterns of the template. Sequences are grouped based on the patterns they possess. The answer to a PBA query is a sequence cuboid (s-cuboid), which is a multidimensional array of cells. Each cell is associated with a pattern instantiated from the query's pattern template. The value of each s-cuboid cell is obtained by applying the aggregate function F to the set of data sequences that belong to that cell. Since a pattern template can involve many variables and can be arbitrarily long, the induced s-cuboid for a PBA query can be huge. For most analytical tasks, however, only iceberg cells with very large aggregate values are of interest. This paper proposes an efficient approach to identifying and evaluating iceberg cells of s-cuboids. Experimental results show that our algorithms are orders of magnitude faster than existing approaches."
1263,,Engagement and Popularity Dynamics of YouTube Videos and Sensitivity to Meta-Data.,"William Hoiles,Anup Aprem,Vikram Krishnamurthy",https://doi.org/10.1109/TKDE.2017.2682858,TKDE,2017,"Videos,YouTube,Neurons,Sensitivity analysis,Learning systems","YouTube, with millions of content creators, has become the preferred destination for viewing videos online. Through the Partner program, YouTube allows content creators to monetize their popular videos. Of significant importance for content creators is which meta-level features (title, tag, thumbnail, and description) are most sensitive for promoting video popularity. The popularity of videos also depends on the social dynamics, i.e., the interaction of the content creators (or channels) with YouTube users. Using real-world data consisting of about 6 million videos spread over 25 thousand channels, we empirically examine the sensitivity of YouTube meta-level features and social dynamics. The key meta-level features that impact the view counts of a video include: first day view count, number of subscribers, contrast of the video thumbnail, Google hits, number of keywords, video category, title length, and number of upper-case letters in the title, respectively, and illustrate that these meta-level features can be used to estimate the popularity of a video. In addition, optimizing the meta-level features after a video is posted increases the popularity of videos. In the context of social dynamics, we discover that there is a causal relationship between views to a channel and the associated number of subscribers. Additionally, insights into the effects of scheduling and video playthrough in a channel are also provided. Our findings provide a useful understanding of user engagement in YouTube."
1264,,User Vitality Ranking and Prediction in Social Networking Services - A Dynamic Network Perspective.,"Richang Hong,Chuan He,Yong Ge,Meng Wang 0001,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2017.2672749,TKDE,2017,"Social network services,Distributed processing,Predictive models,Ranking (statistics)","Social networking services have been prevalent at many online communities such as Twitter.com and Weibo.com, where millions of users keep interacting with each other every day. One interesting and important problem in the social networking services is to rank users based on their vitality in a timely fashion. An accurate ranking list of user vitality could benefit many parties in social network services such as the ads providers and site operators. Although it is very promising to obtain a vitality-based ranking list of users, there are many technical challenges due to the large scale and dynamics of social networking data. In this paper, we propose a unique perspective to achieve this goal, which is quantifying user vitality by analyzing the dynamic interactions among users on social networks. Examples of social network include but are not limited to social networks in microblog sites and academical collaboration networks. Intuitively, if a user has many interactions with his friends within a time period and most of his friends do not have many interactions with their friends simultaneously, it is very likely that this user has high vitality. Based on this idea, we develop quantitative measurements for user vitality and propose our first algorithm for ranking users based vitality. Also, we further consider the mutual influence between users while computing the vitality measurements and propose the second ranking algorithm, which computes user vitality in an iterative way. Other than user vitality ranking, we also introduce a vitality prediction problem, which is also of great importance for many applications in social networking services. Along this line, we develop a customized prediction model to solve the vitality prediction problem. To evaluate the performance of our algorithms, we collect two dynamic social network data sets. The experimental results with both data sets clearly demonstrate the advantage of our ranking and prediction methods."
1265,,Multi-View Unsupervised Feature Selection with Adaptive Similarity and View Weight.,"Chenping Hou,Feiping Nie 0001,Hong Tao,Dongyun Yi",https://doi.org/10.1109/TKDE.2017.2681670,TKDE,2017,"Laplace equations,Sparse matrices,Linear programming,Robustness,Data mining,Algorithm design and analysis,Visualization","With the advent of multi-view data, multi-view learning has become an important research direction in both machine learning and data mining. Considering the difficulty of obtaining labeled data in many real applications, we focus on the multi-view unsupervised feature selection problem. Traditional approaches all characterize the similarity by fixed and pre-defined graph Laplacian in each view separately and ignore the underlying common structures across different views. In this paper, we propose an algorithm named Multi-view Unsupervised Feature Selection with Adaptive Similarity and View Weight (ASVW) to overcome the above mentioned problems. Specifically, by leveraging the learning mechanism to characterize the common structures adaptively, we formulate the objective function by a common graph Laplacian across different views, together with the sparse ℓ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,p</sub>
-norm constraint designed for feature selection. We develop an efficient algorithm to address the non-smooth minimization problem and prove that the algorithm will converge. To validate the effectiveness of ASVW, comparisons are made with some benchmark methods on real-world datasets. We also evaluate our method in the real sports action recognition task. The experimental results demonstrate the effectiveness of our proposed algorithm."
1266,,An Efficient Indexing Method for Skyline Computations with Partially Ordered Domains.,"Yu-Ling Hsueh,Chia-Chun Lin,Chia-Che Chang",https://doi.org/10.1109/TKDE.2017.2656906,TKDE,2017,"Indexing,Partitioning algorithms,Query processing,Encoding,Topology,Cascading style sheets","Efficient processing of skyline queries with partially ordered domains has been intensively addressed in recent years. To further reduce the query processing time to support high-responsive applications, the skyline queries that were previously processed with user preferences similar to those of the new query contribute useful candidate result points. Hence, the answered queries can be cached with both their results and the user preferences such that the query processor can rapidly retrieve the result for a new query only from the result sets of cached queries with compatible user preferences. When caching a significant number of queries accumulated over time, it is essential to adopt effective access methods to index the cached queries to retrieve a set of relevant cached queries for facilitating the cache-based skyline query computations. In this paper, we propose an extended depth-first search indexing method (e-DFS for short) for accessing user preference profiles represented by directed acyclic graphs (DAGs), and emphasize the design of the e-DFS encoding that effectively encodes a user preference profile into a low-dimensional feature point which is eventually indexed by an R-tree. We obtain one or more traversal orders for each node in a DAG by traversing it through a modified version of the depth-first search which is utilized to examine the topology structure and dominance relations to measure closeness or similarity. As a result, e-DFS which combines the criteria of similarity evaluation is able to greatly reduce the search space by filtering out most of the irrelevant cached queries such that the query processor can avoid accessing the entire data set to compute the query results. Extensive experiments are presented to demonstrate the performance and utility of our indexing method, which outperforms the baseline planning techniques by reducing 37 percent of the computational time on average."
1267,,Large-Scale Location Prediction for Web Pages.,"Yuening Hu,Changsung Kang,Jiliang Tang,Dawei Yin,Yi Chang 0001",https://doi.org/10.1109/TKDE.2017.2702631,TKDE,2017,"Web pages,Uniform resource locators,Urban areas,Noise measurement,Prediction algorithms,Web search,Search engines","Location information of Web pages plays an important role in location-sensitive tasks such as Web search ranking for location-sensitive queries. However, such information is usually ambiguous, incomplete, or even missing, which raises the problem of location prediction for Web pages. Meanwhile, Web pages are massive and often noisy, which pose challenges to the majority of existing algorithms for location prediction. In this paper, we propose a novel and scalable location prediction framework for Web pages based on the query-URL click graph. In particular, we introduce a concept of term location vectors to capture location distributions for all terms and develop an automatic approach to learn the importance of each term location vector for location prediction. Empirical results on a large URL set demonstrate that the proposed framework significantly improves the location prediction accuracy comparing with various representative baselines. We further provide a principled way to incorporate the proposed framework into the search ranking task and experimental results on a commercial search engine show that the proposed method remarkably boosts the ranking performance for location-sensitive queries."
1268,,IF-Matching - Towards Accurate Map-Matching with Information Fusion.,"Gang Hu,Jie Shao,Fenglin Liu,Yuan Wang,Heng Tao Shen",https://doi.org/10.1109/TKDE.2016.2617326,TKDE,2017,"Roads,Trajectory,Public transportation,Global Positioning System,Uncertainty,Measurement errors,Sensors","With the advance of various location-acquisition technologies, a myriad of GPS trajectories can be collected every day. However, the raw coordinate data captured by sensors often cannot reflect real positions due to many physical constraints and some rules of law. How to accurately match GPS trajectories to roads on a digital map is an important issue. The problem of map-matching is fundamental for many applications. Unfortunately, many existing methods still cannot meet stringent performance requirements in engineering. In particular, low/unstable sampling rate and noisy/lost data are usually big challenges. Information fusion of different data sources is becoming increasingly promising nowadays. As in practice, some other measurements such as speed and moving direction are collected together with the spatial locations acquired, we can make use of not only location coordinates but all data collected. In this paper, we propose a novel model using the related meta-information to describe a moving object, and present an algorithm called IF-Matching for map-matching. It can handle many ambiguous cases which cannot be correctly matched by existing methods. We run our algorithm with taxi trajectory data on a city-wide road network. Compared with two state-of-the-art algorithms of ST-Matching and the winner of GIS Cup 2012, our approach achieves more accurate results."
1269,,On Minimal Steiner Maximum-Connected Subgraph Queries.,"Jiafeng Hu,Xiaowei Wu 0001,Reynold Cheng,Siqiang Luo,Yixiang Fang",https://doi.org/10.1109/TKDE.2017.2730873,TKDE,2017,"Measurement,Social network services,Proteins,Image edge detection,Bipartite graph,Approximation algorithms,Indexes","Given a graph G and a set Q of query nodes, we examine the Steiner Maximum-Connected Subgraph (SMCS) problem. The SMCS, or G's induced subgraph that contains Q with the largest connectivity, can be useful for customer prediction, product promotion, and team assembling. Despite its importance, the SMCS problem has only been recently studied. Existing solutions evaluate the maximum SMCS, whose number of nodes is the largest among all the SMCSs of Q. However, the maximum SMCS, which may contain a lot of nodes, can be difficult to interpret. In this paper, we investigate the minimal SMCS, which is the minimal subgraph of G with the maximum connectivity containing Q. The minimal SMCS contains much fewer nodes than its maximum counterpart, and is thus easier to be understood. However, the minimal SMCS can be costly to evaluate. We thus propose efficient Expand-Refine algorithms, as well as their approximate versions with accuracy guarantees. We further develop a cache-based processing model to improve the efficiency for an important case when Q consists of a single node. Extensive experiments on large real and synthetic graph datasets validate the effectiveness and efficiency of our approaches."
1270,,Understand Short Texts by Harvesting and Analyzing Semantic Knowledge.,"Wen Hua,Zhongyuan Wang,Haixun Wang,Kai Zheng 0001,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2016.2571687,TKDE,2017,"Semantics,Tagging,Labeling,Vocabulary,Hidden Markov models,Coherence,Context","Understanding short texts is crucial to many applications, but challenges abound. First, short texts do not always observe the syntax of a written language. As a result, traditional natural language processing tools, ranging from part-of-speech tagging to dependency parsing, cannot be easily applied. Second, short texts usually do not contain sufficient statistical signals to support many state-of-the-art approaches for text mining such as topic modeling. Third, short texts are more ambiguous and noisy, and are generated in an enormous volume, which further increases the difficulty to handle them. We argue that semantic knowledge is required in order to better understand short texts. In this work, we build a prototype system for short text understanding which exploits semantic knowledge provided by a well-known knowledgebase and automatically harvested from a web corpus. Our knowledge-intensive approaches disrupt traditional methods for tasks such as text segmentation, part-of-speech tagging, and concept labeling, in the sense that we focus on semantics in all these tasks. We conduct a comprehensive performance evaluation on real-life data. The results show that semantic knowledge is indispensable for short text understanding, and our knowledge-intensive approaches are both effective and efficient in discovering semantics of short texts."
1271,,Two Efficient Hashing Schemes for High-Dimensional Furthest Neighbor Search.,"Qiang Huang,Jianlin Feng,Qiong Fang,Wilfred Ng",https://doi.org/10.1109/TKDE.2017.2752156,TKDE,2017,"Search problems,Data structures,Object recognition,Euclidean distance,Nearest neighbor searches,Query processing","The 
<inline-formula><tex-math notation=""LaTeX"">$c$</tex-math></inline-formula>
-Approximate Furthest Neighbor ( 
<inline-formula><tex-math notation=""LaTeX"">$c$</tex-math></inline-formula>
-AFN) search is a fundamental problem in many applications. However, existing hashing schemes for 
<inline-formula><tex-math notation=""LaTeX"">$c$ </tex-math></inline-formula>
-AFN search are designed for internal memory. The old techniques for external memory, such as furthest point Voronoi diagram and the tree-based methods, are only suitable for the low-dimensional case. In this paper, we introduce a novel concept of the Reverse Locality-Sensitive Hashing (RLSH) family which is directly designed for 
<inline-formula> <tex-math notation=""LaTeX"">$c$</tex-math></inline-formula>
-AFN search. Accordingly, we propose a new reverse query-aware LSH function, which is a random projection coupled with query-aware interval identification. Based on the reverse query-aware LSH functions, we introduce a novel Reverse Query-Aware LSH scheme named RQALSH for high-dimensional 
<inline-formula> <tex-math notation=""LaTeX"">$c$</tex-math></inline-formula>
-AFN search over external memory. Our theoretical studies show that RQALSH enjoys a guarantee on query quality. In addition, in order to further speed up RQALSH, we propose a heuristic variant named RQALSH
<inline-formula><tex-math notation=""LaTeX"">$^*$</tex-math></inline-formula>
 which applies a data-dependent objects selection to largely reduce the number of data objects. In the experiment, we compare with two state-of-the-art hashing schemes QDAFN and DrusillaSelect which have been adapted for external memory. Extensive experiments on four real datasets show that our proposed RQALSH and RQALSH
<inline-formula><tex-math notation=""LaTeX""> $^*$</tex-math></inline-formula>
 schemes significantly outperform these two methods."
1272,,Time Series Management Systems - A Survey.,"Søren Kejser Jensen,Torben Bach Pedersen,Christian Thomsen 0001",https://doi.org/10.1109/TKDE.2017.2740932,TKDE,2017,"Time series analysis,Distributed databases,Query processing,Database languages,Data models,Computational modeling","The collection of time series data increases as more monitoring and automation are being deployed. These deployments range in scale from an Internet of things (IoT) device located in a household to enormous distributed Cyber-Physical Systems (CPSs) producing large volumes of data at high velocity. To store and analyze these vast amounts of data, specialized Time Series Management Systems (TSMSs) have been developed to overcome the limitations of general purpose Database Management Systems (DBMSs) for times series management. In this paper, we present a thorough analysis and classification of TSMSs developed through academic or industrial research and documented through publications. Our classification is organized into categories based on the architectures observed during our analysis. In addition, we provide an overview of each system with a focus on the motivational use case that drove the development of the system, the functionality for storage and querying of time series a system implements, the components the system is composed of, and the capabilities of each system with regard to Stream Processing and Approximate Query Processing (AQP). Last, we provide a summary of research directions proposed by other researchers in the field and present our vision for a next generation TSMS."
1273,,Scalable Graph-Based Semi-Supervised Learning through Sparse Bayesian Model.,"Bingbing Jiang,Huanhuan Chen,Bo Yuan 0006,Xin Yao 0001",https://doi.org/10.1109/TKDE.2017.2749574,TKDE,2017,"Manifolds,Bayes methods,Prediction algorithms,Training,Algorithm design and analysis,Data models,Semisupervised learning","Semi-supervised learning (SSL) concerns the problem of how to improve classifiers’ performance through making use of prior knowledge from unlabeled data. Many SSL methods have been developed to integrate unlabeled data into the classifiers based on either the manifold or cluster assumption in recent years. In particular, the graph-based approaches, following the manifold assumption, have achieved a promising performance in many real-world applications. However, most of them work well on small-scale data sets only and lack probabilistic outputs. In this paper, a scalable graph-based SSL framework through sparse Bayesian model is proposed by defining a graph-based sparse prior. Based on the traditional Bayesian inference technique, a sparse Bayesian SSL algorithm (SBS
<inline-formula> <tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L) is obtained, which can remove the irrelevant unlabeled samples and make probabilistic prediction for out-of-sample data. Moreover, in order to scale SBS
<inline-formula> <tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L to large-scale data sets, an incremental SBS
<inline-formula> <tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L (ISBS
<inline-formula><tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L) is derived. The key idea of ISBS 
<inline-formula><tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L is employing an incremental strategy and sequentially selecting parts of unlabeled samples that contribute to the learning instead of using all available unlabeled samples directly. ISBS
<inline-formula><tex-math notation=""LaTeX"">$^2$</tex-math></inline-formula>
L has lower time and space complexities than previous SSL algorithms with the use of all unlabeled samples. Extensive experiments on various data sets verify that our algorithms can achieve comparable classification effectiveness and efficiency with much better scalability. Finally, the generalization error bound is derived based on robustness analysis."
1274,,Generating Query Facets Using Knowledge Bases.,"Zhengbao Jiang,Zhicheng Dou,Ji-Rong Wen",https://doi.org/10.1109/TKDE.2016.2623782,TKDE,2017,"Knowledge based systems,Public transportation,Search engines,Cameras,Web pages,Films,Google","A query facet is a significant list of information nuggets that explains an underlying aspect of a query. Existing algorithms mine facets of a query by extracting frequent lists contained in top search results. The coverage of facets and facet items mined by these kind of methods might be limited, because only a small number of search results are used. In order to solve this problem, we propose mining query facets by using knowledge bases which contain high-quality structured data. Specifically, we first generate facets based on the properties of the entities which are contained in Freebase and correspond to the query. Second, we mine initial query facets from search results, then expanding them by finding similar entities from Freebase. Experimental results show that our proposed method can significantly improve the coverage of facet items over the state-of-the-art algorithms."
1275,,Personal Web Revisitation by Context and Content Keywords with Relevance Feedback.,"Li Jin,Ling Feng,Gang-Li Liu,Chaokun Wang",https://doi.org/10.1109/TKDE.2017.2672747,TKDE,2017,"Context,Web pages,History,Semantics,Probabilistic logic,Search engines,Browsers","Getting back to previously viewed web pages is a common yet uneasy task for users due to the large volume of personally accessed information on the web. This paper leverages human's natural recall process of using episodic and semantic memory cues to facilitate recall, and presents a personal web revisitation technique called WebPagePrev through context and content keywords. Underlying techniques for context and content memories' acquisition, storage, decay, and utilization for page re-finding are discussed. A relevance feedback mechanism is also involved to tailor to individual's memory strength and revisitation habits. Our 6-month user study shows that: (1) Compared with the existing web revisitation tool Memento, History List Searching method, and Search Engine method, the proposed WebPagePrev delivers the best re-finding quality in finding rate (92.10 percent), average F1-measure (0.4318), and average rank error (0.3145). (2) Our dynamic management of context and content memories including decay and reinforcement strategy can mimic users' retrieval and recall mechanism. With relevance feedback, the finding rate of WebPagePrev increases by 9.82 percent, average F1-measure increases by 47.09 percent, and average rank error decreases by 19.44 percent compared to stable memory management strategy. Among time, location, and activity context factors in WebPagePrev, activity is the best recall cue, and context+content based re-finding delivers the best performance, compared to context based re-finding and content based re-finding."
1276,,Theory-Guided Data Science - A New Paradigm for Scientific Discovery from Data.,"Anuj Karpatne,Gowtham Atluri,James H. Faghmous,Michael S. Steinbach,Arindam Banerjee 0001,Auroop R. Ganguly,Shashi Shekhar,Nagiza F. Samatova,Vipin Kumar",https://doi.org/10.1109/TKDE.2017.2720168,TKDE,2017,"Data science,Data models,Biological system modeling,Mathematical model,Knowledge discovery,Atmospheric modeling,Numerical models","Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science."
1277,,Measuring Concentration of Distances - An Effective and Efficient Empirical Index.,"Sushma Kumari,Balasubramaniam Jayaram",https://doi.org/10.1109/TKDE.2016.2622270,TKDE,2017,"Indexes,Data analysis,Algorithm design and analysis,Dispersion,Search problems,Measurement","High dimensional data analysis gives rise to many challenges. One such that has come to gain a lot of attention recently is the concentration of distances (CoD) phenomenon, which is the inability of distance functions to distinguish points well in high dimensions. CoD affects almost every machine learning and data analysis algorithm in high dimensions. In this work, we present a novel efficient and effective empirical index that not only illustrates whether a distance function tends to concentrate for a given data set, but also enables us to measure the rate of concentration and allows us to compare different distance functions vis-a-vis their rate of concentration. As opposed to existing empirical indices, the proposed empirical measure uses only the internal characteristics of a given data set and hence is applicable on real data sets, which was hitherto not possible."
1278,,Learning Social Circles in Ego-Networks Based on Multi-View Network Structure.,"Chao Lan,Yuhao Yang,Xiaoli Li 0013,Bo Luo,Jun Huan",https://doi.org/10.1109/TKDE.2017.2685385,TKDE,2017,"Standards,Twitter,Upper bound,Privacy,Facebook,Manuals","Automatic social circle detection in ego-networks is a fundamentally important task for social network analysis. So far, most studies focused on how to detect overlapping circles or how to detect based on both network structure and node profiles. This paper asks an orthogonal research question: how to detect circles by leveraging multiple views of the network structure? As a first step, we crawl ego networks from Twitter and model them by six views, including user relationships, user interactions, and user content. We then apply both standard and our modified multi-view spectral clustering techniques to detect circles on these ego-networks. By extensive automatic and manual evaluations, we deliver two major findings: first, multi-view clustering techniques detect better circles than single-view clustering methods; second, our modified clustering technique which presumes sparse networks are incomplete detects better circles than the standard clustering technique which ignores such potential incompleteness. In particular, the second finding makes us conjecture a direct application of standard clustering on potentially incomplete networks may yield biased results. We lightly investigate this issue by deriving a bias upper bound that integrates theories of spectral clustering and matrix perturbation, and discussing how the bound may be affected by several network characteristics."
1279,,Special Section on the International Conference on Data Engineering 2015.,"Wolfgang Lehner,Johannes Gehrke,Kyuseok Shim",https://doi.org/10.1109/TKDE.2016.2639978,TKDE,2017,"Special issues and sections,Meetings,Data analysis,Semantics,Knowledge engineering,Data management","The papers in this special section were presented at the 31st International Conference on Data Engineering that was held in Seoul, Korea, on April 13-17, 2015. 17, 2015. "
1280,,Efficient Retrieval of Bounded-Cost Informative Routes.,"Wengen Li,Jiannong Cao 0001,Jihong Guan,Man Lung Yiu,Shuigeng Zhou",https://doi.org/10.1109/TKDE.2017.2721408,TKDE,2017,"Roads,NP-hard problem,Electronic mail,Lungs,Urban areas,Facebook,Computer science","The widespread location-aware applications produce a vast amount of spatio-textual data that contains both spatial and textual attributes. To make use of this enriched information for users to describe their preferences for travel routes, we propose a Bounded-Cost Informative Route (BCIR) query to retrieve the routes that are the most textually relevant to the user-specified query keywords subject to a travel cost constraint. BCIR query is particularly helpful for tourists and city explorers to plan their travel routes. We will show that BCIR query is an NP-hard problem. To answer BCIR query efficiently, we propose an exact solution with effective pruning techniques and two approximate solutions with performance guarantees. Extensive experiments over real data sets demonstrate that the proposed solutions achieve the expected performance."
1281,,Profiling Entities over Time in the Presence of Unreliable Sources.,"Furong Li,Mong-Li Lee,Wynne Hsu",https://doi.org/10.1109/TKDE.2017.2684804,TKDE,2017,"Organizations,Software,Blogs,Aggregates,Knowledge based systems,Employment,Couplings","To harness the rich amount of information available on the web today, many organizations aggregate public (and private) data to derive knowledge repositories for real-world entities. This paper aims to build historical profiles of real-world entities by integrating temporal records collected from different sources. This problem is challenging not only because entities may change their attribute values over time, but also because information provided by the sources could be unreliable. In this paper, we present a new solution for profiling entities over time. To understand the evolution of entities, we describe a novel transition model which gives the probability that an entity will change to a particular attribute value after some time period. Next, a set of quality metrics are defined for the data sources to capture the exactness and timeliness of their provided values. The transition model and the quality metrics are then built into a source-aware temporal matching algorithm that can link temporal records to entities at the right time and augment entity profiles with correct values. Our suite of experiments demonstrate that the proposed approach is able to outperform the state-of-the-art techniques by constructing more complete and accurate profiles for entities."
1282,,Uncertainty Quantification in Mathematics-Embedded Ontologies Using Stochastic Reduced Order Model.,"Xin Li 0032,José-Fernán Martínez,Martina Eckert,Gregorio Rubio",https://doi.org/10.1109/TKDE.2017.2651819,TKDE,2017,"Ontologies,Uncertainty,Mathematical model,Stochastic processes,Reduced order systems,Computational modeling,Computational efficiency","To resolve one of uncertainty features, randomness, in ontologies, this paper shows how to characterize uncertainty of concepts from a statistical viewpoint. In addition, with a focus on indirect entities, which are computed from direct entities through mathematical models, the uncertainties propagated from those direct entities are important and should be quantified. Thus, a novel algorithm, named Stochastic Reduced Order Model (SROM), is presented to be applied to quantify the ontological uncertainty propagation in presence of multiple input entities. This SROM-based method could approximate the statistics of indirect entities accurately and efficiently by using a very small amount of samples of input entities. The computational cost is considerably reduced while guaranteeing a reasonable degree of accuracy. Furthermore, the predicted statistics of output entities could be regarded as high-level information and be beneficial for other ontological operations, such as ontology filtering and ontology reasoning. The implementation of the SROM algorithm is non-intrusive to the mathematical model; therefore, this algorithm could be applicable to quantify uncertainty in ontologies with any mathematical relationships."
1283,,Causal Decision Trees.,"Jiuyong Li,Saisai Ma,Thuc Duy Le,Lin Liu 0003,Jixue Liu",https://doi.org/10.1109/TKDE.2016.2619350,TKDE,2017,"Decision trees,Bayes methods,Mathematical model,Context,Knowledge engineering,Data analysis,Remuneration","Uncovering causal relationships in data is a major objective of data analytics. Currently, there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree (CDT) where nodes have causal interpretations. Our method follows a well-established causal inference framework and makes use of a classic statistical test to establish the causal relationship between a predictor variable and the outcome variable. At the same time, by taking the advantages of normal decision trees, a CDT provides a compact graphical representation of the causal relationships, and the construction of a CDT is fast as a result of the divide and conquer strategy employed, making CDTs practical for representing and finding causal signals in large data sets. Experiment results demonstrate that CDTs can identify meaningful causal relationships and the CDT algorithm is scalable."
1284,,Beyond Trace Ratio - Weighted Harmonic Mean of Trace Ratios for Multiclass Discriminant Analysis.,"Zhihui Li 0001,Feiping Nie 0001,Xiaojun Chang,Yi Yang 0001",https://doi.org/10.1109/TKDE.2017.2728531,TKDE,2017,"Harmonic analysis,Face recognition,Convergence,Eigenvalues and eigenfunctions,Optimization,Linear discriminant analysis,Electronic mail","Linear discriminant analysis (LDA) is one of the most important supervised linear dimensional reduction techniques which seeks to learn low-dimensional representation from the original high-dimensional feature space through a transformation matrix, while preserving the discriminative information via maximizing the between-class scatter matrix and minimizing the within class scatter matrix. However, the conventional LDA is formulated to maximize the arithmetic mean of trace ratios which suffers from the domination of the largest objectives and might deteriorate the recognition accuracy in practical applications with a large number of classes. In this paper, we propose a new criterion to maximize the weighted harmonic mean of trace ratios, which effectively avoid the domination problem while did not raise any difficulties in the formulation. An efficient algorithm is exploited to solve the proposed challenging problems with fast convergence, which might always find the globally optimal solution just using eigenvalue decomposition in each iteration. Finally, we conduct extensive experiments to illustrate the effectiveness and superiority of our method over both of synthetic datasets and real-life datasets for various tasks, including face recognition, human motion recognition and head pose recognition. The experimental results indicate that our algorithm consistently outperforms other compared methods on all of the datasets."
1285,,Learning Bregman Distance Functions for Structural Learning to Rank.,"Xi Li 0001,Te Pi,Zhongfei Zhang,Xueyi Zhao,Meng Wang 0001,Xuelong Li,Philip S. Yu",https://doi.org/10.1109/TKDE.2017.2654250,TKDE,2017,"Robustness,Support vector machines,Data models,Semantics,Training,Measurement,Analytical models","We study content-based learning to rank from the perspective of learning distance functions. Standardly, the two key issues of learning to rank, feature mappings and score functions, are usually modeled separately, and the learning is usually restricted to modeling a linear distance function such as the Mahalanobis distance. However, the modeling of feature mappings and score functions are mutually interacted, and the patterns underlying the data are probably complicated and nonlinear. Thus, as a general nonlinear distance family, the Bregman distance is a suitable distance function for learning to rank, due to its strong generalization ability for distance functions, and its nonlinearity for exploring the general patterns of data distributions. In this paper, we study learning to rank as a structural learning problem, and devise a Bregman distance function to build the ranking model based on structural SVM. To improve the model robustness to outliers, we develop a robust structural learning framework for the ranking model. The proposed model Robust Structural Bregman distance functions Learning to Rank (RSBLR) is a general and unified framework for learning distance functions to rank. The experiments of data ranking on real-world datasets show the superiority of this method to the state-of-the-art literature, as well as its robustness to the noisily labeled outliers."
1286,,Geo-Social Influence Spanning Maximization.,"Jianxin Li 0001,Timos Sellis,J. Shane Culpepper,Zhenying He,Chengfei Liu,Junhu Wang",https://doi.org/10.1109/TKDE.2017.2690288,TKDE,2017,"Social network services,Indexes,Upper bound,Australia,Integrated circuit modeling,Adaptation models,Companies","Influence maximization is a recent but well-studied problem which helps identify a small set of users that are most likely to “influence” the maximum number of users in a social network. The problem has attracted a lot of attention as it provides a way to improve marketing, branding, and product adoption. However, existing studies rarely consider the physical locations of the users, but location is an important factor in targeted marketing. In this paper, we propose and investigate the problem of influence maximization in location-aware social networks, or, more generally, Geo-social Influence Spanning Maximization. Given a query q composed of a region R, a regional acceptance rate p, and an integer k as a seed selection budget, our aim is to find the maximum geographic spanning regions (MGSR). We refer to this as the MGSR problem. Our approach differs from previous work as we focus more on identifying the maximum spanning geographical regions within a region R, rather than just the number of activated users in the given network like the traditional influence maximization problem [14]. Our research approach can be effectively used for online marketing campaigns that depend on the physical location of social users. To address the MGSR problem, we first prove NP-Hardness. Next, we present a greedy algorithm with a 1 - 1=e approximation ratio to solve the problem, and further improve the efficiency by developing an upper bounded pruning approach. Then, we propose the OIR*-Tree index, which is a hybrid index combining ordered influential node lists with an R*-tree. We show that our index based approach is significantly more efficient than the greedy algorithm and the upper bounded pruning algorithm, especially when k is large. Finally, we evaluate the performance for all of the proposed approaches using three real datasets."
1287,,Enhancing Team Composition in Professional Networks - Problem Definitions and Fast Solutions.,"Liangyue Li,Hanghang Tong,Nan Cao,Kate Ehrlich,Yu-Ru Lin,Norbou Buchler",https://doi.org/10.1109/TKDE.2016.2633464,TKDE,2017,"Kernel,Algorithm design and analysis,Social network services,Indexes,Electronic mail,Natural language processing","In this paper, we study ways to enhance the composition of teams based on new requirements in a collaborative environment. We focus on recommending team members who can maintain the team's performance by minimizing changes to the team's skills and social structure. Our recommendations are based on computing team-level similarity, which includes skill similarity, structural similarity as well as the synergy between the two. Current heuristic approaches are one-dimensional and not comprehensive, as they consider the two aspects independently. To formalize team-level similarity, we adopt the notion of graph kernel of attributed graphs to encompass the two aspects and their interaction. To tackle the computational challenges, we propose a family of fast algorithms by (a) designing effective pruning strategies, and (b) exploring the smoothness between the existing and the new team structures. Extensive empirical evaluations on real world datasets validate the effectiveness and efficiency of our algorithms."
1288,,Scalable Iterative Classification for Sanitizing Large-Scale Datasets.,"Bo Li 0026,Yevgeniy Vorobeychik,Muqun Li,Bradley A. Malin",https://doi.org/10.1109/TKDE.2016.2628180,TKDE,2017,"Data models,Machine learning algorithms,Privacy,Security,Predictive models,Games,Inspection","Cheap ubiquitous computing enables the collection of massive amounts of personal data in a wide variety of domains. Many organizations aim to share such data while obscuring features that could disclose personally identifiable information. Much of this data exhibits weak structure (e.g., text), such that machine learning approaches have been developed to detect and remove identifiers from it. While learning is never perfect, and relying on such approaches to sanitize data can leak sensitive information, a small risk is often acceptable. Our goal is to balance the value of published data and the risk of an adversary discovering leaked identifiers. We model data sanitization as a game between 1) a publisher who chooses a set of classifiers to apply to data and publishes only instances predicted as non-sensitive and 2) an attacker who combines machine learning and manual inspection to uncover leaked identifying information. We introduce a fast iterative greedy algorithm for the publisher that ensures a low utility for a resource-limited adversary. Moreover, using five text data sets we illustrate that our algorithm leaves virtually no automatically identifiable sensitive instances for a state-of-the-art learning algorithm, while sharing over 93 percent of the original data, and completes after at most five iterations."
1289,,Modeling Information Diffusion over Social Networks for Temporal Dynamic Prediction.,"Dong Li,Shengping Zhang,Xin Sun 0003,Huiyu Zhou 0001,Sheng Li,Xuelong Li",https://doi.org/10.1109/TKDE.2017.2702162,TKDE,2017,"Predictive models,TV,Data models,Intelligent agents,Time factors,Flickr","Modeling the process of information diffusion is a challenging problem. Although numerous attempts have been made in order to solve this problem, very few studies are actually able to simulate and predict temporal dynamics of the diffusion process. In this paper, we propose a novel information diffusion model, namely GT model, which treats the nodes of a network as intelligent and rational agents and then calculates their corresponding payoffs, given different choices to make strategic decisions. By introducing time-related payoffs based on the diffusion data, the proposed GT model can be used to predict whether or not the user's behaviors will occur in a specific time interval. The user's payoff can be divided into two parts: social payoff from the user's social contacts and preference payoff from the user's idiosyncratic preference. We here exploit the global influence of the user and the social influence between any two users to accurately calculate the social payoff. In addition, we develop a new method of presenting social influence that can fully capture the temporal dynamics of social influence. Experimental results from two different datasets, Sina Weibo and Flickr demonstrate the rationality and effectiveness of the proposed prediction method with different evaluation metrics."
1290,,Enhancing Binary Classification by Modeling Uncertain Boundary in Three-Way Decisions.,"Yuefeng Li,Libiao Zhang,Yue Xu 0001,Yiyu Yao,Raymond Yiu-Keung Lau,Yutong Wu",https://doi.org/10.1109/TKDE.2017.2681671,TKDE,2017,"Support vector machines,Training,Niobium,Data models,Uncertainty,Area measurement,Standards","Text classification is a process of classifying documents into predefined categories through different classifiers learned from labelled or unlabelled training samples. Many researchers who work on binary text classification attempt to find a more effective way to separate relevant texts from a large data set. However, current text classifiers cannot unambiguously describe the decision boundary between positive and negative objects because of uncertainties caused by text feature selection and the knowledge learning process. This paper proposes a three-way decision model for dealing with the uncertain boundary to improve the binary text classification performance based on the 
<i> rough set</i>
 techniques and centroid solution. It aims to understand the uncertain boundary through partitioning the training samples into three regions (the positive, boundary, and negative regions) by two main boundary vectors 
<inline-formula><tex-math notation=""LaTeX"">$\vec{C_{P}}$</tex-math></inline-formula>
 and 
<inline-formula> <tex-math notation=""LaTeX"">$\vec{C_{N}}$</tex-math></inline-formula>
, created from the labeled positive and negative training subsets, respectively, and further resolve the objects in the boundary region by two derived boundary vectors 
<inline-formula><tex-math notation=""LaTeX"">$\vec{B_{P}}$</tex-math></inline-formula>
 and 
<inline-formula> <tex-math notation=""LaTeX"">$\vec{B_{N}}$</tex-math></inline-formula>
, produced according to the structure of the boundary region. It involves an indirect strategy which is composed of two successive steps in the whole classification process: ‘two-way to three-way’ and ‘three-way to two-way’. Four decision rules are proposed from the training process and applied to the incoming documents for more precise classification. A large number of experiments have been conducted based on the standard data sets RCV1 and Reuters-21578. The experimental results show that the usage of boundary vectors is very effective and efficient for dealing with uncertainties of the decision boundary, and the proposed model has significantly improved the performance of binary text classification in terms of 
<inline-formula><tex-math notation=""LaTeX"">$F_{1}$</tex-math></inline-formula>
 measure and 
<inline-formula> <tex-math notation=""LaTeX"">$AUC$</tex-math></inline-formula>
 area compared with six other popular baseline models."
1291,,Probase+ - Inferring Missing Links in Conceptual Taxonomies.,"Jiaqing Liang,Yanghua Xiao,Haixun Wang,Yi Zhang,Wei Wang 0009",https://doi.org/10.1109/TKDE.2017.2653115,TKDE,2017,"Taxonomy,Automobiles,Semantics,Social network services,Collaboration,Companies,Measurement","Much work has focused on automatically constructing conceptual taxonomies or semantic networks from large text corpora. In this paper, we use a state-of-the-art data-driven conceptual taxonomy, Probase, to show that missing links in taxonomies are the chief problem that hinders their adoption by many real life applications, for the missing links break the inferencing that the conceptual taxonomy claims to support. To solve this problem, we devise a collaborative filtering framework to infer missing links in taxonomies derived from text corpora. We implement our method mainly on Probase, creating a denser taxonomy containing 5.1 million (about 30 percent) more isA relationships, with an accuracy of above 90 percent. We conduct comprehensive experiments to demonstrate the quality of the revised conceptual taxonomies."
1292,,New EIC Editorial.,Xuemin Lin 0001,https://doi.org/10.1109/TKDE.2016.2646898,TKDE,2017,,Presents the introductory editorial for this issue of the publication.
1293,,Detecting Stress Based on Social Interactions in Social Networks.,"Huijie Lin,Jia Jia 0001,Jiezhong Qiu,Yongfeng Zhang,Guangyao Shen,Lexing Xie,Jie Tang 0001,Ling Feng,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2017.2686382,TKDE,2017,"Stress,Psychology,Correlation,Medical services,Twitter,Neural networks","Psychological stress is threatening people's health. It is non-trivial to detect stress timely for proactive care. With the popularity of social media, people are used to sharing their daily activities and interacting with friends on social media platforms, making it feasible to leverage online social network data for stress detection. In this paper, we find that users stress state is closely related to that of his/her friends in social media, and we employ a large-scale dataset from real-world social platforms to systematically study the correlation of users' stress states and social interactions. We first define a set of stress-related textual, visual, and social attributes from various aspects, and then propose a novel hybrid model - a factor graph model combined with Convolutional Neural Network to leverage tweet content and social interaction information for stress detection. Experimental results show that the proposed model can improve the detection performance by 6-9 percent in F1-score. By further analyzing the social interaction data, we also discover several intriguing phenomena, i.e., the number of social structures of sparse connections (i.e., with no delta connections) of stressed users is around 14 percent higher than that of non-stressed users, indicating that the social structure of stressed users' friends tend to be less connected and less complicated than that of non-stressed users."
1294,,Human-Powered Data Cleaning for Probabilistic Reachability Queries on Uncertain Graphs.,"Xin Lin,Yun Peng,Byron Choi,Jianliang Xu",https://doi.org/10.1109/TKDE.2017.2684166,TKDE,2017,"Cleaning,Probabilistic logic,Uncertainty,Databases,Crowdsourcing,Data models,Knowledge engineering","Uncertain graph models are widely used in real-world applications such as knowledge graphs and social networks. To capture the uncertainty, each edge in an uncertain graph is associated with an existential probability that signifies the likelihood of the existence of the edge. One notable issue of querying uncertain graphs is that the results are sometimes uninformative because of the edge uncertainty. In this paper, we consider probabilistic reachability queries, which are one of the fundamental classes of graph queries. To make the results more informative, we adopt a crowdsourcing-based approach to clean the uncertain edges. However, considering the time and monetary cost of crowdsourcing, it is a problem to efficiently select a limited set of edges for cleaning that maximizes the quality improvement. We prove that the edge selection problem is #P-hard. In light of the hardness of the problem, we propose a series of edge selection algorithms, followed by a number of optimization techniques and pruning heuristics for reducing the computation time. Our experimental results demonstrate that our proposed techniques outperform a random selection by up to 27 times in terms of the result quality improvement and the brute-force solution by up to 60 times in terms of the elapsed time."
1295,,Reducing Uncertainty of Probabilistic Top-k Ranking via Pairwise Crowdsourcing.,"Xin Lin,Jianliang Xu,Haibo Hu 0001,Zhe Fan",https://doi.org/10.1109/TKDE.2017.2717830,TKDE,2017,"Crowdsourcing,Probabilistic logic,Uncertainty,Cleaning,Databases,Fans,Data models","Probabilistic top-k ranking is an important and well-studied query operator in uncertain databases. However, the quality of top-k results might be heavily affected by the ambiguity and uncertainty of the underlying data. Uncertainty reduction techniques have been proposed to improve the quality of top-k results by cleaning the original data. Unfortunately, most data cleaning models aim to probe the exact values of the objects individually and therefore do not work well for subjective data types, such as user ratings, which are inherently probabilistic. In this paper, we propose a novel pairwise crowdsourcing model to reduce the uncertainty of top-k ranking using a crowd of domain experts. Given a crowdsourcing task of limited budget, we propose efficient algorithms to select the best object pairs for crowdsourcing that will bring in the highest quality improvement. Extensive experiments show that our proposed solutions outperform a random selection method by up to 30 times in terms of quality improvement of probabilistic top-k ranking queries. In terms of efficiency, our proposed solutions can reduce the elapsed time of a brute-force algorithm from several days to one minute."
1296,,Network Motif Discovery - A GPU Approach.,"Wenqing Lin,Xiaokui Xiao,Xing Xie 0001,Xiaoli Li 0001",https://doi.org/10.1109/TKDE.2016.2566618,TKDE,2017,"Graphics processing units,Frequency estimation,Standards,Biology,Parallel processing,Central Processing Unit,Kernel","The identification of network motifs has important applications in numerous domains, such as pattern detection in biological networks and graph analysis in digital circuits. However, mining network motifs is computationally challenging, as it requires enumerating subgraphs from a real-life graph, and computing the frequency of each subgraph in a large number of random graphs. In particular, existing solutions often require days to derive network motifs from biological networks with only a few thousand vertices. To address this problem, this paper presents a novel study on network motif discovery using Graphical Processing Units (GPUs). The basic idea is to employ GPUs to parallelize a large number of subgraph matching tasks in computing subgraph frequencies from random graphs, so as to reduce the overall computation time of network motif discovery. We explore the design space of GPU-based subgraph matching algorithms, with careful analysis of several crucial factors (such as branch divergences and memory coalescing) that affect the performance of GPU programs. Based on our analysis, we develop a GPU-based solution that (i) considerably differs from existing CPU-based methods in how it enumerates subgraphs, and (ii) exploits the strengths of GPUs in terms of parallelism while mitigating their limitations in terms of the computation power per GPU core. With extensive experiments on a variety of biological networks, we show that our solution is up to two orders of magnitude faster than the best CPU-based approach, and is around 20 times more cost-effective than the latter, when taking into account the monetary costs of the CPU and GPUs used."
1297,,Microscopic and Macroscopic Spatio-Temporal Topic Models for Check-in Data.,"Yu Liu,Martin Ester,Yuqiu Qian,Bo Hu 0012,David W. Cheung",https://doi.org/10.1109/TKDE.2017.2703825,TKDE,2017,"Data models,Urban areas,Analytical models,Microscopy,Twitter,Integrated circuit modeling,Spatial databases","Twitter, together with other online social networks, such as Facebook, and Gowalla have begun to collect hundreds of millions of check-ins. Check-in data captures the spatial and temporal information of user movements and interests. To model and analyze the spatio-temporal aspect of check-in data and discover temporal topics and regions, we first propose a spatio-temporal topic model, i.e., Upstream Spatio-Temporal Topic Model (USTTM). USTTM can discover temporal topics and regions, i.e., a user's choice of region and topic is affected by time in this model. We use continuous time to model check-in data, rather than discretized time, avoiding the loss of information through discretization. In addition, USTTM captures the property that user's interests and activity space will change overtime, and users have different region and topic distributions at different times in USTTM. However, both USTTM and other related models capture “microscopic patterns” within a single city, where users share POIs, and cannot discover “macroscopic” patterns in a global area, where users check-in to different POIs. Therefore, we also propose a macroscopic spatio-temporal topic model, MSTTM, employing words of tweets that are shared between cities to learn the topics of user interests. We perform an experimental evaluation on Twitter and Gowalla data sets from New York City and on a Twitter US data set. In our qualitative analysis, we perform experiments with USTTM to discover temporal topics, e.g., how topic “tourist destinations” changes over time, and to demonstrate that MSTTM indeed discovers macroscopic, generic topics. In our quantitative analysis, we evaluate the effectiveness of USTTM in terms of perplexity, accuracy of POI recommendation, and accuracy of user and time prediction. Our results show that the proposed USTTM achieves better performance than the state-of-the-art models, confirming that it is more natural to model time as an upstream variable affecting the other variables. Finally, the performance of the macroscopic model MSTTM is evaluated on a Twitter US dataset, demonstrating a substantial improvement of POI recommendation accuracy compared to the microscopic models."
1298,,Trajectory Community Discovery and Recommendation by Multi-Source Diffusion Modeling.,"Siyuan Liu,Shuhui Wang",https://doi.org/10.1109/TKDE.2016.2637898,TKDE,2017,"Trajectory,Semantics,Kernel,Feature extraction,Diffusion processes,Velocity measurement,Knowledge discovery","In this paper, we detect communities from trajectories. Existing algorithms for trajectory clustering usually rely on simplex representation and a single proximity-related metric. Unfortunately, additional information markers (e.g., social interactions or semantics in the spatial layout) are ignored, leading to the inability to fully discover the communities in trajectory database. This is especially true for human-generated trajectories, where additional fine-grained markers (e.g., movement velocity at certain locations, or the sequence of semantic spaces visited) are especially useful in capturing latent relationships among community members. To overcome this limitation, we propose TODMIS, a general framework for Trajectory-based community Detection by diffusion modeling on Multiple Information Sources. TODMIS combines additional information with raw trajectory data and construct the diffusion process on multiple similarity metrics. It also learns the consistent graph Laplacians by constructing the multi-modal diffusion process and optimizing the heat kernel coupling on each pair of similarity matrices from multiple information sources. Then, dense sub-graph detection is used to discover the set of distinct communities (including community size) on the coupled multi-graph representation. At last, based on the community information, we propose a novel model for online recommendation. We evaluate TODMIS and our online recommendation methods using different real-life datasets. Experimental results demonstrate the effectiveness and efficiency of our methods."
1299,,Keyword Search on Temporal Graphs.,"Ziyang Liu,Chong Wang,Yi Chen 0001",https://doi.org/10.1109/TKDE.2017.2690637,TKDE,2017,"Peer-to-peer computing,Keyword search,Syntactics,Data models,Social network services,Algorithm design and analysis,Electronic mail","Archiving graph data over history is demanded in many applications, such as social network studies, collaborative projects, scientific graph databases, and bibliographies. Typically people are interested in querying temporal graphs. Existing keyword search approaches for graph-structured data are insufficient for querying temporal graphs. This paper initiates the study of supporting keyword-based queries on temporal graphs. We propose a search syntax that is a moderate extension of keyword search, which allows casual users to easily search temporal graphs with optional predicates and ranking functions related to timestamps. To generate results efficiently, we first propose a best path iterator, which finds the paths between two data nodes in each snapshot that is the “best” with respect to three ranking factors. It prunes invalid or inferior paths and maximizes shared processing among different snapshots. Then, we develop algorithms that efficiently generate top-k query results. Extensive experiments verified the efficiency and effectiveness of our approach."
1300,,Spectral Ensemble Clustering via Weighted K-Means - Theoretical and Practical Evidence.,"Hongfu Liu,Junjie Wu 0002,Tongliang Liu,Dacheng Tao,Yun Fu 0001",https://doi.org/10.1109/TKDE.2017.2650229,TKDE,2017,"Linear programming,Clustering algorithms,Partitioning algorithms,Robustness,Convergence,Complexity theory,Big data","As a promising way for heterogeneous data analytics, consensus clustering has attracted increasing attention in recent decades. Among various excellent solutions, the co-association matrix based methods form a landmark, which redefines consensus clustering as a graph partition problem. Nevertheless, the relatively high time and space complexities preclude it from wide real-life applications. We, therefore, propose Spectral Ensemble Clustering (SEC) to leverage the advantages of co-association matrix in information integration but run more efficiently. We disclose the theoretical equivalence between SEC and weighted K-means clustering, which dramatically reduces the algorithmic complexity. We also derive the latent consensus function of SEC, which to our best knowledge is the first to bridge co-association matrix based methods to the methods with explicit global objective functions. Further, we prove in theory that SEC holds the robustness, generalizability, and convergence properties. We finally extend SEC to meet the challenge arising from incomplete basic partitions, based on which a row-segmentation scheme for big data clustering is proposed. Experiments on various real-world data sets in both ensemble and multi-view clustering scenarios demonstrate the superiority of SEC to some state-of-the-art methods. In particular, SEC seems to be a promising candidate for big data clustering."
1301,,Multi-Behavioral Sequential Prediction with Recurrent Log-Bilinear Model.,"Qiang Liu 0006,Shu Wu,Liang Wang 0001",https://doi.org/10.1109/TKDE.2017.2661760,TKDE,2017,"Predictive models,Context modeling,Context,Markov processes,History,Recurrent neural networks,Correlation","With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role in improving various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are based on a strong independence assumption. As two classical neural network methods for modeling sequences, recurrent neural networks cannot well model short-term contexts, and the log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It can model multiple types of behaviors in historical sequences with behavior-specific transition matrices. RLBL applies a recurrent structure for modeling long-term contexts. It models several items in each hidden layer and employs position-specific transition matrices for modeling short-term contexts. Moreover, considering continuous time difference in behavioral history is a key factor for dynamic prediction, we further extend RLBL and replace position-specific transition matrices with time-specific transition matrices, and accordingly propose a Time-Aware Recurrent Log-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL model and TA-RLBL model yield significant improvements over the competitive compared methods on three datasets, i.e., Movielens-1M dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types."
1302,,A Proactive Workflow Model for Healthcare Operation and Management.,"Chuanren Liu,Hui Xiong 0001,Spiros Papadimitriou,Yong Ge,Keli Xiao",https://doi.org/10.1109/TKDE.2016.2631537,TKDE,2017,"Hidden Markov models,Hospitals,Real-time systems,Analytical models,Data models,Buildings","Advances in real-time location systems have enabled us to collect massive amounts of fine-grained semantically rich location traces, which provide unparalleled opportunities for understanding human activities and generating useful knowledge. This, in turn, delivers intelligence for real-time decision making in various fields, such as workflow management. Indeed, it is a new paradigm to model workflows through knowledge discovery in location traces. To that end, in this paper, we provide a focused study of workflow modeling by integrated analysis of indoor location traces in the hospital environment. In particular, we develop a workflow modeling framework that automatically constructs the workflow states and estimates the parameters describing the workflow transition patterns. More specifically, we propose effective and efficient regularizations for modeling the indoor location traces as stochastic processes. First, to improve the interpretability of the workflow states, we use the geography relationship between the indoor rooms to define a prior of the workflow state distribution. This prior encourages each workflow state to be a contiguous region in the building. Second, to further improve the modeling performance, we show how to use the correlation between related types of medical devices to reinforce the parameter estimation for multiple workflow models. In comparison with our preliminary work [11], we not only develop an integrated workflow modeling framework applicable to general indoor environments, but also improve the modeling accuracy significantly. We reduce the average log-loss by up to 11 percent."
1303,,A Novel and Fast SimRank Algorithm.,"Juan Lu,Zhiguo Gong,Xuemin Lin 0001",https://doi.org/10.1109/TKDE.2016.2626282,TKDE,2017,"Linear systems,Computational modeling,Jacobian matrices,Mathematical model,Convergence,Symmetric matrices,Data models","SimRank is a widely adopted similarity measure for objects modeled as nodes in a graph, based on the intuition that two objects are similar if they are referenced by similar objects. The recursive nature of SimRank definition makes it expensive to compute the similarity score even for a single pair of nodes. This defect limits the applications of SimRank. To speed up the computation, some existing works replace the original model with an approximate model to seek only rough solution of SimRank scores. In this work, we propose a novel solution for computing all-pair SimRank scores. In particular, we propose to convert SimRank to the problem of solving a linear system in matrix form, and further prove that the system is non-singular, diagonally dominate, and symmetric definite positive (for undirected graphs). Those features immediately lead to the adoption of Conjugate Gradient (CG) and Bi-Conjugate Gradient (BiCG) techniques for efficiently computing SimRank scores. As a result, a significant improvement on the convergence rate can be achieved; meanwhile, the sparsity of the adjacency matrix is not damaged all the time. Inspired by the existing common neighbor sharing strategy, we further reduce the computational complexity of the matrix multiplication and resolve the scalable issues. The experimental results show our proposed algorithms significantly outperform the state-of-the-art algorithms."
1304,,Deep Learning of Graphs with Ngram Convolutional Neural Networks.,"Zhiling Luo,Ling Liu 0001,Jianwei Yin,Ying Li 0001,Zhaohui Wu 0001",https://doi.org/10.1109/TKDE.2017.2720734,TKDE,2017,"Machine learning,Convolution,Neural networks,Neurons,Symmetric matrices,Kernel","Convolutional Neural Network (CNN) has gained attractions in image analytics and speech recognition in recent years. However, employing CNN for classification of graphs remains to be challenging. This paper presents the Ngram graph-block based convolutional neural network model for classification of graphs. Our Ngram deep learning framework consists of three novel components. First, we introduce the concept of n-gram block to transform each raw graph object into a sequence of n-gram blocks connected through overlapping regions. Second, we introduce a diagonal convolution step to extract local patterns and connectivity features hidden in these n-gram blocks by performing n-gram normalization. Finally, we develop deeper global patterns based on the local patterns and the ways that they respond to overlapping regions by building a n-gram deep learning model using convolutional neural network. We evaluate the effectiveness of our approach by comparing it with the existing state of art methods using five real graph repositories from bioinformatics and social networks domains. Our results show that the Ngram approach outperforms existing methods with high accuracy and comparable performance."
1305,,Evolutionary Nonnegative Matrix Factorization Algorithms for Community Detection in Dynamic Networks.,"Xiaoke Ma,Di Dong",https://doi.org/10.1109/TKDE.2017.2657752,TKDE,2017,"Heuristic algorithms,Clustering algorithms,Algorithm design and analysis,Optimization,Classification algorithms,Diseases,Time complexity","Discovering evolving communities in dynamic networks is essential to important applications such as analysis for dynamic web content and disease progression. Evolutionary clustering uses the temporal smoothness framework that simultaneously maximizes the clustering accuracy at the current time step and minimizes the clustering drift between two successive time steps. In this paper, we propose two evolutionary nonnegative matrix factorization (ENMF) frameworks for detecting dynamic communities. To address the theoretical relationship among evolutionary clustering algorithms, we first prove the equivalence relationship between ENMF and optimization of evolutionary modularity density. Then, we extend the theory by proving the equivalence between evolutionary spectral clustering and ENMF, which serves as the theoretical foundation for hybrid algorithms. Based on the equivalence, we propose a semi-supervised ENMF (sE-NMF) by incorporating a priori information into ENMF. Unlike the traditional semi-supervised algorithms, a priori information is integrated into the objective function of the algorithm. The main advantage of the proposed algorithm is to escape the local optimal solution without increasing time complexity. The experimental results over a number of artificial and real world dynamic networks illustrate that the proposed method is not only more accurate but also more robust than the state-of-the-art approaches."
1306,,Detecting Sudden and Gradual Drifts in Business Processes from Execution Traces.,"Abderrahmane Maaradji,Marlon Dumas,Marcello La Rosa,Alireza Ostovar",https://doi.org/10.1109/TKDE.2017.2720601,TKDE,2017,"Feature extraction,Space exploration,Delays,Scalability,Insurance","Business processes are prone to unexpected changes, as process workers may suddenly or gradually start executing a process differently in order to adjust to changes in workload, season, or other external factors. Early detection of business process changes enables managers to identify and act upon changes that may otherwise affect process performance. Business process drift detection refers to a family of methods to detect changes in a business process by analyzing event logs extracted from the systems that support the execution of the process. Existing methods for business process drift detection are based on an explorative analysis of a potentially large feature space and in some cases they require users to manually identify specific features that characterize the drift. Depending on the explored feature space, these methods miss various types of changes. Moreover, they are either designed to detect sudden drifts or gradual drifts but not both. This paper proposes an automated and statistically grounded method for detecting sudden and gradual business process drifts under a unified framework. An empirical evaluation shows that the method detects typical change patterns with significantly higher accuracy and lower detection delay than existing methods, while accurately distinguishing between sudden and gradual drifts."
1307,,Using Geodesic Space Density Gradients for Network Community Detection.,"Arif Mahmood,Michael Small,Somaya Ali Al-Máadeed,Nasir M. Rajpoot",https://doi.org/10.1109/TKDE.2016.2632716,TKDE,2017,"Clustering algorithms,Color,Complex systems,Benchmark testing,Diseases,Data structures,Social network services","Many real world complex systems naturally map to network data structures instead of geometric spaces because the only available information is the presence or absence of a link between two entities in the system. To enable data mining techniques to solve problems in the network domain, the nodes need to be mapped to a geometric space. We propose this mapping by representing each network node with its geodesic distances from all other nodes. The space spanned by the geodesic distance vectors is the geodesic space of that network. The position of different nodes in the geodesic space encode the network structure. In this space, considering a continuous density field induced by each node, density at a specific point is the summation of density fields induced by all nodes. We drift each node in the direction of positive density gradient using an iterative algorithm till each node reaches a local maximum. Due to the network structure captured by this space, the nodes that drift to the same region of space belong to the same communities in the original network. We use the direction of movement and final position of each node as important clues for community membership assignment. The proposed algorithm is compared with more than 10 state-of-the-art community detection techniques on two benchmark networks with known communities using Normalized Mutual Information criterion. The proposed algorithm outperformed these methods by a significant margin. Moreover, the proposed algorithm has also shown excellent performance on many real-world networks."
1308,,Feature Grouping-Based Outlier Detection Upon Streaming Trajectories.,"Jiali Mao,Tao Wang,Cheqing Jin,Aoying Zhou",https://doi.org/10.1109/TKDE.2017.2744619,TKDE,2017,"Trajectory,Anomaly detection,Feature extraction,Computational modeling,Proposals,Public transportation","Outlier detection acts as one of the most important analysis tasks for trajectory stream. In stream scenarios, such properties as unlimitedness, time-varying evolutionary, sparsity, and skewness distribution of trajectories pose new challenges to outlier detection technique. Trajectory outlier detection techniques mainly focus on finding trajectory that is dissimilar to the majority of the others, which is based on the hypothesis that they are probably generated by a different mechanism. Most distance-based methods tend to utilize a function (e.g., weighted linear sum) to measure the similarity of two arbitrary objects provided that representative features have been extracted in advance. However, this kind of method is not tailored to identify the outlier which is close to its neighbors according to some features, but behaves significantly different from its neighbors in terms of the other features. To address this issue, we propose a feature grouping-based mechanism that divides all the features into two groups, where the first group (Similarity Feature) is used to find close neighbors and the second group (Difference Feature) is used to find outliers within the similar neighborhood. According to the feature differences among local adjacent objects in one or more time intervals, we present two outlier definitions, including local anomaly trajectory fragment (TF-outlier) and evolutionary anomaly moving object (MO-outlier ). We devise a basic solution and then an optimized algorithm to detect both types of outliers. Experimental results show that our proposal is both effective and efficient to detect outliers upon trajectory data streams."
1309,,Knowledge Base Semantic Integration Using Crowdsourcing.,"Rui Meng,Lei Chen 0002,Yongxin Tong,Chen Jason Zhang",https://doi.org/10.1109/TKDE.2017.2656086,TKDE,2017,"Taxonomy,Semantics,Knowledge based systems,Ontologies,Crowdsourcing,Data integration,Computer science","The semantic Web has enabled the creation of a growing number of knowledge bases (KBs), which are designed independently using different techniques. Integration of KBs has attracted much attention as different KBs usually contain overlapping and complementary information. Automatic techniques for KB integration have been improved but far from perfect. Therefore, in this paper, we study the problem of knowledge base semantic integration using crowd intelligence. There are both classes and instances in a KB, in our work, we propose a novel hybrid framework for KB semantic integration considering the semantic heterogeneity of KB class structures. We first perform semantic integration of the class structures via crowdsourcing, then apply the blocking-based instance matching approach according to the integrated class structure. For class structure (taxonomy) semantic integration, the crowd is leveraged to help identifying the semantic relationships between classes to handle the semantic heterogeneity problem. Under the conditions of both large scale KBs and limited monetary budget for crowdsourcing, we formalize the class structure (taxonomy) semantic integration problem as a Local Tree Based Query Selection (LTQS) problem. We show that the LTQS problem is NP-hard and propose two greedy-based algorithms, i.e., static query selection and adaptive query selection. Furthermore, the KBs are usually of large scales and have millions of instances, direct pairwise-based instance matching is inefficient. Therefore, we adopt the blockingbased strategy for instance matching, taking advantage of the class structure (taxonomy) integration result. The experiments on real large scale KBs verify the effectiveness and efficiency of the proposed approaches."
1310,,"Learning Activity Predictors from Sensor Data - Algorithms, Evaluation, and Applications.","Bryan David Minor,Janardhan Rao Doppa,Diane J. Cook",https://doi.org/10.1109/TKDE.2017.2750669,TKDE,2017,"Prediction algorithms,Smart homes,Training data,Feature extraction,Activity recognition,Environmental factors,Measurement","Recent progress in Internet of Things (IoT) platforms has allowed us to collect large amounts of sensing data. However, there are significant challenges in converting this large-scale sensing data into decisions for real-world applications. Motivated by applications like health monitoring and intervention and home automation we consider a novel problem called Activity Prediction, where the goal is to predict future activity occurrence times from sensor data. In this paper, we make three main contributions. First, we formulate and solve the activity prediction problem in the framework of imitation learning and reduce it to a simple regression learning problem. This approach allows us to leverage powerful regression learners that can reason about the relational structure of the problem with negligible computational overhead. Second, we present several metrics to evaluate activity predictors in the context of real-world applications. Third, we evaluate our approach using real sensor data collected from 24 smart home testbeds. We also embed the learned predictor into a mobile-device-based activity prompter and evaluate the app for nine participants living in smart homes. Our results indicate that our activity predictor performs better than the baseline methods, and offers a simple approach for predicting activities from sensor data."
1311,,NoSE - Schema Design for NoSQL Applications.,"Michael J. Mior,Kenneth Salem,Ashraf Aboulnaga,Rui Liu",https://doi.org/10.1109/TKDE.2017.2722412,TKDE,2017,"Nose,Tools,Physical design,Decision making,Relational databases,Data models","Database design is critical for high performance in relational databases and a myriad of tools exist to aid application designers in selecting an appropriate schema. While the problem of schema optimization is also highly relevant for NoSQL databases, existing tools for relational databases are inadequate in that setting. Application designers wishing to use a NoSQL database instead rely on rules of thumb to select an appropriate schema. We present a system for recommending database schemas for NoSQL applications. Our cost-based approach uses a novel binary integer programming formulation to guide the mapping from the application's conceptual data model to a database schema. We implemented a prototype of this approach for the Cassandra extensible record store. Our prototype, the NoSQL Schema Evaluator (NoSE) is able to capture rules of thumb used by expert designers without explicitly encoding the rules. Automating the design process allows NoSE to produce efficient schemas and to examine more alternatives than would be possible with a manual rule-based approach."
1312,,RAPT - Rare Class Prediction in Absence of True Labels.,"Varun Mithal,Guruprasad Nayak,Ankush Khandelwal,Vipin Kumar,Nikunj C. Oza,Ramakrishna R. Nemani",https://doi.org/10.1109/TKDE.2017.2739739,TKDE,2017,"Training,Satellites,Predictive models,Earth,Buildings,Training data,Urban areas","Many real-world problems involve learning models for rare classes in situations where there are no gold standard labels for training samples but imperfect labels are available for all instances. In this paper, we present RAPT, a three step predictive modeling framework for classifying rare class in such problem settings. The first step of the proposed framework learns a classifier that jointly optimizes precision and recall by only using imperfectly labeled training samples. We also show that, under certain assumptions on the imperfect labels, the quality of this classifier is almost as good as the one constructed using perfect labels. The second and third steps of the framework make use of the fact that imperfect labels are available for all instances to further improve the precision and recall of the rare class. We evaluate the RAPT frameworkon two real-world applications of mapping forest fires and urban extent from earth observing satellite data. The experimental results indicate that RAPTcan be used to identifyforest fires and urban areas with high precision and recall by using imperfect labels, even though obtaining expert annotated samples on a global scale is infeasible in these applications."
1313,,Stochastic Gradient Made Stable - A Manifold Propagation Approach for Large-Scale Optimization.,"Yadong Mu,Wei Liu 0005,Xiaobai Liu,Wei Fan 0001",https://doi.org/10.1109/TKDE.2016.2604302,TKDE,2017,"Stochastic processes,Manifolds,Convergence,Estimation,Optimization,Standards,Convex functions","Stochastic gradient descent (SGD) holds as a classical method to build large scale machine learning models over big data. A stochastic gradient is typically calculated from a limited number of samples (known as mini-batch), which potentially incurs a high variance and causes the estimated parameters to bounce around the optimal solution. To improve the stability of stochastic gradient, recent years have witnessed the proposal of several semi-stochastic gradient descent algorithms, which distinguish themselves from standard SGD by incorporating global information into gradient computation. In this paper, we contribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm to this nascent research area, accelerating the optimization of a large family of composite convex functions. Though theoretically converging faster, prior semi-stochastic algorithms are found to suffer from high iteration complexity, which makes them even slower than SGD in practice on many datasets. In our proposed S3GD, the semi-stochastic gradient is calculated based on efficient manifold propagation, which can be numerically accomplished by sparse matrix multiplications. This way S3GD is able to generate a highly-accurate estimate of the exact gradient from each mini-batch with largely-reduced computational complexity. Theoretic analysis reveals that the proposed S3GD elegantly balances the geometric algorithmic convergence rate against the space and time complexities during the optimization. The efficacy of S3GD is also experimentally corroborated on several large-scale benchmark datasets."
1314,,Classification Under Streaming Emerging New Classes - A Solution Using Completely-Random Trees.,"Xin Mu,Kai Ming Ting,Zhi-Hua Zhou",https://doi.org/10.1109/TKDE.2017.2691702,TKDE,2017,"Detectors,Data models,Unsupervised learning,Training,Streaming media,Context,Support vector machines","This paper investigates an important problem in stream mining, i.e., classification under streaming emerging new classes or SENC. The SENC problem can be decomposed into three subproblems: detecting emerging new classes, classifying known classes, and updating models to integrate each new class as part of known classes. The common approach is to treat it as a classification problem and solve it using either a supervised learner or a semi-supervised learner. We propose an alternative approach by using unsupervised learning as the basis to solve this problem. The proposed method employs completely-random trees which have been shown to work well in unsupervised learning and supervised learning independently in the literature. The completely-random trees are used as a single common core to solve all three subproblems: unsupervised learning, supervised learning, and model update on data streams. We show that the proposed unsupervised-learning-focused method often achieves significantly better outcomes than existing classification-focused methods."
1315,,Enumeration of Maximal Cliques from an Uncertain Graph.,"Arko Provo Mukherjee,Pan Xu 0001,Srikanta Tirthapura",https://doi.org/10.1109/TKDE.2016.2527643,TKDE,2017,"Runtime,Proteins,Algorithm design and analysis,Social network services,Uncertainty,Bioinformatics","We consider the enumeration of dense substructures (maximal cliques) from an uncertain graph. For parameter 0 <; α <; 1, we define the notion of an a-maximal clique in an uncertain graph. We present matching upper and lower bounds on the number of a-maximal cliques possible within a (uncertain) graph. We present an algorithm to enumerate a-maximal cliques whose worst-case runtime is near-optimal, and an experimental evaluation showing the practical utility of the algorithm."
1316,,Bridging Feature Selection and Extraction - Compound Feature Generation.,C. A. Murthy,https://doi.org/10.1109/TKDE.2016.2619712,TKDE,2017,"Feature extraction,Principal component analysis,Prediction algorithms,Data mining,Computational modeling,Compounds,Bridges","Dimensionality reduction is an essential pre-processing technique in many of the data analysis tasks. Popular approaches for dimensionality reduction are Feature Selection (FS) and Feature Extraction (FE). Till now, these approaches are often studied separately or independently so that the final result contains either original or transformed features. In our work, we propose to bridge these two approaches with the aim of finding reduced feature set to contain both kinds (original as well as transformed) of features. A new framework, called Minimum Projection error Minimum Redundancy (MPeMR), is introduced to obtain this result while maintaining orthogonality property among selected original and linear combinations of features. A unified iterative algorithm, for both supervised and unsupervised cases, is also developed under this framework. For each case, the performance of the proposed algorithm is successfully compared with the state-of-the-art methods on real-life data sets."
1317,,Crowdsourced Coverage as a Service - Two-Level Composition of Sensor Cloud Services.,"Azadeh Ghari Neiat,Athman Bouguettaya,Timos Sellis,Sajib Mistry",https://doi.org/10.1109/TKDE.2017.2672738,TKDE,2017,"Quality of service,IEEE 802.11 Standard,Indexing,Planning,Heuristic algorithms,Australia",We present a new two-level composition model for crowdsourced Sensor-Cloud services based on dynamic features such as spatio-temporal aspects. The proposed approach is defined based on a formal Sensor-Cloud service model that abstracts the functionality and non-functional aspects of sensor data on the cloud in terms of spatio-temporal features. A spatio-temporal indexing technique based on the 3D R-tree to enable fast identification of appropriate Sensor-Cloud services is proposed. A novel quality model is introduced that considers dynamic features of sensors to select and compose Sensor-Cloud services. The quality model defines Coverage as a Service which is formulated as a composition of crowdsourced Sensor-Cloud services. We present two new QoS-aware spatio-temporal composition algorithms to select the optimal composition plan. Experimental results validate the performance of the proposed algorithms.
1318,,Data-Driven Answer Selection in Community QA Systems.,"Liqiang Nie,Xiaochi Wei,Dongxiang Zhang,Xiang Wang 0010,Zhipeng Gao,Yi Yang 0001",https://doi.org/10.1109/TKDE.2017.2669982,TKDE,2017,"Training,Feature extraction,Knowledge discovery,Analytical models,Closed-form solutions,Portals,Computer science","Finding similar questions from historical archives has been applied to question answering, with well theoretical underpinnings and great practical success. Nevertheless, each question in the returned candidate pool often associates with multiple answers, and hence users have to painstakingly browse a lot before finding the correct one. To alleviate such problem, we present a novel scheme to rank answer candidates via pairwise comparisons. In particular, it consists of one offline learning component and one online search component. In the offline learning component, we first automatically establish the positive, negative, and neutral training samples in terms of preference pairs guided by our data-driven observations. We then present a novel model to jointly incorporate these three types of training samples. The closed-form solution of this model is derived. In the online search component, we first collect a pool of answer candidates for the given question via finding its similar questions. We then sort the answer candidates by leveraging the offline trained model to judge the preference orders. Extensive experiments on the real-world vertical and general community-based question answering datasets have comparatively demonstrated its robustness and promising performance. Also, we have released the codes and data to facilitate other researchers."
1319,,Confusion-Matrix-Based Kernel Logistic Regression for Imbalanced Data Classification.,"Miho Ohsaki,Peng Wang,Kenji Matsuda,Shigeru Katagiri,Hideyuki Watanabe,Anca Ralescu",https://doi.org/10.1109/TKDE.2017.2682249,TKDE,2017,"Kernel,Linear programming,Logistics,Support vector machines,Training data,Harmonic analysis,Probabilistic logic","There have been many attempts to classify imbalanced data, since this classification is critical in a wide variety of applications related to the detection of anomalies, failures, and risks. Many conventional methods, which can be categorized into sampling, cost-sensitive, or ensemble, include heuristic and task dependent processes. In order to achieve a better classification performance by formulation without heuristics and task dependence, we propose confusion-matrix-based kernel logistic regression (CM-KLOGR). Its objective function is the harmonic mean of various evaluation criteria derived from a confusion matrix, such criteria as sensitivity, positive predictive value, and others for negatives. This objective function and its optimization are consistently formulated on the framework of KLOGR, based on minimum classification error and generalized probabilistic descent (MCE/GPD) learning. Due to the merits of the harmonic mean, KLOGR, and MCE/GPD, CM-KLOGR improves the multifaceted performances in a well-balanced way. This paper presents the formulation of CM-KLOGR and its effectiveness through experiments that comparatively evaluated CM-KLOGR using benchmark imbalanced datasets."
1320,,A Parametric Model Approach for Structural Reconstruction of Scale-Free Networks.,"Pradumn Kumar Pandey,Bibhas Adhikari",https://doi.org/10.1109/TKDE.2017.2725264,TKDE,2017,"Numerical models,Computational modeling,Reconstruction algorithms,Signal to noise ratio,Complex networks,Context modeling,Parametric statistics","We propose a parametric network generation model which we call network reconstruction model (NRM) for structural reconstruction of scale-free real networks with power-law exponent greater than 2 in the tail of its degree distribution. The reconstruction method for a real network is concerned with finding the optimal values of the model parameters by utilizing the powerlaw exponents of model network and the real network. The method is validated for certain real world networks. The usefulness of NRM in order to solve structural reconstruction problem is demonstrated by comparing its performance with some existing popular network generative models. We show that NRM can generate networks which follow edge-densification and densification power-law when the model parameters satisfy an inequality. Computable expressions of the expected number of triangles and expected diameter are obtained for model networks generated by NRM. Finally, we numerically establish that NRM can generate networks with shrinking diameter and modular structure when specific model parameters are chosen."
1321,,Finding Related Forum Posts through Content Similarity over Intention-Based Segmentation.,"Dimitra Papadimitriou,Georgia Koutrika,Yannis Velegrakis,John Mylopoulos",https://doi.org/10.1109/TKDE.2017.2699965,TKDE,2017,"Context,Keyword search,Web sites,Linux,Vocabulary,Monitoring,Business","We study the problem of finding related forum posts to a post at hand. In contrast to traditional approaches for finding related documents that perform content comparisons across the content of the posts as a whole, we consider each post as a set of segments, each written with a different goal in mind. We advocate that the relatedness between two posts should be based on the similarity of their respective segments that are intended for the same goal, i.e., are conveying the same intention. This means that it is possible for the same terms to weigh differently in the relatedness score depending on the intention of the segment in which they are found. We have developed a segmentation method that by monitoring a number of text features can identify the parts of a post where significant jumps occur indicating a point where a segmentation should take place. The generated segments of all the posts are clustered to form intention clusters and then similarities across the posts are calculated through similarities across segments with the same intention. We experimentally illustrate the effectiveness and efficiency of our segmentation method and our overall approach of finding related forum posts."
1322,,Efficient Processing of Skyline Queries Using MapReduce.,"Yoonjae Park,Jun-Ki Min,Kyuseok Shim",https://doi.org/10.1109/TKDE.2017.2654459,TKDE,2017,"Partitioning algorithms,Two dimensional displays,Scalability,Buildings,Big data,Parallel algorithms,Indexes","The skyline operator has attracted considerable attention recently due to its broad applications. However, computing a skyline is challenging today since we have to deal with big data. For data-intensive applications, the MapReduce framework has been widely used recently. In this paper, we propose the efficient parallel algorithm SKY-MR
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 for processing skyline queries using MapReduce. We first build a quadtree-based histogram for space partitioning by deciding whether to split each leaf node judiciously based on the benefit of splitting in terms of the estimated execution time. In addition, we apply the dominance power filtering method to effectively prune non-skyline points in advance. We next partition data based on the regions divided by the quadtree and compute candidate skyline points for each partition using MapReduce. Finally, we check whether each skyline candidate point is actually a skyline point in every partition using MapReduce. We also develop the workload balancing methods to make the estimated execution times of all available machines to be similar. We did experiments to compare SKY-MR
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
 with the state-of-the-art algorithms using MapReduce and confirmed the effectiveness as well as the scalability of SKY-MR
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">+</sup>
."
1323,,Editorial.,"Jian Pei,Xuemin Lin 0001",https://doi.org/10.1109/TKDE.2016.2619598,TKDE,2017,,Presents the editorial for this issue of the publication.
1324,,KDE-Track - An Efficient Dynamic Density Estimator for Data Streams.,"Abdulhakim Ali Qahtan,Suojin Wang,Xiangliang Zhang 0001",https://doi.org/10.1109/TKDE.2016.2626441,TKDE,2017,"Kernel,Estimation,Bandwidth,Spatiotemporal phenomena,Density functional theory,Merging,Time complexity","Recent developments in sensors, global positioning system devices, and smart phones have increased the availability of spatiotemporal data streams. Developing models for mining such streams is challenged by the huge amount of data that cannot be stored in the memory, the high arrival speed, and the dynamic changes in the data distribution. Density estimation is an important technique in stream mining for a wide variety of applications. The construction of kernel density estimators is well studied and documented. However, existing techniques are either expensive or inaccurate and unable to capture the changes in the data distribution. In this paper, we present a method called KDE-Track to estimate the density of spatiotemporal data streams. KDE-Track can efficiently estimate the density function with linear time complexity using interpolation on a kernel model, which is incrementally updated upon the arrival of new samples from the stream. We also propose an accurate and efficient method for selecting the bandwidth value for the kernel density estimator, which increases its accuracy significantly. Both theoretical analysis and experimental validation show that KDE-Track outperforms a set of baseline methods on the estimation accuracy and computing time of complex density structures in data streams."
1325,,Efficient Sensitivity Analysis for Inequality Queries in Probabilistic Databases.,"Biao Qin,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2016.2613538,TKDE,2017,"Sensitivity analysis,Probabilistic logic,Databases,Algorithm design and analysis,Heuristic algorithms,Dynamic programming,Electronic mail","In this paper, we study inequality query (IQ query) processing in tuple independent probabilistic databases, where IQ queries can be categorized into IQ-path, IQ-tree, and IQ-graph queries. We focus on two related issues for IQ queries. One issue is to efficiently compute their probabilities, with the observation that the time complexity of the state-of-the-art algorithm to process IQ-graph queries is high. The other issue is to efficiently perform their sensitivity analysis, which has not been studied before. Here, sensitivity analysis is to identify input tuples that have high influence on the probability of an answer tuple, and the influence of an input tuple is defined as the difference between the output probabilities obtained in two cases, where we assume that the tuple exists in one case and does not exist in the other one. In this paper, we compile the inequality conditions of an IQ query q into a compilation tree T, which encodes the Shannon expansion order. Moreover, we split q into a set of subqueries and each contains only one inequality condition. Using compilation tree and decomposition, we introduce a dynamic programming algorithm called Dec to process an IQ query q in time O(IΦI), where Φ is the lineage of q. An IQ query can be processed by our D
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">ec</sub>
 if and only if its inequality conditions can be compiled into a compilation tree T and the inequality conditions from any node to all of its child nodes must be the same in T. We conduct extensive experiments using real and synthetic datasets to demonstrate the efficiency of our algorithm for computing the probabilities and influences of IQ queries."
1326,,Search Rank Fraud and Malware Detection in Google Play.,"Mahmudur Rahman,Mizanur Rahman,Bogdan Carbunar,Duen Horng Chau",https://doi.org/10.1109/TKDE.2017.2667658,TKDE,2017,"Malware,Google,Feature extraction,Androids,Humanoid robots,Pragmatics,Gold","Fraudulent behaviors in Google Play, the most popular Android app market, fuel search rank abuse and malware proliferation. To identify malware, previous work has focused on app executable and permission analysis. In this paper, we introduce FairPlay, a novel system that discovers and leverages traces left behind by fraudsters, to detect both malware and apps subjected to search rank fraud. FairPlay correlates review activities and uniquely combines detected review relations with linguistic and behavioral signals gleaned from Google Play app data (87 K apps, 2.9 M reviews, and 2.4M reviewers, collected over half a year), in order to identify suspicious apps. FairPlay achieves over 95 percent accuracy in classifying gold standard datasets of malware, fraudulent and legitimate apps. We show that 75 percent of the identified malware apps engage in search rank fraud. FairPlay discovers hundreds of fraudulent apps that currently evade Google Bouncer's detection technology. FairPlay also helped the discovery of more than 1,000 reviews, reported for 193 apps, that reveal a new type of “coercive” review campaign: users are harassed into writing positive reviews, and install and review other apps."
1327,,Facilitating Time Critical Information Seeking in Social Media.,"Suhas Ranganath,Suhang Wang,Xia Hu,Jiliang Tang,Huan Liu 0001",https://doi.org/10.1109/TKDE.2017.2701375,TKDE,2017,"Media,Time factors,Twitter,Hurricanes,Facebook,Real-time systems,Earthquakes","Social media plays a major role in helping people affected by natural calamities. These people use social media to request information and help in situations where time is a critical commodity. However, generic social media platforms like Twitter and Facebook are not conducive for obtaining answers promptly. Algorithms to ensure prompt responders for questions in social media have to understand and model the factors affecting their response time. In this paper, we draw from sociological studies on information seeking and organizational behavior to identify users who can provide timely and relevant responses to questions posted on social media. We first draw from these theories to model the future availability and past response behavior of the candidate responders and integrate these criteria with user relevance. We propose a learning algorithm from these criteria to derive optimal rankings of responders for a given question. We present questions posted on Twitter as a form of information seeking activity in social media and use them to evaluate our framework. Our experiments demonstrate that the proposed framework is useful in identifying timely and relevant responders for questions in social media."
1328,,Advanced Block Nested Loop Join for Extending SSD Lifetime.,"Hongchan Roh,Mincheol Shin,Wonmook Jung,Sanghyun Park",https://doi.org/10.1109/TKDE.2017.2651803,TKDE,2017,"Indexes,Random access memory,Market research,Writing,Error analysis,Flash memories,Nonvolatile memory","Flash technology trends have shown that greater densities between flash memory cells increase read/write error rates and shorten solid-state drive (SSD) device lifetimes. This is critical for enterprise systems, causing such problems as service instability and increased total cost of ownership (TCO) because of SSD replacement. Therefore, numerous studies have focused on decreasing the amount of the DBMS writes. However, there has been no research that focused on decreasing the amount of temporary writes, which are primarily created by join processing. In DBMSs, there are two major join-processing algorithms, i.e., hybrid hash join (HHJ) and sort merge join (SMJ), proven to be the best according to DBMS workload; however, the two algorithms produce temporary writes of intermediate results. Therefore, we instead look to the block-nested loop join (BNLJ); it is well-known that the two algorithms are better than BNLJ, but BNLJ creates no intermediate result writes. It is reasonable to use BNLJ for a major join algorithm if its performance can be enhanced similar to those of HHJ and SMJ, considering BNLJ's advantage of extending SSD lifetimes. Therefore, in this paper, we propose an advanced BNLJ (ANLJ) algorithm that can match the performance of the two main join algorithms."
1329,,Searching Trajectories by Regions of Interest.,"Shuo Shang,Lisi Chen,Christian S. Jensen,Ji-Rong Wen,Panos Kalnis",https://doi.org/10.1109/TKDE.2017.2685504,TKDE,2017,"Trajectory,Correlation,Planning,Spatial databases,Electronic mail,Query processing,Measurement","With the increasing availability of moving-object tracking data, trajectory search is increasingly important. We propose and investigate a novel query type named trajectory search by regions of interest (TSR query). Given an argument set of trajectories, a TSR query takes a set of regions of interest as a parameter and returns the trajectory in the argument set with the highest spatial-density correlation to the query regions. This type of query is useful in many popular applications such as trip planning and recommendation, and location based services in general. TSR query processing faces three challenges: how to model the spatial-density correlation between query regions and data trajectories, how to effectively prune the search space, and how to effectively schedule multiple so-called query sources. To tackle these challenges, a series of new metrics are defined to model spatial-density correlations. An efficient trajectory search algorithm is developed that exploits upper and lower bounds to prune the search space and that adopts a query-source selection strategy, as well as integrates a heuristic search strategy based on priority ranking to schedule multiple query sources. The performance of TSR query processing is studied in extensive experiments based on real and synthetic spatial data."
1330,,Fast Parallel Path Concatenation for Graph Extraction.,"Yingxia Shao,Kai Lei,Lei Chen 0002,Zi Huang,Bin Cui 0001,Zhongyi Liu,Yunhai Tong,Jin Xu",https://doi.org/10.1109/TKDE.2017.2716939,TKDE,2017,"Aggregates,Semantics,Pattern matching,Computational modeling,Data mining,Feature extraction,Electronic mail","Heterogeneous graph is a popular data model to represent the real-world relations with abundant semantics. To analyze heterogeneous graphs, an important step is extracting homogeneous graphs from the heterogeneous graphs, called homogeneous graph extraction. In an extracted homogeneous graph, the relation is defined by a line pattern on the heterogeneous graph and the new attribute values of the relation are calculated by user-defined aggregate functions. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel graph extraction framework, where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan, which determines the order of concatenating paths and generates the final paths in a divide-and-conquer manner. We introduce a cost model to estimate the cost of a plan and discuss three plan selection strategies, among which the best plan can enumerate paths in O(log)(l) iterations, where l is the length of a pattern. Furthermore, to improve the performance of evaluating aggregate functions, we classify the aggregate functions into three categories, i.e., distributive aggregation, algebraic aggregation, and holistic aggregation. Since the distributive and algebraic aggregations can be computed from the partial paths, we speed up the aggregation by computing partial aggregate values during the path enumeration."
1331,,A Survey of Heterogeneous Information Network Analysis.,"Chuan Shi,Yitong Li,Jiawei Zhang 0001,Yizhou Sun,Philip S. Yu",https://doi.org/10.1109/TKDE.2016.2598561,TKDE,2017,"Data mining,Semantics,Publishing,Conferences,Analytical models,Data models","Most real systems consist of a large number of interacting, multi-typed components, while most contemporary researches model them as homogeneous information networks, without distinguishing different types of objects and links in the networks. Recently, more and more researchers begin to consider these interconnected, multi-typed data as heterogeneous information networks, and develop structural analysis approaches by leveraging the rich semantic meaning of structural types of objects and links in the networks. Compared to widely studied homogeneous information network, the heterogeneous information network contains richer structure and semantic information, which provides plenty of opportunities as well as a lot of challenges for data mining. In this paper, we provide a survey of heterogeneous information network analysis. We will introduce basic concepts of heterogeneous information network analysis, examine its developments on different data mining tasks, discuss some advanced topics, and point out some future research directions."
1332,,Guided HTM - Hierarchical Topic Model with Dirichlet Forest Priors.,"Su-Jin Shin,Il-Chul Moon",https://doi.org/10.1109/TKDE.2016.2625790,TKDE,2017,"Vegetation,Probabilistic logic,Resource management,Data models,Encoding,Organizations,Knowledge engineering","Despite the proliferation of topic models, the organization of topics from the probabilistic models needs improvement in two ways: the better structured presentation of topics and the incorporation of domain knowledge on the corpus. The structured presentation, i.e., the hierarchical topic model, helps in categorizing similar topics; the incorporation of domain knowledge enables the concentrated sampling of predefined keywords in the mixture parameter learning. This paper presents a hierarchical topic models with incorporated domain knowledge, called Guided Hierarchical Topic Model (GHTM). Specifically, we allocated the prior information from the knowledge to the Dirichlet Forest prior. From the prior adjustment, we obtained the topic tree guided by the domain knowledge. This paper also contributes in enumerating four different knowledge extraction methods and applying the extracted knowledge to GHTM. We evaluated the performance of GHTM in terms of the hierarchical clustering accuracy, and we found a significant improvement of hierarchical clustering measured by F-measures. This improvement is also verified by the perplexity analyses. Additionally, we measured topic quality with KL-divergence and visualization, and these confirm the ability to better separate topic distributions. Finally, we tested the hierarchical topic quality through human experiments, and this also revealed significant improvements originating from the guidance."
1333,,Fully Scalable Methods for Distributed Tensor Factorization.,"Kijung Shin,Lee Sael,U Kang",https://doi.org/10.1109/TKDE.2016.2610420,TKDE,2017,"Tensile stress,Memory management,Matrix decomposition,Scalability,Convergence,Optimization,Indexes","Given a high-order large-scale tensor, how can we decompose it into latent factors? Can we process it on commodity computers with limited memory? These questions are closely related to recommender systems, which have modeled rating data not as a matrix but as a tensor to utilize contextual information such as time and location. This increase in the order requires tensor-factorization methods scalable with both the order and size of a tensor. In this paper, we propose two distributed tensor factorization methods, CDTF and SALS. Both methods are scalable with all aspects of data and show a trade-off between convergence speed and memory requirements. CDTF, based on coordinate descent, updates one parameter at a time, while SALS generalizes on the number of parameters updated at a time. In our experiments, only our methods factorized a five-order tensor with 1 billion observable entries, 10 M mode length, and 1 K rank, while all other state-of-the-art methods failed. Moreover, our methods required several orders of magnitude less memory than their competitors. We implemented our methods on MAPREDUCE with two widely-applicable optimization techniques: local disk caching and greedy row assignment. They speeded up our methods up to 98.2× and also the competitors up to 5.9×."
1334,,Matching Heterogeneous Events with Patterns.,"Shaoxu Song,Yu Gao,Chaokun Wang,Xiaochen Zhu,Jianmin Wang 0001,Philip S. Yu",https://doi.org/10.1109/TKDE.2017.2690912,TKDE,2017,"Pattern matching,Pragmatics,Dictionaries,Marine vehicles,Upper bound,Frequency conversion,Web services","A large amount of heterogeneous event data are increasingly generated, e.g., in online systems for Web services or operational systems in enterprises. Owing to the difference between event data and traditional relational data, the matching of heterogeneous events is highly non-trivial. While event names are often opaque (e.g., merely with obscure IDs), the existing structure-based matching techniques for relational data also fail to perform owing to the poor discriminative power of dependency relationships between events. We note that interesting patterns exist in the occurrence of events, which may serve as discriminative features in event matching. In this paper, we formalize the problem of matching events with patterns. A generic pattern based matching framework is proposed, which is compatible with the existing structure based techniques. To improve the matching efficiency, we devise several bounds of matching scores for pruning. Recognizing the NP-hardness of the optimal event matching problem with patterns, we propose efficient heuristic. Finally, extensive experiments demonstrate the effectiveness of our pattern based matching compared with approaches adapted from existing techniques, and the efficiency improved by the bounding, pruning and heuristic methods."
1335,,Influential Node Tracking on Dynamic Social Network - An Interchange Greedy Approach.,"Guojie Song,Yuanhao Li,Xiaodong Chen,Xinran He,Jie Tang 0001",https://doi.org/10.1109/TKDE.2016.2620141,TKDE,2017,"Social network services,Heuristic algorithms,Upper bound,Approximation algorithms,Algorithm design and analysis,Electronic mail,Greedy algorithms","As both social network structure and strength of influence between individuals evolve constantly, it requires tracking the influential nodes under a dynamic setting. To address this problem, we explore the Influential Node Tracking (INT) problem as an extension to the traditional Influence Maximization problem (IM) under dynamic social networks. While the Influence Maximization problem aims at identifying a set of k nodes to maximize the joint influence under one static network, the INT problem focuses on tracking a set of influential nodes that keeps maximizing the influence as the network evolves. Utilizing the smoothness of the evolution of the network structure, we propose an efficient algorithm, Upper Bound Interchange Greedy (UBI) and a variant, UBI+. Instead of constructing the seed set from the ground, we start from the influential seed set we found previously and implement node replacement to improve the influence coverage. Furthermore, by using a fast update method by calculating the marginal gain of nodes, our algorithm can scale to dynamic social networks with millions of nodes. Empirical experiments on three real large-scale dynamic social networks show that our UBI and its variants, UBI+ achieves better performance in terms of both influence coverage and running time."
1336,,Linking Heterogeneous Data in the Semantic Web Using Scalable and Domain-Independent Candidate Selection.,"Dezhao Song,Yi Luo,Jeff Heflin",https://doi.org/10.1109/TKDE.2016.2606399,TKDE,2017,"Semantic Web,Joining processes,Ontologies,Scalability,Databases,Data engineering","Due to the decentralized nature of the Semantic Web, the same real-world entity may be described in various data sources with different ontologies and assigned syntactically distinct identifiers. In order to facilitate data utilization and consumption in the Semantic Web, without compromising the freedom of people to publish their data, one critical problem is to appropriately interlink such heterogeneous data. This interlinking process is sometimes referred to as Entity Matching, i.e., finding which identifiers refer to the same real-world entity. In this paper, we propose two candidate selection algorithms to improve the scalability of entity matching systems. First of all, we propose HistSim that utilizes the matching histories of the instances to prune instance pairs that are not sufficiently similar to the same pool of other instances. A sigmoid function based thresholding method is proposed to automatically adjust the threshold for such commonality on-the-fly. Furthermore, we propose DisNGram that selects candidate instance pairs by computing a character-level similarity metric on discriminating literal values that are chosen using domain-independent unsupervised learning. Instances are indexed on the chosen predicates' literal values to enable efficient look-up for similar instances. Finally, in order to be able to handle heterogeneous datasets with a large number of predicates, a mechanism for automatically determining predicate comparability is proposed. We evaluate our two candidate selection algorithms against six state-of-the-art systems on three Semantic Web datasets, and demonstrate that our proposed algorithms frequently outperform state-of-the-art systems on F1-score and runtime."
1337,,Hierarchical Contextual Attention Recurrent Neural Network for Map Query Suggestion.,"Jun Song 0004,Jun Xiao 0001,Fei Wu 0001,Haishan Wu,Tong Zhang,Zhongfei (Mark) Zhang,Wenwu Zhu 0001",https://doi.org/10.1109/TKDE.2017.2700392,TKDE,2017,"Context modeling,Data models,Recurrent neural networks,Computational modeling,Predictive models,Correlation","The query logs from an on-line map query system provide rich cues to understand the behaviors of human crowds. With the growing ability of collecting large scale query logs, the query suggestion has been a topic of recent interest. In general, query suggestion aims at recommending a list of relevant queries w.r.t. users' inputs via an appropriate learning of crowds' query logs. In this paper, we are particularly interested in map query suggestions (e.g., the predictions of location-related queries) and propose a novel model Hierarchical Contextual Attention Recurrent Neural Network (HCAR-NN) for map query suggestion in an encoding-decoding manner. Given crowds map query logs, our proposed HCAR-NN not only learns the local temporal correlation among map queries in a query session (e.g., queries in a short-term interval are relevant to accomplish a search mission), but also captures the global longer range contextual dependencies among map query sessions in query logs (e.g., how a sequence of queries within a short-term interval has an influence on another sequence of queries). We evaluate our approach over millions of queries from a commercial search engine (i.e., Baidu Map). Experimental results show that the proposed approach provides significant performance improvements over the competitive existing methods in terms of classical metrics (i.e., Recall@K and MRR) as well as the prediction of crowds' search missions."
1338,,Temporal Conformance Analysis and Explanation of Clinical Guidelines Execution - An Answer Set Programming Approach.,"Matteo Spiotta,Paolo Terenziani,Daniele Theseider Dupré",https://doi.org/10.1109/TKDE.2017.2734084,TKDE,2017,"Hip,Surgery,Delays,Guidelines,Cognition,Programming","Clinical Guidelines (CGs) provide general evidence-based recommendations and physicians often have to resort also to their Basic Medical Knowledge (BMK) to cope with specific patients. In this paper, we explore the interplay between CGs and BMK from the viewpoint of a-posteriori conformance analysis, intended as the adherence of a specific execution log to both the CG and the BMK. In this paper, we consider also the temporal dimension: the guideline may include temporal constraints for the execution of actions, and its adaptation to a specific patient and context may add or modify conditions and temporal constraints for actions. We propose an approach for analyzing execution traces in Answer Set Programming with respect to a guideline and BMK, pointing out discrepancies - including temporal discrepancies - with respect to the different knowledge sources, and providing explanations regarding how the applications of the CG and the BMK have interacted, especially in case strictly adhering to both is not possible."
1339,,Reachability Querying - Can It Be Even Faster?,"Jiao Su,Qing Zhu,Hao Wei,Jeffrey Xu Yu",https://doi.org/10.1109/TKDE.2016.2631160,TKDE,2017,"Indexes,IP networks,Complexity theory,Testing,Labeling,Social network services,Ontologies","As an important graph operator, reachability query has been extensively studied over decades, which is to check whether a vertex can reach another vertex over a large directed graph G with n vertices and m edges. The efforts made in the reported studies have greatly improved the query time of answering reachability queries online, while reducing the offline index construction time to construct an index with a reasonable size given the approach taken, where an entry in an index for a vertex is called a label of the vertex. Among all the work, the recent development of IP (Independent Permutation) employs randomness using k-min-wise independent permutations to process reachability queries, and shows the advantages for both query time and index construction time. In this paper, we propose a new Bloom filter Labeling, denoted as BFL. We show that the probability to answer reachability queries by BFL can be bounded, and BFL has high pruning power to answer more reachability queries directly. We give algorithms and analyze the pruning power of BFL. We conduct extensive studies using 19 large datasets. We show that BFL with an interval label performs best in the index construction time for all 19 cases, and performs best in query time for 16 out of 19 cases."
1340,,App Miscategorization Detection - A Case Study on Google Play.,"Didi Surian,Suranga Seneviratne,Aruna Seneviratne,Sanjay Chawla",https://doi.org/10.1109/TKDE.2017.2686851,TKDE,2017,"Google,Games,Australia,Data models,Support vector machines,Brain modeling,Mobile communication","An ongoing challenge in the rapidly evolving app market ecosystem is to maintain the integrity of app categories. At the time of registration, app developers have to select, what they believe, is the most appropriate category for their apps. Besides the inherent ambiguity of selecting the right category, the approach leaves open the possibility of misuse and potential gaming by the registrant. Periodically, the app store will refine the list of categories available and potentially reassign the apps. However, it has been observed that the mismatch between the description of the app and the category it belongs to, continues to persist. Although some common mechanisms (e.g., a complaint-driven or manual checking) exist, they limit the response time to detect miscategorized apps and still open the challenge on categorization. We introduce FRAC+: (FR)amework for (A)pp (C)ategorization. FRAC+ has the following salient features: (i) it is based on a data-driven topic model and automatically suggests the categories appropriate for the app store, and (ii) it can detect miscategorizated apps. Extensive experiments attest to the performance of FRAC+. Experiments on GOOGLE Play shows that FRAC+'s topics are more aligned with GOOGLE's new categories and 0.35-1.10 percent game apps are detected to be miscategorized."
1341,,Signature-Based Trajectory Similarity Join.,"Na Ta 0001,Guoliang Li 0001,Yongqing Xie,Changqi Li,Shuang Hao,Jianhua Feng",https://doi.org/10.1109/TKDE.2017.2651821,TKDE,2017,"Trajectory,Bidirectional control,Indexes,Public transportation,Complexity theory,Navigation,Roads","Emerging vehicular trajectory data have opened up opportunities to benefit many real-world applications, e.g., frequent trajectory based navigation systems, road planning, car pooling, etc. The similarity join is a key operation to enable such applications, which finds similar trajectory pairs from two large collections of trajectories. Existing similarity metrics on trajectories rely on aligning sampling points of two trajectories. However, due to different sampling rates or different vehicular speeds, the sample points in similar trajectories may not be aligned. To address this problem, we propose a new bi-directional mapping similarity (BDS), which allows a sample point of a trajectory to align to the closest location (which may not be a sample point) on the other trajectory, and vice versa. Since it is expensive to enumerate every two trajectories and compute their similarity, we propose Strain-Join, a signature-based trajectory similarity join framework. Strain-Join first generates signatures for each trajectory such that if two trajectories do not share common signatures, they cannot be similar. In order to utilize this property to prune dissimilar pairs, we devise several techniques to generate high-quality signatures and propose an efficient filtering algorithm to prune dissimilar pairs. For the pairs not pruned by the filtering algorithm, we propose effective verification algorithms to verify whether they are similar. Experimental results on real datasets show that our algorithm outperforms state-of-the-art techniques in terms of both effectiveness and efficiency."
1342,,"Managing Temporal Constraints with Preferences - Representation, Reasoning, and Querying.","Paolo Terenziani,Antonella Andolina,Luca Piovesan",https://doi.org/10.1109/TKDE.2017.2697852,TKDE,2017,"Cognition,Guidelines,Probabilistic logic,Reliability,Measurement,Electronic mail,Heuristic algorithms","Representing and managing temporal knowledge, in the form of temporal constraints, is a crucial task in many areas, including knowledge representation, planning, and scheduling. The current literature in the area is moving from the treatment of “crisp” temporal constraints to fuzzy or probabilistic constraints, to account for preferences and\or uncertainty. Given a set of temporal constraints, the evaluation of the tightest implied constraints is a fundamental task, which is essential also to provide reliable query-answering facilities. However, while such tasks have been widely addressed for “crisp” temporal constraints, they have not attracted enough attention in the “non-crisp” context yet. We overcome such a limitation, by (i) extending quantitative temporal constraints to cope with preferences, (ii) defining a temporal reasoning algorithm which evaluates the tightest temporal constraints, and (iii) providing suitable query-answering facilities based on it."
1343,1,Local Suppression and Splitting Techniques for Privacy Preserving Publication of Trajectories.,"Manolis Terrovitis,Giorgos Poulis,Nikos Mamoulis,Spiros Skiadopoulos",https://doi.org/10.1109/TKDE.2017.2675420,TKDE,2017,"Trajectory,Companies,Databases,Data privacy,Privacy,Credit cards,Radiofrequency identification","We study the problem of preserving user privacy in the publication of location sequences. Consider a database of trajectories, corresponding to movements of people, captured by their transactions when they use credit cards, RFID debit cards, or NFC (http://en.wikipedia.org/wiki/Near_field_communication) compliant devices. We show that, if such trajectories are published exactly (by only hiding the identities of persons that followed them), one can use partial trajectory knowledge as a quasi-identifier for the remaining locations in the sequence. We devise four intuitive techniques, based on combinations of locations suppression and trajectories splitting, and we show that they can prevent privacy breaches while keeping published data accurate for aggregate query answering and frequent subsets data mining."
1344,,Comparative Relation Generative Model.,"Maksim Tkachenko,Hady Wirawan Lauw",https://doi.org/10.1109/TKDE.2016.2640281,TKDE,2017,"Earth Observing System,Image quality,Context,Digital cameras,Information systems,Roads","Online reviews are important decision aids to consumers. Other than helping users to evaluate individual products, reviews also support comparison shopping by comparing two (or more) products based on a specific aspect. However, making a comparison across two different reviews, written by different authors, is not always equitable due to the different standards and preferences of authors. Therefore, we focus on comparative sentences, whereby two products are compared directly by a review author within a sentence. We study the problem of comparative relation mining. Given a set of comparative sentences, each relating a pair of entities, our objective is three-fold: to interpret the comparative direction in each sentence, to identify the aspect of each sentence, and to determine the relative merits of each entity with respect to that aspect. This requires mining comparative relations at two levels of resolution: at the sentence level, and at the entity level. Our insight is that there is a significant synergy between the two levels. We propose a generative model for comparative text, which jointly models comparative directions at the sentence level, and ranking at the entity level. This model is tested comprehensively on Amazon reviews dataset with good empirical outperformance over pipelined baselines."
1345,,Continuous Top-k Monitoring on Document Streams.,"Leong Hou U,Junjie Zhang,Kyriakos Mouratidis,Ye Li",https://doi.org/10.1109/TKDE.2017.2657622,TKDE,2017,"Monitoring,Knowledge engineering,Servers,Indexing,Context,Sorting","The efficient processing of document streams plays an important role in many information filtering systems. Emerging applications, such as news update filtering and social network notifications, demand presenting end-users with the most relevant content to their preferences. In this work, user preferences are indicated by a set of keywords. A central server monitors the document stream and continuously reports to each user the top-k documents that are most relevant to her keywords. Our objective is to support large numbers of users and high stream rates, while refreshing the top-k results almost instantaneously. Our solution abandons the traditional frequency-ordered indexing approach. Instead, it follows an identifier-ordering paradigm that suits better the nature of the problem. When complemented with a novel, locally adaptive technique, our method offers (i) proven optimality w.r.t. the number of considered queries per stream event, and (ii) an order of magnitude shorter response time (i.e., time to refresh the query results) than the current state-of-the-art."
1346,,Mining Competitors from Large Unstructured Datasets.,"George Valkanas,Theodoros Lappas,Dimitrios Gunopulos",https://doi.org/10.1109/TKDE.2017.2705101,TKDE,2017,"Bars,Business,Indexes,Electronic mail,Standards,Data mining,Context","In any competitive business, success is based on the ability to make an item more appealing to customers than the competition. A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness between two items? Who are the main competitors of a given item? What are the features of an item that most affect its competitiveness? Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution. In this paper, we present a formal definition of the competitiveness between two items, based on the market segments that they can both cover. Our evaluation of competitiveness utilizes customer reviews, an abundant source of information that is available in a wide range of domains. We present efficient methods for evaluating competitiveness in large review datasets and address the natural problem of finding the top-k competitors of a given item. Finally, we evaluate the quality of our results and the scalability of our approach using multiple datasets from different domains."
1347,,Semiring Rank Matrix Factorization.,"Thanh Le Van,Siegfried Nijssen,Matthijs van Leeuwen,Luc De Raedt",https://doi.org/10.1109/TKDE.2017.2688374,TKDE,2017,"Sparse matrices,Data mining,Linear algebra,Additives,Gene expression,Computer science,Bioinformatics","Rank data, in which each row is a complete or partial ranking of available items (columns), is ubiquitous. Among others, it can be used to represent preferences of users, levels of gene expression, and outcomes of sports events. It can have many types of patterns, among which consistent rankings of a subset of the items in multiple rows, and multiple rows that rank the same subset of the items highly. In this article, we show that the problems of finding such patterns can be formulated within a single generic framework that is based on the concept of semiring matrix factorization. In this framework, we employ the max-product semiring rather than the plus-product semiring common in traditional linear algebra. We apply this semiring matrix factorization framework on two tasks: sparse rank matrix factorization and rank matrix tiling. Experiments on both synthetic and real world datasets show that the framework is capable of discovering different types of structure as well as obtaining high quality solutions."
1348,,Dynamic Facet Ordering for Faceted Product Search Engines.,"Damir Vandic,Steven S. Aanen,Flavius Frasincar,Uzay Kaymak",https://doi.org/10.1109/TKDE.2017.2652461,TKDE,2017,"User interfaces,Search engines,Semantics,Navigation,Electronic mail,Impurities,Digital audio players","Faceted browsing is widely used in Web shops and product comparison sites. In these cases, a fixed ordered list of facets is often employed. This approach suffers from two main issues. First, one needs to invest a significant amount of time to devise an effective list. Second, with a fixed list of facets, it can happen that a facet becomes useless if all products that match the query are associated to that particular facet. In this work, we present a framework for dynamic facet ordering in e-commerce. Based on measures for specificity and dispersion of facet values, the fully automated algorithm ranks those properties and facets on top that lead to a quick drill-down for any possible target product. In contrast to existing solutions, the framework addresses e-commerce specific aspects, such as the possibility of multiple clicks, the grouping of facets by their corresponding properties, and the abundance of numeric facets. In a large-scale simulation and user study, our approach was, in general, favorably compared to a facet list created by domain experts, a greedy approach as baseline, and a state-of-the-art entropy-based solution."
1349,,Pre-Processing Censored Survival Data Using Inverse Covariance Matrix Based Calibration.,"Bhanukiran Vinzamuri,Yan Li 0052,Chandan K. Reddy",https://doi.org/10.1109/TKDE.2017.2719028,TKDE,2017,"Estimation,Standards,Covariance matrices,Correlation,Calibration,Data models,Algorithm design and analysis","Censoring is a common phenomenon that arises in many longitudinal studies where an event of interest could not be recorded within the given time frame. Censoring causes missing time-to-event labels, and this effect is compounded when dealing with datasets which have high amounts of censored instances. In addition, dependent censoring in the data, where censoring is dependent on the covariates in the data leads to bias in standard survival estimators. This motivates us to develop an approach for pre-processing censored data which calibrates the right censored (RC) times in an attempt to reduce the bias in the survival estimators. This calibration is done using an imputation method which estimates the sparse inverse covariance matrix over the dataset in an iterative convergence framework. During estimation, we apply row and column-based regularization to account for both row and column-wise correlations between different instances while imputing them. This is followed by comparing these imputed censored times with the original RC times to obtain the final calibrated RC times. These calibrated RC times can now be used in the survival dataset in place of the original RC times for more effective prediction. One of the major benefits of our calibration approach is that it is a pre-processing method for censored data which can be used in conjunction with any survival prediction algorithm and improve its performance. We evaluate the goodness of our approach using a wide array of survival prediction algorithms which are applied over crowdfunding data, electronic health records (EHRs), and synthetic censored datasets. Experimental results indicate that our calibration method improves the AUC values of survival prediction algorithms, compared to applying them directly on the original survival data."
1350,,DRIMUX - Dynamic Rumor Influence Minimization with User Experience in Social Networks.,"Biao Wang,Ge Chen,Luoyi Fu,Li Song,Xinbing Wang",https://doi.org/10.1109/TKDE.2017.2728064,TKDE,2017,"Minimization,Integrated circuit modeling,Heuristic algorithms,Facebook,Chaos","With the soaring development of large scale online social networks, online information sharing is becoming ubiquitous everyday. Various information is propagating through online social networks including both the positive and negative. In this paper, we focus on the negative information problems such as the online rumors. Rumor blocking is a serious problem in large-scale social networks. Malicious rumors could cause chaos in society and hence need to be blocked as soon as possible after being detected. In this paper, we propose a model of dynamic rumor influence minimization with user experience (DRIMUX). Our goal is to minimize the influence of the rumor (i.e., the number of users that have accepted and sent the rumor) by blocking a certain subset of nodes. A dynamic Ising propagation model considering both the global popularity and individual attraction of the rumor is presented based on a realistic scenario. In addition, different from existing problems of influence minimization, we take into account the constraint of user experience utility. Specifically, each node is assigned a tolerance time threshold. If the blocking time of each user exceeds that threshold, the utility of the network will decrease. Under this constraint, we then formulate the problem as a network inference problem with survival theory, and propose solutions based on maximum likelihood principle. Experiments are implemented based on large-scale real world networks and validate the effectiveness of our method."
1351,,Learning on Big Graph - Label Inference and Regularization with Anchor Hierarchy.,"Meng Wang 0001,Weijie Fu,Shijie Hao,Hengchang Liu,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2017.2654445,TKDE,2017,"Manifolds,Computational modeling,Data models,Optimization,Laplace equations,Computational efficiency,Semisupervised learning","Several models have been proposed to cope with the rapidly increasing size of data, such as Anchor Graph Regularization (AGR). The AGR approach significantly accelerates graph-based learning by exploring a set of anchors. However, when a dataset becomes much larger, AGR still faces a big graph which brings dramatically increasing computational costs. To overcome this issue, we propose a novel Hierarchical Anchor Graph Regularization (HAGR) approach by exploring multiple-layer anchors with a pyramid-style structure. In HAGR, the labels of datapoints are inferred from the coarsest anchors layer by layer in a coarse-to-fine manner. The label smoothness regularization is performed on all datapoints, and we demonstrate that the optimization process only involves a small-size reduced Laplacian matrix. We also introduce a fast approach to construct our hierarchical anchor graph based on an approximate nearest neighbor search technique. Experiments on million-scale datasets demonstrate the effectiveness and efficiency of the proposed HAGR approach over existing methods. Results show that the HAGR approach is even able to achieve a good performance within 3 minutes in an 8-million-example classification task."
1352,,Nodes&apos; Evolution Diversity and Link Prediction in Social Networks.,"Huan Wang 0005,Wenbin Hu,Zhenyu Qiu,Bo Du 0001",https://doi.org/10.1109/TKDE.2017.2728527,TKDE,2017,"Social network services,Cultural differences,Prediction algorithms,Algorithm design and analysis,Predictive models,Organizations,Image edge detection","Recently, social networks have witnessed a massive surge in popularity. A key issue in social network research is network evolution analysis, which assumes that all the autonomous nodes in a social network follow uniform evolution mechanisms. However, different nodes in a social network should have different evolution mechanisms to generate different edges. This is proposed as the underlying idea to ensure the nodes' evolution diversity in this paper. Our approach involves identifying the micro-level node evolution that generates different edges by introducing the existing link prediction methods from the perspectives of nodes. We also propose the edge generation coefficient to evaluate the extent to which an edge's generation can be explained by a link prediction method. To quantify the nodes' evolution diversity, we define the diverse evolution distance. Furthermore, a diverse node adaption algorithm is proposed to indirectly analyze the evolution of the entire network based on the nodes' evolution diversity. Extensive experiments on disparate real-world networks demonstrate that the introduction of the nodes' evolution diversity is important and beneficial for analyzing the network evolution. The diverse node adaption algorithm outperforms other state-of-the-art link prediction algorithms in terms of both accuracy and universality. The greater the nodes' evolution diversity, the more obvious its advantages."
1353,,Probabilistic Models for Ad Viewability Prediction on the Web.,"Chong Wang,Achir Kalra,Li Zhou,Cristian Borcea,Yi Chen 0001",https://doi.org/10.1109/TKDE.2017.2705688,TKDE,2017,"Advertising,Predictive models,Probabilistic logic,Pricing,Real-time systems,Layout,Data models","Online display advertising has becomes a billion-dollar industry, and it keeps growing. Advertisers attempt to send marketing messages to attract potential customers via graphic banner ads on publishers' webpages. Advertisers are charged for each view of a page that delivers their display ads. However, recent studies have discovered that more than half of the ads are never shown on users' screens due to insufficient scrolling. Thus, advertisers waste a great amount of money on these ads that do not bring any return on investment. Given this situation, the Interactive Advertising Bureau calls for a shift toward charging by viewable impression, i.e., charge for ads that are viewed by users. With this new pricing model, it is helpful to predict the viewability of an ad. This paper proposes two probabilistic latent class models (PLC) that predict the viewability of any given scroll depth for a user-page pair. Using a real-life dataset from a large publisher, the experiments demonstrate that our models outperform comparison systems."
1354,,Automating Characterization Deployment in Distributed Data Stream Management Systems.,"Chunkai Wang,Xiaofeng Meng 0001,Qi Guo 0001,Zujian Weng,Chen Yang 0009",https://doi.org/10.1109/TKDE.2017.2751606,TKDE,2017,"Predictive models,Dynamic scheduling,Media streaming,Delta-sigma modulation,Real-time systems,Query processing,Learning systems","Distributed data stream management systems (DDSMS) are usually composed of upper layer relational query systems (RQS) and lower layer stream processing systems (SPS). When users submit new queries to RQS, a query planner needs to be converted into a directed acyclic graph (DAG) consisting of tasks which are running on SPS. Based on different query requests and data stream properties, SPS need to configure different deployments strategies. However, how to dynamically predict deployment configurations of SPS to ensure the processing throughput and low resource usage is a great challenge. This article presents OrientStream, a framework for automating characterization deployment in DDSMS using incremental machine learning techniques. By introducing the data-level, query plan-level, operator-level, and cluster-level's four-level feature extraction mechanism, we first use the different query workloads as training sets to predict the resource usage by DDSMS, and select the optimal resource configuration from candidate settings based on the current query requests and stream properties, then migrate the operator state by introducing dynamic reconfiguration. Finally, we validate our approach on the open source SPS-Storm. In view of the application scenarios with long monitoring cycle and non-frequent data fluctuation, experiments show that OrientStream can reduce CPU usage of 8-15 percent and memory usage of 38-48 percent, respectively."
1355,,Knowledge Graph Embedding - A Survey of Approaches and Applications.,"Quan Wang 0002,Zhendong Mao,Bin Wang 0004,Li Guo 0001",https://doi.org/10.1109/TKDE.2017.2754499,TKDE,2017,"Statistical analysis,Knowledge discovery,Graphical models,Matrix decomposition,Systematics,Market research,Semantics","Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth."
1356,,Learning Online Trends for Interactive Query Auto-Completion.,"Yingfei Wang,Hua Ouyang,Hongbo Deng,Yi Chang 0001",https://doi.org/10.1109/TKDE.2017.2738639,TKDE,2017,"Bayes methods,Market research,Search engines,Electronic mail,Context modeling,Predictive models,Decision making","Query auto-completion (QAC) is widely used by modern search engines to assist users by predicting their intended queries. Most QAC approaches rely on deterministic batch learning algorithms trained from past query log data. However, query popularities keep changing all the time and QAC operates in a real-time scenario where users interact with the search engine continually. So, ideally, QAC must be timely and adaptive enough to reflect time-sensitive changes in an online fashion. Second, due to the vertical position bias, a query suggestion with a higher rank tends to attract more clicks regardless of user's original intention. Hence, in the long run, it is important to place some lower ranked yet potentially more relevant queries to higher positions to collect more valuable user feedbacks. In order to tackle these issues, we propose to formulate QAC as a ranked Multi-Armed Bandits (MAB) problem which enjoys theoretical soundness. To utilize prior knowledge from query logs, we propose to use Bayesian inference and Thompson Sampling to solve this MAB problem. Extensive experiments on large scale datasets show that our QAC algorithm has the capacity to adaptively learn temporal trends, and outperforms existing QAC algorithms in ranking qualities."
1357,,Feature Selection by Maximizing Independent Classification Information.,"Jun Wang 0023,Jin-Mao Wei,Zhenglu Yang,Shu-Qin Wang",https://doi.org/10.1109/TKDE.2017.2650906,TKDE,2017,"Redundancy,Mutual information,Target recognition,Computers,Upper bound,Correlation,Entropy","Feature selection approaches based on mutual information can be roughly categorized into two groups. The first group minimizes the redundancy of features between each other. The second group maximizes the new classification information of features providing for the selected subset. A critical issue is that large new information does not signify little redundancy, and vice versa. Features with large new information but with high redundancy may be selected by the second group, and features with low redundancy but with little relevance with classes may be highly scored by the first group. Existing approaches fail to balance the importance of both terms. As such, a new information term denoted as Independent Classification Information is proposed in this paper. It assembles the newly provided information and the preserved information negatively correlated with the redundant information. Redundancy and new information are properly unified and equally treated in the new term. This strategy helps find the predictive features providing large new information and little redundancy. Moreover, independent classification information is proved as a loose upper bound of the total classification information of feature subset. Its maximization is conducive to achieve a high global discriminative performance. Comprehensive experiments demonstrate the effectiveness of the new approach."
1358,,Activity Maximization by Effective Information Diffusion in Social Networks.,"Zhefeng Wang,Yu Yang 0001,Jian Pei,Lingyang Chu,Enhong Chen",https://doi.org/10.1109/TKDE.2017.2740284,TKDE,2017,"Integrated circuit modeling,Social network services,Computational modeling,Upper bound,Approximation algorithms,Diffusion processes","In a social network, even about the same information the excitement between different users are different. If we want to spread a piece of new information and maximize the expected total amount of excitement, which seed users should we choose? This problem indeed is substantially different from the renowned influence maximization problem and cannot be tackled using the existing approaches. In this paper, motivated by the demand in a few interesting applications, we model the novel problem of activity maximization, and tackle the problem systematically. We first analyze the complexity and the approximability of the problem. We develop an upper bound and a lower bound that are submodular so that the Sandwich framework can be applied. We then devise a polling-based randomized algorithm that guarantees a data dependent approximation factor. Our experiments on four real data sets clearly verify the effectiveness and scalability of our method, as well as the advantage of our method against the other heuristic methods."
1359,,LS-Join - Local Similarity Join on String Collections.,"Jiaying Wang,Xiaochun Yang 0001,Bin Wang 0015,Chengfei Liu",https://doi.org/10.1109/TKDE.2017.2687460,TKDE,2017,"Indexes,Data integration,Earth Observing System,Cleaning,Computer science,Digital cameras","String similarity join, as an essential operation in applications including data integration and data cleaning, has attracted significant attention in the research community. Previous studies focus on global similarity join. In this paper, we study local similarity join with edit distance constraints, which finds string pairs from two string collections that have similar substrings. We study two kinds of local similarity join problems: checking local similar pairs and locating local similar pairs. We first consider the case where if two strings are locally similar to each other, they must share a common gram of a certain length. We show how to do efficient local similarity verification based on a matching gram pair. We propose two pruning techniques and an incremental method to further improve the efficiency of finding matching gram pairs. Then, we devise a method to locate the longest similar substring pair for two local similar strings. We conducted a comprehensive experimental study to evaluate the efficiency of these techniques."
1360,,Efficient Distance-Aware Influence Maximization in Geo-Social Networks.,"Xiaoyang Wang 0002,Ying Zhang 0001,Wenjie Zhang 0001,Xuemin Lin 0001",https://doi.org/10.1109/TKDE.2016.2633472,TKDE,2017,"Social network services,Integrated circuit modeling,Approximation algorithms,Indexes,Computational modeling,Australia","Given a social network and a positive integer k, the influence maximization problem aims to identify a set of k nodes in that can maximize the influence spread under a certain propagation model. As the proliferation of geo-social networks, location-aware promotion is becoming more necessary in real applications. In this paper, we study the distance-aware influence maximization (DAIM) problem, which advocates the importance of the distance between users and the promoted location. Unlike the traditional influence maximization problem, DAIM treats users differently based on their distances from the promoted location. In this situation, the k nodes selected are different when the promoted location varies. In order to handle the large number of queries and meet the online requirement, we develop two novel index-based approaches, MIA-DA and RIS-DA, by utilizing the information over some pre-sampled query locations. MIA-DA is a heuristic method which adopts the maximum influence arborescence (MIA) model to approximate the influence calculation. In addition, different pruning strategies as well as a priority-based algorithm are proposed to significantly reduce the searching space. To improve the effectiveness, in RIS-DA, we extend the reverse influence sampling (RIS) model and come up with an unbiased estimator for the DAIM problem. Through carefully analyzing the sample size needed for indexing, RIS-DA is able to return a 1 - 1=e - ε approximate solution with at least 1 - δ probability for any given query. Finally, we demonstrate the efficiency and effectiveness of proposed methods over real geo-social networks."
1361,,Bring Order into the Samples - A Novel Scalable Method for Influence Maximization.,"Xiaoyang Wang 0002,Ying Zhang 0001,Wenjie Zhang 0001,Xuemin Lin 0001,Chen Chen 0017",https://doi.org/10.1109/TKDE.2016.2624734,TKDE,2017,"Integrated circuit modeling,Social network services,Acceleration,Mathematical model,Scalability,Optimization","As a key problem in viral marketing, influence maximization has been extensively studied in the literature. Given a positive integer k, a social network g and a certain propagation model, it aims to find a set of k nodes that have the largest influence spread. The state-of-the-art method IMM is based on the reverse influence sampling (RIS) framework. By using the martingale technique, it greatly outperforms the previous methods in efficiency. However, IMM still has limitations in scalability due to the high overhead of deciding a tight sample size. In this paper, instead of spending the effort on deciding a tight sample size, we present a novel bottom-ksketch based RIS framework, namely BKRIS, which brings the order of samples into the RIS framework. By applying the sketch technique, we can derive early termination conditions to significantly accelerate the seed set selection procedure. Moreover, we provide a cost-effective method to find a proper sample size to bound the quality of returned result. In addition, we provide several optimization techniques to reduce the cost of generating samples' order and efficiently deal with the worst-case scenario. We demonstrate the efficiency and effectiveness of the proposed method over 10 real world datasets. Compared with the IMM approach, BKRIS can achieve up to two orders of magnitude speedup with almost the same influence spread. In the largest dataset with 1.8 billion edges, BKRIS can return 50 seeds in 1.3 seconds and return 5,000 seeds in 36.6 seconds. It takes IMM 55.32 second and 3,664.97 seconds, respectively."
1362,,Incremental Subgraph Feature Selection for Graph Classification.,"Haishuai Wang,Peng Zhang 0001,Xingquan Zhu 0001,Ivor Wai-Hung Tsang,Ling Chen 0006,Chengqi Zhang,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2016.2616305,TKDE,2017,"Feature extraction,Social network services,Data mining,Data models,Electronic mail,Algorithm design and analysis,Biological system modeling","Graph classification is an important tool for analyzing data with structure dependency, where subgraphs are often used as features for learning. In reality, the dimension of the subgraphs crucially depends on the threshold setting of the frequency support parameter, and the number may become extremely large. As a result, subgraphs may be incrementally discovered to form a feature stream and require the underlying graph classifier to effectively discover representative subgraph features from the subgraph feature stream. In this paper, we propose a primal-dual incremental subgraph feature selection algorithm (ISF) based on a max-margin graph classifier. The ISF algorithm constructs a sequence of solutions that are both primal and dual feasible. Each primal-dual pair shrinks the dual gap and renders a better solution for the optimal subgraph feature set. To avoid bias of ISF algorithm on short-pattern subgraph features, we present a new incremental subgraph join feature selection algorithm (ISJF) by forcing graph classifiers to join short-pattern subgraphs and generate long-pattern subgraph features. We evaluate the performance of the proposed models on both synthetic networks and real-world social network data sets. Experimental results demonstrate the effectiveness of the proposed methods."
1363,,I Know What You Want to Express - Sentence Element Inference by Incorporating External Knowledge Base.,"Xiaochi Wei,Heyan Huang,Liqiang Nie,Hanwang Zhang,Xianling Mao,Tat-Seng Chua",https://doi.org/10.1109/TKDE.2016.2622705,TKDE,2017,"Natural languages,Knowledge based systems,Semantics,Knowledge discovery,Face,Internet,Encyclopedias","Sentence auto-completion is an important feature that saves users many keystrokes in typing the entire sentence by providing suggestions as they type. Despite its value, the existing sentence auto-completion methods, such as query completion models, can hardly be applied to solving the object completion problem in sentences with the form of (subject, verb, object), due to the complex natural language description and the data deficiency problem. Towards this goal, we treat an SVO sentence as a three-element triple (subject, sentence pattern, object), and cast the sentence object completion problem as an element inference problem. These elements in all triples are encoded into a unified low-dimensional embedding space by our proposed TRANSFER model, which leverages the external knowledge base to strengthen the representation learning performance. With such representations, we can provide reliable candidates for the desired missing element by a linear model. Extensive experiments on a real-world dataset have well-validated our model. Meanwhile, we have successfully applied our proposed model to factoid question answering systems for answer candidate selection, which further demonstrates the applicability of the TRANSFER model."
1364,,Efficient Keyword-Aware Representative Travel Route Recommendation.,"Yu Ting Wen,Jinyoung Yeo,Wen-Chih Peng,Seung-won Hwang",https://doi.org/10.1109/TKDE.2017.2690421,TKDE,2017,"Bars,Feature extraction,Social network services,Planning,Semantics,Data mining,Reconstruction algorithms","With the popularity of social media (e.g., Facebook and Flicker), users can easily share their check-in records and photos during their trips. In view of the huge number of user historical mobility records in social media, we aim to discover travel experiences to facilitate trip planning. When planning a trip, users always have specific preferences regarding their trips. Instead of restricting users to limited query options such as locations, activities, or time periods, we consider arbitrary text descriptions as keywords about personalized requirements. Moreover, a diverse and representative set of recommended travel routes is needed. Prior works have elaborated on mining and ranking existing routes from check-in data. To meet the need for automatic trip organization, we claim that more features of Places of Interest (POIs) should be extracted. Therefore, in this paper, we propose an efficient Keyword-aware Representative Travel Route framework that uses knowledge extraction from users' historical mobility records and social interactions. Explicitly, we have designed a keyword extraction module to classify the POI-related tags, for effective matching with query keywords. We have further designed a route reconstruction algorithm to construct route candidates that fulfill the requirements. To provide befitting query results, we explore Representative Skyline concepts, that is, the Skyline routes which best describe the trade-offs among different POI features. To evaluate the effectiveness and efficiency of the proposed algorithms, we have conducted extensive experiments on real location-based social network datasets, and the experiment results show that our methods do indeed demonstrate good performance compared to state-of-the-art works."
1365,,Dependency Analysis of Accuracy Estimates in k-Fold Cross Validation.,"Tzu-Tsung Wong,Nai-Yu Yang",https://doi.org/10.1109/TKDE.2017.2740926,TKDE,2017,"Training,Classification algorithms,Testing,Sociology,Statistical analysis,Software","A standard procedure for evaluating the performance of classification algorithms is k-fold cross validation. Since the training sets for any pair of iterations in k-fold cross validation are overlapping when the number of folds is larger than two, the resulting accuracy estimates are considered to be dependent. In this paper, the overlapping of training sets is shown to be irrelevant in determining whether two fold accuracies are dependent or not. Then a statistical method is proposed to test the appropriateness of assuming independence for the accuracy estimates in k-fold cross validation. This method is applied on 20 data sets, and the experimental results suggest that it is generally appropriate to assume that the fold accuracies are independent. The cross validation of non-overlapping training sets can make fold accuracies to be dependent. However, this dependence almost has no impact on estimating the sample variance of fold accuracies, and hence they can generally be assumed to be independent."
1366,,Temporal Interaction and Causal Influence in Community-Based Question Answering.,"Fei Wu 0001,Xinyu Duan,Jun Xiao 0001,Zhou Zhao,Siliang Tang,Yin Zhang 0006,Yueting Zhuang",https://doi.org/10.1109/TKDE.2017.2720737,TKDE,2017,"Knowledge discovery,Semantics,Computer architecture,Recurrent neural networks,Microprocessors,Adaptation models","During the last decade, community-based question answering (CQA) sites have accumulated a vast amount of questions and their crowdsourced answers over time. How to efficiently identify the quality of answers that are relevant to a given question has become an active line of research in CQA. The major challenge of CQA is the accurate selection of high-quality answers w.r.t given questions. Previous approaches tend to model the semantic matching between individual pair of one question and its corresponding answer (how fitting an answer is to a posted question). However, these works ignore the temporal interactions between answers (how previous answers influence the late posted answers). For example, a rational user likely adapts others' opinions, revises his inclinations, and posts a more appropriate answer after understanding the given question and previously posted answers. As a result, this paper devises an architecture named Temporal Interaction and Causal Influence LSTM (TC-LSTM) to effectively leverage not only the causal influence between question-answer (how appropriate an answer is for a given question) but also the temporal interactions between answers-answer (how a high-quality answer gradually forms). In particular, long short-term memory (LSTM) is used to capture the explicit question-answer influence and the implicit answers-answer interactions. Experiments are conducted on SemEval 2015 CQA dataset for answer classification task and Baidu Zhidao Dataset for answer ranking task. The experimental results show the advantage of our model comparing with other state-of-the-art methods."
1367,,Modeling the Evolution of Users&apos; Preferences and Social Links in Social Networking Services.,"Le Wu,Yong Ge,Qi Liu 0003,Enhong Chen,Richang Hong,Junping Du,Meng Wang 0001",https://doi.org/10.1109/TKDE.2017.2663422,TKDE,2017,"Social network services,Predictive models,Data models,Fuses,Collaboration,Buildings,Electronic mail","Sociologists have long converged that the evolution of a Social Networking Service(SNS) is driven by the interplay between users' preferences (reflected in user-item interaction behavior) and the social network structure (reflected in user-user interaction behavior). Nevertheless, traditional approaches either modeled these two kinds of behaviors in isolation or relied on a static assumption of a SNS. Thus, it is still unclear how do the roles of the dynamic social network structure and users' historical preferences affect the evolution of SNSs. Furthermore, can transforming the underlying social theories in the platform evolution modeling process benefit both behavior prediction tasks? In this paper, we incorporate the underlying social theories to explain and model the evolution of users' two kinds of behaviors in SNSs. Specifically, we present two kinds of representations for users' behaviors: a direct (latent) representation that presumes users' behaviors are represented directly (latently) by their historical behaviors. Under each representation, we associate each user's two kinds of behaviors with two vectors at each time. Then, for each representation, we propose the corresponding learning model to fuse the interplay between users' two kinds of behaviors. Finally, extensive experimental results demonstrate the effectiveness of our proposed models for both user preference prediction and social link suggestion."
1368,,Semantic Bootstrapping - A Theoretical Perspective.,"Wentao Wu 0001,Hongsong Li,Haixun Wang,Kenny Q. Zhu",https://doi.org/10.1109/TKDE.2016.2619347,TKDE,2017,"Syntactics,Semantics,Electronic mail,Cats,Information retrieval,Data mining","Knowledge acquisition is an iterative process. Most previous work has focused on bootstrapping techniques based on syntactic patterns, that is, each iteration finds more syntactic patterns for subsequent extraction. However, syntactic bootstrapping is incapable of resolving the inherent ambiguities in the syntactic patterns. The precision of the extracted results is thus often poor. On the other hand, semantic bootstrapping bootstraps directly on knowledge rather than on syntactic patterns, that is, it uses existing knowledge to understand the text and acquire more knowledge. It has been shown that semantic bootstrapping can achieve superb precision while retaining good recall. Nonetheless, the working mechanism of semantic bootstrapping remains elusive. In this paper, we present a detailed analysis of semantic bootstrapping from a theoretical perspective. We show that the efficiency and effectiveness of semantic bootstrapping can be theoretically guaranteed. Our experimental evaluation results substantiate the theoretical analysis."
1369,,On Spectral Analysis of Signed and Dispute Graphs - Application to Community Structure.,"Leting Wu,Xintao Wu,Aidong Lu,Yuemeng Li",https://doi.org/10.1109/TKDE.2017.2684809,TKDE,2017,"Social network services,Laplace equations,Spectral analysis,Clustering algorithms,Eigenvalues and eigenfunctions,Clustering methods,Partitioning algorithms","This paper presents a spectral analysis of signed networks from both theoretical and practical aspects. On the theoretical aspect, we conduct theoretical studies based on results from matrix perturbation for analyzing community structures of complex signed networks and show how the negative edges affect distributions and patterns of node spectral coordinates in the spectral space. We prove and demonstrate that node spectral coordinates form orthogonal clusters for two types of signed networks: graphs with dense inter-community mixed sign edges and 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-dispute graphs where inner-community connections are absent or very sparse but inter-community connections are dense with negative edges. The cluster orthogonality pattern is different from the line orthogonality pattern (i.e., node spectral coordinates form orthogonal lines) observed in the networks with 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-block structure. We show why the line orthogonality pattern does not hold in the spectral space for these two types of networks. On the practical aspect, we have developed a clustering method to study signed networks and 
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math> </inline-formula>
-dispute networks. Empirical evaluations on both synthetic networks (with up to one million nodes) and real networks show our algorithm outperforms existing clustering methods on signed networks in terms of accuracy and efficiency."
1370,,Online Transfer Learning with Multiple Homogeneous or Heterogeneous Sources.,"Qingyao Wu,Hanrui Wu,Xiaoming Zhou,Mingkui Tan,Yonghui Xu,Yuguang Yan,Tianyong Hao",https://doi.org/10.1109/TKDE.2017.2685597,TKDE,2017,"Training,Silicon,Kernel,Machine learning algorithms,Data mining,Training data,Computer vision","Transfer learning techniques have been broadly applied in applications where labeled data in a target domain are difficult to obtain while a lot of labeled data are available in related source domains. In practice, there can be multiple source domains that are related to the target domain, and how to combine them is still an open problem. In this paper, we seek to leverage labeled data from multiple source domains to enhance classification performance in a target domain where the target data are received in an online fashion. This problem is known as the online transfer learning problem. To achieve this, we propose novel online transfer learning paradigms in which the source and target domains are leveraged adaptively. We consider two different problem settings: homogeneous transfer learning and heterogeneous transfer learning. The proposed methods work in an online manner, where the weights of the source domains are adjusted dynamically. We provide the mistake bounds of the proposed methods and perform comprehensive experiments on real-world data sets to demonstrate the effectiveness of the proposed algorithms."
1371,,Collaboratively Training Sentiment Classifiers for Multiple Domains.,"Fangzhao Wu,Zhigang Yuan,Yongfeng Huang 0001",https://doi.org/10.1109/TKDE.2017.2669975,TKDE,2017,"Collaboration,User-generated content,Motion pictures,Training,Robustness,Benchmark testing,Parallel algorithms","We propose a collaborative multi-domain sentiment classification approach to train sentiment classifiers for multiple domains simultaneously. In our approach, the sentiment information in different domains is shared to train more accurate and robust sentiment classifiers for each domain when labeled data is scarce. Specifically, we decompose the sentiment classifier of each domain into two components, a global one and a domain-specific one. The global model can capture the general sentiment knowledge and is shared by various domains. The domain-specific model can capture the specific sentiment expressions in each domain. In addition, we extract domain-specific sentiment knowledge from both labeled and unlabeled samples in each domain and use it to enhance the learning of domain-specific sentiment classifiers. Besides, we incorporate the similarities between domains into our approach as regularization over the domain-specific sentiment classifiers to encourage the sharing of sentiment information between similar domains. Two kinds of domain similarity measures are explored, one based on textual content and the other one based on sentiment expressions. Moreover, we introduce two efficient algorithms to solve the model of our approach. Experimental results on benchmark datasets show that our approach can effectively improve the performance of multi-domain sentiment classification and significantly outperform baseline methods."
1372,,"Storing, Tracking, and Querying Provenance in Linked Data.","Marcin Wylot,Philippe Cudré-Mauroux,Manfred Hauswirth,Paul Groth",https://doi.org/10.1109/TKDE.2017.2690299,TKDE,2017,"Resource description framework,W3C,Triples (Data structure),Query processing","The proliferation of heterogeneous Linked Data on the Web poses new challenges to database systems. In particular, the capacity to store, track, and query provenance data is becoming a pivotal feature of modern triplestores. We present methods extending a native RDF store to efficiently handle the storage, tracking, and querying of provenance in RDF data. We describe a reliable and understandable specification of the way results were derived from the data and how particular pieces of data were combined to answer a query. Subsequently, we present techniques to tailor queries with provenance data. We empirically evaluate the presented methods and show that the overhead of storing and tracking provenance is acceptable. Finally, we show that tailoring a query with provenance information can also significantly improve the performance of query execution."
1373,,On Fault Tolerance for Distributed Iterative Dataflow Processing.,"Chen Xu 0001,Markus Holzemer,Manohar Kaul,Juan Soto 0001,Volker Markl",https://doi.org/10.1109/TKDE.2017.2690431,TKDE,2017,"Machine learning,Fault tolerance,Iterative methods,Distributed processing","Large-scale graph and machine learning analytics widely employ distributed iterative processing. Typically, these analytics are a part of a comprehensive workflow, which includes data preparation, model building, and model evaluation. General-purpose distributed dataflow frameworks execute all steps of such workflows holistically. This holistic view enables these systems to reason about and automatically optimize the entire pipeline. Here, graph and machine learning analytics are known to incur a long runtime since they require multiple passes over the data until convergence is reached. Thus, fault tolerance and a fast-recovery from any intermittent failure is critical for efficient analysis. In this paper, we propose novel fault-tolerant mechanisms for graph and machine learning analytics that run on distributed dataflow systems. We seek to reduce checkpointing costs and shorten failure recovery times. For graph processing, rather than writing checkpoints that block downstream operators, our mechanism writes checkpoints in an unblocking manner that does not break pipelined tasks. In contrast to the conventional approach for unblocking checkpointing (e.g., that manage checkpoints independently for immutable datasets), we inject the checkpoints of mutable datasets into the iterative dataflow itself. Hence, our mechanism is iteration-aware by design. This simplifies the system architecture and facilitates coordinating checkpoint creation during iterative graph processing. Moreover, we are able to rapidly rebound, via confined recovery, by exploiting the fact that log files exist locally on healthy nodes and managing to avoid a complete recomputation from scratch. In addition, we propose replica recovery for machine learning algorithms, whereby we employ a broadcast variable that enables us to quickly recover without having to introduce any checkpoints. In order to evaluate our fault tolerance strategies, we conduct both a theoretical study and experimental analyses using Apache Flink and discover that they outperform blocking checkpointing and complete recovery."
1374,,A Unified Framework for Metric Transfer Learning.,"Yonghui Xu,Sinno Jialin Pan,Hui Xiong,Qingyao Wu,Ronghua Luo,Huaqing Min,Hengjie Song",https://doi.org/10.1109/TKDE.2017.2669193,TKDE,2017,"Kernel,Euclidean distance,Training data,Mobile handsets,Learning systems,Knowledge transfer","Transfer learning has been proven to be effective for the problems where training data from a source domain and test data from a target domain are drawn from different distributions. To reduce the distribution divergence between the source domain and the target domain, many previous studies have been focused on designing and optimizing objective functions with the Euclidean distance to measure dissimilarity between instances. However, in some real-world applications, the Euclidean distance may be inappropriate to capture the intrinsic similarity or dissimilarity between instances. To deal with this issue, in this paper, we propose a metric transfer learning framework (MTLF) to encode metric learning in transfer learning. In MTLF, instance weights are learned and exploited to bridge the distributions of different domains, while Mahalanobis distance is learned simultaneously to maximize the intra-class distances and minimize the inter-class distances for the target domain. Unlike previous work where instance weights and Mahalanobis distance are trained in a pipelined framework that potentially leads to error propagation across different components, MTLF attempts to learn instance weights and a Mahalanobis distance in a parallel framework to make knowledge transfer across domains more effective. Furthermore, we develop general solutions to both classification and regression problems on top of MTLF, respectively. We conduct extensive experiments on several real-world datasets on object recognition, handwriting recognition, and WiFi location to verify the effectiveness of MTLF compared with a number of state-of-the-art methods."
1375,,Efficient Algorithms for the Identification of Top-k Structural Hole Spanners in Large Social Networks.,"Wenzheng Xu,Mojtaba Rezvani,Weifa Liang,Jeffrey Xu Yu,Chengfei Liu",https://doi.org/10.1109/TKDE.2017.2651825,TKDE,2017,"Social network services,Bridges,Estimation,Electronic mail,Computer science,Collaboration,Australia","Recent studies show that individuals in a social network can be divided into different groups of densely connected communities, and these individuals who bridge different communities, referred to as structural hole spanners, have great potential to acquire resources/information from communities and thus benefit from the access. Structural hole spanners are crucial in many real applications such as community detections, diffusion controls, viral marketing, etc. In spite of their importance, little attention has been paid to them. Particularly, how to accurately characterize the structural hole spanners and how to devise efficient yet scalable algorithms to find them in a large social network are fundamental issues. In this paper, we study the top-k structural hole spanner problem. We first provide a novel model to measure the quality of structural hole spanners through exploiting the structural hole spanner properties. Due to its NP-hardness, we then devise two efficient yet scalable algorithms, by developing innovative filtering techniques that can filter out unlikely solutions as quickly as possible, while the proposed techniques are built up on fast estimations of the upper and lower bounds on the cost of an optimal solution and make use of articulation points in real social networks. We finally conduct extensive experiments to validate the effectiveness of the proposed model, and to evaluate the performance of the proposed algorithms using real world datasets. The experimental results demonstrate that the proposed model can capture the characteristics of structural hole spanners accurately, and the structural hole spanners found by the proposed algorithms are much better than those by existing algorithms in all considered social networks, while the running times of the proposed algorithms are very fast."
1376,,Online Multi-Task Learning Framework for Ensemble Forecasting.,"Jianpeng Xu,Pang-Ning Tan,Jiayu Zhou,Lifeng Luo",https://doi.org/10.1109/TKDE.2017.2662006,TKDE,2017,"Predictive models,Forecasting,Time series analysis,Computational modeling,Prediction algorithms,Data models,Numerical models","Ensemble forecasting is a widely-used numerical prediction method for modeling the evolution of nonlinear dynamic systems. To predict the future state of such systems, a set of ensemble member forecasts is generated from multiple runs of computer models, where each run is obtained by perturbing the starting condition or using a different model representation of the system. The ensemble mean or median is typically chosen as a point estimate for the ensemble member forecasts. These approaches are limited in that they assume each ensemble member is equally skillful and may not preserve the temporal autocorrelation of the predicted time series. To overcome these limitations, we present an online multi-task learning framework called ORION to estimate the optimal weights for combining the ensemble member forecasts. Unlike other existing formulations, the proposed framework is novel in that its learning algorithm must backtrack and revise its previous forecasts before making future predictions if the earlier forecasts were incorrect when verified against new observation data. We termed this strategy as online learning with restart. Our proposed framework employs a graph Laplacian regularizer to ensure consistency of the predicted time series. It can also accommodate different types of loss functions, including ϵ-insensitive and quantile loss functions, the latter of which is particularly useful for extreme value prediction. A theoretical proof demonstrating the convergence of our algorithm is also given. Experimental results on seasonal soil moisture forecasts from 12 major river basins in North America demonstrate the superiority of ORION compared to other baseline algorithms."
1377,,Patient Flow Prediction via Discriminative Learning of Mutually-Correcting Processes.,"Hongteng Xu,Weichang Wu,Shamim Nemati,Hongyuan Zha",https://doi.org/10.1109/TKDE.2016.2618925,TKDE,2017,"Hidden Markov models,Copper,Data models,Robustness,Medical diagnostic imaging,Hospitals","Over the past decade, the rate of care unit (CU) use in the United States has been increasing. With an aging population and ever-growing demand for medical care, effective management of patients' transitions among different care facilities will prove indispensible for shortening the length of hospital stays, improving patient outcomes, allocating critical care resources, and reducing preventable re-admissions. In this paper, we focus on an important problem of predicting the so-called “patient flow” from longitudinal electronic health records (EHRs), which has not been explored via existing machine learning techniques. By treating a sequence of transition events as a point process, we develop a novel framework for modeling patient flow through various CUs and jointly predicting patients' destination CUs and duration days. Instead of learning a generative point process model via maximum likelihood estimation, we propose a novel discriminative learning algorithm aiming at improving the prediction of transition events in the case of sparse data. By parameterizing the proposed model as a mutually-correcting process, we formulate the estimation problem via generalized linear models, which lends itself to efficient learning based on alternating direction method of multipliers (ADMM). Furthermore, we achieve simultaneous feature selection and learning by adding a group-lasso regularizer to the ADMM algorithm. Additionally, for suppressing the negative influence of data imbalance on the learning of model, we synthesize auxiliary training data for the classes with extremely few samples, and improve the robustness of our learning method accordingly. Testing on real-world data, we show that our method obtains superior performance in terms of accuracy of predicting the destination CU transition and duration of each CU occupancy."
1378,,Fusing Complete Monotonic Decision Trees.,"Hang Xu,Wenjian Wang,Yuhua Qian",https://doi.org/10.1109/TKDE.2017.2725832,TKDE,2017,"Decision trees,Information systems,Data mining,Vegetation,Education,Entropy,Algorithm design and analysis","Monotonic classification is a kind of classification task in which a monotonicity constraint exist between features and class, i.e., if sample x
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sub>
 has a higher value in each feature than sample x
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</sub>
, it should be assigned to a class with a higher level than the level of x
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">j</sub>
's class. Several methods have been proposed, but they have some limits such as with limited kind of data or limited classification accuracy. In our former work, the classification accuracy on monotonic classification has been improved by fusing monotonic decision trees, but it always has a complex classification model. This work aims to find a monotonic classifier to process both nominal and numeric data by fusing complete monotonic decision trees. Through finding the completed feature subsets based on discernibility matrix on ordinal dataset, a set of monotonic decision trees can be obtained directly and automatically, on which the rank is still preserved. Fewer decision trees are needed, which will serve as base classifiers to construct a decision forest fused complete monotonic decision trees. The experiment results on 10 datasets demonstrate that the proposed method can reduce the number of base classifiers effectively and then simplify classification model, and obtain good classification performance simultaneously."
1379,,RaPare - A Generic Strategy for Cold-Start Rating Prediction Problem.,"Jingwei Xu 0001,Yuan Yao 0001,Hanghang Tong,Xianping Tao,Jian Lu 0001",https://doi.org/10.1109/TKDE.2016.2615039,TKDE,2017,"Recommender systems,Collaboration,Interviews,Optimization,Algorithm design and analysis,Calibration,Electronic mail","In recent years, recommender system is one of indispensable components in many e-commerce websites. One of the major challenges that largely remains open is the cold-start problem, which can be viewed as a barrier that keeps the cold-start users/items away from the existing ones. In this paper, we aim to break through this barrier for cold-start users/items by the assistance of existing ones. In particular, inspired by the classic Elo Rating System, which has been widely adopted in chess tournaments, we propose a novel rating comparison strategy (RAPARE) to learn the latent profiles of cold-start users/items. The centerpiece of our RAPARE is to provide a fine-grained calibration on the latent profiles of cold-start users/items by exploring the differences between cold-start and existing users/items. As a generic strategy, our proposed strategy can be instantiated into existing methods in recommender systems. To reveal the capability of RAPARE strategy, we instantiate our strategy on two prevalent methods in recommender systems, i.e., the matrix factorization based and neighborhood based collaborative filtering. Experimental evaluations on five real data sets validate the superiority of our approach over the existing methods in cold-start scenario."
1380,,Bayesian Nonparametric Relational Topic Model through Dependent Gamma Processes.,"Junyu Xuan,Jie Lu 0001,Guangquan Zhang 0001,Richard Yi Da Xu,Xiangfeng Luo",https://doi.org/10.1109/TKDE.2016.2636182,TKDE,2017,"Bayes methods,Data models,Stochastic processes,Indexes,Electronic mail,Probability distribution,Social network services","Traditional relational topic models provide a successful way to discover the hidden topics from a document network. Many theoretical and practical tasks, such as dimensional reduction, document clustering, and link prediction, could benefit from this revealed knowledge. However, existing relational topic models are based on an assumption that the number of hidden topics is known a priori, which is impractical in many real-world applications. Therefore, in order to relax this assumption, we propose a nonparametric relational topic model using stochastic processes instead of fixed-dimensional probability distributions in this paper. Specifically, each document is assigned a Gamma process, which represents the topic interest of this document. Although this method provides an elegant solution, it brings additional challenges when mathematically modeling the inherent network structure of typical document network, i.e., two spatially closer documents tend to have more similar topics. Furthermore, we require that the topics are shared by all the documents. In order to resolve these challenges, we use a subsampling strategy to assign each document a different Gamma process from the global Gamma process, and the subsampling probabilities of documents are assigned with a Markov Random Field constraint that inherits the document network structure. Through the designed posterior inference algorithm, we can discover the hidden topics and its number simultaneously. Experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and, more importantly, the number of topics."
1381,,A Survey on Context Learning.,"Guangxu Xun,Xiaowei Jia,Vishrawas Gopalakrishnan,Aidong Zhang",https://doi.org/10.1109/TKDE.2016.2614508,TKDE,2017,"Context,Context modeling,Semantics,Analytical models,Computational modeling,Mathematical model,Smoothing methods","Learning semantics based on context information has been researched in many research areas for decades. Context information can not only be directly used as the input data, but also sometimes used as auxiliary knowledge to improve existing models. This survey aims at providing a structured and comprehensive overview of the research on context learning. We summarize and group the existing literature into four categories, Explicit Analysis, Implicit Analysis, Neural Network Models, and Composite Models, based on the underlying techniques adopted by them. For each category, we talk about the basic idea and techniques, and also introduce how context information is utilized as the model input or incorporated into the model to enhance the performance or extend the domain of application as auxiliary knowledge. In addition, we discuss the advantages and disadvantages of each model from both the technical and practical point of view."
1382,,An Approach for Building Efficient and Accurate Social Recommender Systems Using Individual Relationship Networks.,"Surong Yan,Kwei-Jay Lin,Xiaolin Zheng,Wenyu Zhang 0001,Xiaoqing Feng",https://doi.org/10.1109/TKDE.2017.2717984,TKDE,2017,"Recommender systems,Social network services,Computational modeling,Data models,Complexity theory,Scalability,Sparse matrices","Social recommender system, using social relation networks as additional input to improve the accuracy of traditional recommender systems, has become an important research topic. However, most existing methods utilize the entire user relationship network with no consideration to its huge size, sparsity, imbalance, and noise issues. This may degrade the efficiency and accuracy of social recommender systems. This study proposes a new approach to manage the complexity of adding social relation networks to recommender systems. Our method first generates an individual relationship network (IRN) for each user and item by developing a novel fitting algorithm of relationship networks to control the relationship propagation and contracting. We then fuse matrix factorization with social regularization and the neighborhood model using IRN's to generate recommendations. Our approach is quite general, and can also be applied to the item-item relationship network by switching the roles of users and items. Experiments on four datasets with different sizes, sparsity levels, and relationship types show that our approach can improve predictive accuracy and gain a better scalability compared with state-of-the-art social recommendation methods."
1383,,A Scalable Data Chunk Similarity Based Compression Approach for Efficient Big Sensing Data Processing on Cloud.,"Chi Yang,Jinjun Chen",https://doi.org/10.1109/TKDE.2016.2531684,TKDE,2017,"Sensors,Big data,Cloud computing,Data compression,Data models,Scalability","Big sensing data is prevalent in both industry and scientific research applications where the data is generated with high volume and velocity. Cloud computing provides a promising platform for big sensing data processing and storage as it provides a flexible stack of massive computing, storage, and software services in a scalable manner. Current big sensing data processing on Cloud have adopted some data compression techniques. However, due to the high volume and velocity of big sensing data, traditional data compression techniques lack sufficient efficiency and scalability for data processing. Based on specific on-Cloud data compression requirements, we propose a novel scalable data compression approach based on calculating similarity among the partitioned data chunks. Instead of compressing basic data units, the compression will be conducted over partitioned data chunks. To restore original data sets, some restoration functions and predictions will be designed. MapReduce is used for algorithm implementation to achieve extra scalability on Cloud. With real world meteorological big sensing data experiments on U-Cloud platform, we demonstrate that the proposed scalable compression approach based on data chunk similarity can significantly improve data compression efficiency with affordable data accuracy loss."
1384,,Stochastic Blockmodeling and Variational Bayes Learning for Signed Network Analysis.,"Bo Yang 0002,Xueyan Liu,Yang Li 0030,Xuehua Zhao",https://doi.org/10.1109/TKDE.2017.2700304,TKDE,2017,"Stochastic processes,Bayes methods,Data models,Approximation algorithms,Computational modeling,Biological system modeling,Prediction algorithms","Signed networks with positive and negative links attract considerable interest in their studying since they contain more information than unsigned networks. Community detection and sign (or attitude) prediction are still primary challenges, as the fundamental problems of signed network analysis. For this, a generative Bayesian approach is presented wherein 1) a signed stochastic blockmodel is proposed to characterize the community structure in the context of signed networks, by explicit formulating the distributions of the density and frustration of signed links from a stochastic perspective, and 2) a model learning algorithm is advanced by theoretical deriving a variational Bayes EM for the parameter estimation and variation-based approximate evidence for the model selection. The comparison of the above approach with the state-of-the-art methods on synthetic and real-world networks, shows its advantage in the community detection and sign prediction for the exploratory networks."
1385,,Discrete Nonnegative Spectral Clustering.,"Yang Yang 0002,Fumin Shen,Zi Huang,Heng Tao Shen,Xuelong Li",https://doi.org/10.1109/TKDE.2017.2701825,TKDE,2017,"Clustering algorithms,Predictive models,Robustness,Optimization,Matrix decomposition,Biological system modeling,Laplace equations","Spectral clustering has been playing a vital role in various research areas. Most traditional spectral clustering algorithms comprise two independent stages (e.g., first learning continuous labels and then rounding the learned labels into discrete ones), which may cause unpredictable deviation of resultant cluster labels from genuine ones, thereby leading to severe information loss and performance degradation. In this work, we study how to achieve discrete clustering as well as reliably generalize to unseen data. We propose a novel spectral clustering scheme which deeply explores cluster label properties, including discreteness, nonnegativity, and discrimination, as well as learns robust out-of-sample prediction functions. Specifically, we explicitly enforce a discrete transformation on the intermediate continuous labels, which leads to a tractable optimization problem with a discrete solution. Besides, we preserve the natural nonnegative characteristic of the clustering labels to enhance the interpretability of the results. Moreover, to further compensate the unreliability of the learned clustering labels, we integrate an adaptive robust module with ℓ
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2p</sub>
 loss to learn prediction function for grouping unseen data. We also show that the out-of-sample component can inject discriminative knowledge into the learning of cluster labels under certain conditions. Extensive experiments conducted on various data sets have demonstrated the superiority of our proposal as compared to several existing clustering approaches."
1386,,Tracking Influential Individuals in Dynamic Networks.,"Yu Yang 0001,Zhefeng Wang,Jian Pei,Enhong Chen",https://doi.org/10.1109/TKDE.2017.2734667,TKDE,2017,"Computational modeling,Social network services,Heuristic algorithms,Peer-to-peer computing,Integrated circuit modeling,Algorithm design and analysis","In this paper, we tackle a challenging problem inherent in a series of applications: tracking the influential nodes in dynamic networks. Specifically, we model a dynamic network as a stream of edge weight updates. This general model embraces many practical scenarios as special cases, such as edge and node insertions, deletions as well as evolving weighted graphs. Under the popularly adopted linear threshold model and independent cascade model, we consider two essential versions of the problem: finding the nodes whose influences passing a user specified threshold and finding the top-k most influential nodes. Our key idea is to use the polling-based methods and maintain a sample of random RR sets so that we can approximate the influence of nodes with provable quality guarantees. We develop an efficient algorithm that incrementally updates the sample random RR sets against network changes. We also design methods to determine the proper sample sizes for the two versions of the problem so that we can provide strong quality guarantees and, at the same time, be efficient in both space and time. In addition to the thorough theoretical results, our experimental results on five real network data sets clearly demonstrate the effectiveness and efficiency of our algorithms."
1387,,Robust Online Multi-Task Learning with Correlative and Personalized Structures.,"Peng Yang 0010,Peilin Zhao,Xin Gao 0001",https://doi.org/10.1109/TKDE.2017.2703106,TKDE,2017,"Approximation algorithms,Matrix decomposition,Periodic structures,Prediction algorithms,Robustness,Algorithm design and analysis,Closed-form solutions","Multi-Task Learning (MTL) can enhance a classifier's generalization performance by learning multiple related tasks simultaneously. Conventional MTL works under the offline or batch setting, and suffers from expensive training cost and poor scalability. To address such inefficiency issues, online learning techniques have been applied to solve MTL problems. However, most existing algorithms of online MTL constrain task relatedness into a presumed structure via a single weight matrix, which is a strict restriction that does not always hold in practice. In this paper, we propose a robust online MTL framework that overcomes this restriction by decomposing the weight matrix into two components: The first one captures the low-rank common structure among tasks via a nuclear norm and the second one identifies the personalized patterns of outlier tasks via a group lasso. Theoretical analysis shows the proposed algorithm can achieve a sub-linear regret with respect to the best linear model in hindsight. Even though the above framework achieves good performance, the nuclear norm that simply adds all nonzero singular values together may not be a good low-rank approximation. To improve the results, we use a log-determinant function as a non-convex rank approximation. The gradient scheme is applied to optimize log-determinant function and can obtain a closed-form solution for this refined problem. Experimental results on a number of real-world applications verify the efficacy of our method."
1388,,Scalable Algorithms for CQA Post Voting Prediction.,"Yuan Yao 0001,Hanghang Tong,Feng Xu 0007,Jian Lu 0001",https://doi.org/10.1109/TKDE.2017.2696535,TKDE,2017,"Heuristic algorithms,Prediction algorithms,Couplings,Kernel,Lips,Linearity,Predictive models","Community Question Answering (CQA) sites, such as Stack Overflow and Yahoo! Answers, have become very popular in recent years. These sites contain rich crowdsourcing knowledge contributed by the site users in the form of questions and answers, and these questions and answers can satisfy the information needs of more users. In this article, we aim at predicting the voting scores of questions/answers shortly after they are posted in the CQA sites. To accomplish this task, we identify three key aspects that matter with the voting of a post, i.e., the non-linear relationships between features and output, the question and answer coupling, and the dynamic fashion of data arrivals. A family of algorithms are proposed to model the above three key aspects. Some approximations and extensions are also proposed to scale up the computation. We analyze the proposed algorithms in terms of optimality, correctness, and complexity. Extensive experimental evaluations conducted on two real data sets demonstrate the effectiveness and efficiency of our algorithms."
1389,,Multimodal KB Harvesting for Emerging Spatial Entities.,"Jinyoung Yeo,Hyunsouk Cho,Jin-Woo Park,Seung-won Hwang",https://doi.org/10.1109/TKDE.2017.2651805,TKDE,2017,"Flickr,Twitter,Encyclopedias,Electronic publishing,Internet","New entities are being created daily. Though the novelty of these entities naturally attracts mentions, due to lack of prior knowledge, it is more challenging to collect knowledge about such entities than pre-existing entities, whose KBs are comprehensively annotated through LBSNs and EBSNs. In this paper, we focus on knowledge harvesting for emerging spatial entities (ESEs), such as new businesses and venues, assuming we have only a list of ESE names. Existing techniques for knowledge base (KB) harvesting are primarily associated with information extraction from textual corpora. In contrast, we propose a multimodal method for event detection based on the complementary interaction of image, text, and user information between multi-source platforms, namely Flickr and Twitter. We empirically validate our harvesting approaches improve the quality of KB with enriched place and event knowledge."
1390,,Spatial-Aware Hierarchical Collaborative Deep Learning for POI Recommendation.,"Hongzhi Yin,Weiqing Wang 0001,Hao Wang 0005,Ling Chen 0006,Xiaofang Zhou 0001",https://doi.org/10.1109/TKDE.2017.2741484,TKDE,2017,"Semantics,Collaboration,Machine learning,Feature extraction,Probabilistic logic,Additives,Data models","Point-of-interest (POI) recommendation has become an important way to help people discover attractive and interesting places, especially when they travel out of town. However, the extreme sparsity of user-POI matrix and cold-start issues severely hinder the performance of collaborative filtering-based methods. Moreover, user preferences may vary dramatically with respect to the geographical regions due to different urban compositions and cultures. To address these challenges, we stand on recent advances in deep learning and propose a Spatial-Aware Hierarchical Collaborative Deep Learning model (SH-CDL). The model jointly performs deep representation learning for POIs from heterogeneous features and hierarchically additive representation learning for spatial-aware personal preferences. To combat data sparsity in spatial-aware user preference modeling, both the collective preferences of the public in a given target region and the personal preferences of the user in adjacent regions are exploited in the form of social regularization and spatial smoothing. To deal with the multimodal heterogeneous features of the POIs, we introduce a late feature fusion strategy into our SH-CDL model. The extensive experimental analysis shows that our proposed model outperforms the state-of-the-art recommendation models, especially in out-of-town and cold-start recommendation scenarios."
1391,,Adaptive Ensembling of Semi-Supervised Clustering Solutions.,"Zhiwen Yu 0002,Zongqiang Kuang,Jiming Liu 0001,Hongsheng Chen,Jun Zhang 0003,Jane You,Hau-San Wong,Guoqiang Han",https://doi.org/10.1109/TKDE.2017.2695615,TKDE,2017,"Clustering algorithms,Measurement,Algorithm design and analysis,Matrix decomposition,Cancer,Bioinformatics,Kernel","Conventional semi-supervised clustering approaches have several shortcomings, such as (1) not fully utilizing all useful must-link and cannot-link constraints, (2) not considering how to deal with high dimensional data with noise, and (3) not fully addressing the need to use an adaptive process to further improve the performance of the algorithm. In this paper, we first propose the transitive closure based constraint propagation approach, which makes use of the transitive closure operator and the affinity propagation to address the first limitation. Then, the random subspace based semi-supervised clustering ensemble framework with a set of proposed confidence factors is designed to address the second limitation and provide more stable, robust, and accurate results. Next, the adaptive semi-supervised clustering ensemble framework is proposed to address the third limitation, which adopts a newly designed adaptive process to search for the optimal subspace set. Finally, we adopt a set of nonparametric tests to compare different semi-supervised clustering ensemble approaches over multiple datasets. The experimental results on 20 real high dimensional cancer datasets with noisy genes and 10 datasets from UCI datasets and KEEL datasets show that (1) The proposed approaches work well on most of the real-world datasets. (2) It outperforms other state-of-the-art approaches on 12 out of 20 cancer datasets, and 8 out of 10 UCI machine learning datasets."
1392,,A Generic Method for Accelerating LSH-Based Similarity Join Processing.,"Chenyun Yu,Sarana Nutanong,Hangyu Li,Cong Wang 0001,Xingliang Yuan",https://doi.org/10.1109/TKDE.2016.2638838,TKDE,2017,"Algorithm design and analysis,Error analysis,Indexes,Search problems,Acceleration,Approximation algorithms,Measurement","Locality sensitive hashing (LSH) is an efficient method for solving the problem of approximate similarity search in highdimensional spaces. Through LSH, a high-dimensional similarity join can be processed in the same way as hash join, making the cost of joining two large datasets linear. By judicially analyzing the properties of multiple LSH algorithms, we propose a generic method to speed up the process of joining two large datasets using LSH. The crux of our method lies in the waywhich we identify a set of representative points to reduce the number of LSH lookups. Theoretical analyzes show that our proposed method can greatly reduce the number of lookup operations and retain the same result accuracy compared to executing LSH lookups for every query point. Furthermore, we demonstrate the generality of our method by showing that the same principle can be applied to LSH algorithms for three different metrics: the Euclidean distance (QALSH), Jaccard similarity measure (MinHash), and Hamming distance (sequence hashing). Results from experimental studies using real datasets confirm our error analyzes and show significant improvements of our method overthe state-of-the-art LSH method: to achieve over 0.95 recall, we only need to operate LSH lookups for at most 15 percent of the query points."
1393,,Keyword Search over Distributed Graphs with Compressed Signature.,"Ye Yuan 0001,Xiang Lian,Lei Chen 0002,Jeffrey Xu Yu,Guoren Wang,Yongjiao Sun",https://doi.org/10.1109/TKDE.2017.2656079,TKDE,2017,"Keyword search,Search problems,Algorithm design and analysis,Servers,Partitioning algorithms,Distributed databases,Resource description framework","Graph keyword search has drawn many research interests, since graph models can generally represent both structured and unstructured databases and keyword searches can extract valuable information for users without the knowledge of the underlying schema and query language. In practice, data graphs can be extremely large, e.g., a Web-scale graph containing billions of vertices. The state-of-the-art approaches employ centralized algorithms to process graph keyword searches, and thus they are infeasible for such large graphs, due to the limited computational power and storage space of a centralized server. To address this problem, we investigate keyword search for Web-scale graphs deployed in a distributed environment. We first give a naive search algorithm to answer the query efficiently. However, the naive search algorithm uses a flooding search strategy that incurs large time and network overhead. To remedy this shortcoming, we then propose a signature-based search algorithm. Specifically, we design a vertex signature that encodes the shortest-path distance from a vertex to any given keyword in the graph. As a result, we can find query answers by exploring fewer paths, so that the time and communication costs are low. Moreover, we reorganize the graph data in the cluster after its initial random partitioning so that the signature-based techniques are more effective. Finally, our experimental results demonstrate the feasibility of our proposed approach in performing keyword searches over Web-scale graph data."
1394,,Query Reorganization Algorithms for Efficient Boolean Information Filtering.,"Lefteris Zervakis,Christos Tryfonopoulos,Spiros Skiadopoulos,Manolis Koubarakis",https://doi.org/10.1109/TKDE.2016.2620140,TKDE,2017,"Indexing,Clustering algorithms,Servers,Data models,Vegetation","In the information filtering paradigm, clients subscribe to a server with continuous queries that express their information needs and get notified every time appropriate information is published. To perform this task in an efficient way, servers employ indexing schemes that support fast matches of the incoming information with the query database. Such indexing schemes involve (i) main-memory trie-based data structures that cluster similar queries by capturing common elements between them and (ii) efficient filtering mechanisms that exploit this clustering to achieve high throughput and low filtering times. However, state-of-the-art indexing schemes are sensitive to the query insertion order and cannot adopt to an evolving query workload, degrading the filtering performance over time. In this paper, we present an adaptive trie-based algorithm that outperforms current methods by relying on query statistics to reorganise the query database. Contrary to previous approaches, we show that the nature of the constructed tries, rather than their compactness, is the determining factor for efficient filtering performance. Our algorithm does not depend on the order of insertion of queries in the database, manages to cluster queries even when clustering possibilities are limited, and achieves more than 96 percent filtering time improvement over its state-of-the-art competitors. Finally, we demonstrate that our solution is easily extensible to multi-core machines."
1395,,Citywide Traffic Volume Estimation Using Trajectory Data.,"Xianyuan Zhan,Yu Zheng 0004,Xiuwen Yi,Satish V. Ukkusuri",https://doi.org/10.1109/TKDE.2016.2621104,TKDE,2017,"Roads,Volume measurement,Feature extraction,Trajectory,Global Positioning System,Vehicles,Solid modeling","Traffic volume estimation at the city scale is an important problem useful to many transportation operations and urban applications. This paper proposes a hybrid framework that integrates both state-of-art machine learning techniques and well-established traffic flow theory to estimate citywide traffic volume. In addition to typical urban context features extracted from multiple sources, we extract a special set of features from GPS trajectories based on the implications of traffic flow theory, which provide extra information on the speed-flow relationship. Using the network-wide speed information estimated from a travel speed estimation model, a volume related high level feature is first learned using an unsupervised graphical model. A volume re-interpretation model is then introduced to map the volume related high level feature to the predicted volume using a small amount of ground truth data for training. The framework is evaluated using a GPS trajectory dataset from 33,000 Beijing taxis and volume ground truth data obtained from 4,980 video clips. The results demonstrate effectiveness and potential of the proposed framework in citywide traffic volume estimation."
1396,,Enabling Kernel-Based Attribute-Aware Matrix Factorization for Rating Prediction.,"Jia-Dong Zhang,Chi-Yin Chow,Jin Xu",https://doi.org/10.1109/TKDE.2016.2641439,TKDE,2017,"Sparse matrices,Recommender systems,Context,Adaptation models,Predictive models,Collaboration,Social network services","In recommender systems, one key task is to predict the personalized rating of a user to a new item and then return the new items having the top predicted ratings to the user. Recommender systems usually apply collaborative filtering techniques (e.g., matrix factorization) over a sparse user-item rating matrix to make rating prediction. However, the collaborative filtering techniques are severely affected by the data sparsity of the underlying user-item rating matrix and often confront the cold-start problems for new items and users. Since the attributes of items and social links between users become increasingly accessible in the Internet, this paper exploits the rich attributes of items and social links of users to alleviate the rating sparsity effect and tackle the cold-start problems. Specifically, we first propose a Kernel-based Attribute-aware Matrix Factorization model called KAMF to integrate the attribute information of items into matrix factorization. KAMF can discover the nonlinear interactions among attributes, users, and items, which mitigate the rating sparsity effect and deal with the cold-start problem for new items by nature. Further, we extend KAMF to address the cold-start problem for new users by utilizing the social links between users. Finally, we conduct a comprehensive performance evaluation for KAMF using two large-scale real-world data sets recently released in Yelp and MovieLens. Experimental results show that KAMF achieves significantly superior performance against other state-of-the-art rating prediction techniques."
1397,,Disambiguation-Free Partial Label Learning.,"Min-Ling Zhang,Fei Yu 0010,Cai-Zhi Tang",https://doi.org/10.1109/TKDE.2017.2721942,TKDE,2017,"Training,Encoding,Silicon,Face,Painting,Predictive models,Collaboration","In partial label learning, each training example is associated with a set of candidate labels among which only one is the ground-truth label. The common strategy to induce predictive model is trying to disambiguate the candidate label set, i.e., differentiating the modeling outputs of individual candidate labels. Specifically, disambiguation by differentiation can be conducted either by identifying the ground-truth label iteratively or by treating each candidate label equally. Nonetheless, the disambiguation strategy is prone to be misled by the false positive labels co-occurring with ground-truth label. In this paper, a new partial label learning strategy is studied which refrains from conducting disambiguation. Specifically, by adapting error-correcting output codes (ECOC), a simple yet effective approach named PL-ECOC is proposed by utilizing candidate label set as an entirety. During training phase, to build binary classifier w.r.t. each column coding, any partially labeled example will be regarded as a positive or negative training example only if its candidate label set entirely falls into the coding dichotomy. During testing phase, class label for the unseen instance is determined via loss-based decoding which considers binary classifiers' empirical performance and predictive margin. Extensive experiments show that PL-ECOC performs favorably against state-of-the-art partial label learning approaches."
1398,,Feature Constrained Multi-Task Learning Models for Spatiotemporal Event Forecasting.,"Liang Zhao 0002,Qian Sun,Jieping Ye,Feng Chen 0001,Chang-Tien Lu,Naren Ramakrishnan",https://doi.org/10.1109/TKDE.2017.2657624,TKDE,2017,"Forecasting,Predictive models,Urban areas,Twitter,Data models,Spatiotemporal phenomena","Spatial event forecasting from social media is potentially extremely useful but suffers from critical challenges, such as the dynamic patterns of features (keywords) and geographic heterogeneity (e.g., spatial correlations, imbalanced samples, and different populations in different locations). Most existing approaches (e.g., LASSO regression, dynamic query expansion, and burst detection) address some, but not all, of these challenges. Here, we propose a novel multi-task learning framework that aims to concurrently address all the challenges involved. Specifically, given a collection of locations (e.g., cities), forecasting models are built for all the locations simultaneously by extracting and utilizing appropriate shared information that effectively increases the sample size for each location, thus improving the forecasting performance. The new model combines both static features derived from a predefined vocabulary by domain experts and dynamic features generated from dynamic query expansion in a multi-task feature learning framework. Different strategies to balance homogeneity and diversity between static and dynamic terms are also investigated. And, efficient algorithms based on Iterative Group Hard Thresholding are developed to achieve efficient and effective model training and prediction. Extensive experimental evaluations on Twitter data from civil unrest and influenza outbreak datasets demonstrate the effectiveness and efficiency of our proposed approach."
1399,,Robust Dual Clustering with Adaptive Manifold Regularization.,"Nengwen Zhao,Lefei Zhang,Bo Du 0001,Qian Zhang 0009,Jane You,Dacheng Tao",https://doi.org/10.1109/TKDE.2017.2732986,TKDE,2017,"Clustering algorithms,Manifolds,Data mining,Data models,Robustness,Clustering methods,Data structures","In recent years, various data clustering algorithms have been proposed in the data mining and engineering communities. However, there are still drawbacks in traditional clustering methods which are worth to be further investigated, such as clustering for the high dimensional data, learning an ideal affinity matrix which optimally reveals the global data structure, discovering the intrinsic geometrical and discriminative properties of the data space, and reducing the noises influence brings by the complex data input. In this paper, we propose a novel clustering algorithm called robust dual clustering with adaptive manifold regularization (RDC), which simultaneously performs dual matrix factorization tasks with the target of an identical cluster indicator in both of the original and projected feature spaces, respectively. Among which, the l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2,1</sub>
-norm is used instead of the conventional l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
-norm to measure the loss, which helps to improve the model robustness by relieving the influences by the noises and outliers. In order to better consider the intrinsic geometrical and discriminative data structure, we incorporate the manifold regularization term on the cluster indicator by using a particularly learned affinity matrix which is more suitable for the clustering task. Moreover, a novel augmented lagrangian method (ALM) based procedure is designed to effectively and efficiently seek the optimal solution of the proposed RDC optimization. Numerous experiments on the representative data sets demonstrate the superior performance of the proposed method compares to the existing clustering algorithms."
1400,,Maximizing Acceptance in Rejection-Aware Spatial Crowdsourcing.,"Libin Zheng,Lei Chen 0002",https://doi.org/10.1109/TKDE.2017.2676771,TKDE,2017,"Crowdsourcing,Servers,Heuristic algorithms,Throughput,Dynamic programming,Mobile computing,Mobile communication","With the rapid development of mobile networks and the widespread usage of mobile devices, spatial crowdsourcing, which refers to assigning location-based tasks to moving workers, has drawn increasing attention. One of the important issues in spatial crowdsourcing is task assignment, which allocates tasks to appropriate workers. However, existing works generally assume that no rejection would happen after the task assignment is completed by the server. Ignorance of such an operation can lead to low system throughput. Thus, in this paper, we take workers' rejection into consideration and try to maximize workers' acceptance in order to improve the system throughput. Specifically, we first formally define the problem of maximizing workers' acceptance in rejection-aware spatial crowdsourcing. Unfortunately, the problem is NP-hard. We propose two exact solutions to obtain the optimal assignment, but they are not efficient enough and not scalable for large inputs. Then, we present four approximation approaches for improving the efficiency. Finally, we show the effectiveness of the proposed pruning strategy for the exact solutions and the superiority of the proposed Greedy algorithm over other approximation methods through extensive experiments."
1401,,Efficient Clue-Based Route Search on Road Networks.,"Bolong Zheng,Han Su,Wen Hua,Kai Zheng 0001,Xiaofang Zhou 0001,GuoHui Li 0001",https://doi.org/10.1109/TKDE.2017.2703848,TKDE,2017,"Indexes,Heuristic algorithms,Roads,Search problems,Approximation algorithms,Context,Dynamic programming","With the advances in geo-positioning technologies and location-based services, it is nowadays quite common for road networks to have textual contents on the vertices. Previous work on identifying an optimal route that covers a sequence of query keywords has been studied in recent years. However, in many practical scenarios, an optimal route might not always be desirable. For example, a personalized route query is issued by providing some clues that describe the spatial context between PoIs along the route, where the result can be far from the optimal one. Therefore, in this paper, we investigate the problem of clue-based route search (CRS), which allows a user to provide clues on keywords and spatial relationships. First, we propose a greedy algorithm and a dynamic programming algorithm as baselines. To improve efficiency, we develop a branch-and-bound algorithm that prunes unnecessary vertices in query processing. In order to quickly locate candidate, we propose an AB-tree that stores both the distance and keyword information in tree structure. To further reduce the index size, we construct a PB-tree by utilizing the virtue of 2-hop label index to pinpoint the candidate. Extensive experiments are conducted and verify the superiority of our algorithms and index structures."
1402,,A Quality-Sensitive Method for Learning from Crowds.,"Jinhong Zhong,Peng Yang 0008,Ke Tang",https://doi.org/10.1109/TKDE.2017.2738643,TKDE,2017,"Reliability,Supervised learning,Crowdsourcing,Optimization,Support vector machines,Noise measurement,Learning systems","In real-world applications, the oracle who can label all instances correctly may not exist or may be too expensive to acquire. Alternatively, crowdsourcing provides an easy way to get labels at a low cost from multiple non-expert annotators. During the past few years, much attention has been paid to learning from such crowdsourcing data, namely Learning from Crowds (LFC). Despite their proper statistical foundations, the existing methods for LFC still suffer from several disadvantages, such as needing prior knowledge to select the expertise model to represent the behavior of annotators, involving non-convex optimization problems, or restricting the classifier type being used. This paper addresses LFC from a quality-sensitive perspective and presents a novel framework named QS-LFC. Through reformulating the original LFC problem as a quality-sensitive learning problem, the above-mentioned disadvantages of existing methods can be avoided. Further, a support vector machine (SVM) implementation of QS-LFC is proposed. Experimental results on both synthetic and real-world data sets demonstrate that QS-LFC can achieve better generalization performance and is more robust to the noisy labels, than the existing methods."
1403,,Modeling and Learning Distributed Word Representation with Metadata for Question Retrieval.,"Guangyou Zhou,Jimmy Xiangji Huang",https://doi.org/10.1109/TKDE.2017.2665625,TKDE,2017,"Metadata,Semantics,Context modeling,Knowledge discovery,Computational modeling,Kernel,Aggregates","Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved."
1404,,Query Expansion with Enriched User Profiles for Personalized Search Utilizing Folksonomy Data.,"Dong Zhou,Xuan Wu,Wenyu Zhao,Séamus Lawless,Jianxun Liu",https://doi.org/10.1109/TKDE.2017.2668419,TKDE,2017,"Semantics,Web search,Web pages,Vocabulary,Search problems,History,Collaboration","Query expansion has been widely adopted in Web search as a way of tackling the ambiguity of queries. Personalized search utilizing folksonomy data has demonstrated an extreme vocabulary mismatch problem that requires even more effective query expansion methods. Co-occurrence statistics, tag-tag relationships, and semantic matching approaches are among those favored by previous research. However, user profiles which only contain a user's past annotation information may not be enough to support the selection of expansion terms, especially for users with limited previous activity with the system. We propose a novel model to construct enriched user profiles with the help of an external corpus for personalized query expansion. Our model integrates the current state-of-the-art text representation learning framework, known as word embeddings, with topic models in two groups of pseudo-aligned documents. Based on user profiles, we build two novel query expansion techniques. These two techniques are based on topical weights-enhanced word embeddings, and the topical relevance between the query and the terms inside a user profile, respectively. The results of an in-depth experimental evaluation, performed on two real-world datasets using different external corpora, show that our approach outperforms traditional techniques, including existing non-personalized and personalized query expansion methods."
1405,,Computing Semantic Similarity of Concepts in Knowledge Graphs.,"Ganggao Zhu,Carlos Angel Iglesias",https://doi.org/10.1109/TKDE.2016.2610428,TKDE,2017,"Semantics,Integrated circuits,Measurement,Knowledge based systems,Taxonomy,Motion pictures,Knowledge engineering","This paper presents a method for measuring the semantic similarity between concepts in Knowledge Graphs (KGs) such as WordNet and DBpedia. Previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts (e.g., path length and depth), or only on the Information Content (IC) of concepts. We propose a semantic similarity method, namely wpath, to combine these two approaches, using IC to weight the shortest path length between concepts. Conventional corpus-based IC is computed from the distributions of concepts over textual corpus, which is required to prepare a domain corpus containing annotated concepts and has high computational cost. As instances are already extracted from textual corpus and annotated by concepts in KGs, graph-based IC is proposed to compute IC based on the distributions of concepts over instances. Through experiments performed on well known word similarity datasets, we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods. Moreover, in a real category classification evaluation, the wpath method has shown the best performance in terms of accuracy and F score."
1406,1,Differentially Private Data Publishing and Analysis - A Survey.,"Tianqing Zhu,Gang Li 0009,Wanlei Zhou,Philip S. Yu",https://doi.org/10.1109/TKDE.2017.2697856,TKDE,2017,"Privacy,Data privacy,Publishing,Data analysis,Sensitivity,Algorithm design and analysis,Data models","Differential privacy is an essential and prevalent privacy model that has been widely explored in recent decades. This survey provides a comprehensive and structured overview of two research directions: differentially private data publishing and differentially private data analysis. We compare the diverse release mechanisms of differentially private data publishing given a variety of input data in terms of query type, the maximum number of queries, efficiency, and accuracy. We identify two basic frameworks for differentially private data analysis and list the typical algorithms used within each framework. The results are compared and discussed based on output accuracy and efficiency. Further, we propose several possible directions for future research and possible applications."
1407,,Unsupervised Visual Hashing with Semantic Assistant for Content-Based Image Retrieval.,"Lei Zhu 0002,Jialie Shen,Liang Xie 0001,Zhiyong Cheng",https://doi.org/10.1109/TKDE.2016.2562624,TKDE,2017,"Visualization,Semantics,Correlation,Boats,Image retrieval,Oceans,Flickr","As an emerging technology to support scalable content-based image retrieval (CBIR), hashing has recently received great attention and became a very active research domain. In this study, we propose a novel unsupervised visual hashing approach called semantic-assisted visual hashing (SAVH). Distinguished from semi-supervised and supervised visual hashing, its core idea is to effectively extract the rich semantics latently embedded in auxiliary texts of images to boost the effectiveness of visual hashing without any explicit semantic labels. To achieve the target, a unified unsupervised framework is developed to learn hash codes by simultaneously preserving visual similarities of images, integrating the semantic assistance from auxiliary texts on modeling high-order relationships of inter-images, and characterizing the correlations between images and shared topics. Our performance study on three publicly available image collections: Wiki, MIR Flickr, and NUS-WIDE indicates that SAVH can achieve superior performance over several state-of-the-art techniques."
1408,,SAP - Improving Continuous Top-K Queries Over Streaming Data.,"Rui Zhu 0003,Bin Wang 0015,Xiaochun Yang 0001,Baihua Zheng,Guoren Wang",https://doi.org/10.1109/TKDE.2017.2662236,TKDE,2017,"Partitioning algorithms,Maintenance engineering,Monitoring,Temperature sensors,Heuristic algorithms,Complexity theory,Indexes","Continuous top-k query over streaming data is a fundamental problem in database. In this paper, we focus on the sliding window scenario, where a continuous top-k query returns the top-k objects within each query window on the data stream. Existing algorithms support this type of queries via incrementally maintaining a subset of objects in the window and try to retrieve the answer from this subset as much as possible whenever the window slides. However, since all the existing algorithms are sensitive to query parameters and data distribution, they all suffer from expensive incremental maintenance cost. In this paper, we propose a self-adaptive partition framework to support continuous top-k query. It partitions the window into sub-windows and only maintains a small number of candidates with highest scores in each sub-window. Based on this framework, we have developed several partition algorithms to cater for different object distributions and query parameters. To our best knowledge, it is the first algorithm that achieves logarithmic complexity w.r.t. k for incrementally maintaining the candidate set even in the worstcase scenarios."
1409,,SimRank on Uncertain Graphs.,"Rong Zhu,Zhaonian Zou,Jianzhong Li 0001",https://doi.org/10.1109/TKDE.2017.2725275,TKDE,2017,"Proteins,Uncertainty,Approximation algorithms,Measurement uncertainty,Linear matrix inequalities,Markov processes,Erbium","SimRank is a similarity measure between vertices in a graph. Recently, many algorithms have been proposed to efficiently evaluate SimRank similarities. However, the existing algorithms either overlook uncertainty in graph structures or depends on an unreasonable assumption. In this paper, we study SimRank on uncertain graphs. Following the random-walk-based formulation of SimRank on deterministic graphs and the possible world model of uncertain graphs, we first define random walks on uncertain graphs and show that our definition of random walks satisfies Markov's property. We formulate our SimRank measure based on random walks on uncertain graphs. We discover a critical difference between random walks on uncertain graphs and random walks on deterministic graphs, which makes all existing SimRank computation algorithms on deterministic graphs inapplicable to uncertain graphs. For SimRank computation, we consider computing both single-pair SimRank and single-source top-K SimRank. We propose three algorithms, namely the sampling algorithm with high efficiency, the two-phase algorithm with comparable efficiency and higher accuracy, and a speeding-up algorithm with much higher efficiency. Meanwhile, we present an optimized algorithm for efficient computing the single-source top-K SimRank. The experimental results verify the effectiveness of our SimRank measure and the efficiency of the proposed SimRank computation algorithms."
1410,,Bag-of-Discriminative-Words (BoDW) Representation via Topic Modeling.,"Yueting Zhuang,Hanqi Wang,Jun Xiao 0001,Fei Wu 0001,Yi Yang 0001,Weiming Lu 0001,Zhongfei Zhang",https://doi.org/10.1109/TKDE.2017.2658571,TKDE,2017,"Computational modeling,Visualization,Data models,Analytical models,Vocabulary,Semantics,Bayes methods","Many of the words in a given document either deliver facts (objective) or express opinions (subjective), respectively, depending on the topics they are involved in. For example, given a bunch of documents, the word “bug” assigned to the topic “order Hemiptera” apparently remarks one object (i.e., one kind of insects), while the same word assigned to the topic “software” probably conveys a negative opinion. Motivated by the intuitive assumption that different words have varying degrees of discriminative power in delivering the objective sense or the subjective sense with respect to their assigned topics, a model named as discriminatively objective-subjective LDA (dosLDA) is proposed in this paper. The essential idea underlying the proposed dosLDA is that a pair of objective and subjective selection variables are explicitly employed to encode the interplay between topics and discriminative power for the words in documents in a supervised manner. As a result, each document is appropriately represented as “bag-of-discriminativewords” (BoDW). The experiments reported on documents and images demonstrate that dosLDA not only performs competitively over traditional approaches in terms of topic modeling and document classification, but also has the ability to discern the discriminative power of each word in terms of its objective or subjective sense with respect to its assigned topic."
1411,,Unsupervised Single and Multiple Views Feature Extraction with Structured Graph.,"Wenzhang Zhuge,Feiping Nie 0001,Chenping Hou,Dongyun Yi",https://doi.org/10.1109/TKDE.2017.2725263,TKDE,2017,"Feature extraction,Learning systems,Periodic structures,Linear programming,Algorithm design and analysis,Laplace equations,Loss measurement","Many feature extraction methods reduce the dimensionality of data based on the input graph matrix. The graph construction which reflects relationships among raw data points is crucial to the quality of resulting low-dimensional representations. To improve the quality of graph and make it more suitable for feature extraction tasks, we incorporate a new graph learning mechanism into feature extraction and add an interaction between the learned graph and the low-dimensional representations. Based on this learning mechanism, we propose a novel framework, termed as unsupervised single view feature extraction with structured graph (FESG), which learns both a transformation matrix and an ideal structured graph containing the clustering information. Moreover, we propose a novel way to extend FESG framework for multi-view learning tasks. The extension is named as unsupervised multiple views feature extraction with structured graph (MFESG), which learns an optimal weight for each view automatically without requiring an additional parameter. To show the effectiveness of the framework, we design two concrete formulations within FESG and MFESG, together with two efficient solving algorithms. Promising experimental results on plenty of real-world datasets have validated the effectiveness of our proposed algorithms."
1412,,"Towards Real-Time, Country-Level Location Classification of Worldwide Tweets.","Arkaitz Zubiaga,Alex Voss,Rob Procter,Maria Liakata,Bo Wang 0034,Adam Tsakalidis",https://doi.org/10.1109/TKDE.2017.2698463,TKDE,2017,"Urban areas,Geology,Twitter,Real-time systems,Metadata,History","The increase of interest in using social media as a source for research has motivated tackling the challenge of automatically geolocating tweets, given the lack of explicit location information in the majority of tweets. In contrast to much previous work that has focused on location classification of tweets restricted to a specific country, here we undertake the task in a broader context by classifying global tweets at the country level, which is so far unexplored in a real-time scenario. We analyze the extent to which a tweet's country of origin can be determined by making use of eight tweet-inherent features for classification. Furthermore, we use two datasets, collected a year apart from each other, to analyze the extent to which a model trained from historical tweets can still be leveraged for classification of new tweets. With classification experiments on all 217 countries in our datasets, as well as on the top 25 countries, we offer some insights into the best use of tweet-inherent features for an accurate country-level classification of tweets. We find that the use of a single feature, such as the use of tweet content alone-the most widely used feature in previous work-leaves much to be desired. Choosing an appropriate combination of both tweet content and metadata can actually lead to substantial improvements of between 20 and 50 percent. We observe that tweet content, the user's self-reported location and the user's real name, all of which are inherent in a tweet and available in a real-time scenario, are particularly useful to determine the country of origin. We also experiment on the applicability of a model trained on historical tweets to classify new tweets, finding that the choice of a particular combination of features whose utility does not fade over time can actually lead to comparable performance, avoiding the need to retrain. However, the difficulty of achieving accurate classification increases slightly for countries with multiple commonalities, especially for English and Spanish speaking countries."
105,,Don&apos;t Let Google Know I&apos;m Lonely.,"Pol Mac Aonghusa,Douglas J. Leith",https://doi.org/10.1145/2937754,TOPS,2016,[],
106,1,A New Framework for Privacy-Preserving Aggregation of Time-Series Data.,"Fabrice Benhamouda,Marc Joye,Benoît Libert",https://doi.org/10.1145/2873069,TOPS,2016,[],
107,,On the Workflow Satisfiability Problem with Class-Independent Constraints for Hierarchical Organizations.,"Jason Crampton,Andrei Gagarin,Gregory Z. Gutin,Mark Jones 0001,Magnus Wahlström",https://doi.org/10.1145/2988239,TOPS,2016,[],
108,,Detection of Rogue Certificates from Trusted Certificate Authorities Using Deep Neural Networks.,"Zheng Dong,Kevin Kane,L. Jean Camp",https://doi.org/10.1145/2975591,TOPS,2016,[],
109,,Looks Like Eve - Exposing Insider Threats Using Eye Movement Biometrics.,"Simon Eberz,Kasper Bonne Rasmussen,Vincent Lenders,Ivan Martinovic",https://doi.org/10.1145/2904018,TOPS,2016,[],
110,,MAC Precomputation with Applications to Secure Memory.,"Juan A. Garay,Vladimir Kolesnikov,Rae McLellan",https://doi.org/10.1145/2943780,TOPS,2016,[],
111,,Inhibiting and Detecting Offline Password Cracking Using ErsatzPasswords.,"Christopher N. Gutierrez,Mohammed H. Almeshekah,Eugene H. Spafford,Mikhail J. Atallah,Jeffrey Avery",http://dl.acm.org/citation.cfm?id=2996457,TOPS,2016,[],
112,1,General Graph Data De-Anonymization - From Mobility Traces to Social Networks.,"Shouling Ji,Weiqing Li,Mudhakar Srivatsa,Jing (Selena) He,Raheem A. Beyah",https://doi.org/10.1145/2894760,TOPS,2016,[],
113,,How to Train Your Browser - Preventing XSS Attacks Using Contextual Script Fingerprints.,"Dimitris Mitropoulos,Konstantinos Stroggylos,Diomidis Spinellis,Angelos D. Keromytis",https://doi.org/10.1145/2939374,TOPS,2016,[],
114,1,Privacy-Preserving Publishing of Hierarchical Data.,"Ismet Ozalp,Mehmet Emre Gursoy,Mehmet Ercan Nergiz,Yücel Saygin",https://doi.org/10.1145/2976738,TOPS,2016,[],
115,,Efficient and Accurate Behavior-Based Tracking of Malware-Control Domains in Large ISP Networks.,"Babak Rahbarinia,Roberto Perdisci,Manos Antonakakis",https://doi.org/10.1145/2960409,TOPS,2016,[],
116,,Toward Robotic Robbery on the Touch Screen.,"Abdul Serwadda,Vir V. Phoha,Zibo Wang,Rajesh Kumar,Diksha Shukla",https://doi.org/10.1145/2898353,TOPS,2016,[],
117,,Behavioral Study of Users When Interacting with Active Honeytokens.,"Asaf Shabtai,Maya Bercovitch,Lior Rokach,Ya&apos;akov (Kobi) Gal,Yuval Elovici,Erez Shmueli",https://doi.org/10.1145/2854152,TOPS,2016,[],
118,,Designing Password Policies for Strength and Usability.,"Richard Shay,Saranga Komanduri,Adam L. Durity,Phillip (Seyoung) Huh,Michelle L. Mazurek,Sean M. Segreti,Blase Ur,Lujo Bauer,Nicolas Christin,Lorrie Faith Cranor",https://doi.org/10.1145/2891411,TOPS,2016,[],
119,,An Efficient User Verification System Using Angle-Based Mouse Movement Biometrics.,"Nan Zheng,Aaron Paloski,Haining Wang",https://doi.org/10.1145/2893185,TOPS,2016,[],
1413,,Venus - Scalable Real-Time Spatial Queries on Microblogs with Adaptive Load Shedding.,"Amr Magdy 0001,Mohamed F. Mokbel,Sameh Elnikety,Suman Nath,Yuxiong He",https://doi.org/10.1109/TKDE.2015.2493531,TKDE,2016,"Venus,Real-time systems,Memory management,Accuracy,Indexing,Twitter","Microblogging services have become among the most popular services on the web in the last few years. This led to significant increase in data size, speed, and applications. This paper presents 
<i>Venus</i>
; a system that supports real-time spatial queries on microblogs. 
<i>Venus</i>
 supports its queries on a spatial boundary 
<inline-formula><tex-math notation=""LaTeX"">$R$</tex-math></inline-formula>
 and a temporal boundary 
<inline-formula><tex-math notation=""LaTeX"">$T$</tex-math></inline-formula>
, from which only the top- 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 microblogs are returned in the query answer based on a spatio-temporal ranking function. Supporting such queries requires 
<i>Venus</i>
 to digest hundreds of millions of real-time microblogs in main-memory with high rates, yet, it provides low query responses and efficient memory utilization. To this end, 
<i>Venus</i>
 employs: (1) an efficient in-memory spatio-temporal index that digests high rates of incoming microblogs in real time, (2) a scalable query processor that prune the search space, 
<inline-formula><tex-math notation=""LaTeX"">$R$</tex-math></inline-formula>
 and 
<inline-formula><tex-math notation=""LaTeX"">$T$</tex-math></inline-formula>
, effectively to provide low query latency on millions of items in real time, and (3) a group of memory optimization techniques that provide system administrators with different options to save significant memory resources while keeping the query accuracy almost perfect. 
<i>Venus</i>
 memory optimization techniques make use of the local arrival rates of microblogs to smartly shed microblogs that are old enough not to contribute to any query answer. In addition, 
<i>Venus</i>
 can adaptively, in real time, adjust its load shedding based on both the spatial distribution and the parameters of incoming query loads. All 
<i>Venus</i>
 components can accommodate different spatial and temporal ranking functions that are able to capture the importance of each dimension differently depending on the applications requirements. Extensive experimental results based on real Twitter data and actual locations of Bing search queries show that 
<i>Venus</i>
 supports high arrival rates of up to 64 K microblogs/second and average query latency of 4 msec."
1414,,Incremental Consolidation of Data-Intensive Multi-Flows.,"Petar Jovanovic 0001,Oscar Romero 0001,Alkis Simitsis,Alberto Abelló",https://doi.org/10.1109/TKDE.2016.2515609,TKDE,2016,"Coal,Business,Complexity theory,Bismuth,Data models,Semantics,Data integration","Business intelligence (BI) systems depend on efficient integration of disparate and often heterogeneous data. The integration of data is governed by data-intensive flows and is driven by a set of information requirements. Designing such flows is in general a complex process, which due to the complexity of business environments is hard to be done manually. In this paper, we deal with the challenge of efficient design and maintenance of data-intensive flows and propose an incremental approach, namely CoAl , for semi-automatically consolidating data-intensive flows satisfying a given set of information requirements. CoAl works at the logical level and consolidates data flows from either high-level information requirements or platform-specific programs. As CoAl integrates a new data flow, it opts for maximal reuse of existing flows and applies a customizable cost model tuned for minimizing the overall cost of a unified solution. We demonstrate the efficiency and effectiveness of our approach through an experimental evaluation using our implemented prototype."
1415,,Efficient Recovery of Missing Events.,"Jianmin Wang 0001,Shaoxu Song,Xiaochen Zhu,Xuemin Lin 0001,Jiaguang Sun 0001",https://doi.org/10.1109/TKDE.2016.2594785,TKDE,2016,"Business,Petri nets,Engineering drawings,Indexes,Routing,Sun,Data mining","For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance."
1416,,GenPerm - A Unified Method for Detecting Non-Overlapping and Overlapping Communities.,"Tanmoy Chakraborty 0002,Suhansanu Kumar,Niloy Ganguly,Animesh Mukherjee 0001,Sanjukta Bhowmick",https://doi.org/10.1109/TKDE.2016.2554119,TKDE,2016,"Measurement,Inference algorithms,Prediction algorithms,Algorithm design and analysis,Government,Detection algorithms,Indexes","Detection of non-overlapping and overlapping communities are essentially the same problem. However, current algorithms focus either on finding overlapping or non-overlapping communities. We present a generalized framework that can identify both non-overlapping and overlapping communities, without any prior input about the network or its community distribution. To do so, we introduce a vertex-based metric, GenPerm, that quantifies by how much a vertex belongs to each of its constituent communities. Our community detection algorithm is based on maximizing the GenPerm over all the vertices in the network. We demonstrate, through experiments over synthetic and real-world networks, that GenPerm is more effective than other metrics in evaluating community structure. Further, we show that due to its vertex-centric property, GenPerm can be used to unfold several inferences beyond community detection, such as core-periphery analysis and message spreading. Our algorithm for maximizing GenPerm outperforms six state-of-the-art algorithms in accurately predicting the ground-truth labels. Finally, we discuss the problem of resolution limit in overlapping communities and demonstrate that maximizing GenPerm can mitigate this problem."
1417,,The Moving K Diversified Nearest Neighbor Query.,"Yu Gu 0002,Guanli Liu,Jianzhong Qi 0001,Hongfei Xu,Ge Yu 0001,Rui Zhang 0003",https://doi.org/10.1109/TKDE.2016.2593464,TKDE,2016,"Approximation algorithms,Prefetching,Nearest neighbor searches,Usability,Query processing,Trajectory,Indexes","As a major type of continuous spatial queries, the moving 
<inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>
 nearest neighbor (
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
NN) query has been studied extensively. However, most existing studies have focused on only the query efficiency. In this paper, we consider further the usability of the query results, in particular the diversification of the returned data points. We thereby formulate a new type of query named the 
<i>moving <inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math><alternatives><inline-graphic xlink:href=""gu-ieq4-2593464.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> diversified nearest neighbor query (M<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:href=""gu-ieq5-2593464.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>DNN)</i>
. This type of query continuously reports the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 diversified nearest neighbors while the query object is moving. Here, the degree of diversity of the 
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math> </inline-formula>
NN set is defined on the distance between the objects in the 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
NN set. Computing the 
<inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>
 diversified nearest neighbors is an NP-hard problem. We propose an algorithm to maintain incrementally the 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 diversified nearest neighbors to reduce the query processing costs. We further propose two approximate algorithms to obtain even higher query efficiency with precision bounds. We verify the effectiveness and efficiency of the proposed algorithms both theoretically and empirically. The results confirm the superiority of the proposed algorithms over the baseline algorithm."
1418,,To Combat Multi-Class Imbalanced Problems by Means of Over-Sampling Techniques.,"Lida Abdi,Sattar Hashemi",https://doi.org/10.1109/TKDE.2015.2458858,TKDE,2016,"Mathematical model,Training,Accuracy,Eigenvalues and eigenfunctions,Machine learning algorithms,Algorithm design and analysis,Benchmark testing","Class imbalance problem is quite pervasive in our nowadays human practice. This problem basically refers to the skewness in the data underlying distribution which, in turn, imposes many difficulties on typical machine learning algorithms. To deal with the emerging issues arising from multi-class skewed distributions, existing efforts are mainly divided into two categories: model-oriented solutions and data-oriented techniques. Focusing on the latter, this paper presents a new over-sampling technique which is inspired by Mahalanobis distance. The presented over-sampling technique, called MDO (Mahalanobis Distance-based Over-sampling technique), generates synthetic samples which have the same Mahalanobis distance from the considered class mean as other minority class examples. By preserving the covariance structure of the minority class instances and intelligently generating synthetic samples along the probability contours, new minority class instances are modelled better for learning algorithms. Moreover, MDO can reduce the risk of overlapping between different class regions which are considered as a serious challenge in multi-class problems. Our theoretical analyses and empirical observations across wide spectrum multi-class imbalanced benchmarks indicate that MDO is the method of choice by offering statistical superior MAUC and precision compared to the popular over-sampling techniques."
1419,,On Learning of Choice Models with Interactive Attributes.,Manish Aggarwal,https://doi.org/10.1109/TKDE.2016.2563434,TKDE,2016,"Biological system modeling,Decision making,Predictive models,Data models,Computational modeling,Machine learning algorithms,Optimization","Introducing recent advances in the machine learning techniques to state-of-the-art discrete choice models, we develop an approach to infer the unique and complex decision making process of a decision-maker (DM), which is characterized by the DM's priorities and attitudinal character, along with the attributes interaction, to name a few. On the basis of exemplary preference information in the form of pairwise comparisons of alternatives, our method seeks to induce a DM's preference model in terms of the parameters of recent discrete choice models. To this end, we reduce our learning function to a constrained non-linear optimization problem. Our learning approach is a simple one that takes into consideration the interaction among the attributes along with the priorities and the unique attitudinal character of a DM. The experimental results on standard benchmark datasets suggest that our approach is not only intuitively appealing and easily interpretable but also competitive to state-of-the-art methods."
1420,,D-ToSS - A Distributed Throwaway Spatial Index Structure for Dynamic Location Data.,"Afsin Akdogan,Cyrus Shahabi,Ugur Demiryurek",https://doi.org/10.1109/TKDE.2016.2572697,TKDE,2016,"Servers,Computer architecture,Microprocessors,Distributed databases,Spatial indexes,Query processing","Many applications deal with moving object datasets, e.g., mobile phone social networking, scientific simulations, and ride-sharing services. These applications need to handle a tremendous number of spatial objects that continuously move and execute spatial queries to explore their surroundings. To manage such update-heavy workloads, several throwaway index structures have recently been proposed, where a static index is rebuilt periodically from scratch rather than updated incrementally. It has been shown that throwaway indices outperform specialized moving-object indices that maintain location updates incrementally. However, throwaway indices suffer from scalability due to their single-server design and the only distributed throwaway index (D-MOVIES), extension of a centralized approach, does not scale out as the number of servers increases, especially during query processing phase."
1421,,Recurring and Novel Class Detection Using Class-Based Ensemble for Evolving Data Stream.,"Tahseen Al-Khateeb,Mohammad M. Masud,Khaled Al-Naami,Sadi Evren Seker,Ahmad M. Mustafa,Latifur Khan,Zouheir Trabelsi,Charu C. Aggarwal,Jiawei Han 0001",https://doi.org/10.1109/TKDE.2015.2507123,TKDE,2016,"Data models,Electronic mail,Benchmark testing,Twitter,Market research,Decision trees,Error analysis","Streaming data is one of the attention receiving sources for concept-evolution studies. When a new class occurs in the data stream it can be considered as a new concept and so the concept-evolution. One attractive problem occurring in the concept-evolution studies is the recurring classes from our previous study. In data streams, a class can disappear and reappear after a while. Existing studies on data stream classification techniques either misclassify the recurring class or falsely identify the recurring classes as novel classes. Because of the misclassification or false novel classification, the error rates increases on those studies. In this paper we address the problem by defining a novel ensemble technique “class-based” ensemble which replaces the traditional “chunk-based” approach in order to detect the recurring classes. We discuss the details of two different approaches in class-based ensemble and explain and compare them in detail. Different than the previous studies in the field, we also prove the superiority of both “class-based” ensemble method over state-of-art techniques via empirical approach on a number of benchmark data sets including Web comments as text mining challenge."
1422,,kNNVWC - An Efficient k-Nearest Neighbors Approach Based on Various-Widths Clustering.,"Abdul Mohsen Almalawi,Adil Fahad,Zahir Tari,Muhammad Aamir Cheema,Ibrahim Khalil",https://doi.org/10.1109/TKDE.2015.2460735,TKDE,2016,"Indexes,Clustering algorithms,Partitioning algorithms,Approximation algorithms,Search problems,Vegetation,Routing","The k-nearest neighbor approach (k-NN) has been extensively used as a powerful non-parametric technique in many scientific and engineering applications. However, this approach incurs a large computational cost. Hence, this issue has become an active research field. In this work, a novel k-NN approach based on various-widths clustering, named kNNVWC, to efficiently find k-NNs for a query object from a given data set, is presented. kNNVWC does clustering using various widths, where a data set is clustered with a global width first and each produced cluster that meets the predefined criteria is recursively clustered with its own local width that suits its distribution. This reduces the clustering time, in addition to balancing the number of produced clusters and their respective sizes. Maximum efficiency is achieved by using triangle inequality to prune unlikely clusters. Experimental results demonstrate that kNNVWC performs well in finding k-NNs for query objects compared to a number of k-NN search algorithms, especially for a data set with high dimensions, various distributions and large size."
1423,,A Comprehensive Approach to &apos;Now&apos; in Temporal Relational Databases - Semantics and Representation.,"Luca Anselma,Luca Piovesan,Abdul Sattar 0001,Bela Stantic,Paolo Terenziani",https://doi.org/10.1109/TKDE.2016.2588490,TKDE,2016,"Semantics,Data models,Algebra,Relational databases,Indexing,Intelligent systems","Now-related temporal data play an important role in many applications. Clifford et al.'s approach is a milestone to model the semantics of `now' in temporal relational databases. Several relational representation models for now-related data have been presented; however, the semantics of such representations has not been explicitly studied. Additionally, the definition of a relational algebra to query now-related data is an open problem. We propose the first integrated approach that provides both a neat semantics for now-related data and a compact 1NF representation (data model and relational algebra) for them. Additionally, our approach also extends current approaches to consider (i) domains where it is not always possible to know when changes in the world are recorded in the database and (ii) now-related data with a bound on their persistency in the future. To do so, we explicitly model the notion of temporal indeterminacy in the future for now-related data. The properties of our approach are also analyzed both from a theoretical (semantic correctness and reducibility of the algebra) and from an experimental point of view. Experiments show that, despite the fact that our approach is a major extension to current temporal relational approaches, no significant overhead is added to deal with `now'."
1424,,An Optimization Model for Clustering Categorical Data Streams with Drifting Concepts.,"Liang Bai,Xueqi Cheng,Jiye Liang,Huawei Shen",https://doi.org/10.1109/TKDE.2016.2594068,TKDE,2016,"Optimization,Data models,Clustering algorithms,Linear programming,Indexes,Algorithm design and analysis,Market research","There is always a lack of a cluster validity function and optimization strategy to find out clusters and catch the evolution trend of cluster structures on a categorical data stream. Therefore, this paper presents an optimization model for clustering categorical data streams. In the model, a cluster validity function is proposed as the objective function to evaluate the effectiveness of the clustering model while each new input data subset is flowing. It simultaneously considers the certainty of the clustering model and the continuity with the last clustering model in the clustering process. An iterative optimization algorithm is proposed to solve an optimal solution of the objective function with some constraints. Furthermore, we strictly derive a detection index for drifting concepts from the optimization model. We propose a detection method that integrates the detection index and the optimization model to catch the evolution trend of cluster structures on a categorical data stream. The new method can effectively avoid ignoring the effect of the clustering validity on the detection result. Finally, using the experimental studies on several real data sets, we illustrate the effectiveness of the proposed algorithm in clustering categorical data streams, compared with existing data-streams clustering algorithms."
1425,,Discovering the k Representative Skyline Over a Sliding Window.,"Mei Bai,Junchang Xin,Guoren Wang,Luming Zhang,Roger Zimmermann,Ye Yuan 0001,Xindong Wu 0001",https://doi.org/10.1109/TKDE.2016.2546242,TKDE,2016,"Time complexity,Greedy algorithms,Approximation algorithms,Acceleration,Algorithm design and analysis,Indexes,Decision making","<i>A representative skyline</i>
 contains 
<inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>
 skyline points that can represent its corresponding full skyline. The existing measuring criteria of 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 representative skylines are specifically designed for static data, and they cannot effectively handle streaming data. In this paper, we focus on the problem of calculating the 
<inline-formula> <tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 representative skyline over data streams. First, we propose a new criterion to choose 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 skyline points as the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 representative skyline for data stream environments, termed the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
 largest dominance skyline ( 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-LDS), which is representative to the entire data set and is highly stable over the streaming data. Second, we propose an efficient exact algorithm, called Prefix-based Algorithm (PBA), to solve the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula>
-LDS problem in a 2-dimensional space. The time complexity of PBA is only 
<inline-formula><tex-math notation=""LaTeX""> $\mathcal {O}((M-k)\times k)$</tex-math></inline-formula>
 where 
<inline-formula><tex-math notation=""LaTeX"">$M$</tex-math> </inline-formula>
 is the size of the full skyline set. Third, the 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-LDS problem for a 
<inline-formula> <tex-math notation=""LaTeX"">$d$</tex-math></inline-formula>
-dimensional (
<inline-formula><tex-math notation=""LaTeX"">$d\ge 3$</tex-math> </inline-formula>
) space turns out to be very complex. Therefore, a greedy algorithm is designed to answer 
<inline-formula><tex-math notation=""LaTeX""> $k$</tex-math> </inline-formula>
-LDS queries. To further accelerate the calculation, we propose a 
<inline-formula> <tex-math notation=""LaTeX"">$\epsilon$</tex-math></inline-formula>
-greedy algorithm which can achieve an approximate factor of 
<inline-formula><tex-math notation=""LaTeX""> $\frac{1}{(1+\epsilon)}(1-\frac{1}{\sqrt{e}})$</tex-math></inline-formula>
. Experimental results on both synthetic and real-world data show that our 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math> </inline-formula>
-LDS significantly outperforms its competitors in data stream environments. Furthermore, we demonstrate that the proposed 
<inline-formula> <tex-math notation=""LaTeX"">$\epsilon$</tex-math></inline-formula>
-greedy algorithm can solve 
<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>
-LDS efficiently and with a competitive accuracy."
1426,,Cold-Start Recommendation with Provable Guarantees - A Decoupled Approach.,"Iman Barjasteh,Rana Forsati,Dennis Ross,Abdol-Hossein Esfahanian,Hayder Radha",https://doi.org/10.1109/TKDE.2016.2522422,TKDE,2016,"Collaboration,Matrix decomposition,Knowledge engineering,Data engineering,Recommender systems,Motion pictures,Electronic mail","Although the matrix completion paradigm provides an appealing solution to the collaborative filtering problem in recommendation systems, some major issues, such as data sparsity and cold-start problems, still remain open. In particular, when the rating data for a subset of users or items is entirely missing, commonly known as the 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> cold-start</i>
 problem, the standard matrix completion methods are inapplicable due the non-uniform sampling of available ratings. In recent years, there has been considerable interest in dealing with cold-start users or items that are principally based on the idea of exploiting other sources of information to compensate for this lack of rating data. In this paper, we propose a novel and general algorithmic framework based on matrix completion that simultaneously exploits the similarity information among users and items to alleviate the cold-start problem. In contrast to existing methods, our proposed recommender algorithm, dubbed DecRec, 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">decouples</i>
 the following two aspects of the cold-start problem to effectively exploit the side information: (i) the completion of a rating sub-matrix, which is generated by excluding cold-start users/items from the original rating matrix; and (ii) the transduction of knowledge from existing ratings to cold-start items/users using side information. This crucial difference prevents the error propagation of completion and transduction, and also significantly boosts the performance when appropriate side information is incorporated. The recovery error of the proposed algorithm is analyzed theoretically and, to the best of our knowledge, this is the first algorithm that addresses the cold-start problem with provable guarantees on performance. Additionally, we also address the problem where both cold-start user and item challenges are present simultaneously. We conduct thorough experiments on real datasets that complement our theoretical results. These experiments demonstrate the effectiveness of the proposed algorithm in handling the cold-start users/items problem and mitigating data sparsity issue."
1427,,Inference of Regular Expressions for Text Extraction from Examples.,"Alberto Bartoli,Andrea De Lorenzo,Eric Medvet,Fabiano Tarlao",https://doi.org/10.1109/TKDE.2016.2515587,TKDE,2016,"Proposals,Data mining,Training data,Training,Complexity theory,Optimization,Electronic mail","A large class of entity extraction tasks from text that is either semistructured or fully unstructured may be addressed by regular expressions, because in many practical cases the relevant entities follow an underlying syntactical pattern and this pattern may be described by a regular expression. In this work, we consider the long-standing problem of synthesizing such expressions automatically, based solely on examples of the desired behavior. We present the design and implementation of a system capable of addressing extraction tasks of realistic complexity. Our system is based on an evolutionary procedure carefully tailored to the specific needs of regular expression generation by examples. The procedure executes a search driven by a multiobjective optimization strategy aimed at simultaneously improving multiple performance indexes of candidate solutions while at the same time ensuring an adequate exploration of the huge solution space. We assess our proposal experimentally in great depth, on a number of challenging datasets. The accuracy of the obtained solutions seems to be adequate for practical usage and improves over earlier proposals significantly. Most importantly, our results are highly competitive even with respect to human operators. A prototype is available as a web application at <;uri xlink:type=""simple"">http://regex.inginf.units.it<;/uri>."
1428,,Correction to &quot;Inference of Regular Expressions for Text Extraction from Examples&quot;.,"Alberto Bartoli,Andrea De Lorenzo,Eric Medvet,Fabiano Tarlao",https://doi.org/10.1109/TKDE.2016.2557978,TKDE,2016,"Data mining,Electronic mail,Libraries","Presents corrections to typographical errors in the paper, ""Inference of regular expressions for text extraction from examples,"" (Bartoli, A., et al), IEEE Trans. Knowl. Data Eng., vol. 28, no. 5, pp. 1217–1230, May 2016."
1429,,A Game Theory Inspired Approach to Stable Core Decomposition on Weighted Networks.,"Srinka Basu,Ujjwal Maulik",https://doi.org/10.1109/TKDE.2015.2508817,TKDE,2016,"Analytical models,Benchmark testing,Complex networks,Complex systems,Topology,Algorithm design and analysis,Organizations","Meso-scale structural analysis, like core decomposition has uncovered groups of nodes that play important roles in the underlying complex systems. The existing core decomposition approaches generally focus on node properties like degree and strength. The node centric approaches can only capture a limited information about the local neighborhood topology. In the present work, we propose a group density based core analysis approach that overcome the drawbacks of the node centric approaches. The proposed algorithmic approach focuses on weight density, cohesiveness, and stability of a substructure. The method also assigns an unique score to every node that rank the nodes based on their degree of core-ness. To determine the correctness of the proposed method, we propose a synthetic benchmark with planted core structure. A performance test on the null model is carried out using a weighted lattice without core structures. We further test the stability of the approach against random noise. The experimental results prove the superiority of our algorithm over the state-of-the-arts. We finally analyze the core structures of several popular weighted network models and real life weighted networks. The experimental results reveal important node ranking and hierarchical organization of the complex networks, which give us better insight about the underlying systems."
1430,,Clustering and Summarizing Protein-Protein Interaction Networks - A Survey.,"Sourav S. Bhowmick,Boon-Siew Seah",https://doi.org/10.1109/TKDE.2015.2492559,TKDE,2016,"Proteins,Clustering algorithms,Biological processes,Bioinformatics,Data mining,Cells (biology)","The increasing availability and significance of large-scale protein-protein interaction (PPI) data has resulted in a flurry of research activity to comprehend the organization, processes, and functioning of cells by analyzing these data at network level. Network clustering, that analyzes the topological and functional properties of a PPI network to identify clusters of interacting proteins, has gained significant popularity in the bioinformatics as well as data mining research communities. Many studies since the last decade have shown that clustering PPI networks is an effective approach for identifying functional modules, revealing functions of unknown proteins, etc. In this paper, we examine this issue by classifying, discussing, and comparing a wide ranging approaches proposed by the bioinformatics community to cluster PPI networks. A pervasive desire of this review is to emphasize the uniqueness of the network clustering problem in the context of PPI networks and highlight why generic network clustering algorithms proposed by the data mining community cannot be directly adopted to address this problem effectively. We also review a closely related problem to PPI network clustering, network summarization, which can enable us to make sense out of the information contained in large PPI networks by generating multi-level functional summaries."
1431,,Discovering Team Structures in Soccer from Spatiotemporal Data.,"Alina Bialkowski,Patrick Lucey,Peter Carr 0001,Iain A. Matthews,Sridha Sridharan,Clinton Fookes",https://doi.org/10.1109/TKDE.2016.2581158,TKDE,2016,"Trajectory,Probability density function,Heating,Entropy,Spatiotemporal phenomena,Graphical models,Distribution functions","In team sports like soccer, utilizing tracking data for analysis is challenging due to the dynamic and multi-agent nature of the data. The biggest issue surrounds the changing of positions or “roles” between players on a frame-to-frame basis, which causes misalignment of the data and makes it difficult to perform team analysis. In this paper, we present an unsupervised method to learn a formation template which allows us to “align” the tracking data at the frame level. Not only does this approach give important contextual information to facilitate large-scale analysis (e.g., we know when a player is in the left-wing position compared to left-back), it also yields the team structure or “formation” which serves as a strong descriptor for identifying a team's style. The utility of the approach is demonstrated on a full season of player and ball tracking data from a professional soccer league consisting of over 21.5 million frames of player tracking data."
1432,,Incremental Evolving Domain Adaptation.,"Adeleh Bitarafan,Mahdieh Soleymani Baghshah,Marzieh Gheisari",https://doi.org/10.1109/TKDE.2016.2551241,TKDE,2016,"Distributed databases,Kernel,Manifolds,Face recognition,Training data,Lighting,Object recognition","Almost all of the existing domain adaptation methods assume that all test data belong to a single stationary target distribution. However, in many real world applications, data arrive sequentially and the data distribution is continuously evolving. In this paper, we tackle the problem of adaptation to a continuously evolving target domain that has been recently introduced. We assume that the available data for the source domain are labeled but the examples of the target domain can be unlabeled and arrive sequentially. Moreover, the distribution of the target domain can evolve continuously over time. We propose the Evolving Domain Adaptation (EDA) method that first finds a new feature space in which the source domain and the current target domain are approximately indistinguishable. Therefore, source and target domain data are similarly distributed in the new feature space and we use a semi-supervised classification method to utilize both the unlabeled data of the target domain and the labeled data of the source domain. Since test data arrives sequentially, we propose an incremental approach both for finding the new feature space and for semi-supervised classification. Experiments on several real datasets demonstrate the superiority of our proposed method in comparison to the other recent methods."
1433,,Cross-Domain Sentiment Classification Using Sentiment Sensitive Embeddings.,"Danushka Bollegala,Tingting Mu,John Yannis Goulermas",https://doi.org/10.1109/TKDE.2015.2475761,TKDE,2016,"Training,Linear programming,Feature extraction,Logistics,Frequency-domain analysis,Joints,Learning systems","Unsupervised Cross-domain Sentiment Classification is the task of adapting a sentiment classifier trained on a particular domain (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">source domain</i>
), to a different domain (
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">target domain</i>
), without requiring any labeled data for the target domain. By adapting an existing sentiment classifier to previously unseen target domains, we can avoid the cost for manual data annotation for the target domain. We model this problem as embedding learning, and construct three objective functions that capture: (a) distributional properties of 
<italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> pivots</i>
 (i.e., common features that appear in both source and target domains), (b) label constraints in the source domain documents, and (c) geometric properties in the unlabeled documents in both source and target domains. Unlike prior proposals that first learn a lower-dimensional embedding independent of the source domain sentiment labels, and next a sentiment classifier in this embedding, our joint optimisation method learns embeddings that are sensitive to sentiment classification. Experimental results on a benchmark dataset show that by jointly optimising the three objectives we can obtain better performances in comparison to optimising each objective function separately, thereby demonstrating the importance of task-specific embedding learning for cross-domain sentiment classification. Among the individual objective functions, the best performance is obtained by (c). Moreover, the proposed method reports cross-domain sentiment classification accuracies that are statistically comparable to the current state-of-the-art embedding learning methods for cross-domain sentiment classification."
1434,,Improving Collaborative Recommendation via User-Item Subgroups.,"Jiajun Bu,Xin Shen,Bin Xu 0005,Chun Chen 0001,Xiaofei He 0001,Deng Cai 0001",https://doi.org/10.1109/TKDE.2016.2566622,TKDE,2016,"Collaboration,Recommender systems,Clustering algorithms,Motion pictures,Data models,Prediction algorithms","Collaborative filtering (CF) is out of question the most widely adopted and successful recommendation approach. A typical CF-based recommender system associates a user with a group of like-minded users based on their individual preferences over all the items, either explicit or implicit, and then recommends to the user some unobserved items enjoyed by the group. However, we find that two users with similar tastes on one item subset may have totally different tastes on another set. In other words, there exist many user-item subgroups each consisting of a subset of items and a group of like-minded users on these items. It is more reasonable to predict preferences through one user's correlated subgroups, but not the entire user-item matrix. In this paper, to find meaningful subgroups, we formulate a new Multiclass Co-Clustering (MCoC) model, which captures relations of user-to-item, user-to-user, and item-to-item simultaneously. Then, we combine traditional CF algorithms with subgroups for improving their top- 
<inline-formula><tex-math notation=""LaTeX"">$N$</tex-math></inline-formula>
 recommendation performance. Our approach can be seen as a new extension of traditional clustering CF models. Systematic experiments on several real data sets have demonstrated the effectiveness of our proposed approach."
1435,,Adaptive Replication Management in HDFS Based on Supervised Learning.,"Dinh-Mao Bui,Shujaat Hussain,Eui-Nam Huh,Sungyoung Lee",https://doi.org/10.1109/TKDE.2016.2523510,TKDE,2016,"Encoding,Big data,Fault tolerance,Fault tolerant systems,Measurement,Computers","The number of applications based on Apache Hadoop is dramatically increasing due to the robustness and dynamic features of this system. At the heart of Apache Hadoop, the Hadoop Distributed File System (HDFS) provides the reliability and high availability for computation by applying a static replication by default. However, because of the characteristics of parallel operations on the application layer, the access rate for each data file in HDFS is completely different. Consequently, maintaining the same replication mechanism for every data file leads to detrimental effects on the performance. By rigorously considering the drawbacks of the HDFS replication, this paper proposes an approach to dynamically replicate the data file based on the predictive analysis. With the help of probability theory, the utilization of each data file can be predicted to create a corresponding replication strategy. Eventually, the popular files can be subsequently replicated according to their own access potentials. For the remaining low potential files, an erasure code is applied to maintain the reliability. Hence, our approach simultaneously improves the availability while keeping the reliability in comparison to the default scheme. Furthermore, the complexity reduction is applied to enhance the effectiveness of the prediction when dealing with Big Data."
1436,,Prefix-Adaptive and Time-Sensitive Personalized Query Auto Completion.,"Fei Cai,Shangsong Liang,Maarten de Rijke",https://doi.org/10.1109/TKDE.2016.2568179,TKDE,2016,"Context,Market research,Context modeling,Predictive models,Web search,Adaptation models","Query auto completion (QAC) methods recommend queries to search engine users when they start entering a query. Current QAC methods mostly rank query completions based on their past popularity, i.e., on the number of times they have previously been submitted as a query. However, query popularity changes over time and may vary drastically across users. Accordingly, the ranking of query completions should be adjusted. Previous time-sensitive and user-specific QAC methods have been developed separately, yielding significant improvements over methods that are neither time-sensitive nor personalized. We propose a hybrid QAC method that is both time-sensitive and personalized. We extend it to handle long-tail prefixes, which we achieve by assigning optimal weights to the contribution from time-sensitivity and personalization. Using real-world search log datasets, we return top 
<inline-formula><tex-math notation=""LaTeX""> $N$</tex-math> </inline-formula>
 query suggestions ranked by predicted popularity as estimated from popularity trends and cyclic popularity behavior; we rerank them by integrating similarities to a user's previous queries (both in the current session and in previous sessions). Our method outperforms state-of-the-art time-sensitive QAC baselines, achieving total improvements of between 3 and 7 percent in terms of mean reciprocal rank (MRR). After optimizing the weights, our extended model achieves MRR improvements of between 4 and 8 percent."
1437,,Relaxed Functional Dependencies - A Survey of Approaches.,"Loredana Caruccio,Vincenzo Deufemia,Giuseppe Polese",https://doi.org/10.1109/TKDE.2015.2472010,TKDE,2016,"Databases,XML,Nervous system,Cardiology,Semantics,Context,Computational fluid dynamics","Recently, there has been a renovated interest in functional dependencies due to the possibility of employing them in several advanced database operations, such as data cleaning, query relaxation, record matching, and so forth. In particular, the constraints defined for canonical functional dependencies have been relaxed to capture inconsistencies in real data, patterns of semantically related data, or semantic relationships in complex data types. In this paper, we have surveyed 35 of such functional dependencies, providing a classification criteria, motivating examples, and a systematic analysis of them."
1438,,SPIRIT - A Tree Kernel-Based Method for Topic Person Interaction Detection.,"Yung-Chun Chang,Chien Chin Chen,Wen-Lian Hsu",https://doi.org/10.1109/TKDE.2016.2566620,TKDE,2016,"Feature extraction,Proteins,Syntactics,Semantics,Kernel,Protein engineering,Context","The development of a topic in a set of topic documents is constituted by a series of person interactions at a specific time and place. Knowing the interactions of the persons mentioned in these documents is helpful for readers to better comprehend the documents. In this paper, we propose a topic person interaction detection method called SPIRIT, which classifies the text segments in a set of topic documents that convey person interactions. We design the rich interactive tree structure to represent syntactic, context, and semantic information of text, and this structure is incorporated into a tree-based convolution kernel to identify interactive segments. Experiment results based on real world topics demonstrate that the proposed rich interactive tree structure effectively detects the topic person interactions and that our method outperforms many well-known relation extraction and protein-protein interaction methods."
1439,,Distributed In-Memory Processing of All k Nearest Neighbor Queries.,"Georgios Chatzimilioudis,Constantinos Costa,Demetrios Zeinalipour-Yazti,Wang-Chien Lee,Evaggelia Pitoura",https://doi.org/10.1109/TKDE.2015.2503768,TKDE,2016,"Servers,Partitioning algorithms,Scalability,Distributed algorithms,Query processing,Load management,Social network services","A wide spectrum of Internet-scale mobile applications, ranging from social networking, gaming and entertainment to emergency response and crisis management, all require efficient and scalable All k Nearest Neighbor (AkNN) computations over millions of moving objects every few seconds to be operational. Most traditional techniques for computing AkNN queries are centralized, lacking both scalability and efficiency. Only recently, distributed techniques for shared-nothing cloud infrastructures have been proposed to achieve scalability for large datasets. These batch-oriented algorithms are sub-optimal due to inefficient data space partitioning and data replication among processing units. In this paper, we present Spitfire, a distributed algorithm that provides a scalable and high-performance AkNN processing framework. Our proposed algorithm deploys a fast load-balanced partitioning scheme along with an efficient replication-set selection algorithm, to provide fast main-memory computations of the exact AkNN results in a batch-oriented manner. We evaluate, both analytically and experimentally, how the pruning efficiency of the Spitfire algorithm plays a pivotal role in reducing communication and response time up to an order of magnitude, compared to three other state-of-the-art distributed AkNN algorithms executed in distributed main-memory."
1440,,A Computer-Assistance Learning System for Emotional Wording.,"Wei-Fan Chen,Mei-Hua Chen,Ming-Lung Chen,Lun-Wei Ku",https://doi.org/10.1109/TKDE.2015.2507579,TKDE,2016,"Computer aided instruction,Sentiment analysis,Text analysis,Dictionaries,Learning systems","Language learners' limited lexical knowledge leads to imprecise wording. This is especially true when they attempt to express their emotions. Many learners rely heavily on the traditional thesaurus. Unfortunately, this fails to provide appropriate suggestions for lexical choices. To better aid English-as-a-second-language learners with word choices, we propose RESOLVE, which provides ranked synonyms of emotion words based on contextual information. RESOLVE suggests precise emotion words regarding the events in the relevant context. Patterns are learned to capture emotion events, and various factors are considered in the scoring function for ranking emotion words. We also describe an online writing system developed using RESOLVE and evaluate its effectiveness for learning assistance with a writing task. Experimental results showed that RESOLVE yielded a superior performance on NDCG@5 which significantly outperformed both PMI and SVM approaches, and offered better suggestions than Roget's Thesaurus and PIGAI (an online automated essay scoring system). Moreover, when applying it to the writing task, students' appropriateness with emotion words was 30 percent improved. Less-proficient learners benefited more from RESOLVE than highly-proficient learners. Post-tests also showed that after using RESOLVE, less-proficient learners' ability to use emotion words approached that of highly-proficient learners. RESOLVE thus enables learners to use precise emotion words."
1441,,Metric All-k-Nearest-Neighbor Search.,"Lu Chen 0001,Yunjun Gao,Gang Chen 0001,Haida Zhang",https://doi.org/10.1109/TKDE.2015.2453954,TKDE,2016,"Indexes,Extraterrestrial measurements,Algorithm design and analysis,Search problems,Upper bound,Data mining","An all-k-nearest-neighbor (AkNN) query finds from a given object set O, k nearest neighbors for each object in a specified query set Q. This operation is common in many applications such as GIS, data mining, and image analysis. Although it has received much attention in the Euclidean space, there is little prior work on the metric space. In this paper, we study the problem of AkNN retrieval in metric spaces, termed metric AkNN(MAkNN) search, and propose efficient algorithms for supporting MAkNN queries with arbitrary k value. Our methods utilize dynamic disk-based metric indexes (e.g., M-tree), employ a series of pruning rules, take advantage of grouping, reuse, pre-processing, and progressive pruning techniques, require no detailed representations of objects, and can be applied as long as the distance metric satisfies the triangle inequality. In addition, we extend our approaches to tackle metric self-AkNN (MSAkNN) search, a natural variation of MAkNN queries, where the query set Q is identical to the object set O. Extensive experiments using both real and synthetic data sets demonstrate, compared with state-of-the-art euclidean AkNN, MAkNN, and MSAkNN algorithms, the performance of our proposed algorithms and the effectiveness of our presented techniques."
1442,,Mining Health Examination Records - A Graph-Based Approach.,"Ling Chen 0004,Xue Li 0001,Quan Z. Sheng,Wen-Chih Peng,John Bennett,Hsiao-Yun Hu,Nicole Huang",https://doi.org/10.1109/TKDE.2016.2561278,TKDE,2016,"Semisupervised learning,Diseases,Data mining,Prediction algorithms,Electronic mail,Australia","General health examination is an integral part of healthcare in many countries. Identifying the participants at risk is important for early warning and preventive intervention. The fundamental challenge of learning a classification model for risk prediction lies in the unlabeled data that constitutes the majority of the collected dataset. Particularly, the unlabeled data describes the participants in health examinations whose health conditions can vary greatly from healthy to very-ill. There is no ground truth for differentiating their states of health. In this paper, we propose a graph-based, semi-supervised learning algorithm called SHG-Health (Semi-supervised Heterogeneous Graph on Health) for risk predictions to classify a progressively developing situation with the majority of the data unlabeled. An efficient iterative algorithm is designed and the proof of convergence is given. Extensive experiments based on both real health examination datasets and synthetic datasets are performed to show the effectiveness and efficiency of our method."
1443,,Node Immunization on Large Graphs - Theory and Algorithms.,"Chen Chen 0022,Hanghang Tong,B. Aditya Prakash,Charalampos E. Tsourakakis,Tina Eliassi-Rad,Christos Faloutsos,Duen Horng Chau",https://doi.org/10.1109/TKDE.2015.2465378,TKDE,2016,"Eigenvalues and eigenfunctions,Immune system,Electronic mail,Robustness,Approximation methods,Computational complexity,Computers","Given a large graph, like a computer communication network, which k nodes should we immunize (or monitor, or remove), to make it as robust as possible against a computer virus attack? This problem, referred to as the node immunization problem, is the core building block in many high-impact applications, ranging from public health, cybersecurity to viral marketing. A central component in node immunization is to find the best k bridges of a given graph. In this setting, we typically want to determine the relative importance of a node (or a set of nodes) within the graph, for example, how valuable (as a bridge) a person or a group of persons is in a social network. First of all, we propose a novel `bridging' score Dλ, inspired by immunology, and we show that its results agree with intuition for several realistic settings. Since the straightforward way to compute Dλ is computationally intractable, we then focus on the computational issues and propose a surprisingly efficient way (O(nk
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 + m)) to estimate it. Experimental results on real graphs show that (1) the proposed `bridging' score gives mining results consistent with intuition; and (2) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives."
1444,,Recommendation for Repeat Consumption from User Implicit Feedback.,"Jun Chen 0004,Chaokun Wang,Jianmin Wang 0001,Philip S. Yu",https://doi.org/10.1109/TKDE.2016.2593720,TKDE,2016,"Recommender systems,Feature extraction,Hidden Markov models,Predictive models,Tensile stress,Solids,Memory management","Recommender system has been studied as a useful tool to discover novel items for users while fitting their personalized interest. Thus, the previously consumed items are usually out of consideration due to the “lack” of novelty. However, as time elapses, people may forget those previously consumed and preferred items which could become “novel” again. Meanwhile, repeat consumption accounts for a major portion of people's observed activities; examples include: eating regularly at a same restaurant, or repeatedly listening to the same songs. Therefore, we believe that recommending repeat consumption will have a real utility at certain times. In this paper, we formulate the problem of recommendation for repeat consumption with user implicit feedback. A time-sensitive personalized pairwise ranking (TS-PPR) method based on user behavioral features is proposed to address this problem. The proposed method factorizes the temporal user-item interactions via learning the mappings from the behavioral features in observable space to the preference features in latent space, and combines users' static and dynamic preferences together in recommendation. An empirical study on real-world data sets shows encouraging results."
1445,,Conflict-Aware Weighted Bipartite B-Matching and Its Application to E-Commerce.,"Cheng Chen 0019,Lan Zheng,Venkatesh Srinivasan 0001,Alex Thomo,Kui Wu 0001,Anthony Sukow",https://doi.org/10.1109/TKDE.2016.2527003,TKDE,2016,"Bipartite graph,Advertising,Companies,Internet,Recommender systems,Algorithm design and analysis,Resource management","The weighted bipartite b-matching problem (WBM) plays a significant role in many real-world applications, including resource allocation, scheduling, Internet advertising, and E-commerce. WBM has been widely studied and efficient matching algorithms are well known. In this work, we study a novel variant of WBM, called conflict-aware WBM (CA-WBM), where conflict constraints are present between vertices of the bipartite graph. In CA-WBM, if two vertices (on the same side) are in conflict, they may not be included in the matching result simultaneously. We present a generalized formulation of CA-WBM in the context of E-commerce, where diverse matching results are often desired (e.g., movies of different genres and merchants selling products of different categories). While WBM is efficiently solvable in polynomial-time, we show that CA-WBM is NP-hard. We propose approximate and randomized algorithms to solve CA-WBM and show that they achieve close to optimal solutions via comprehensive experiments using synthetic datasets. We derive a theoretical bound on the approximation ratio of a greedy algorithm for CA-WBM and show that it is scalable on a large-scale real-world dataset."
1446,,Task Assignment on Multi-Skill Oriented Spatial Crowdsourcing.,"Peng Cheng 0003,Xiang Lian,Lei Chen 0002,Jinsong Han,Jizhong Zhao",https://doi.org/10.1109/TKDE.2016.2550041,TKDE,2016,"Crowdsourcing,Adaptive algorithms,Electronic mail,Painting,Cleaning,Remuneration,Knowledge engineering","With the rapid development of mobile devices and crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, the spatial crowdsourcing refers to sending location-based requests to workers, based on their current positions. In this paper, we consider a spatial crowdsourcing scenario, in which each worker has a set of qualified skills, whereas each spatial task (e.g., repairing a house, decorating a room, and performing entertainment shows for a ceremony) is time-constrained, under the budget constraint, and required a set of skills. Under this scenario, we will study an important problem, namely 
<i>multi-skill spatial crowdsourcing</i>
 (MS-SC), which finds an optimal worker-and-task assignment strategy, such that skills between workers and tasks match with each other, and workers’ benefits are maximized under the budget constraint. We prove that the MS-SC problem is NP-hard and intractable. Therefore, we propose three effective heuristic approaches, including greedy, 
<inline-formula><tex-math notation=""LaTeX"">$g$</tex-math></inline-formula>
-divide-and-conquer and cost-model-based adaptive algorithms to get worker-and-task assignments. Through extensive experiments, we demonstrate the efficiency and effectiveness of our MS-SC processing approaches on both real and synthetic data sets."
120,,Model Checking Distributed Mandatory Access Control Policies.,"Perry Alexander,Lee Pike,Peter Loscocco,George Coker",https://doi.org/10.1145/2785966,TOPS,2015,[],
121,,Randomization-Based Intrusion Detection System for Advanced Metering Infrastructure.,"Muhammad Qasim Ali,Ehab Al-Shaer",https://doi.org/10.1145/2814936,TOPS,2015,[],
122,,A Large-Scale Evaluation of High-Impact Password Strength Meters.,"Xavier de Carné de Carnavalet,Mohammad Mannan",https://doi.org/10.1145/2739044,TOPS,2015,[],
123,,Using Architecture to Reason about Information Security.,"Stephen Chong,Ron van der Meyden",https://doi.org/10.1145/2829949,TOPS,2015,[],
124,,vVote - A Verifiable Voting System.,"Chris Culnane,Peter Y. A. Ryan,Steve A. Schneider,Vanessa Teague",https://doi.org/10.1145/2746338,TOPS,2015,[],
125,,A Visualizable Evidence-Driven Approach for Authorship Attribution.,"Steven H. H. Ding,Benjamin C. M. Fung,Mourad Debbabi",https://doi.org/10.1145/2699910,TOPS,2015,[],
126,,CacheAudit - A Tool for the Static Analysis of Cache Side Channels.,"Goran Doychev,Boris Köpf,Laurent Mauborgne,Jan Reineke",https://doi.org/10.1145/2756550,TOPS,2015,[],
127,,Dynamic Provable Data Possession.,"C. Christopher Erway,Alptekin Küpçü,Charalampos Papamanthou,Roberto Tamassia",https://doi.org/10.1145/2699909,TOPS,2015,[],
128,,Misbehavior in Bitcoin - A Study of Double-Spending and Accountability.,"Ghassan O. Karame,Elli Androulaki,Marc Roeschlin,Arthur Gervais,Srdjan Capkun",https://doi.org/10.1145/2732196,TOPS,2015,[],
129,,Gatling - Automatic Performance Attack Discovery in Large-Scale Distributed Systems.,"Hyojeong Lee,Jeff Seibert,Dylan Fistrovic,Charles Edwin Killian,Cristina Nita-Rotaru",https://doi.org/10.1145/2714565,TOPS,2015,[],
130,,Attacking the Internet Using Broadcast Digital Television.,"Yossef Oren,Angelos D. Keromytis",https://doi.org/10.1145/2723159,TOPS,2015,[],
131,,Silence is Golden - Exploiting Jamming and Radio Silence to Communicate.,"Roberto Di Pietro,Gabriele Oligeri",https://doi.org/10.1145/2699906,TOPS,2015,[],
132,,Cryptographic Theory Meets Practice - Efficient and Privacy-Preserving Payments for Public Transport.,"Andy Rupp,Foteini Baldimtsi,Gesine Hinterwälder,Christof Paar",https://doi.org/10.1145/2699904,TOPS,2015,[],
133,,Pareto-Optimal Adversarial Defense of Enterprise Systems.,"Edoardo Serra,Sushil Jajodia,Andrea Pugliese 0001,Antonino Rullo,V. S. Subrahmanian",https://doi.org/10.1145/2699907,TOPS,2015,[],
134,,Integrity Attacks on Real-Time Pricing in Electric Power Grids.,"Rui Tan,Varun Badrinath Krishna,David K. Y. Yau,Zbigniew T. Kalbarczyk",https://doi.org/10.1145/2790298,TOPS,2015,[],
135,,"Picture Gesture Authentication - Empirical Analysis, Automated Attacks, and Scheme Evaluation.","Ziming Zhao 0001,Gail-Joon Ahn,Hongxin Hu",https://doi.org/10.1145/2701423,TOPS,2015,[],
136,,Comparing Vulnerability Severity and Exploits Using Case-Control Studies.,"Luca Allodi,Fabio Massacci",https://doi.org/10.1145/2630069,TOPS,2014,[],
137,,Sophisticated Access Control via SMT and Logical Frameworks.,"Konstantine Arkoudas,Ritu Chadha,C. Jason Chiang",https://doi.org/10.1145/2595222,TOPS,2014,[],
138,,Know Your Enemy - Compromising Adversaries in Protocol Analysis.,"David A. Basin,Cas Cremers",https://doi.org/10.1145/2658996,TOPS,2014,[],
139,,Exposure - A Passive DNS Analysis Service to Detect and Report Malicious Domains.,"Leyla Bilge,Sevil Sen,Davide Balzarotti,Engin Kirda,Christopher Kruegel",https://doi.org/10.1145/2584679,TOPS,2014,[],
140,,SpartanRPC - Remote Procedure Call Authorization in Wireless Sensor Networks.,"Peter C. Chapin,Christian Skalka",https://doi.org/10.1145/2644809,TOPS,2014,[],
141,,Cross-Domain Password-Based Authenticated Key Exchange Revisited.,"Liqun Chen,Hoon Wei Lim,Guomin Yang",https://doi.org/10.1145/2584681,TOPS,2014,[],
142,,An Anti-Phishing System Employing Diffused Information.,"Teh-Chung Chen,Torin Stepan,Scott Dick,James Miller 0001",https://doi.org/10.1145/2584680,TOPS,2014,[],
143,,Off-Path TCP Injection Attacks.,"Yossi Gilad,Amir Herzberg",https://doi.org/10.1145/2597173,TOPS,2014,[],
144,,Mutual Authentication and Trust Bootstrapping towards Secure Disk Encryption.,"Johannes Götzfried,Tilo Müller",https://doi.org/10.1145/2663348,TOPS,2014,[],
145,,A Framework for Expressing and Enforcing Purpose-Based Privacy Policies.,"Mohammad Jafari,Reihaneh Safavi-Naini,Philip W. L. Fong,Ken Barker 0001",https://doi.org/10.1145/2629689,TOPS,2014,[],
146,,StopWatch - A Cloud Architecture for Timing Channel Mitigation.,"Peng Li,Debin Gao,Michael K. Reiter",https://doi.org/10.1145/2670940,TOPS,2014,[],
147,,Rumpole - An Introspective Break-Glass Access Control Language.,"Srdjan Marinovic,Naranker Dulay,Morris Sloman",https://doi.org/10.1145/2629502,TOPS,2014,[],
148,,Security Analysis of Accountable Anonymity in Dissent.,"Ewa Syta,Henry Corrigan-Gibbs,Shu-Chun Weng,David Wolinsky,Bryan Ford,Aaron Johnson",https://doi.org/10.1145/2629621,TOPS,2014,[],
149,,Automated Anomaly Detector Adaptation using Adaptive Threshold Tuning.,"Muhammad Qasim Ali,Ehab Al-Shaer,Hassan Khan,Syed Ali Khayam",https://doi.org/10.1145/2445566.2445569,TOPS,2013,[],
150,,Enforceable Security Policies Revisited.,"David A. Basin,Vincent Jugé,Felix Klaedtke,Eugen Zalinescu",https://doi.org/10.1145/2487222.2487225,TOPS,2013,[],
151,,Secure and verifiable outsourcing of large-scale biometric computations.,"Marina Blanton,Yihua Zhang,Keith B. Frikken",https://doi.org/10.1145/2535523,TOPS,2013,[],
152,,DriverGuard - Virtualization-Based Fine-Grained Protection on I/O Flows.,"Yueqiang Cheng,Xuhua Ding,Robert H. Deng",https://dl.acm.org/citation.cfm?id=2505123,TOPS,2013,[],
153,,Leakage Mapping - A Systematic Methodology for Assessing the Side-Channel Information Leakage of Cryptographic Implementations.,"William E. Cobb,Rusty O. Baldwin,Eric D. Laspe",https://doi.org/10.1145/2487222.2487224,TOPS,2013,[],
154,,On the Parameterized Complexity and Kernelization of the Workflow Satisfiability Problem.,"Jason Crampton,Gregory Z. Gutin,Anders Yeo",https://doi.org/10.1145/2487222.2487226,TOPS,2013,[],
155,,An experimental security analysis of two satphone standards.,"Benedikt Driessen,Ralf Hund,Carsten Willems,Christof Paar,Thorsten Holz",https://doi.org/10.1145/2535522,TOPS,2013,[],
156,,Role Mining with Probabilistic Models.,"Mario Frank 0001,Joachim M. Buhmann,David A. Basin",https://doi.org/10.1145/2445566.2445567,TOPS,2013,[],
157,,Bridging the Semantic Gap in Virtual Machine Introspection via Online Kernel Data Redirection.,"Yangchun Fu,Zhiqiang Lin",https://dl.acm.org/citation.cfm?id=2505124,TOPS,2013,[],
158,,Fragmentation Considered Vulnerable.,"Yossi Gilad,Amir Herzberg",https://doi.org/10.1145/2445566.2445568,TOPS,2013,[],
159,,Mohawk - Abstraction-Refinement and Bound-Estimation for Verifying Access Control Policies.,"Karthick Jayaraman,Mahesh V. Tripunitara,Vijay Ganesh,Martin C. Rinard,Steve J. Chapin",https://doi.org/10.1145/2445566.2445570,TOPS,2013,[],
160,,Modelling Access Propagation in Dynamic Systems.,"Thomas Leonard,Martin Hall-May,Mike Surridge",https://doi.org/10.1145/2516951.2516952,TOPS,2013,[],
161,,CPM - Masking Code Pointers to Prevent Code Injection Attacks.,"Pieter Philippaerts,Yves Younan,Stijn Muylle,Frank Piessens,Sven Lachmund,Thomas Walter 0001",https://doi.org/10.1145/2487222.2487223,TOPS,2013,[],
162,,Examining a Large Keystroke Biometrics Dataset for Statistical-Attack Openings.,"Abdul Serwadda,Vir V. Phoha",https://dl.acm.org/citation.cfm?id=2516960,TOPS,2013,[],
163,,Bringing java&apos;s wild native world under control.,"Mengtao Sun,Gang Tan,Joseph Siefers,Bin Zeng 0004,Greg Morrisett",https://doi.org/10.1145/2535505,TOPS,2013,[],
164,,Access privacy and correctness on untrusted storage.,"Peter Williams,Radu Sion",https://doi.org/10.1145/2535524,TOPS,2013,[],
165,,On Protection by Layout Randomization.,"Martín Abadi,Gordon D. Plotkin",https://doi.org/10.1145/2240276.2240279,TOPS,2012,[],
166,,Dynamic enforcement of abstract separation of duty constraints.,"David A. Basin,Samuel J. Burri,Günter Karjoth",https://doi.org/10.1145/2382448.2382451,TOPS,2012,[],
167,,Verified Cryptographic Implementations for TLS.,"Karthikeyan Bhargavan,Cédric Fournet,Ricardo Corin,Eugen Zalinescu",https://doi.org/10.1145/2133375.2133378,TOPS,2012,[],
168,,Adversarial stylometry - Circumventing authorship recognition to preserve privacy and anonymity.,"Michael Brennan,Sadia Afroz,Rachel Greenstadt",https://doi.org/10.1145/2382448.2382450,TOPS,2012,[],
169,,Efficient Attributes for Anonymous Credentials.,"Jan Camenisch,Thomas Groß 0001",https://doi.org/10.1145/2133375.2133379,TOPS,2012,[],
170,,Towards Practical Identification of HF RFID Devices.,"Boris Danev,Srdjan Capkun,Ramya Jayaram Masti,Thomas S. Benjamin",https://doi.org/10.1145/2240276.2240278,TOPS,2012,[],
171,,Effectiveness and detection of denial-of-service attacks in tor.,"Norman Danner,Sam DeFabbia-Kane,Danny Krizanc,Marc Liberatore",https://doi.org/10.1145/2382448.2382449,TOPS,2012,[],
172,,Probabilistic analysis of onion routing in a black-box model.,"Joan Feigenbaum,Aaron Johnson,Paul F. Syverson",https://doi.org/10.1145/2382448.2382452,TOPS,2012,[],
173,,LOT - A Defense Against IP Spoofing and Flooding Attacks.,"Yossi Gilad,Amir Herzberg",https://doi.org/10.1145/2240276.2240277,TOPS,2012,[],
174,,Corrective Enforcement - A New Paradigm of Security Policy Enforcement by Monitors.,"Raphaël Khoury,Nadia Tawbi",https://doi.org/10.1145/2240276.2240281,TOPS,2012,[],
175,,Information Leaks in Structured Peer-to-Peer Anonymous Communication Systems.,"Prateek Mittal,Nikita Borisov",https://doi.org/10.1145/2133375.2133380,TOPS,2012,[],
176,,"Return-Oriented Programming - Systems, Languages, and Applications.","Ryan Roemer,Erik Buchanan,Hovav Shacham,Stefan Savage",https://doi.org/10.1145/2133375.2133377,TOPS,2012,[],
177,,Guest Editorial - Special Issue on Computer and Communications Security.,"Paul F. Syverson,Somesh Jha",https://doi.org/10.1145/2133375.2133376,TOPS,2012,[],
178,,BAF and FI-BAF - Efficient and Publicly Verifiable Cryptographic Schemes for Secure Logging in Resource-Constrained Systems.,"Attila Altay Yavuz,Peng Ning,Michael K. Reiter",https://doi.org/10.1145/2240276.2240280,TOPS,2012,[],
179,,Security Seals on Voting Machines - A Case Study.,Andrew W. Appel,https://doi.org/10.1145/2019599.2019603,TOPS,2011,[],
180,,Remote data checking using provable data possession.,"Giuseppe Ateniese,Randal C. Burns,Reza Curtmola,Joseph Herring,Osama Khan,Lea Kissner,Zachary N. J. Peterson,Dawn Song",https://doi.org/10.1145/1952982.1952994,TOPS,2011,[],
181,,PEREA - Practical TTP-free revocation of repeatedly misbehaving anonymous users.,"Man Ho Au,Patrick P. Tsang,Apu Kapadia",https://doi.org/10.1145/2043628.2043630,TOPS,2011,[],
182,,Formal Reasoning about Physical Properties of Security Protocols.,"David A. Basin,Srdjan Capkun,Patrick Schaller,Benedikt Schmidt",https://doi.org/10.1145/2019599.2019601,TOPS,2011,[],
183,,Detecting and resolving policy misconfigurations in access-control systems.,"Lujo Bauer,Scott Garriss,Michael K. Reiter",https://doi.org/10.1145/1952982.1952984,TOPS,2011,[],
184,,Server-side verification of client behavior in online games.,"Darrell Bethea,Robert A. Cochran,Michael K. Reiter",https://doi.org/10.1145/2043628.2043633,TOPS,2011,[],
185,,Relations among privacy notions.,"Jens-Matthias Bohli,Andreas Pashalidis",https://doi.org/10.1145/1952982.1952986,TOPS,2011,[],
186,,"Access control via belnap logic - Intuitive, expressive, and analyzable policy composition.","Glenn Bruns,Michael Huth",https://doi.org/10.1145/1952982.1952991,TOPS,2011,[],
187,,Privacy-preserving distributed network troubleshooting - bridging the gap between theory and practice.,"Martin Burkhart,Xenofontas A. Dimitropoulos",https://doi.org/10.1145/2043628.2043632,TOPS,2011,[],
188,,Lightweight RFID authentication with forward and backward security.,"Mike Burmester,Jorge Munilla",https://doi.org/10.1145/1952982.1952993,TOPS,2011,[],
189,,The Frog-Boiling Attack - Limitations of Secure Network Coordinate Systems.,"Eric Chan-Tin,Victor Heorhiadi,Nicholas Hopper,Yongdae Kim",https://doi.org/10.1145/2043621.2043627,TOPS,2011,[],
190,,Private and Continual Release of Statistics.,"T.-H. Hubert Chan,Elaine Shi,Dawn Song",https://doi.org/10.1145/2043621.2043626,TOPS,2011,[],
191,,Access controls for oblivious and anonymous systems.,"Scott E. Coull,Matthew Green 0001,Susan Hohenberger",https://doi.org/10.1145/1952982.1952992,TOPS,2011,[],
192,,Practical and efficient cryptographic enforcement of interval-based access control policies.,Jason Crampton,https://doi.org/10.1145/1952982.1952996,TOPS,2011,[],
193,,Authenticated Dictionaries - Real-World Costs and Trade-Offs.,"Scott A. Crosby,Dan S. Wallach",https://doi.org/10.1145/2019599.2019602,TOPS,2011,[],
194,,Cross-application data provenance and policy enforcement.,Brian Demsky,https://doi.org/10.1145/1952982.1952988,TOPS,2011,[],
195,,Practical defenses against pollution attacks in wireless network coding.,"Jing Dong 0006,Reza Curtmola,Cristina Nita-Rotaru",https://doi.org/10.1145/1952982.1952989,TOPS,2011,[],
196,,Modeling key compromise impersonation attacks on group key exchange protocols.,"M. Choudary Gorantla,Colin Boyd,Juan Manuel González Nieto,Mark Manulis",https://doi.org/10.1145/2043628.2043629,TOPS,2011,[],
197,,Guest Editorial SACMAT 2009 and 2010.,"James Joshi,Barbara Carminati",https://doi.org/10.1145/2043621.2043622,TOPS,2011,[],
198,,Group-Centric Secure Information-Sharing Models for Isolated Groups.,"Ram Krishnan,Jianwei Niu 0001,Ravi S. Sandhu,William H. Winsborough",https://doi.org/10.1145/2043621.2043623,TOPS,2011,[],
199,,"Access Control Policy Translation, Verification, and Minimization within Heterogeneous Data Federations.","Gregory Leighton,Denilson Barbosa",https://doi.org/10.1145/2043621.2043625,TOPS,2011,[],
200,,Introduction to special section SACMAT&apos;08.,Ninghui Li,https://doi.org/10.1145/1952982.1952983,TOPS,2011,[],
201,,On two RFID privacy notions and their relations.,"Yingjiu Li,Robert H. Deng,Junzuo Lai,Changshe Ma",https://doi.org/10.1145/2043628.2043631,TOPS,2011,[],
202,,False data injection attacks against state estimation in electric power grids.,"Yao Liu 0007,Peng Ning,Michael K. Reiter",https://doi.org/10.1145/1952982.1952995,TOPS,2011,[],
203,,Combining Discretionary Policy with Mandatory Information Flow in Operating Systems.,"Ziqing Mao,Ninghui Li,Hong Chen,Xuxian Jiang",https://doi.org/10.1145/2043621.2043624,TOPS,2011,[],
204,,Robust and efficient authentication of video stream broadcasting.,"Gabriele Oligeri,Stefano Chessa,Roberto Di Pietro,Gaetano Giunta",https://doi.org/10.1145/1952982.1952987,TOPS,2011,[],
205,,Nexus authorization logic (NAL) - Design rationale and applications.,"Fred B. Schneider,Kevin Walsh,Emin Gün Sirer",https://doi.org/10.1145/1952982.1952990,TOPS,2011,[],
206,,"Empowering End Users to Confine Their Own Applications - The Results of a Usability Study Comparing SELinux, AppArmor, and FBAC-LSM.","Z. Cliffe Schreuders,Tanya Jane McGill,Christian Payne",https://doi.org/10.1145/2019599.2019604,TOPS,2011,[],
207,,Checksum-Aware Fuzzing Combined with Dynamic Taint Analysis and Symbolic Execution.,"Tielei Wang,Tao Wei,Guofei Gu,Wei Zou",https://doi.org/10.1145/2019599.2019600,TOPS,2011,[],
208,,Authorization recycling in hierarchical RBAC systems.,"Qiang Wei,Jason Crampton,Konstantin Beznosov,Matei Ripeanu",https://doi.org/10.1145/1952982.1952985,TOPS,2011,[],
209,,Practical Oblivious Outsourced Storage.,"Peter Williams,Radu Sion,Miroslava Sotáková",https://doi.org/10.1145/2019599.2019605,TOPS,2011,[],
210,,CANTINA+ - A Feature-Rich Machine Learning Framework for Detecting Phishing Web Sites.,"Guang Xiang,Jason I. Hong,Carolyn Penstein Rosé,Lorrie Faith Cranor",https://doi.org/10.1145/2019599.2019606,TOPS,2011,[],
211,,Security of multithreaded programs by compilation.,"Gilles Barthe,Tamara Rezk,Alejandro Russo,Andrei Sabelfeld",https://doi.org/10.1145/1805974.1895977,TOPS,2010,[],
212,,A logic for state-modifying authorization policies.,"Moritz Y. Becker,Sebastian Nanz",https://doi.org/10.1145/1805974.1805976,TOPS,2010,[],
213,,CANDID - Dynamic candidate evaluations for automatic prevention of SQL injection attacks.,"Prithvi Bisht,Parthasarathy Madhusudan,V. N. Venkatakrishnan",https://doi.org/10.1145/1698750.1698754,TOPS,2010,[],
214,,Editorial ESORICS 2007.,"Joachim Biskup,Javier López 0001",https://doi.org/10.1145/1805974.1805975,TOPS,2010,[],
215,,Attribute-Based Messaging - Access Control and Confidentiality.,"Rakeshbabu Bobba,Omid Fatemieh,Fariba Khan,Arindam Khan 0001,Carl A. Gunter,Himanshu Khurana,Manoj Prabhakaran",https://doi.org/10.1145/1880022.1880025,TOPS,2010,[],
216,,A framework to enforce access control over data streams.,"Barbara Carminati,Elena Ferrari,Jianneng Cao,Kian-Lee Tan",https://doi.org/10.1145/1805974.1805984,TOPS,2010,[],
217,,Combining fragmentation and encryption to protect privacy in data storage.,"Valentina Ciriani,Sabrina De Capitani di Vimercati,Sara Foresti,Sushil Jajodia,Stefano Paraboschi,Pierangela Samarati",https://doi.org/10.1145/1805974.1805978,TOPS,2010,[],
218,,A logical specification and analysis for SELinux MLS policy.,"Boniface Hicks,Sandra Julieta Rueda,Luke St. Clair,Trent Jaeger,Patrick D. McDaniel",https://doi.org/10.1145/1805974.1805982,TOPS,2010,[],
219,,How much anonymity does network latency leak?,"Nicholas Hopper,Eugene Y. Vasserman,Eric Chan-Tin",https://doi.org/10.1145/1698750.1698753,TOPS,2010,[],
220,,Stealthy malware detection and monitoring through VMM-based &quot;out-of-the-box&quot; semantic view reconstruction.,"Xuxian Jiang,Xinyuan Wang,Dongyan Xu",https://doi.org/10.1145/1698750.1698752,TOPS,2010,[],
221,,Pairing-Based Onion Routing with Improved Forward Secrecy.,"Aniket Kate,Gregory M. Zaverucha,Ian Goldberg",https://doi.org/10.1145/1880022.1880023,TOPS,2010,[],
222,,On the consistency of distributed proofs with hidden subtrees.,"Adam J. Lee,Kazuhiro Minami,Marianne Winslett",https://doi.org/10.1145/1805974.1805981,TOPS,2010,[],
223,,Authenticated Index Structures for Aggregation Queries.,"Feifei Li 0001,Marios Hadjieleftheriou,George Kollios,Leonid Reyzin",https://doi.org/10.1145/1880022.1880026,TOPS,2010,[],
224,,Key Evolution Systems in Untrusted Update Environments.,"Benoît Libert,Jean-Jacques Quisquater,Moti Yung",https://doi.org/10.1145/1880022.1880031,TOPS,2010,[],
225,,Authenticated error-correcting codes with applications to multicast authentication.,"Anna Lysyanskaya,Roberto Tamassia,Nikos Triandopoulos",https://doi.org/10.1145/1698750.1698757,TOPS,2010,[],
226,,Mining Roles with Multiple Objectives.,"Ian M. Molloy,Hong Chen,Tiancheng Li,Qihua Wang,Ninghui Li,Elisa Bertino,Seraphin B. Calo,Jorge Lobo 0001",https://doi.org/10.1145/1880022.1880030,TOPS,2010,[],
227,,Split-ballot voting - Everlasting privacy with distributed trust.,"Tal Moran,Moni Naor",https://doi.org/10.1145/1698750.1698756,TOPS,2010,[],
228,,Identity Escrow Protocol and Anonymity Analysis in the Applied Pi-Calculus.,"Aybek Mukhamedov,Mark Dermot Ryan",https://doi.org/10.1145/1880022.1880035,TOPS,2010,[],
229,,Privacy-aware role-based access control.,"Qun Ni,Elisa Bertino,Jorge Lobo 0001,Carolyn Brodie,Clare-Marie Karat,John Karat,Alberto Trombetta",https://doi.org/10.1145/1805974.1805980,TOPS,2010,[],
230,,Storage-Based Intrusion Detection.,"Adam G. Pennington,John Linwood Griffin,John S. Bucy,John D. Strunk,Gregory R. Ganger",https://doi.org/10.1145/1880022.1880024,TOPS,2010,[],
231,,New payload attribution methods for network forensic investigations.,"Miroslav Ponec,Paul Giura,Joel Wein,Hervé Brönnimann",https://doi.org/10.1145/1698750.1698755,TOPS,2010,[],
232,,A Simple and Generic Construction of Authenticated Encryption with Associated Data.,Palash Sarkar 0001,https://doi.org/10.1145/1880022.1880027,TOPS,2010,[],
233,,MPSS - Mobile Proactive Secret Sharing.,"David A. Schultz,Barbara Liskov,Moses D. Liskov",https://doi.org/10.1145/1880022.1880028,TOPS,2010,[],
234,,Editorial SACMAT 2007.,Bhavani M. Thuraisingham,https://doi.org/10.1145/1805974.1805979,TOPS,2010,[],
235,,BLAC - Revoking Repeatedly Misbehaving Anonymous Users without Relying on TTPs.,"Patrick P. Tsang,Man Ho Au,Apu Kapadia,Sean W. Smith",https://doi.org/10.1145/1880022.1880033,TOPS,2010,[],
236,,The role mining problem - A formal perspective.,"Jaideep Vaidya,Vijayalakshmi Atluri,Qi Guo",https://doi.org/10.1145/1805974.1895983,TOPS,2010,[],
237,,Guest editorial - Special issue on computer and communications security.,"Sabrina De Capitani di Vimercati,Paul F. Syverson",https://doi.org/10.1145/1698750.1698751,TOPS,2010,[],
238,,Deterring voluntary trace disclosure in re-encryption mix-networks.,"XiaoFeng Wang 0001,Philippe Golle,Markus Jakobsson,Alex Tsow",https://doi.org/10.1145/1698750.1698758,TOPS,2010,[],
239,,Satisfiability and Resiliency in Workflow Authorization Systems.,"Qihua Wang,Ninghui Li",https://doi.org/10.1145/1880022.1880034,TOPS,2010,[],
240,,Uncovering Spoken Phrases in Encrypted Voice over IP Conversations.,"Charles V. Wright,Lucas Ballard,Scott E. Coull,Fabian Monrose,Gerald M. Masson",https://doi.org/10.1145/1880022.1880029,TOPS,2010,[],
241,,Robust Decentralized Virtual Coordinate Systems in Adversarial Environments.,"David Zage,Cristina Nita-Rotaru",https://doi.org/10.1145/1880022.1880032,TOPS,2010,[],
242,,"Control-flow integrity principles, implementations, and applications.","Martín Abadi,Mihai Budiu,Úlfar Erlingsson,Jay Ligatti",https://doi.org/10.1145/1609956.1609960,TOPS,2009,[],
243,,Dynamic and Efficient Key Management for Access Hierarchies.,"Mikhail J. Atallah,Marina Blanton,Nelly Fazio,Keith B. Frikken",https://doi.org/10.1145/1455526.1455531,TOPS,2009,[],
244,,New Techniques for Private Stream Searching.,"John Bethencourt,Dawn Xiaodong Song,Brent Waters",https://doi.org/10.1145/1455526.1455529,TOPS,2009,[],
245,,Universally Composable RFID Identification and Authentication Protocols.,"Mike Burmester,Tri Van Le,Breno de Medeiros,Gene Tsudik",https://doi.org/10.1145/1513601.1513603,TOPS,2009,[],
246,,IP Covert Channel Detection.,"Serdar Cabuk,Carla E. Brodley,Clay Shields",https://doi.org/10.1145/1513601.1513604,TOPS,2009,[],
247,,Enforcing access control in Web-based social networks.,"Barbara Carminati,Elena Ferrari,Andrea Perego",https://doi.org/10.1145/1609956.1609962,TOPS,2009,[],
248,,Opportunities and Limits of Remote Timing Attacks.,"Scott A. Crosby,Dan S. Wallach,Rudolf H. Riedi",https://doi.org/10.1145/1455526.1455530,TOPS,2009,[],
249,,Cryptanalysis of the random number generator of the Windows operating system.,"Leo Dorrendorf,Zvi Gutterman,Benny Pinkas",https://doi.org/10.1145/1609956.1609966,TOPS,2009,[],
250,,Maintaining control while delegating trust - Integrity constraints in trust management.,"Sandro Etalle,William H. Winsborough",https://doi.org/10.1145/1609956.1609961,TOPS,2009,[],
251,,Defining strong privacy for RFID.,"Ari Juels,Stephen A. Weis",https://doi.org/10.1145/1609956.1609963,TOPS,2009,[],
252,,Automated trust negotiation using cryptographic credentials.,"Jiangtao Li 0001,Ninghui Li,William H. Winsborough",https://doi.org/10.1145/1609956.1609958,TOPS,2009,[],
253,,Resiliency Policies in Access Control.,"Ninghui Li,Qihua Wang,Mahesh V. Tripunitara",https://doi.org/10.1145/1513601.1513602,TOPS,2009,[],
254,,Alcatraz - An Isolated Environment for Experimenting with Untrusted Software.,"Zhenkai Liang,Weiqing Sun,V. N. Venkatakrishnan,R. Sekar 0001",https://doi.org/10.1145/1455526.1455527,TOPS,2009,[],
255,,Run-Time Enforcement of Nonsafety Policies.,"Jay Ligatti,Lujo Bauer,David Walker",https://doi.org/10.1145/1455526.1455532,TOPS,2009,[],
256,,Introduction to ACM TISSEC special issue on CCS 2005.,Catherine A. Meadows,https://doi.org/10.1145/1609956.1609957,TOPS,2009,[],
257,,Efficient and secure protocols for privacy-preserving set operations.,"Yingpeng Sang,Hong Shen 0001",https://doi.org/10.1145/1609956.1609965,TOPS,2009,[],
258,,Compact and Anonymous Role-Based Authorization Chain.,"Danfeng Yao,Roberto Tamassia",https://doi.org/10.1145/1455526.1455528,TOPS,2009,[],
259,,Compromising anonymous communication systems using blind source separation.,"Ye Zhu,Riccardo Bettati",https://doi.org/10.1145/1609956.1609964,TOPS,2009,[],
260,,Keyboard acoustic emanations revisited.,"Li Zhuang,Feng Zhou,J. D. Tygar",https://doi.org/10.1145/1609956.1609959,TOPS,2009,[],
261,,Puppetnets - Misusing Web Browsers as a Distributed Attack Infrastructure.,"Spyros Antonatos,Periklis Akritidis,Vinh The Lam,Kostas G. Anagnostakis",https://doi.org/10.1145/1455518.1477941,TOPS,2008,[],
262,,Characterization and Improvement of Time-Memory Trade-Off Based on Perfect Tables.,"Gildas Avoine,Pascal Junod,Philippe Oechslin",https://doi.org/10.1145/1380564.1380565,TOPS,2008,[],
263,,ODSBR - An on-demand secure Byzantine resilient routing protocol for wireless ad hoc networks.,"Baruch Awerbuch,Reza Curtmola,David Holmer,Cristina Nita-Rotaru,Herbert Rubens",https://doi.org/10.1145/1284680.1341892,TOPS,2008,[],
264,,Status-Based Access Control.,"Steve Barker,Marek J. Sergot,Duminda Wijesekera",https://doi.org/10.1145/1410234.1410235,TOPS,2008,[],
265,,New Multiparty Signature Schemes for Network Routing Applications.,"Alexandra Boldyreva,Craig Gentry,Adam O&apos;Neill,Dae Hyun Yum",https://doi.org/10.1145/1410234.1410237,TOPS,2008,[],
266,,Dynamic access-control policies on XML encrypted data.,"Luc Bouganim,François Dang Ngoc,Philippe Pucheral",https://doi.org/10.1145/1284680.1284684,TOPS,2008,[],
267,,On the Existence of Unconditionally Privacy-Preserving Auction Protocols.,"Felix Brandt 0001,Tuomas Sandholm",https://doi.org/10.1145/1330332.1330338,TOPS,2008,[],
268,,EXE - Automatically Generating Inputs of Death.,"Cristian Cadar,Vijay Ganesh,Peter M. Pawlowski,David L. Dill,Dawson R. Engler",https://doi.org/10.1145/1455518.1455522,TOPS,2008,[],
269,,Epidemic thresholds in real networks.,"Deepayan Chakrabarti,Yang Wang 0008,Chenxi Wang,Jure Leskovec,Christos Faloutsos",https://doi.org/10.1145/1284680.1284681,TOPS,2008,[],
270,,Distributed Authentication of Program Integrity Verification in Wireless Sensor Networks.,"Katharine Chang,Kang G. Shin",https://doi.org/10.1145/1341731.1341735,TOPS,2008,[],
271,,Provably Secure Timed-Release Public Key Encryption.,"Jung Hee Cheon,Nicholas Hopper,Yongdae Kim,Ivan Osipkov",https://doi.org/10.1145/1330332.1330336,TOPS,2008,[],
272,,Secure Time Synchronization in Sensor Networks.,"Saurabh Ganeriwal,Christina Pöpper,Srdjan Capkun,Mani B. Srivastava",https://doi.org/10.1145/1380564.1380571,TOPS,2008,[],
273,,Controlled physical random functions and applications.,"Blaise Gassend,Marten van Dijk,Dwaine E. Clarke,Emina Torlak,Srinivas Devadas,Pim Tuyls",https://doi.org/10.1145/1284680.1284683,TOPS,2008,[],
274,1,Data Collection with Self-Enforcing Privacy.,"Philippe Golle,Frank McSherry,Ilya Mironov",https://doi.org/10.1145/1455518.1477940,TOPS,2008,[],
275,,Secrecy in Multiagent Systems.,"Joseph Y. Halpern,Kevin R. O&apos;Neill",https://doi.org/10.1145/1410234.1410239,TOPS,2008,[],
276,,Using First-Order Logic to Reason about Policies.,"Joseph Y. Halpern,Vicky Weissman",https://doi.org/10.1145/1380564.1380569,TOPS,2008,[],
277,,Formal foundations for hybrid hierarchies in GTRBAC.,"James B. D. Joshi,Elisa Bertino,Arif Ghafoor,Yue Zhang 0002",https://doi.org/10.1145/1284680.1284682,TOPS,2008,[],
278,,On the Construction of Practical Key Predistribution Schemes for Distributed Sensor Networks Using Combinatorial Designs.,"Jooyoung Lee,Douglas R. Stinson",https://doi.org/10.1145/1330332.1330333,TOPS,2008,[],
279,,Enforcing Safety and Consistency Constraints in Policy-Based Authorization Systems.,"Adam J. Lee,Marianne Winslett",https://doi.org/10.1145/1455518.1455520,TOPS,2008,[],
280,,The Traust Authorization Service.,"Adam J. Lee,Marianne Winslett,Jim Basney,Von Welch",https://doi.org/10.1145/1330295.1330297,TOPS,2008,[],
281,,Attack-Resistant Location Estimation in Wireless Sensor Networks.,"Donggang Liu,Peng Ning,An Liu,Cliff Wang,Wenliang Du",https://doi.org/10.1145/1380564.1380570,TOPS,2008,[],
282,,RIPPS - Rogue Identifying Packet Payload Slicer Detecting Unauthorized Wireless Hosts Through Network Traffic Conditioning.,"Chad D. Mano,Andrew Blaich,Qi Liao,Yingxin Jiang,David A. Cieslak,David Salyers,Aaron Striegel",https://doi.org/10.1145/1330332.1330334,TOPS,2008,[],
283,,XACML Policy Integration Algorithms.,"Pietro Mazzoleni,Bruno Crispo,Swaminathan Sivasubramanian,Elisa Bertino",https://doi.org/10.1145/1330295.1330299,TOPS,2008,[],
284,,On predictive models and user-drawn graphical passwords.,"Paul C. van Oorschot,Julie Thorpe",https://doi.org/10.1145/1284680.1284685,TOPS,2008,[],
285,,Verifying Completeness of Relational Query Answers from Online Servers.,"HweeHwa Pang,Kian-Lee Tan",https://doi.org/10.1145/1330332.1330337,TOPS,2008,[],
286,,Redoubtable Sensor Networks.,"Roberto Di Pietro,Luigi V. Mancini,Alessandro Mei,Alessandro Panconesi,Jaikumar Radhakrishnan",https://doi.org/10.1145/1341731.1341734,TOPS,2008,[],
287,,An Analytic Framework for Modeling and Detecting Access Layer Misbehavior in Wireless Networks.,"Svetlana Radosavac,George V. Moustakides,John S. Baras,Iordanis Koutsopoulos",https://doi.org/10.1145/1380564.1380567,TOPS,2008,[],
288,,Editorial.,Indrakshi Ray,https://doi.org/10.1145/1330295.1330296,TOPS,2008,[],
289,,Evaluation of Intrusion Detection Systems Under a Resource Constraint.,"Young U. Ryu,Hyeun-Suk Rhee",https://doi.org/10.1145/1380564.1380566,TOPS,2008,[],
290,,Noninvasive Methods for Host Certification.,"Patrick Traynor,Michael Chien,Scott Weaver,Boniface Hicks,Patrick D. McDaniel",https://doi.org/10.1145/1341731.1341737,TOPS,2008,[],
291,,Editorial.,Gene Tsudik,https://doi.org/10.1145/1341731.1341732,TOPS,2008,[],
292,,A Graph Based Approach Toward Network Forensics Analysis.,"Wei Wang 0266,Thomas E. Daniels",https://doi.org/10.1145/1410234.1410238,TOPS,2008,[],
293,,Fast and Black-box Exploit Detection and Signature Generation for Commodity Software.,"XiaoFeng Wang 0001,Zhuowei Li,Jong Youl Choi,Jun Xu 0003,Michael K. Reiter,Chongkyung Kil",https://doi.org/10.1145/1455518.1455523,TOPS,2008,[],
294,,Passive-Logging Attacks Against Anonymous Communications Systems.,"Matthew K. Wright,Micah Adler,Brian Neil Levine,Clay Shields",https://doi.org/10.1145/1330332.1330335,TOPS,2008,[],
295,,Guest Editorial - Special Issue on Computer and Communications Security.,"Rebecca N. Wright,Sabrina De Capitani di Vimercati",https://doi.org/10.1145/1455518.1455519,TOPS,2008,[],
296,,Thwarting E-mail Spam Laundering.,"Mengjun Xie,Heng Yin,Haining Wang",https://doi.org/10.1145/1455518.1455525,TOPS,2008,[],
297,,Message Dropping Attacks in Overlay Networks - Attack Detection and Attacker Identification.,"Liang Xie 0002,Sencun Zhu",https://doi.org/10.1145/1341731.1341736,TOPS,2008,[],
298,,Distributed and Secure Bootstrapping of Mobile Ad Hoc Networks - Framework and Constructions.,"Shouhuai Xu,Srdjan Capkun",https://doi.org/10.1145/1410234.1410236,TOPS,2008,[],
299,,SDAP - A Secure Hop-by-Hop Data Aggregation Protocol for Sensor Networks.,"Yi Yang 0002,Xinran Wang,Sencun Zhu,Guohong Cao",https://doi.org/10.1145/1380564.1380568,TOPS,2008,[],
300,,Private Information - To Reveal or not to Reveal.,"Danfeng Yao,Keith B. Frikken,Mikhail J. Atallah,Roberto Tamassia",https://doi.org/10.1145/1410234.1410240,TOPS,2008,[],
301,,Toward a Usage-Based Security Framework for Collaborative Computing Systems.,"Xinwen Zhang,Masayuki Nakae,Michael J. Covington,Ravi S. Sandhu",https://doi.org/10.1145/1330295.1330298,TOPS,2008,[],
302,,A Framework for Identifying Compromised Nodes in Wireless Sensor Networks.,"Qing Zhang,Ting Yu,Peng Ning",https://doi.org/10.1145/1341731.1341733,TOPS,2008,[],
303,,Just fast keying in the pi calculus.,"Martín Abadi,Bruno Blanchet,Cédric Fournet",https://doi.org/10.1145/1266977.1266978,TOPS,2007,[],
304,,Specification and verification of security requirements in a programming model for decentralized CSCW systems.,"Tanvir Ahmed 0002,Anand R. Tripathi",https://doi.org/10.1145/1237500.1237503,TOPS,2007,[],
305,,Guest editorial - Special issue on access control models and technologies.,Gail-Joon Ahn,https://doi.org/10.1145/1210263.1216576,TOPS,2007,[],
306,,Secure sessions for Web services.,"Karthikeyan Bhargavan,Ricardo Corin,Cédric Fournet,Andrew D. Gordon",https://doi.org/10.1145/1237500.1237504,TOPS,2007,[],
307,,Provably secure authenticated group Diffie-Hellman key exchange.,"Emmanuel Bresson,Olivier Chevassut,David Pointcheval",https://doi.org/10.1145/1266977.1266979,TOPS,2007,[],
308,,GEO-RBAC - A spatially aware RBAC.,"Maria Luisa Damiani,Elisa Bertino,Barbara Catania,Paolo Perlasca",https://doi.org/10.1145/1210263.1210265,TOPS,2007,[],
309,,Relevancy-based access control and its evaluation on versioned XML documents.,"Mizuho Iwaihara,Ryotaro Hayashi 0002,Somchai Chatvichienchai,Chutiporn Anutariya,Vilas Wuwongse",https://doi.org/10.1145/1210263.1210266,TOPS,2007,[],
310,,On mutually exclusive roles and separation-of-duty.,"Ninghui Li,Mahesh V. Tripunitara,Ziad Bizri",https://doi.org/10.1145/1237500.1237501,TOPS,2007,[],
311,,On interdomain routing security and pretty secure BGP (psBGP).,"Paul C. van Oorschot,Tao Wan,Evangelos Kranakis",https://doi.org/10.1145/1266977.1266980,TOPS,2007,[],
312,,Batch zero-knowledge proof and verification and its applications.,"Kun Peng,Colin Boyd,Ed Dawson",https://doi.org/10.1145/1237500.1237502,TOPS,2007,[],
313,,PP-trust-X - A system for privacy preserving trust negotiations.,"Anna Cinzia Squicciarini,Elisa Bertino,Elena Ferrari,Federica Paci,Bhavani M. Thuraisingham",https://doi.org/10.1145/1266977.1266981,TOPS,2007,[],
314,,Modeling network intrusion detection alerts for correlation.,"Jingmin Zhou,Mark R. Heckman,Brennen Reynolds,Adam Carlson,Matt Bishop",https://doi.org/10.1145/1210263.1210267,TOPS,2007,[],
315,,Improved proxy re-encryption schemes with applications to secure distributed storage.,"Giuseppe Ateniese,Kevin Fu,Matthew Green 0001,Susan Hohenberger",https://doi.org/10.1145/1127345.1127346,TOPS,2006,[],
316,,Accountability protocols - Formalized and verified.,"Giampaolo Bella,Lawrence C. Paulson",https://doi.org/10.1145/1151414.1151416,TOPS,2006,[],
317,,Battery power-aware encryption.,"Rajarathnam Chandramouli,Satish Bapatla,K. P. Subbalakshmi,R. N. Uma",https://doi.org/10.1145/1151414.1151417,TOPS,2006,[],
318,,Foundations and applications for secure triggers.,"Ariel Futoransky,Emiliano Kargieman,Carlos Sarraute,Ariel Waissbein",https://doi.org/10.1145/1127345.1127349,TOPS,2006,[],
319,,A framework for password-based authenticated key exchange1.,"Rosario Gennaro,Yehuda Lindell",https://doi.org/10.1145/1151414.1151418,TOPS,2006,[],
320,,A practical revocation scheme for broadcast encryption using smartcards.,"Noam Kogan,Yuval Shavitt,Avishai Wool",https://doi.org/10.1145/1178618.1178622,TOPS,2006,[],
321,,Improved efficiency for revocation schemes via Newton interpolation.,"Noam Kogan,Tamir Tassa",https://doi.org/10.1145/1187441.1187444,TOPS,2006,[],
322,,Security analysis in role-based access control.,"Ninghui Li,Mahesh V. Tripunitara",https://doi.org/10.1145/1187441.1187442,TOPS,2006,[],
323,,Auditing sum-queries to make a statistical database secure.,"Francesco M. Malvestuto,Mauro Mezzini,Marina Moscarini",https://doi.org/10.1145/1127345.1127347,TOPS,2006,[],
324,,Methods and limitations of security policy reconciliation.,"Patrick D. McDaniel,Atul Prakash 0001",https://doi.org/10.1145/1178618.1178620,TOPS,2006,[],
325,,Controlled and cooperative updates of XML documents in byzantine and failure-prone distributed systems.,"Giovanni Mella,Elena Ferrari,Elisa Bertino,Yunhua Koglin",https://doi.org/10.1145/1187441.1187443,TOPS,2006,[],
326,,XML access control using static analysis.,"Makoto Murata,Akihiko Tozawa,Michiharu Kudo,Satoshi Hada",https://doi.org/10.1145/1178618.1178621,TOPS,2006,[],
327,,Anomalous system call detection.,"Darren Mutz,Fredrik Valeur,Giovanni Vigna,Christopher Krügel",https://doi.org/10.1145/1127345.1127348,TOPS,2006,[],
328,,An effective role administration model using organization structure.,"Sejong Oh,Ravi S. Sandhu,Xinwen Zhang",https://doi.org/10.1145/1151414.1151415,TOPS,2006,[],
329,,On countering online dictionary attacks with login histories and humans-in-the-loop.,"Paul C. van Oorschot,Stuart G. Stubblebine",https://doi.org/10.1145/1178618.1178619,TOPS,2006,[],
330,,Safety in automated trust negotiation.,"William H. Winsborough,Ninghui Li",https://doi.org/10.1145/1178618.1178623,TOPS,2006,[],
331,,Preface.,Vijay Atluri,https://doi.org/10.1145/1053283.1053285,TOPS,2005,[],
332,,Randomized instruction set emulation.,"Elena Gabriela Barrantes,David H. Ackley,Stephanie Forrest,Darko Stefanovic",https://doi.org/10.1145/1053283.1053286,TOPS,2005,[],
333,,X-GTRBAC - an XML-based policy specification framework and architecture for enterprise-wide access control.,"Rafae Bhatti,Arif Ghafoor,Elisa Bertino,James Joshi",https://doi.org/10.1145/1065545.1065547,TOPS,2005,[],
334,,X-gtrbac admin - A decentralized administration model for enterprise-wide access control.,"Rafae Bhatti,Basit Shafiq,Elisa Bertino,Arif Ghafoor,James Joshi",https://doi.org/10.1145/1108906.1108909,TOPS,2005,[],
335,,Modeling and assessing inference exposure in encrypted databases.,"Alberto Ceselli,Ernesto Damiani,Sabrina De Capitani di Vimercati,Sushil Jajodia,Stefano Paraboschi,Pierangela Samarati",https://doi.org/10.1145/1053283.1053289,TOPS,2005,[],
336,,The concept of layered proving trees and its application to the automation of security protocol verification.,"Reiner Dojen,Tom Coffey",https://doi.org/10.1145/1085126.1085128,TOPS,2005,[],
337,,A pairwise key predistribution scheme for wireless sensor networks.,"Wenliang Du,Jing Deng 0001,Yunghsiang S. Han,Pramod K. Varshney,Jonathan Katz,Aram Khalili",https://doi.org/10.1145/1065545.1065548,TOPS,2005,[],
338,,Guest editorial - Special issue on access control models and technologies.,Elena Ferrari,https://doi.org/10.1145/1108906.1108907,TOPS,2005,[],
339,,Keystroke analysis of free text.,"Daniele Gunetti,Claudia Picardi",https://doi.org/10.1145/1085126.1085129,TOPS,2005,[],
340,,Access control to people location information.,"Urs Hengartner,Peter Steenkiste",https://doi.org/10.1145/1108906.1108910,TOPS,2005,[],
341,,Establishing pairwise keys in distributed sensor networks.,"Donggang Liu,Peng Ning,Rongfang Li",https://doi.org/10.1145/1053283.1053287,TOPS,2005,[],
342,,"Incentive-based modeling and inference of attacker intent, objectives, and strategies.","Peng Liu 0005,Wanyu Zang,Meng Yu 0001",https://doi.org/10.1145/1053283.1053288,TOPS,2005,[],
343,,Editorial.,Ravi S. Sandhu,https://doi.org/10.1145/1053283.1053284,TOPS,2005,[],
344,,Trusted paths for browsers.,"Zishuang (Eileen) Ye,Sean W. Smith,Denise L. Anthony",https://doi.org/10.1145/1065545.1065546,TOPS,2005,[],
345,,Formal model and policy specification of usage control.,"Xinwen Zhang,Francesco Parisi-Presicce,Ravi S. Sandhu,Jaehong Park",https://doi.org/10.1145/1108906.1108908,TOPS,2005,[],
346,,APSS - proactive secret sharing in asynchronous systems.,"Lidong Zhou,Fred B. Schneider,Robbert van Renesse",https://doi.org/10.1145/1085126.1085127,TOPS,2005,[],
347,,Just fast keying - Key agreement in a hostile internet.,"William Aiello,Steven M. Bellovin,Matt Blaze,Ran Canetti,John Ioannidis,Angelos D. Keromytis,Omer Reingold",https://doi.org/10.1145/996943.996946,TOPS,2004,[],
348,,On the performance of group key agreement protocols.,"Yair Amir,Yongdae Kim,Cristina Nita-Rotaru,Gene Tsudik",https://doi.org/10.1145/1015040.1015045,TOPS,2004,[],
349,,Verifiable encryption of digital signatures and applications.,Giuseppe Ateniese,https://doi.org/10.1145/984334.984335,TOPS,2004,[],
350,,Breaking and provably repairing the SSH authenticated encryption scheme - A case study of the Encode-then-Encrypt-and-MAC paradigm.,"Mihir Bellare,Tadayoshi Kohno,Chanathip Namprempre",https://doi.org/10.1145/996943.996945,TOPS,2004,[],
351,,The session token protocol for forensics and traceback.,"Brian D. Carrier,Clay Shields",https://doi.org/10.1145/1015040.1015041,TOPS,2004,[],
352,,Content-triggered trust negotiation.,"Adam Hess,Jason E. Holt,Jared Jacobson,Kent E. Seamons",https://doi.org/10.1145/1015040.1015044,TOPS,2004,[],
353,,A key-chain-based keying scheme for many-to-many secure group communication.,"Dijiang Huang,Deep Medhi",https://doi.org/10.1145/1042031.1042033,TOPS,2004,[],
354,,Consistency analysis of authorization hook placement in the Linux security modules framework.,"Trent Jaeger,Antony Edwards,Xiaolan Zhang 0001",https://doi.org/10.1145/996943.996944,TOPS,2004,[],
355,,Tree-based group key agreement.,"Yongdae Kim,Adrian Perrig,Gene Tsudik",https://doi.org/10.1145/984334.984337,TOPS,2004,[],
356,,"Use of nested certificates for efficient, dynamic, and trust preserving public key infrastructure.","Albert Levi,M. Ufuk Çaglayan,Çetin Kaya Koç",https://doi.org/10.1145/984334.984336,TOPS,2004,[],
357,,Crypto-based identifiers (CBIDs) - Concepts and applications.,"Gabriel Montenegro,Claude Castelluccia",https://doi.org/10.1145/984334.984338,TOPS,2004,[],
358,,Techniques and tools for analyzing intrusion alerts.,"Peng Ning,Yun Cui,Douglas S. Reeves,Dingbang Xu",https://doi.org/10.1145/996943.996947,TOPS,2004,[],
359,,Hypothesizing and reasoning about attacks missed by intrusion detection systems.,"Peng Ning,Dingbang Xu",https://doi.org/10.1145/1042031.1042036,TOPS,2004,[],
360,,The UCONABC usage control model.,"Jaehong Park,Ravi S. Sandhu",https://doi.org/10.1145/984334.984339,TOPS,2004,[],
361,,Client-side caching for TLS.,"Hovav Shacham,Dan Boneh,Eric Rescorla",https://doi.org/10.1145/1042031.1042034,TOPS,2004,[],
362,,An integrated approach to engineer and enforce context constraints in RBAC environments.,"Mark Strembeck,Gustaf Neumann",https://doi.org/10.1145/1015040.1015043,TOPS,2004,[],
363,,A key recovery attack on the 802.11b wired equivalent privacy protocol (WEP).,"Adam Stubblefield,John Ioannidis,Aviel D. Rubin",https://doi.org/10.1145/996943.996948,TOPS,2004,[],
364,,Traducement - A model for record security.,"Tom Walcott,Matt Bishop",https://doi.org/10.1145/1042031.1042035,TOPS,2004,[],
365,,Modular authorization and administration.,"Horst F. Wedde,Mario Lischka",https://doi.org/10.1145/1015040.1015042,TOPS,2004,[],
366,,The predecessor attack - An analysis of a threat to anonymous communications systems.,"Matthew K. Wright,Micah Adler,Brian Neil Levine,Clay Shields",https://doi.org/10.1145/1042031.1042032,TOPS,2004,[],
367,,Flexible access control policy specification with constraint logic programming.,"Steve Barker,Peter J. Stuckey",https://doi.org/10.1145/950191.950194,TOPS,2003,[],
368,,A logical framework for reasoning about access control models.,"Elisa Bertino,Barbara Catania,Elena Ferrari,Paolo Perlasca",https://doi.org/10.1145/605434.605437,TOPS,2003,[],
369,,"BlueBoX - A policy-driven, host-based intrusion detection system.","Suresh Chari,Pau-Chen Cheng",https://doi.org/10.1145/762476.762477,TOPS,2003,[],
370,,Administrative scope - A foundation for role-based administrative models.,"Jason Crampton,George Loizou",https://doi.org/10.1145/762476.762478,TOPS,2003,[],
371,,Public-key support for group collaboration.,"Carl M. Ellison,Steve Dohrmann",https://doi.org/10.1145/950191.950195,TOPS,2003,[],
372,,On the relationship between strand spaces and multi-agent systems.,"Joseph Y. Halpern,Riccardo Pucella",https://doi.org/10.1145/605434.605436,TOPS,2003,[],
373,,Policy management using access control spaces.,"Trent Jaeger,Xiaolan Zhang 0001,Antony Edwards",https://doi.org/10.1145/937527.937528,TOPS,2003,[],
374,,Clustering intrusion detection alarms to support root cause analysis.,Klaus Julisch,https://doi.org/10.1145/950191.950192,TOPS,2003,[],
375,,Access control with IBM Tivoli access manager.,Günter Karjoth,https://doi.org/10.1145/762476.762479,TOPS,2003,[],
376,,Delegation logic - A logic-based approach to distributed authorization.,"Ninghui Li,Benjamin N. Grosof,Joan Feigenbaum",https://doi.org/10.1145/605434.605438,TOPS,2003,[],
377,,Efficient multicast stream authentication using erasure codes.,"Jung-Min Park 0001,Edwin K. P. Chong,Howard Jay Siegel",https://doi.org/10.1145/762476.762480,TOPS,2003,[],
378,,A secure and private system for subscription-based remote services.,"Pino Persiano,Ivan Visconti",https://doi.org/10.1145/950191.950193,TOPS,2003,[],
379,,OCB - A block-cipher mode of operation for efficient authenticated encryption.,"Phillip Rogaway,Mihir Bellare,John Black",https://doi.org/10.1145/937527.937529,TOPS,2003,[],
380,,Certificate-based authorization policy in a PKI environment.,"Mary R. Thompson,Abdelilah Essiari,Srilekha Mudumbai",https://doi.org/10.1145/950191.950196,TOPS,2003,[],
381,,A propositional policy algebra for access control.,"Duminda Wijesekera,Sushil Jajodia",https://doi.org/10.1145/762476.762481,TOPS,2003,[],
382,,Supporting structured credentials and sensitive policies through interoperable strategies for automated trust negotiation.,"Ting Yu,Marianne Winslett,Kent E. Seamons",https://doi.org/10.1145/605434.605435,TOPS,2003,[],
383,,A rule-based framework for role-based delegation and revocation.,"Longhua Zhang,Gail-Joon Ahn,Bei-tseng Chu",https://doi.org/10.1145/937527.937530,TOPS,2003,[],
384,,An authorization model for temporal and derived data - securing information portals.,"Vijayalakshmi Atluri,Avigdor Gal",https://doi.org/10.1145/504909.504912,TOPS,2002,[],
385,,A model of OASIS role-based access control and its support for active security.,"Jean Bacon,Ken Moody,Walt Yao",https://doi.org/10.1145/581271.581276,TOPS,2002,[],
386,,User authentication through keystroke dynamics.,"Francesco Bergadano,Daniele Gunetti,Claudia Picardi",https://doi.org/10.1145/581271.581272,TOPS,2002,[],
387,,Remus - a security-enhanced operating system.,"Massimo Bernaschi,Emanuele Gabrielli,Luigi V. Mancini",https://doi.org/10.1145/504909.504911,TOPS,2002,[],
388,,Secure and selective dissemination of XML documents.,"Elisa Bertino,Elena Ferrari",https://doi.org/10.1145/545186.545190,TOPS,2002,[],
389,,Trust management for IPsec.,"Matt Blaze,John Ioannidis,Angelos D. Keromytis",https://doi.org/10.1145/505586.505587,TOPS,2002,[],
390,,An algebra for composing access control policies.,"Piero A. Bonatti,Sabrina De Capitani di Vimercati,Pierangela Samarati",https://doi.org/10.1145/504909.504910,TOPS,2002,[],
391,,A fine-grained access control system for XML documents.,"Ernesto Damiani,Sabrina De Capitani di Vimercati,Stefano Paraboschi,Pierangela Samarati",https://doi.org/10.1145/505586.505590,TOPS,2002,[],
392,,An algebraic approach to IP traceback.,"Drew Dean,Matthew K. Franklin,Adam Stubblefield",https://doi.org/10.1145/505586.505588,TOPS,2002,[],
393,,The economics of information security investment.,"Lawrence A. Gordon,Martin P. Loeb",https://doi.org/10.1145/581271.581274,TOPS,2002,[],
394,,A methodology for analyzing the performance of authentication protocols.,"Alan Harbitter,Daniel A. Menascé",https://doi.org/10.1145/581271.581275,TOPS,2002,[],
395,,A graph-based formalism for RBAC.,"Manuel Koch,Luigi V. Mancini,Francesco Parisi-Presicce",https://doi.org/10.1145/545186.545191,TOPS,2002,[],
396,,Information leakage from optical emanations.,"Joe Loughry,David A. Umphress",https://doi.org/10.1145/545186.545189,TOPS,2002,[],
397,,"Simple, state-based approaches to program-based anomaly detection.","Christoph C. Michael,Anup K. Ghosh",https://doi.org/10.1145/545186.545187,TOPS,2002,[],
398,,Termination in language-based systems.,"Algis Rudys,Dan S. Wallach",https://doi.org/10.1145/505586.505589,TOPS,2002,[],
399,,Improving the granularity of access control for Windows 2000.,"Michael M. Swift,Anne Hopkins,Peter Brundrett,Cliff Van Dyke,Praerit Garg,Shannon Chan,Mario Goertzel,Gregory Jensenworth",https://doi.org/10.1145/581271.581273,TOPS,2002,[],
400,,Token-based scanning of source code for security problems.,"John Viega,J. T. Bloch,Tadayoshi Kohno,Gary McGraw",https://doi.org/10.1145/545186.545188,TOPS,2002,[],
401,,TRBAC - A temporal role-based access control model.,"Elisa Bertino,Piero A. Bonatti,Elena Ferrari",https://doi.org/10.1145/501978.501979,TOPS,2001,[],
402,,A nested transaction model for multilevel secure database management systems.,"Elisa Bertino,Barbara Catania,Elena Ferrari",https://doi.org/10.1145/503339.503340,TOPS,2001,[],
403,,Real-time protocol analysis for detecting link-state routing protocol attacks.,"Ho-Yen Chang,Shyhtsun Felix Wu,Y. Frank Jou",https://doi.org/10.1145/383775.383776,TOPS,2001,[],
404,,Proposed NIST standard for role-based access control.,"David F. Ferraiolo,Ravi S. Sandhu,Serban I. Gavrila,D. Richard Kuhn,Ramaswamy Chandramouli",https://doi.org/10.1145/501978.501980,TOPS,2001,[],
405,,Practical safety in flexible access control models.,"Trent Jaeger,Jonathon Tidswell",https://doi.org/10.1145/501963.501966,TOPS,2001,[],
406,,An unknown key-share attack on the MQV key agreement protocol.,Burton S. Kaliski Jr.,https://doi.org/10.1145/501978.501981,TOPS,2001,[],
407,,The SecureRing group communication system.,"Kim Potter Kihlstrom,Louise E. Moser,P. M. Melliar-Smith",https://doi.org/10.1145/503339.503341,TOPS,2001,[],
408,,Abstraction-based intrusion detection in distributed environments.,"Peng Ning,Sushil Jajodia,Xiaoyang Sean Wang",https://doi.org/10.1145/503339.503342,TOPS,2001,[],
409,,Role-based access control on the web.,"Joon S. Park,Ravi S. Sandhu,Gail-Joon Ahn",https://doi.org/10.1145/383775.383777,TOPS,2001,[],
410,,The architecture and performance of security protocols in the ensemble group communication system - Using diamonds to guard the castle.,"Ohad Rodeh,Kenneth P. Birman,Danny Dolev",https://doi.org/10.1145/501978.501982,TOPS,2001,[],
411,,An authorization model for a public key management service.,"Pierangela Samarati,Michael K. Reiter,Sushil Jajodia",https://doi.org/10.1145/503339.503343,TOPS,2001,[],
412,,Secure virtual enclaves - Supporting coalition use of distributed application technologies.,"Deborah Shands,Jay Jacobs,Richard Yee,E. John Sebes",https://doi.org/10.1145/501963.501964,TOPS,2001,[],
413,,"Cost profile of a highly assured, secure operating system.",Richard E. Smith,https://doi.org/10.1145/383775.383778,TOPS,2001,[],
414,,Secure password-based cipher suite for TLS.,"Michael Steiner 0001,Peter Buhler,Thomas Eirich,Michael Waidner",https://doi.org/10.1145/501963.501965,TOPS,2001,[],
415,,Role-based authorization constraints specification.,"Gail-Joon Ahn,Ravi S. Sandhu",https://doi.org/10.1145/382912.382913,TOPS,2000,[],
416,,The base-rate fallacy and the difficulty of intrusion detection.,Stefan Axelsson,https://doi.org/10.1145/357830.357849,TOPS,2000,[],
417,,Signature schemes based on the strong RSA assumption.,"Ronald Cramer,Victor Shoup",https://doi.org/10.1145/357830.357847,TOPS,2000,[],
418,,Xor-trees for efficient anonymous multicast and reception.,"Shlomi Dolev,Rafail Ostrovsky",https://doi.org/10.1145/354876.354877,TOPS,2000,[],
419,,Balancing cooperation and risk in intrusion detection.,Deborah A. Frincke,https://doi.org/10.1145/353323.353324,TOPS,2000,[],
420,,A framework for constructing features and models for intrusion detection systems.,"Wenke Lee,Salvatore J. Stolfo",https://doi.org/10.1145/382912.382914,TOPS,2000,[],
421,,Testing Intrusion detection systems - a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory.,John McHugh,https://doi.org/10.1145/382912.382923,TOPS,2000,[],
422,,Scalable multicast security with dynamic recipient groups.,"Refik Molva,Alain Pannetrat",https://doi.org/10.1145/357830.357834,TOPS,2000,[],
423,,Configuring role-based access control to enforce mandatory and discretionary access control policies.,"Sylvia L. Osborn,Ravi S. Sandhu,Qamar Munawer",https://doi.org/10.1145/354876.354878,TOPS,2000,[],
424,,Enforceable security policies.,Fred B. Schneider,https://doi.org/10.1145/353323.353382,TOPS,2000,[],
425,,Reflection as a mechanism for software integrity verification.,Diomidis Spinellis,https://doi.org/10.1145/353323.353383,TOPS,2000,[],
426,,Key management for encrypted broadcast.,Avishai Wool,https://doi.org/10.1145/354876.354879,TOPS,2000,[],
427,,The Specification and Enforcement of Authorization Constraints in Workflow Management Systems.,"Elisa Bertino,Elena Ferrari,Vijayalakshmi Atluri",https://doi.org/10.1145/300830.300837,TOPS,1999,[],
428,,A Role-Based Access Control Model and Reference Implementation within a Corporate Intranet.,"David F. Ferraiolo,John F. Barkley,D. Richard Kuhn",https://doi.org/10.1145/300830.300834,TOPS,1999,[],
429,,On secure and pseudonymous client-relationships with multiple servers.,"Eran Gabber,Phillip B. Gibbons,David M. Kristol,Yossi Matias,Alain J. Mayer",https://doi.org/10.1145/330382.330386,TOPS,1999,[],
430,,Public-Key Cryptography and Password Protocols.,"Shai Halevi,Hugo Krawczyk",https://doi.org/10.1145/322510.322514,TOPS,1999,[],
431,,Strength of two data encryption standard implementations under timing attacks.,"Alejandro Hevia,Marcos A. Kiwi",https://doi.org/10.1145/330382.330390,TOPS,1999,[],
432,,Flexible Control of Downloaded Executable Content.,"Trent Jaeger,Atul Prakash 0001,Jochen Liedtke,Nayeem Islam",https://doi.org/10.1145/317087.317091,TOPS,1999,[],
433,,Temporal Sequence Learning and Data Reduction for Anomaly Detection.,"Terran Lane,Carla E. Brodley",https://doi.org/10.1145/322510.322526,TOPS,1999,[],
434,,The Role Graph Model and Conflict of Interest.,"Matunda Nyanchama,Sylvia L. Osborn",https://doi.org/10.1145/300830.300832,TOPS,1999,[],
435,,Inductive Analysis of the Internet Protocol TLS.,Lawrence C. Paulson,https://doi.org/10.1145/322510.322530,TOPS,1999,[],
436,,Authentication Metric Analysis and Design.,"Michael K. Reiter,Stuart G. Stubblebine",https://doi.org/10.1145/317087.317088,TOPS,1999,[],
437,,Editorial.,Ravi S. Sandhu,https://doi.org/10.1145/300830.317086,TOPS,1999,[],
438,,The ARBAC97 Model for Role-Based Administration of Roles.,"Ravi S. Sandhu,Venkata Bhamidipati,Qamar Munawer",https://doi.org/10.1145/300830.300839,TOPS,1999,[],
439,,Secure Audit Logs to Support Computer Forensics.,"Bruce Schneier,John Kelsey",https://doi.org/10.1145/317087.317089,TOPS,1999,[],
440,,Unlinkable serial transactions - protocols and applications.,"Stuart G. Stubblebine,Paul F. Syverson,David M. Goldschlag",https://doi.org/10.1145/330382.330384,TOPS,1999,[],
441,,Design of a High-Performance ATM Firewall.,"Jun Xu 0014,Mukesh Singhal",https://doi.org/10.1145/322510.322520,TOPS,1999,[],
442,,High Dictionary Compression for Proactive Password Checking.,"Francesco Bergadano,Bruno Crispo,Giancarlo Ruffo",https://doi.org/10.1145/290163.290164,TOPS,1998,[],
443,,Exception-Based Information Flow Control in Object-Oriented Systems.,"Elisa Bertino,Sabrina De Capitani di Vimercati,Elena Ferrari,Pierangela Samarati",https://doi.org/10.1145/290163.290167,TOPS,1998,[],
444,,Crowds - Anonymity for Web Transactions.,"Michael K. Reiter,Aviel D. Rubin",https://doi.org/10.1145/290163.290168,TOPS,1998,[],
445,,Editorial.,Ravi S. Sandhu,https://doi.org/10.1145/290163.290166,TOPS,1998,[],
446,,The Multilevel Relational (MLR) Data Model.,"Ravi S. Sandhu,Fang Chen",https://doi.org/10.1145/290163.290171,TOPS,1998,[],
